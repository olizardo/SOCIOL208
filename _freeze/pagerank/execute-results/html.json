{
  "hash": "7f663e761ca0be1e39ededc1a8b44cbd",
  "result": {
    "markdown": "---\ntitle: \"PageRank Prestige Scoring\"\nexecute: \n  eval: true\n  echo: true\n  output: true\n  warning: false\n  message: false\nformat: \n   html:\n      code-line-numbers: true\n---\n\n\nThe model of status distribution implied by the [Eigenvector Centrality approach](prestige.qmd) implies that each actor distributes the same amount of status *independently* of the number of connections they have. Status just replicates indefinitely. Thus, a node with a 100 friends has 100 status units to distribute to each of them and a node with a 10 friends has 10 units. \n\nThis is why the eigenvector idea rewards nodes who are connected to popular others more. Even though everyone begins with a single unit of status, well-connected nodes by degree end up having more of it to distribute. \n\n## A Degree-Normalized Model of Status \n\nBut what if status propagated in the network proportionately to the number of connections one had? For instance, if someone has 100 friends and they only had so much time or energy, they would only have a fraction of status to distribute to others than a person with 10 friends. \n\nIn that case, the node with a hundred friends would only have 1/100 of status units to distribute to each of their connections while the node with 10 friends would have 1/10 units. Under this formulation, being connected to *discerning* others, that is people who only connect to a few, is better than being connected to others who connect to everyone else indiscriminately. \n\nHow would we implement this model? First, let's create a variation of the undirected friendship nomination adjacency matrix called the $\\mathbf{P}$ matrix. It is defined like this:\n\n$$\n\\mathbf{P} = \\mathbf{D}_{out}^{-1}\\mathbf{A}\n$$\n\nWhere $\\mathbf{A}$ is our old friend the adjacency matrix, and $\\mathbf{D}_{out}^{-1}$ is a matrix containing the *inverse* of each node outdegree along the diagonals and zeroes in every other cell. \n\nIn `R` we can create the $\\mathbf{D}_{out}^{-1}$ matrix using the native `diag` function like this (using an undirected version of the Krackhardt high-tech managers friendship network):\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   library(networkdata)\n   library(igraph)\n   g <- as_undirected(ht_friends, mode = \"collapse\")\n   A <- as.matrix(as_adjacency_matrix(g))\n   D.o <- diag(1/rowSums(A))\n```\n:::\n\n\nRecalling that the function `rowSums` gives us the row sums of the adjacency matrix, which is the same as each node's outdegree.\n\nWe can check out that the  $\\mathbf{D}_{out}^{-1}$ indeed contains the quantities we seek by looking at its first few rows and columns:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   round(D.o[1:10, 1:10], 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n [1,] 0.11  0.0 0.00 0.00  0.0 0.00 0.00  0.0 0.00  0.00\n [2,] 0.00  0.1 0.00 0.00  0.0 0.00 0.00  0.0 0.00  0.00\n [3,] 0.00  0.0 0.17 0.00  0.0 0.00 0.00  0.0 0.00  0.00\n [4,] 0.00  0.0 0.00 0.14  0.0 0.00 0.00  0.0 0.00  0.00\n [5,] 0.00  0.0 0.00 0.00  0.1 0.00 0.00  0.0 0.00  0.00\n [6,] 0.00  0.0 0.00 0.00  0.0 0.14 0.00  0.0 0.00  0.00\n [7,] 0.00  0.0 0.00 0.00  0.0 0.00 0.33  0.0 0.00  0.00\n [8,] 0.00  0.0 0.00 0.00  0.0 0.00 0.00  0.2 0.00  0.00\n [9,] 0.00  0.0 0.00 0.00  0.0 0.00 0.00  0.0 0.17  0.00\n[10,] 0.00  0.0 0.00 0.00  0.0 0.00 0.00  0.0 0.00  0.12\n```\n:::\n:::\n\n\nWe can then create the $\\mathbf{P}$ matrix corresponding to the undirected version of the Krackhardt friendship network using matrix multiplication like this:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   P <- D.o %*% A\n```\n:::\n\n\nRecalling that `%*%` is the `R` matrix multiplication operator.\n\nSo the resulting $\\mathbf{P}$ is the original adjacency matrix, in which each non-zero entry is equal to one divided by the outdegree of the corresponding node in each row. \n\nHere are the first 10 rows and columns of the new matrix:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   round(P[1:10, 1:10], 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n [1,] 0.00 0.11 0.00 0.11 0.00 0.00 0.00 0.11 0.00  0.00\n [2,] 0.10 0.00 0.00 0.10 0.10 0.10 0.00 0.00 0.00  0.00\n [3,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.17\n [4,] 0.14 0.14 0.00 0.00 0.00 0.00 0.00 0.14 0.00  0.00\n [5,] 0.00 0.10 0.00 0.00 0.00 0.00 0.00 0.00 0.10  0.10\n [6,] 0.00 0.14 0.00 0.00 0.00 0.00 0.14 0.00 0.14  0.00\n [7,] 0.00 0.00 0.00 0.00 0.00 0.33 0.00 0.00 0.00  0.00\n [8,] 0.20 0.00 0.00 0.20 0.00 0.00 0.00 0.00 0.00  0.20\n [9,] 0.00 0.00 0.00 0.00 0.17 0.17 0.00 0.00 0.00  0.17\n[10,] 0.00 0.00 0.12 0.00 0.12 0.00 0.00 0.12 0.12  0.00\n```\n:::\n:::\n\n\nNote that the entries are now numbers between zero and one and the matrix is *asymmetric*; that is, $p_{ij}$ is not necessarily equal to $p_{ji}$. In fact $p_{ij}$ will only be equal to $p_{ji}$ when $k_i = k_j$ (nodes have the same degree). Each cell in the matrix is thus equal to $1/k_i$ where $k_i$ is the degree of the node in row $i$. \n\nMoreover the rows of $\\mathbf{P}$ sum to one:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   rowSums(P)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n```\n:::\n:::\n\n\nWhich means that the $\\mathbf{P}$ matrix is **row stochastic**. That is the \"outdegree\" of each node in the matrix is forced to sum to a fixed number. Substantively this means that we are equalizing the total amount of prestige or status that each node can distribute in the system to a fixed quantity. \n\nThis means that nodes with a lot of out-neighbors will dissipate this quantity by distributing it across a larger number of recipients (hence their corresponding non-zero entries in the rows of $\\mathbf{P}$) will be a small number) and nodes with a few out-neighbors will have more to distribute. \n\nAnother thing to note is that while the sums of the $\\mathbf{P}$ matrix sum to a fixed number (1.0) the sums of the *columns* of the same matrix do not:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   round(colSums(P), 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 1.11 1.34 0.63 0.86 1.56 1.06 0.37 0.51 0.61 1.21 2.33 0.92 0.17 0.87 1.08\n[16] 0.53 2.73 0.54 1.21 0.60 0.77\n```\n:::\n:::\n\n\nThis means that inequalities in the system will be tied to the *indegree* of each node in the $\\mathbf{P}$ matrix, which is given by either the column sums of the matrix (as we just saw) or the *row sums* of the *transpose* of the same matrix $\\mathbf{P}^T$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   round(rowSums(t(P)), 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 1.11 1.34 0.63 0.86 1.56 1.06 0.37 0.51 0.61 1.21 2.33 0.92 0.17 0.87 1.08\n[16] 0.53 2.73 0.54 1.21 0.60 0.77\n```\n:::\n:::\n\n\nThis will come in handy in a second.\n\nThe $\\mathbf{P}$ matrix has many interpretations, but here it just quantifies the idea that the amount of centrality each node can distribute is proportional to their degree, and that the larger the degree, the less there is to distribute (the smaller each cell $p_{ij}$ will be). Meanwhile, it is clear that nodes that are pointed to by many other nodes who themselves don't point to many others have a larger indegree in $\\mathbf{P}$. \n\nNow we can just adapt the the model of status distribution we used for [eigenvector centrality](prestige.qmd) but this time using the $\\mathbf{P}$ rather than the $\\mathbf{A}$ matrix. Note that because we are interested in the status that comes **into** each node we use the *transpose* of $\\mathbf{P}$ rather than $\\mathbf{P}$, just like we did for the @bonacich72 status score. \n\nHere's our old status game function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   status1 <- function(w) {\n      x <- rep(1, nrow(w)) #initial status vector set to all ones of length equal to the number of nodes\n      d <- 1 #initial delta\n      k <- 0 #initializing counter\n      while (d > 1e-10) {\n          o.x <- x #old status scores\n          x <- w %*% o.x #new scores a function of old scores and adjacency matrix\n          x <- x/norm(x, type = \"E\") #normalizing new status scoress\n          d <- abs(sum(abs(x) - abs(o.x))) #delta between new and old scores\n          k <- k + 1 #incrementing while counter\n      }\n   return(as.vector(x))\n   }\n```\n:::\n\n\nAt each step, the status of a node is equivalent to the sum of the status scores of their in-neighbors, with more discerning in-neighbors passing along more status than less discerning ones:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   s2 <- status1(t(P))\n   s2 <- s2/max(s2)\n   round(s2, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 0.500 0.556 0.333 0.389 0.556 0.389 0.167 0.278 0.333 0.444 0.778 0.444\n[13] 0.111 0.333 0.500 0.278 1.000 0.222 0.556 0.278 0.333\n```\n:::\n:::\n\n\nWhat if I told you that these numbers are the same as the leading eigenvector of $\\mathbf{P}^T$?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   s2 <- abs(eigen(t(P))$vector[, 1])\n   s2 <- s2/max(s2)\n   round(s2, 3) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 0.500 0.556 0.333 0.389 0.556 0.389 0.167 0.278 0.333 0.444 0.778 0.444\n[13] 0.111 0.333 0.500 0.278 1.000 0.222 0.556 0.278 0.333\n```\n:::\n:::\n\n\nAnd, of course, the (normalized) scores produced by this approach are identical to those computed by the `page_rank` function in `igraph` with \"damping factor\" (to be explained in a second  ) set to 1.0:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   pr <- page_rank(g, damping = 1, algo = \"arpack\")$vector\n   round(pr/max(pr), 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 0.500 0.556 0.333 0.389 0.556 0.389 0.167 0.278 0.333 0.444 0.778 0.444\n[13] 0.111 0.333 0.500 0.278 1.000 0.222 0.556 0.278 0.333\n```\n:::\n:::\n\n\nSo the distributional model of status is the same one implemented in the PageRank algorithm!\n\n## PageRank as a Markov Difussion Model\n\nRemember how we said earlier that there are multiple ways of thinking about $\\mathbf{P}$? Another way of thinking about the $\\mathbf{P}$ matrix is as characterizing the behavior of a **random walker** in the graph. At any time point $t$ the walker (a piece of information, a virus, or status itself) sits on node $i$ and the with probability $p_{ij}$ jumps to $j$, who is one of node $i$'s out-neighbors. The probabilities for each $i$ and $j$ combination are stored in the matrix $\\mathbf{P}$. \n\nSo our status game can best be understood as a special case of a **diffusion** game, where what's being diffused through the network is status itself. Let's see how this would work. \n\nImagine we want to spread something through the Krackhardt managers friendship network like a rumor or a piece of information. We start with a seed node $i$ and then track \"where\" the rumor is at each time step in the network (where the location is a person in the network). The rules of the game are the Markov diffusion model we described above. At each time step the rumor sits on some $j$ and it diffuses to one of $j$'s neighbors $k$ with probability $p_{jk}$. Where the rumor has been before that time step does not affect where it goes in the present.\n\nThis sequence of transmission events is called a **markov chain**, and when it happens in a graph it is called a **random walk** on the graph. The node at which the rumor sits at a given time $t$ is called the **state** of the markov chain at $t$. For instance, the following function prints out *every* state of the markov chain for some series of steps $q$, given a network transition matrix $\\mathbf{P}$ (the `w` argument in the function):\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   markov.chain1 <- function(w, seed = 1, q = 100) {\n      state <- 0 #initializing state vector\n      i <- seed #setting seed node\n      for (t in 1:q) {\n         state[t] <- sample(1:ncol(w), 1, prob = w[i, ]) #new state of the chain\n         i <- state[t] #new source node\n      }\n   return(state)\n   }\n```\n:::\n\n\nThe function above sets the \"seed\" node to that given in the argument `seed` (by default, node 1) in line 2. Then, in line 4, it enters the `for` loop to run `q` times (in this case 100 times). In line 6 the `state` vector at `t` is set to a random node $j$ sampled from the $\\mathbf{P}$ matrix with probability equal to the entry $p_{ij}$ in the $i^{th}$ row of the matrix. \n\nFor instance when it comes to first node that row looks like:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   round(P[1, ], 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 0.00 0.11 0.00 0.11 0.00 0.00 0.00 0.11 0.00 0.00 0.11 0.11 0.00 0.00 0.11\n[16] 0.11 0.11 0.00 0.11 0.00 0.00\n```\n:::\n:::\n\n\nWhich means that nodes {2, 4, 8, 11, 12, 15, 16, 17, 19} (the neighbors of node 1) have an 11% chance each of being sampled in line 6 and the other ones have no chance. Then in line 7 the new source node is set to whatever neighbor of $i$ was sampled in line 6. \n\nHere's a markov chain state sequence of length 100 from the managers friendship network, starting with node 1 as the seed:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   markov.chain1(P)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  [1]  8  1  4 12 11 15 17  2  5  9 15  1 16 10  8  1 12  4 17  7 14 17  3 11 15\n [26] 14 19 14 19 14 17  6  2  4 17 16  4 12  6 17 10  5 17  2  6 12  6 15  6 17\n [51]  5 10 20 10  8  4  2 11  5 21  6  9 15 19  3 10  9 10 20 10  8 10  5 19  5\n [76] 19 12 17 11  2 17 11 17  4  1  8 11 13 11 17  2  6  9 11  8 11 17 21 12 17\n```\n:::\n:::\n\n\nWe can of course create a longer one by changing the `q` argument while setting the seed node to 5:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   markov.chain1(P, q = 500, seed = 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  [1] 21  6  9 10  9  5 11  1 19 20 11  5 17 16  2 17 14  3 17  4 17  7 14  3 15\n [26] 17 15  5 17  9 10  8 10 16  1  4  8  1 17  6 21 18  2 19  3 11 19  5 11  1\n [51] 12 17  8 17  4 12 19  5  9 17 15  6  2 19 17  9 17  7  6  9 17  6 17  6 15\n [76] 14 17  4 16  2 21  6 21  5 13 11 18  2 21  5 17 11  2 16  2 21 12 11 20 10\n[101]  5 19  2 17  8  1  4 17 11 12  1  2  4 11  3 10  8  4  2 21  6 21 17 21 12\n[126] 17  9 11 12  6 17  9 11  1  8  1 19 12  1 19 17 20 11  2 18 11 20 10  3 14\n[151]  3 15  9 15  9 17 21 12 17  9  5  9  5 10  3 11 15 17  6  7  6 12 19  5 11\n[176] 15  5 21  2 21 18 11  1 12 10 20 17  8  1 11 17  8  4 17 14  7 17  5  9  6\n[201]  7 14 17 16  2 11  1  8 11 12 17  2 21  2 11 19  2  5 19  3 17 11 15 19 11\n[226] 19  3 14 19 12 17 16 10 16 10 12  6 12 19 14  5  9 17  9  5 10  9  5 17  8\n[251] 11  2 18 11  4 16 10  3 14 19 12 17  2  6  9 11  9  5  9 10 12 11  1 12  1\n[276]  2 16  2 18  2 17  1 11  5 21  6  7  6  2  6 15  1 11  8 10 17  6  9  5 13\n[301]  5 11 15  6 17  2 19  1  4 17 10  5 21 18  2 17 15  3 19  5  9 17  3 11  4\n[326]  2  5 10 12 19 17 19 20 18  2 21  2  5 14 19 17 14  7  6 12  6 17 21  6 21\n[351]  6 17  6  2 11 18  2 17 14  7 14  5 10 16 17  2  5 15  1  2 21 12  6 15 14\n[376]  5 19 12 10  3 10  3 10 12 21 18 20 11 20 19  1 17  5  9 17 12 10 12  6 17\n[401]  1  2 18 21  5 11  2  5 19 20 19 12 21  5  2 18 11  5 15 19 20 11  4  8 11\n[426] 18 11  8  4 16  1 17  6  7 14 15  6 21  5 17  1 19 15 19 15  1 17  4  2 16\n[451]  1 17  4 12 11 12 19 14 15  9 10 20 17 20 19 15  6  7 17  5  9  6  7  6  2\n[476] 18 20 19 15  5 15 19  2  6 21 18 11 13 11 20 17  1  8 10  9  5 19  5  2 17\n```\n:::\n:::\n\n\nNote that one thing that happens here is that the rumor goes through some nodes more often than others. Another thing that you may be thinking is that the odds that a node will keep repeating itself in this chain has to do with how many neighbors they have since that increases the chances that they will be chosen in the `sample` line of the function. If you think that, you will be right!\n\nOne thing we can do with the long vector of numbers above is compute the *probability* that the chain will be in some state or another after a number of steps $q$. To do that, all we have to do is find out the number of times each of the numbers (from one through twenty one) repeats itself, and then divide by the total number of steps. \n\nIn `R` we can do that like this, using `q = 50000`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   states <- markov.chain1(P, q = 50000, seed = 1)\n   count <- table(states)\n   p <- count/length(states)\n   names(p) <- names(count)\n   round(p, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n0.06 0.06 0.04 0.04 0.06 0.05 0.02 0.03 0.04 0.05 0.09 0.05 0.01 0.04 0.06 0.03 \n  17   18   19   20   21 \n0.12 0.02 0.06 0.03 0.04 \n```\n:::\n:::\n\n\nLine 1 computes the states of the markov chain after 50000 iterations using our `markov.chain1` function. Then line 2 uses the native `R` function `table` as a handy trick to compute how many times each node shows up in the chain stored in the `count` object: \n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\n   count\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nstates\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n2752 3064 1920 2110 3182 2314  991 1501 2030 2534 4367 2499  656 1895 2907 1591 \n  17   18   19   20   21 \n5827 1226 3149 1564 1921 \n```\n:::\n:::\n\n\nFinally, line 4 divides these numbers by the length of the chain to get the probability. \n\nNote that the numbers stored in the `p` vector are readily interpretable. For instance, the `0.06` in the first spot tells us that if we were to run this chain many times and check where the rumor is at step fifty-thousand, there is 6% chance that the rumor will be sitting on node 1, while there is a 11% chance that it would be sitting on node 17, a 3% chance that it will be on node 18, and so forth.  \n\nLike well behaved probabilities, these numbers sum to 1.0:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   sum(p)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n:::\n\n\nWe can incorporate these steps in to a new and improved function like thus:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   markov.chain2 <- function(w, seed = 1, q = 100) {\n      state <- 0\n      i <- seed\n      for (t in 1:q) {\n         state[t] <- sample(1:ncol(w), 1, prob = w[i, ])\n         i <- state[t]\n      }\n   count <- table(states)\n   p <- count/length(states)\n   names(p) <- names(count)\n   return(p)\n   }\n```\n:::\n\n\nWhich now does everything in one step:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   markov.chain2(P, q = 500)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      1       2       3       4       5       6       7       8       9      10 \n0.05504 0.06128 0.03840 0.04220 0.06364 0.04628 0.01982 0.03002 0.04060 0.05068 \n     11      12      13      14      15      16      17      18      19      20 \n0.08734 0.04998 0.01312 0.03790 0.05814 0.03182 0.11654 0.02452 0.06298 0.03128 \n     21 \n0.03842 \n```\n:::\n:::\n\n\nThere is another way to compute these probabilities more directly from the $\\mathbf{P}$ matrix. The basic idea is that at any time $t$, the distribution of probabilities across nodes in the network stored in the vector $\\mathbf{x}$ is given by:\n\n$$\n\\mathbf{x}(t) = \\mathbf{P}^T\\mathbf{x}(t-1)\n$$\n\nWith the initial probability vector given by:\n\n$$\n\\mathbf{x}(0) = \\mathbf{e}^{(i)}\n$$\n\nWhere $e^{(i)}$ is a vector containing all zeros except for the $i^{th}$ spot, where it contains a one, indicating the initial seed node. \n\nHere's an `R` function that implements this idea:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   markov.chain3 <- function(w, init = 1, steps = 100) {\n      x <- rep(0, nrow(w))\n      x[init] <- 1\n      P.t <- t(w)\n      for (t in 1:steps) {\n         x <- P.t %*% x\n         }\n   x <- as.vector(x)\n   names(x) <- 1:ncol(w)\n   return(x)\n   }\n```\n:::\n\n\nAnd we can see what it spits out:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   p3 <- markov.chain3(P)\n   round(p3, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n0.06 0.06 0.04 0.04 0.06 0.04 0.02 0.03 0.04 0.05 0.09 0.05 0.01 0.04 0.06 0.03 \n  17   18   19   20   21 \n0.11 0.03 0.06 0.03 0.04 \n```\n:::\n:::\n\n\nWhich are the same as our more complicated function above.\n\nNow you may have noticed this already, but these are the same numbers produced by the the PageRank status game!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   names(pr) <- 1:21\n   round(pr, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n0.06 0.06 0.04 0.04 0.06 0.04 0.02 0.03 0.04 0.05 0.09 0.05 0.01 0.04 0.06 0.03 \n  17   18   19   20   21 \n0.11 0.03 0.06 0.03 0.04 \n```\n:::\n:::\n\n\nThis gives us another (and perhaps) more intuitive interpretation of what the PageRank prestige ranking is all about. Nodes have more prestige if they are more \"central\" in a network where something is spreading via a random walk process. Higher ranked nodes will be visited more often by the random walker, less-highly-ranked nodes less. \n\nNote that if a random walker is just a *web surfer* then it makes sense that a more highly visited page should be more prestigious than a less frequently visited page [@brin_page98].\n\n## PageRank with Damping and Teleportation in Directed Graphs\n\nPageRank of course was designed to deal with **directed graphs** (like the World Wide Web). So let's load up the version of the Krackhardt's Managers data that contains the **advice network** which is an unambiguously directed relation. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n   g <- ht_advice\n   A <- as.matrix(as_adjacency_matrix(g))\n```\n:::\n\n\nA plot of the advice network is shown in @fig-krack. \n\n\n::: {.cell}\n::: {.cell-output-display}\n![Krackhardt's managers network](pagerank_files/figure-html/fig-krack-1.png){#fig-krack width=768}\n:::\n:::\n\n\nWe then compute the $\\mathbf{P}$ matrix corresponding to this network:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   D.o <- diag(1/rowSums(A))\n   P <- D.o %*% A \n```\n:::\n\n\nOne issue that arises in computing the $\\mathbf{P}$ for directed graphs is that there could be nodes with no out-neighbors (so-called sink nodes) or like node 6 in @fig-krack, who has just one out-neighbor (e.g., seeks advice from just one person), in which case the probability is 1.0 that if the random walker is at node 6 it will go to node 21. \n\nTo avoid this issue the original designers of the PageRank algorithm [@brin_page98] added a \"fudge\" factor: That is, with probability $\\alpha$ the random walker should hop from node to node following the directed links in the graph. But once in a while with probability $1-\\alpha$ the walker should decide to \"teleport\" (with uniform probability) to *any* node in the graph whether it is an out-neighbor of the current node or not.\n\nHow do we do that? Well we need to \"fix\" the $\\mathbf{P}$ matrix to allow for such behavior. So instead of $\\mathbf{P}$ we estimate our distributive status model on the matrix $\\mathbf{G}$ (yes, for **G**oogle):\n\n$$\n   \\mathbf{G} = \\alpha \\mathbf{P} + (1 - \\alpha) \\mathbf{E}\n$$\n\nWhere $\\mathbf{E}$ is a matrix of the same dimensions as $\\mathbf{P}$ but containing $1/n$ in every cell indicating that every node has an equal chance of being \"teleported\" to.\n\nSo, fixing $\\alpha = 0.85$ (the standard value chosen by @brin_page98 in their original paper) our $\\mathbf{G}$ matrix would be:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   n <- vcount(g)\n   E <- matrix(1/n, n, n)\n   G <- (0.85 * P) + (0.15 * E)\n```\n:::\n\n\nAnd then we just play our status distribution game on the transpose of $\\mathbf{G}$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   s3 <- round(status1(t(G)), 3)\n   round(s3/max(s3), 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 0.302 0.573 0.165 0.282 0.100 0.437 0.635 0.255 0.092 0.180 0.237 0.242\n[13] 0.087 0.268 0.097 0.168 0.258 0.440 0.087 0.200 1.000\n```\n:::\n:::\n\n\nWhich is the same answer you would get from the `igraph` function `page_rank` by setting the \"damping\" parameter to 0.85:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   pr <- page_rank(g, damping = 0.85, algo = \"arpack\")$vector\n   round(pr/max(pr), 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 0.302 0.573 0.165 0.281 0.100 0.437 0.635 0.256 0.091 0.180 0.237 0.242\n[13] 0.086 0.269 0.097 0.169 0.258 0.440 0.086 0.200 1.000\n```\n:::\n:::\n\n\nWe can see therefore that the damping parameter simply controls the extent to which the PageRank ranking is driven by the directed connectivity of the $\\mathbf{P}$ matrix, versus a stochastic or random component. ",
    "supporting": [
      "pagerank_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}