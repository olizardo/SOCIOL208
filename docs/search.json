[
  {
    "objectID": "ahn.html",
    "href": "ahn.html",
    "title": "Assigning Nodes to Multiple Communities",
    "section": "",
    "text": "In the previous handout, we examined various approaches to community and dense subgraph detection. What they all have in common is they assign nodes into non-overlapping groups. That is, nodes are either in one group or another but cannot belong to multiple groups at once. While this may make sense for a lot of substantive settings, it might not make sense for other ones, where multiple group memberships are normal (e.g., think of high school)."
  },
  {
    "objectID": "blondel.html",
    "href": "blondel.html",
    "title": "Role Similarity Across Graphs",
    "section": "",
    "text": "Sometimes we may want to figure out how similar a given node’s position in one social network is to that of another node in a different network. This calls for a method that could allow us to compare how similar a node in one graph is to other nodes in another graph.\nA particularly interesting version of this problem arises when we have information on the same set of nodes across different set of relations. In that case, we may be interested in answering the question as to whether nodes occupy similar or dissimilar positions across the networks defined by the different relations.\nBlondel et al. (2004) describe an approach that can help us make headway on this problem. They use a similar iterative procedure that we saw can be used to compute status scores from directed graphs (like PageRank and HITS) but this time to compute similarity scores between pairs of nodes across graphs.\nThe idea, just like with the status scores, is that the two set of nodes in each graph start with the same set of similarity scores, and then we update them as we traverse the connectivity structure of the two graphs.\nSo let’s say the adjacency matrix of the first graph is \\(\\mathbf{A}\\) and that of the second graph is \\(\\mathbf{B}\\). The first graph has \\(n_A\\) number of nodes and the corresponding quantity in the second graph is \\(n_B\\) our target similarity matrix \\(\\mathbf{Z}\\), comparing the node sets in the two graphs, will therefore be of dimensions \\(n_B \\times n_A\\).\nWe initialize \\(z_{ij}(0) = 1\\) for all \\(i\\) and \\(j\\); that is, \\(\\mathbf{Z}(0)\\) is a matrix full of ones. At each time step subsequent to that, we fill up the \\(\\mathbf{Z}\\) matrix with new values according to:\n\\[\n   \\mathbf{Z}(t + 1) = \\mathbf{B}\\mathbf{Z(t)}\\mathbf{A}^T + \\mathbf{B}^T\\mathbf{Z(t)}\\mathbf{A}\n\\]\nTo ensure convergence, we then normalize the \\(\\mathbf{Z}\\) matrix after every update using our trusty Euclidean norm:\n\\[\n\\mathbf{Z}(t > 0) = \\frac{\\mathbf{Z}}{||\\mathbf{Z}||_2}\n\\]"
  },
  {
    "objectID": "blondel.html#computing-node-similarities-across-different-graphs",
    "href": "blondel.html#computing-node-similarities-across-different-graphs",
    "title": "Role Similarity Across Graphs",
    "section": "Computing Node Similarities Across Different Graphs",
    "text": "Computing Node Similarities Across Different Graphs\nLet us see how this would work with real data. We will compare two subgraphs of the larger law_advice network (Lazega 2001) from the networkdata package. This is a directed advice-seeking network so a node goes from advisee to adviser.\nWe create two subgraphs. One composed of older male partners (aged fifty or older) and the other composed of the women in the firm (both parterns and associates). They look lik this:\n\n\n\n\n\n\nOlder Men Partners\n\n\n\n\n\n\n\n\n\nWomen Lawyers\n\n\n\n\n\nA function to compute the Blondel similarity as described earlier can be written as:\n\n   blondel.sim <- function(A, B) {\n      K <- matrix(1, nrow(B), nrow(A))\n      if (is.null(rownames(A)) == TRUE) {\n         rownames(A) <- 1:nrow(A)\n         colnames(A) <- 1:nrow(A)\n         }\n      if (is.null(rownames(B)) == TRUE) {\n         rownames(B) <- 1:nrow(B)\n         colnames(B) <- 1:nrow(B)\n         }\n      k <- 1\n      diff <- 1\n      old.diff <- 2\n      while (diff != old.diff | k %% 2 == 0) {\n         old.diff <- diff\n         K.old <- K\n         K <- (B %*% K.old %*% t(A)) + (t(B) %*% K.old %*% A)\n         K <- K/norm(K, type = \"F\")\n         diff <- abs(sum(abs(K)) - sum(abs(K.old)))\n         k <- k + 1\n      }\n   for (j in 1:ncol(K)) {\n      K[, j] <- K[, j]/max(K[, j])\n      }\n   rownames(K) <- rownames(B)\n   colnames(K) <- rownames(A)\n   return(list(K = K, k = k, diff = diff))\n   }\n\nWhich is modeled after our status game function but instead of computing a vector of scores we are populating a whole matrix!\nThe basic task is to figure out which nodes from the first matrix are most similar to which nodes from the second. That is, given these two networks can be identify actors who play similar roles in each?\nAnd here are the results presented in tabular form:\n\n   library(kableExtra)\n   A <- as.matrix(as_adjacency_matrix(g1))\n   B <- as.matrix(as_adjacency_matrix(g2))\n   K <- round(blondel.sim(A, B)[[1]], 2)\n   kbl(K, align = \"c\", format = \"html\", row.names = TRUE) %>% \n      column_spec(1, bold = TRUE) %>% \n      kable_styling(full_width = TRUE,\n                     bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\n \n  \n      \n    1 \n    2 \n    3 \n    4 \n    5 \n    6 \n    7 \n    8 \n    9 \n    10 \n    11 \n    12 \n    13 \n  \n \n\n  \n    1 \n    0.25 \n    0.22 \n    0.41 \n    0.48 \n    0.45 \n    0.11 \n    0.47 \n    0.24 \n    0.20 \n    0.30 \n    0.47 \n    0.25 \n    0.36 \n  \n  \n    2 \n    0.39 \n    0.38 \n    0.38 \n    0.27 \n    0.16 \n    0.37 \n    0.15 \n    0.38 \n    0.34 \n    0.38 \n    0.36 \n    0.34 \n    0.34 \n  \n  \n    3 \n    0.64 \n    0.63 \n    0.54 \n    0.33 \n    0.09 \n    0.67 \n    0.10 \n    0.63 \n    0.56 \n    0.58 \n    0.51 \n    0.55 \n    0.51 \n  \n  \n    4 \n    0.65 \n    0.66 \n    0.52 \n    0.26 \n    0.00 \n    0.71 \n    0.00 \n    0.63 \n    0.60 \n    0.59 \n    0.46 \n    0.58 \n    0.50 \n  \n  \n    5 \n    1.00 \n    1.00 \n    0.99 \n    0.73 \n    0.28 \n    1.00 \n    0.34 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n  \n  \n    6 \n    0.66 \n    0.67 \n    0.56 \n    0.33 \n    0.04 \n    0.71 \n    0.06 \n    0.65 \n    0.64 \n    0.62 \n    0.53 \n    0.62 \n    0.57 \n  \n  \n    7 \n    0.05 \n    0.04 \n    0.12 \n    0.14 \n    0.16 \n    0.00 \n    0.15 \n    0.04 \n    0.04 \n    0.08 \n    0.12 \n    0.05 \n    0.09 \n  \n  \n    8 \n    0.32 \n    0.31 \n    0.37 \n    0.32 \n    0.24 \n    0.26 \n    0.24 \n    0.30 \n    0.29 \n    0.34 \n    0.37 \n    0.31 \n    0.34 \n  \n  \n    9 \n    0.49 \n    0.44 \n    0.88 \n    1.00 \n    1.00 \n    0.20 \n    1.00 \n    0.47 \n    0.40 \n    0.63 \n    0.96 \n    0.50 \n    0.71 \n  \n  \n    10 \n    0.31 \n    0.28 \n    0.73 \n    0.89 \n    0.89 \n    0.04 \n    0.91 \n    0.29 \n    0.30 \n    0.47 \n    0.82 \n    0.39 \n    0.60 \n  \n  \n    11 \n    0.86 \n    0.86 \n    1.00 \n    0.82 \n    0.58 \n    0.76 \n    0.57 \n    0.86 \n    0.79 \n    0.92 \n    0.97 \n    0.82 \n    0.86 \n  \n  \n    12 \n    0.15 \n    0.15 \n    0.18 \n    0.16 \n    0.04 \n    0.15 \n    0.08 \n    0.15 \n    0.20 \n    0.17 \n    0.21 \n    0.20 \n    0.23 \n  \n  \n    13 \n    0.34 \n    0.34 \n    0.42 \n    0.37 \n    0.26 \n    0.29 \n    0.28 \n    0.33 \n    0.34 \n    0.38 \n    0.43 \n    0.36 \n    0.40 \n  \n  \n    14 \n    0.18 \n    0.17 \n    0.32 \n    0.35 \n    0.33 \n    0.09 \n    0.34 \n    0.17 \n    0.16 \n    0.23 \n    0.35 \n    0.19 \n    0.27 \n  \n  \n    15 \n    0.40 \n    0.41 \n    0.41 \n    0.28 \n    0.16 \n    0.39 \n    0.15 \n    0.40 \n    0.38 \n    0.41 \n    0.37 \n    0.38 \n    0.36 \n  \n  \n    16 \n    0.53 \n    0.51 \n    0.92 \n    0.99 \n    0.87 \n    0.29 \n    0.91 \n    0.52 \n    0.54 \n    0.69 \n    1.00 \n    0.61 \n    0.80 \n  \n  \n    17 \n    0.46 \n    0.44 \n    0.70 \n    0.74 \n    0.49 \n    0.33 \n    0.58 \n    0.46 \n    0.53 \n    0.55 \n    0.81 \n    0.57 \n    0.71 \n  \n  \n    18 \n    0.34 \n    0.31 \n    0.67 \n    0.81 \n    0.63 \n    0.15 \n    0.72 \n    0.34 \n    0.41 \n    0.46 \n    0.82 \n    0.47 \n    0.67 \n  \n\n\n\n\n\nIn the table each column is normalized by its maximum, so a 1.0 in that column tells us that that node (from the first network) is maximally similar to the corresponding row node (from the second network).\nFor instance, node 5 in the women’s lawyers graph (a highly central node in terms of being an adviser) is most similar to node 1 in the older men partner’s graph (also a highly central node in terms of being an adviser).\nNode 9 in the women lawyer’s graph, who’s mostly an advise-seeker, is most similar to node 5 in the older men partner graph who’s also an advise-seeker. So it looks like it works!"
  },
  {
    "objectID": "blondel.html#equivalence-to-hits",
    "href": "blondel.html#equivalence-to-hits",
    "title": "Role Similarity Across Graphs",
    "section": "Equivalence to HITS",
    "text": "Equivalence to HITS\nOne neat thing that Blondel et al. (2004) show is that we can also take a network and compare it to ideal-typical small graphs and get scores for how much each node in the observed network resembles each of the nodes in the hypothetical ideal-typical structure.\nMore specifically, they show that if we can run their algorithm to compare any network to the following two-node graph:\n\n   g <- make_empty_graph(2)\n   g <- add_edges(g, c(1,2))\n   V(g)$name <- c(\"Hub\", \"Authority\")\n   plot(g, \n        edge.arrow.size=1, \n        vertex.color=\"tan2\", \n        vertex.size=12, vertex.frame.color=\"lightgray\", \n        vertex.label.color=\"black\", vertex.label.cex=1.5, \n        vertex.label.dist=-3)\n\n\n\n\nIn which case, the result “similarity” scores, will be equivalent to the Hub and Authority scores!\nWe can check that this is the case for the women’s lawyers advice graph:\n\n   A <- as.matrix(as_adjacency_matrix(g))\n   B <- as.matrix(as_adjacency_matrix(g2))\n   K <- round(blondel.sim(A, B)[[1]], 4)\n   tab <- cbind(K, Hub.Score = round(hits_scores(g2)$hub, 4), \n         Auth.Score = round(hits_scores(g2)$authority, 4))\n   kbl(tab, align = \"c\", format = \"html\", row.names = TRUE) %>% \n      column_spec(1, bold = TRUE) %>% \n      kable_styling(full_width = TRUE,\n                     bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\n \n  \n      \n    Hub \n    Authority \n    Hub.Score \n    Auth.Score \n  \n \n\n  \n    1 \n    0.3810 \n    0.0000 \n    0.3810 \n    0.0000 \n  \n  \n    2 \n    0.1434 \n    0.3415 \n    0.1434 \n    0.3415 \n  \n  \n    3 \n    0.0813 \n    0.6021 \n    0.0813 \n    0.6021 \n  \n  \n    4 \n    0.0000 \n    0.7826 \n    0.0000 \n    0.7826 \n  \n  \n    5 \n    0.1654 \n    1.0000 \n    0.1654 \n    1.0000 \n  \n  \n    6 \n    0.0000 \n    0.7826 \n    0.0000 \n    0.7826 \n  \n  \n    7 \n    0.1434 \n    0.0000 \n    0.1434 \n    0.0000 \n  \n  \n    8 \n    0.2235 \n    0.2858 \n    0.2235 \n    0.2858 \n  \n  \n    9 \n    1.0000 \n    0.0386 \n    1.0000 \n    0.0386 \n  \n  \n    10 \n    0.8973 \n    0.0000 \n    0.8973 \n    0.0000 \n  \n  \n    11 \n    0.6110 \n    0.6559 \n    0.6110 \n    0.6559 \n  \n  \n    12 \n    0.0000 \n    0.2095 \n    0.0000 \n    0.2095 \n  \n  \n    13 \n    0.2239 \n    0.3364 \n    0.2239 \n    0.3364 \n  \n  \n    14 \n    0.3197 \n    0.0523 \n    0.3197 \n    0.0523 \n  \n  \n    15 \n    0.1434 \n    0.4036 \n    0.1434 \n    0.4036 \n  \n  \n    16 \n    0.8432 \n    0.2174 \n    0.8432 \n    0.2174 \n  \n  \n    17 \n    0.4089 \n    0.3188 \n    0.4089 \n    0.3188 \n  \n  \n    18 \n    0.5222 \n    0.0955 \n    0.5222 \n    0.0955 \n  \n\n\n\n\n\nThe first two columns are the scores using the function to compute the Blondel et al. similarity to each of the two nodes in the Hub/Authority micro-graph and the third and fourth columns are the scores we get from the igraph function hits scores, which as we can see, are identical."
  },
  {
    "objectID": "blondel.html#computing-a-brokerage-score",
    "href": "blondel.html#computing-a-brokerage-score",
    "title": "Role Similarity Across Graphs",
    "section": "Computing a Brokerage Score",
    "text": "Computing a Brokerage Score\nOf course in a directed graph, there are more than two ideal typical “roles.” In addition to “sender” (Hub) or “receiver” (Authority) we may also have “intermediaries” or “pass along” nodes. We can thus get an “intermediary” score for each node by comparing any network to the following three-node graph:\n\n   g <- make_empty_graph(3)\n   g <- add_edges(g, c(1,2, 2,3))\n   V(g)$name <- c(\"Hub\", \"Broker\", \"Authority\")\n   plot(g, \n        edge.arrow.size=1, \n        vertex.color=\"tan2\", \n        vertex.size=12, vertex.frame.color=\"lightgray\", \n        vertex.label.color=\"black\", vertex.label.cex=1.5, \n        vertex.label.dist=2)\n\n\n\n\nHere are the results for the women lawyers graph:\n\n   A <- as.matrix(as_adjacency_matrix(g))\n   B <- as.matrix(as_adjacency_matrix(g2))\n   K <- round(blondel.sim(A, B)[[1]], 4)\n   kbl(K, align = \"c\", format = \"html\", row.names = TRUE) %>% \n      column_spec(1, bold = TRUE) %>% \n      kable_styling(full_width = TRUE,\n                     bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\n \n  \n      \n    Hub \n    Broker \n    Authority \n  \n \n\n  \n    1 \n    0.5190 \n    0.3321 \n    0.1135 \n  \n  \n    2 \n    0.1318 \n    0.3472 \n    0.3600 \n  \n  \n    3 \n    0.0998 \n    0.4586 \n    0.6923 \n  \n  \n    4 \n    0.0000 \n    0.4888 \n    0.7547 \n  \n  \n    5 \n    0.5241 \n    0.8968 \n    1.0000 \n  \n  \n    6 \n    0.0954 \n    0.5091 \n    0.7547 \n  \n  \n    7 \n    0.1318 \n    0.1234 \n    0.0000 \n  \n  \n    8 \n    0.2501 \n    0.3584 \n    0.2755 \n  \n  \n    9 \n    1.0000 \n    0.8236 \n    0.2000 \n  \n  \n    10 \n    0.9913 \n    0.6859 \n    0.0318 \n  \n  \n    11 \n    0.5445 \n    1.0000 \n    0.7371 \n  \n  \n    12 \n    0.1971 \n    0.1427 \n    0.1530 \n  \n  \n    13 \n    0.3175 \n    0.4117 \n    0.2970 \n  \n  \n    14 \n    0.3710 \n    0.2875 \n    0.0918 \n  \n  \n    15 \n    0.1318 \n    0.4205 \n    0.3780 \n  \n  \n    16 \n    0.9998 \n    0.8747 \n    0.2453 \n  \n  \n    17 \n    0.7783 \n    0.5841 \n    0.3102 \n  \n  \n    18 \n    0.9644 \n    0.5162 \n    0.1302 \n  \n\n\n\n\n\nColumns one and three gives us versions of the Hub and Authority scores (respectively), but column two now gives us a “score” for how much the row node resembles and intermediary (or broker) in the network. We can see that the “purest” broker in the women’s advice network is node 11."
  },
  {
    "objectID": "cube.html",
    "href": "cube.html",
    "title": "The Cube",
    "section": "",
    "text": "Brandes, Borgatti, and Freeman (2016) discuss the centrality “cube,” an interesting and intuitive way to understand the way betweenness centrality works, as well as the dual connection between closeness and betweenness.\nLet us illustrate using a simple example. We begin by creating an Erdos-Renyi graph with eight nodes and connection probability \\(p = 0.5\\):\nThe resulting graph looks like:\nThe basic innovation behind the centrality cube is to store the intermediation information among every node triplet in the graph \\(s\\), \\(r\\), \\(b\\) (standing for “sender,” receiver,” and “broker”) in a three dimensional array rather than the usual two dimensional matrix.\nThe three dimensional array can be thought of as a “cube” by stacking multiple reachability matrices between every pair \\(s\\) and \\(r\\) along a three dimensional dimension \\(b\\). So each “b-slice” of the cube will contain the number of times node \\(b\\) stands in a shortest path between \\(s\\) and \\(r\\) divided by the total number of paths between \\(s\\) and \\(r\\) which as you recall computes the pairwise betweenness of \\(b\\) with respect to \\(s\\) and \\(r\\).\nLet’s see how that works."
  },
  {
    "objectID": "cube.html#building-the-cube",
    "href": "cube.html#building-the-cube",
    "title": "The Cube",
    "section": "Building the Cube",
    "text": "Building the Cube\nWe begin by writing a simple user-defined function to count the total number of shortest paths between each pair of nodes:\n\n   nsp <- function(x) {\n      n <- vcount(g)\n      S <- matrix(0, n, n)\n      for (i in 1:n) {\n            for (j in 1:n) {\n                  if (j %in% neighbors(x, i) == FALSE) {\n                     S[i, j] <- length(all_shortest_paths(x, i, j)$vpaths)\n               }\n            }\n         }\n      return(S)\n   }\n\nThe function is called nsp and takes a graph as input and returns and matrix called \\(\\mathbf{S}\\) with entries \\(s_{ij}\\) equal to the total number of shortest paths between \\(i\\) and \\(j\\). This is done by computing the length of the list returned by the all_shortest_paths function in igraph for each pair of non-adjacent nodes. This is done in two steps.\n\nFirst, we check whether \\(j\\) is a neighbor of \\(i\\) using the neighbors function in igraph. The neighbors function takes a graph and a node id as input and returns a vector of that node’s neighbors in the graph. We want the function to update the \\(S\\) matrix only when \\(i\\) and \\(j\\) are not adjacent (indirectly connected).\nSecond, we use the all_shortest_paths function to actually compute the number of shortest paths between \\(i\\) and \\(j\\). This function takes three inputs: (1) A graph object, (2) a sender node id, and (3) a receiver node id (which can be a vector of receiver nodes), and returns a list of the paths between the sender and receiver nodes in the form of vectors of node ids defining each path as elements of a list called “vpaths.”\n\nNow we are ready to write a user defined function to build the cube. Here’s a not-so-efficient (programming wise) but working example:\n\n   cube <- function(g) {\n      n <- vcount(g)\n      c <- array(rep(n^3, 0), c(n, n, n))\n      S <- nsp(g)\n      for (b in 1:n) {\n         for (s in 1:n) {\n            for (r in 1:n) {\n               if (s != r & r %in% neighbors(g, s) == FALSE) {\n                  p.sr <- all_shortest_paths(g, s, r)$vpaths\n                  b.sr <- lapply(p.sr, function(x) {x[-c(1, length(x))]}) \n                  c[s, r, b] <- sum(as.numeric(sapply(b.sr, function(x) {b %in% x})))\n                  c[s, r, b] <- c[s, r, b]/S[s, r]\n               }\n            }\n         }\n      }\n   c[is.na(c)] <- 0\n   return(c)\n   }\n\nIn line 1, we name the function cube. Line 3 initializes the array in R. It takes a string of zeros of length \\(n^3\\) where \\(n\\) is the number of nodes and crams them into an \\(n \\times n \\times n\\) array. In this case, since \\(n = 8\\), this means eight empty matrices of dimensions \\(8 \\times 8\\) stacked together to form our cube full of zeros. The \\(ijk^{th}\\) cell of the array corresponds to sender \\(i\\), receiver \\(j\\) and broker node \\(k\\). Line 4 computes the matrix \\(S\\) containing the number of shortest paths between every sender and receiver node in the graph.\nLines 5-15 populate the cube with the required information using an (inefficient) triple for loop. As noted some, useful igraph functions come into play here:\n\nIn line 8 the if conditional inside the triple loop uses the neighbors function in igraph and checks that node \\(r\\) is not a neighbor of \\(s\\) (if they are directly connected then node \\(b\\) cannot be a broker).\nAfter we check that \\(s\\) and \\(r\\) are not neighbors, we use the all_shortest_paths function in igraph to get all the shortest paths between \\(s\\) and \\(r\\) in line 9.\nLine 10 uses some lapply magic to drop the source and receiver nodes from the list of node ids vectors returned by the all_shortest_paths function.\nLine 11 uses additional sapply magic and the base R function %in% to check how many times each broker node \\(b\\) shows up in that list of shortest paths as an inner node between \\(s\\) and \\(r\\); we put that number in the \\(ijk^{th}\\) cell of the array, and loop through all triplets until we are done.\nLine 12 takes the number computed in line 11 and divides by the total number of shortest paths between \\(s\\) and \\(r\\) which is the betweenness ratio we are seeking."
  },
  {
    "objectID": "cube.html#exploring-the-cube",
    "href": "cube.html#exploring-the-cube",
    "title": "The Cube",
    "section": "Exploring the Cube",
    "text": "Exploring the Cube\nOnce we have our array, we can create all kinds of interesting sub-matrices containing the intermediation information in the graph by summing rows and columns of the array along different dimensions.\nFirst, let us see what’s in the cube. We can query specific two-dimensional sub-matrices using an extension of the usual format for querying matrices in R for three-dimensional arrays. For instance this:\n\n   srb <- cube(g)\n   round(srb[, , 2], 2)\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n[1,]  0.0    0 0.00    0 0.00  0.5    0 0.00\n[2,]  0.0    0 0.00    0 0.00  0.0    0 0.00\n[3,]  0.0    0 0.00    0 0.00  0.0    0 0.33\n[4,]  0.0    0 0.00    0 0.00  0.0    0 0.00\n[5,]  0.0    0 0.00    0 0.00  0.0    0 0.33\n[6,]  0.5    0 0.00    0 0.00  0.0    0 1.00\n[7,]  0.0    0 0.00    0 0.00  0.0    0 0.00\n[8,]  0.0    0 0.33    0 0.33  1.0    0 0.00\n\n\nCreates a three-dimensional matrix of pairwise betweenness probabilities and assigns it to the srb object in line 1, and looks at the \\(s_{\\bullet} \\times r_{\\bullet} \\times b_2\\) entry in line 2.\nEach entry in the matrix is the probability that node 2 stands on a shortest path between the row sender and the column receiver node. For instance, the 0.33 in the entry corresponding to row 5 and column 8 tells us that node 2 stands in one third of the shortest paths between nodes 5 and 8 (there are 3 distinct shortest paths between 5 and 8).\nBecause each sub-matrix in the cube is a matrix, we can do the usual matrix operations on them. For instance, let’s take the row sums of the \\(s\\) to \\(r\\) matrix corresponding to node 3 as the broker. This can be done like this:\n\n   d.3 <- rowSums(srb[ , , 3])\n   names(d.3) <- 1:vcount(g)\n   d.3\n\n  1   2   3   4   5   6   7   8 \n1.5 2.0 0.0 3.0 6.0 3.5 3.0 1.0 \n\n\nAs Brandes, Borgatti, and Freeman (2016), note this vector gives us the dependence of each node in the graph on node 3. Obviously node 3 doesn’t depend on itself so there is a zero on the third spot in the vector. As is clear from the plot, node 5 is the most dependent on 3 for intermediation with the rest of the nodes in the graph.\nWe can also pick a particular sender and receiver node and sum all their dyadic entries in the cube across the third (broker) dimension:\n\n   sum(srb[1, 6, ])\n\n[1] 2\n\n\nThis number is equivalent to the geodesic distance between the nodes minus one:\n\n   distances(g)[1, 6] - 1\n\n[1] 2"
  },
  {
    "objectID": "cube.html#betweenness-and-closeness-in-the-cube",
    "href": "cube.html#betweenness-and-closeness-in-the-cube",
    "title": "The Cube",
    "section": "Betweenness and Closeness in the Cube",
    "text": "Betweenness and Closeness in the Cube\nThe betweenness centrality of each node is encoded in the cube, because we already computed the main ratio that the measure depends on. For instance, let’s look at the matrix composed by taking the slice of cube that corresponds to node 3 as a broker:\n\n   srb[, , 3]\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n[1,]  0.0  0.0    0  0.0    1  0.5  0.0    0\n[2,]  0.0  0.0    0  0.5    1  0.0  0.5    0\n[3,]  0.0  0.0    0  0.0    0  0.0  0.0    0\n[4,]  0.0  0.5    0  0.0    1  1.0  0.5    0\n[5,]  1.0  1.0    0  1.0    0  1.0  1.0    1\n[6,]  0.5  0.0    0  1.0    1  0.0  1.0    0\n[7,]  0.0  0.5    0  0.5    1  1.0  0.0    0\n[8,]  0.0  0.0    0  0.0    1  0.0  0.0    0\n\n\nThe sum of the all the cells in this matrix (divided by two) correspond to node 3’s betweenness centrality:\n\n   sum(srb[, , 3])/2\n\n[1] 10\n\n   betweenness(g)[3]\n\n[1] 10\n\n\nSo to get each node’s betweenness we just can just sum up the entries in each of the cube’s sub-matrices:\n\n   b.cube <- round(colSums(srb, dims = 2)/2, 2)\n   b.igraph <- round(betweenness(g), 2)\n   names(b.cube) <- 1:vcount(g)\n   names(b.igraph) <- 1:vcount(g)\n   b.cube\n\n    1     2     3     4     5     6     7     8 \n 0.00  2.17 10.00  0.67  0.00  0.00  3.17  4.00 \n\n   b.igraph\n\n    1     2     3     4     5     6     7     8 \n 0.00  2.17 10.00  0.67  0.00  0.00  3.17  4.00 \n\n\nNote the neat trick of using the argument dims = 2 in the usual colSums command. This tells colSums that we are dealing with a three dimensional matrix, and that what we want is the sum of the columns across the cube’s third dimension (the brokers). Note also that we divide the cube betweenness by two because we are summing identical entries across the upper and lower triangle of the symmetric dyadic brokerage matrices inside the cube (not surprisingly, node 3 is the top betweenness centrality node).\nAs Brandes, Borgatti, and Freeman (2016) point out, using the cube info, we can build a matrix of dependencies between each pair of nodes. In this matrix, the rows correspond to a sender (or receiver) node, the columns to a broker node and the \\(sb^{th}\\) entry contains the sum of the proportion of paths containing the broker nodes that starts with the sender node and end with some other node in the graph.\nHere’s a function that uses the cube info to build the dependency matrix that Brandes, Borgatti, and Freeman (2016) talk about using the cube as input:\n\n   dep.ij <- function(c) {\n      n <- nrow(c)\n      dep.ij <- rowSums(c[, , 1])\n      for (i in 2:n) {\n         dep.ij <- cbind(dep.ij,  rowSums(c[, , i]))\n         \n         }\n      rownames(dep.ij) <- 1:n\n      colnames(dep.ij) <- 1:n\n      return(dep.ij)\n   }\n\nThis function just takes the various vectors formed by the row sums of the sender-receiver matrix across each value of the third dimension (which is just each node in the graph when playing the broker role). It then returns a regular old \\(n \\times n\\) containing the info.\nHere’s the result when applied to our little example:\n\n   library(kableExtra)\n   kbl(round(dep.ij(srb), 2), \n       format = \"html\", align = \"c\", row.names = TRUE,\n       caption = \"Dependence Matrix.\") %>% \n      column_spec(1, bold = TRUE) %>%\n   kable_styling(bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\nDependence Matrix.\n \n  \n      \n    1 \n    2 \n    3 \n    4 \n    5 \n    6 \n    7 \n    8 \n  \n \n\n  \n    1 \n    0 \n    0.50 \n    1.5 \n    0.00 \n    0 \n    0 \n    2.50 \n    2.5 \n  \n  \n    2 \n    0 \n    0.00 \n    2.0 \n    0.00 \n    0 \n    0 \n    0.00 \n    2.0 \n  \n  \n    3 \n    0 \n    0.33 \n    0.0 \n    0.33 \n    0 \n    0 \n    1.33 \n    0.0 \n  \n  \n    4 \n    0 \n    0.00 \n    3.0 \n    0.00 \n    0 \n    0 \n    0.00 \n    2.0 \n  \n  \n    5 \n    0 \n    0.33 \n    6.0 \n    0.33 \n    0 \n    0 \n    1.33 \n    0.0 \n  \n  \n    6 \n    0 \n    1.50 \n    3.5 \n    0.00 \n    0 \n    0 \n    0.50 \n    0.5 \n  \n  \n    7 \n    0 \n    0.00 \n    3.0 \n    0.00 \n    0 \n    0 \n    0.00 \n    1.0 \n  \n  \n    8 \n    0 \n    1.67 \n    1.0 \n    0.67 \n    0 \n    0 \n    0.67 \n    0.0 \n  \n\n\n\n\n\nNote that this is valued matrix that is also asymmetric. Take for instance, node 3. Every node in the graph depends on node 3 for access to other nodes, but node 3 does not depend on nodes 1, 5, 6, or 8.\nInterestingly, as Brandes, Borgatti, and Freeman (2016) also show, the betweenness centrality also can be calculated from the dependency matrix! All we need to do is compute the column sums, equivalent to in-degree in the directed dependence network:\n\n   round(colSums(dep.ij(srb))/2, 2)\n\n    1     2     3     4     5     6     7     8 \n 0.00  2.17 10.00  0.67  0.00  0.00  3.17  4.00 \n\n\nEven more interestingly, closeness centrality is also in the dependence matrix! It is given by the outdegree of each actor in the directed dependence network, corresponding to the row sums of the matrix (shifted by a constant given by \\(n-1\\)).\n\n   c.c <- rowSums(distances(g))\n   c.d <- rowSums(dep.ij(srb)) + (vcount(g) - 1)\n   names(c.c) <- 1:vcount(g)\n   names(c.d) <- 1:vcount(g)\n   round(1/c.c, 3)\n\n    1     2     3     4     5     6     7     8 \n0.071 0.091 0.111 0.083 0.067 0.077 0.091 0.091 \n\n   round(1/c.d, 3)\n\n    1     2     3     4     5     6     7     8 \n0.071 0.091 0.111 0.083 0.067 0.077 0.091 0.091 \n\n\nHere we see that node 3 is also the top in closeness, followed closely (pun intended) by nodes 2, 7, and 8. This makes sense because an actor with high closeness is one that has low dependence on key nodes to be able to reach others.\nOf course, closeness is also in the cube because of the mathematical relationship we saw earlier between the sum of entries between senders and receivers across brokers in the cube and the geodesic distance.\nFor instance, let’s get the matrix corresponding to node 3’s role as sender across all brokers and receivers:\n\n   round(srb[3, , ], 2)\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n[1,]    0 0.00    0 0.00    0    0 1.00    0\n[2,]    0 0.00    0 0.00    0    0 0.00    0\n[3,]    0 0.00    0 0.00    0    0 0.00    0\n[4,]    0 0.00    0 0.00    0    0 0.00    0\n[5,]    0 0.00    0 0.00    0    0 0.00    0\n[6,]    0 0.00    0 0.00    0    0 0.00    0\n[7,]    0 0.00    0 0.00    0    0 0.00    0\n[8,]    0 0.33    0 0.33    0    0 0.33    0\n\n\nThe entries of this matrix give us the probability that node 3 is the sender, whenever the row node is the is the broker (an inner node in the path) and the column node is the receiver.\nFor instance, the value 0.3 in row 8 and column 1 tells us that node 3 is the sender node in one third of the paths that end in node 1 and feature node 8 as a broker.\nInterestingly, the sums of the entries in this matrix are equivalent to the sum of the geodesic distances between node 3 and every other node in the graph shifted by a constant (\\(n- 1\\)):\n\n   sum(srb[3, , ]) + (vcount(g) - 1)\n\n[1] 9\n\n   sum(distances(g)[3, ])\n\n[1] 9\n\n\nSo the closeness centrality can be computed from the cube as follows:\n\n   c <- 0\n   n <- vcount(g)\n   for (i in 1:n) {\n      c[i] <- 1/(sum(srb[i, , ]) + (n - 1))\n   }\n   round(c, 3)\n\n[1] 0.071 0.091 0.111 0.083 0.067 0.077 0.091 0.091\n\n\nWhich is the same as:\n\n   round(closeness(g), 3)\n\n[1] 0.071 0.091 0.111 0.083 0.067 0.077 0.091 0.091"
  },
  {
    "objectID": "handout1.html",
    "href": "handout1.html",
    "title": "Basic Network Statistics",
    "section": "",
    "text": "Here we will analyze a small network and computer some basic statistics of interest. The first thing we need to do is get some data! For this purpose, we will use the package networkdata (available here). To install the package, use the following code:\n\n   #install.packages(\"remotes\") \n   remotes::install_github(\"schochastics/networkdata\")\n\nTo load the network datasets in the networkdata just type:\n\n   library(networkdata)\n\nThe package contains a bunch of human and animal social networks to browse through them, type:\n\n   data(package = \"networkdata\")\n\nWe will pick one of the movies for this analysis, namely, Pulp Fiction. This is movie_559. In the movie network two characters are linked by an edge if they appear in a scene together. The networkdata data sets come in igraph format, so we need to load that package (or install it using install.packages if you haven’t done that yet).\n\n   #install.packages(\"igraph\") \n   library(igraph)\n   g <- movie_559"
  },
  {
    "objectID": "handout1.html#number-of-nodes-and-edges",
    "href": "handout1.html#number-of-nodes-and-edges",
    "title": "Basic Network Statistics",
    "section": "Number of Nodes and Edges",
    "text": "Number of Nodes and Edges\nNow we are ready to compute some basic network statistics. As with any network, we want to know what the number of nodes and the number of edges (links) are. Since this is a relatively small network, we can begin by listing the actors.\n\n   V(g)\n\n+ 38/38 vertices, named, from 9e7cc7a:\n [1] BRETT           BUDDY           BUTCH           CAPT KOONS     \n [5] ED SULLIVAN     ENGLISH DAVE    ESMARELDA       FABIENNE       \n [9] FOURTH MAN      GAWKER #2       HONEY BUNNY     JIMMIE         \n[13] JODY            JULES           LANCE           MANAGER        \n[17] MARSELLUS       MARVIN          MAYNARD         MIA            \n[21] MOTHER          PATRON          PEDESTRIAN      PREACHER       \n[25] PUMPKIN         RAQUEL          ROGER           SPORTSCASTER #1\n[29] SPORTSCASTER #2 THE GIMP        THE WOLF        VINCENT        \n[33] WAITRESS        WINSTON         WOMAN           YOUNG MAN      \n[37] YOUNG WOMAN     ZED            \n\n\nThe function V takes the igraph network object as input and returns an igraph.vs object as output (short for “igraph vertex sequence”), listing the names (if given as a graph attribute) of each node. The first line also tells us that there are 38 nodes in this network.\nThe igraph.vs object operates much like an R character vector, so we can query its length to figure out the number of nodes:\n\n   length(V(g))\n\n[1] 38\n\n\nThe analogue function for edges in igraph is E which also takes the network object as input and returns an object of class igraph.es (“igraph edge sequence”) as output:\n\n   E(g)\n\n+ 102/102 edges from 9e7cc7a (vertex names):\n [1] BRETT      --MARSELLUS       BRETT      --MARVIN         \n [3] BRETT      --ROGER           BRETT      --VINCENT        \n [5] BUDDY      --MIA             BUDDY      --VINCENT        \n [7] BRETT      --BUTCH           BUTCH      --CAPT KOONS     \n [9] BUTCH      --ESMARELDA       BUTCH      --GAWKER #2      \n[11] BUTCH      --JULES           BUTCH      --MARSELLUS      \n[13] BUTCH      --PEDESTRIAN      BUTCH      --SPORTSCASTER #1\n[15] BUTCH      --ENGLISH DAVE    BRETT      --FABIENNE       \n[17] BUTCH      --FABIENNE        FABIENNE   --JULES          \n[19] FOURTH MAN --JULES           FOURTH MAN --VINCENT        \n+ ... omitted several edges\n\n\nThis tells us that there are 102 edges (connected dyads) in the network. Some of these include Brett and Marsellus and Fabienne and Jules, but not all can be listed for reasons of space.\nigraph also has two dedicated functions that return the number of nodes and edges in the graph in one fell swoop. They are called vcount and ecount and take the graph object as input:\n\n   vcount(g)\n\n[1] 38\n\n   ecount(g)\n\n[1] 102"
  },
  {
    "objectID": "handout1.html#graph-density",
    "href": "handout1.html#graph-density",
    "title": "Basic Network Statistics",
    "section": "Graph Density",
    "text": "Graph Density\nOnce we have the number of edges and nodes, we can calculate the most basic derived statistic in a network, which is the density. Since the movie network is an undirected graph, the density is given by:\n\\[\n   \\frac{2m}{n(n-1)}\n\\]\nWhere \\(m\\) is the number of edges and \\(n\\) is the number of nodes, or in our case:\n\n   (2 * 102) / (38 * (38 - 1))\n\n[1] 0.1450925\n\n\nOf course, igraph has a dedicated function called edge_density to compute the density too, which takes the igraph object as input:\n\n   edge_density(g)\n\n[1] 0.1450925"
  },
  {
    "objectID": "handout1.html#degree",
    "href": "handout1.html#degree",
    "title": "Basic Network Statistics",
    "section": "Degree",
    "text": "Degree\nThe next set of graph metrics are based on the degree of the graph. We can list the graph’s degree set using the igraph function degree:\n\n   degree(g)\n\n          BRETT           BUDDY           BUTCH      CAPT KOONS     ED SULLIVAN \n              7               2              17               5               2 \n   ENGLISH DAVE       ESMARELDA        FABIENNE      FOURTH MAN       GAWKER #2 \n              4               1               3               2               3 \n    HONEY BUNNY          JIMMIE            JODY           JULES           LANCE \n              8               3               4              16               4 \n        MANAGER       MARSELLUS          MARVIN         MAYNARD             MIA \n              5              10               6               3              11 \n         MOTHER          PATRON      PEDESTRIAN        PREACHER         PUMPKIN \n              5               5               3               3               8 \n         RAQUEL           ROGER SPORTSCASTER #1 SPORTSCASTER #2        THE GIMP \n              3               6               2               1               2 \n       THE WOLF         VINCENT        WAITRESS         WINSTON           WOMAN \n              3              25               4               3               5 \n      YOUNG MAN     YOUNG WOMAN             ZED \n              4               4               2 \n\n\nThe degree function takes the igraph network object as input and returns a plain old R named vector as output with the names being the names attribute of vertices in the network object.\nUsually we are interested in who are the “top nodes” in the network by degree (a kind of centrality). To figure that out, all we need to do is sort the degree set (to generate the graph’s degree sequence) and list the top entries:\n\n   d <- degree(g)\n   d.sort <- sort(d, decreasing = TRUE)\n   d.sort[1:8]\n\n    VINCENT       BUTCH       JULES         MIA   MARSELLUS HONEY BUNNY \n         25          17          16          11          10           8 \n    PUMPKIN       BRETT \n          8           7 \n\n\nLine 1 stores the degrees in an object “d”, line 2 creates a “sorted” version of the same object (from bigger to smaller) and line 3 shows the first eight entries of the sorted degree sequence.\nBecause the degree vector “d” is just a regular old vector we can use native R mathematical operations to figure out things like the sum, maximum, minimum, and average degree of the graph:\n\n   sum(d)\n\n[1] 204\n\n   max(d)\n\n[1] 25\n\n   min(d)\n\n[1] 1\n\n   mean(d)\n\n[1] 5.368421\n\n\nSo the sum of degrees is 204, the maximum degree is 25 (belonging to Vincent), the minimum is one, and the average is about 5.4.\nNote that these numbers recreate some well-known equalities in graph theory:\n\nThe sum of degrees is twice the number of edges (the first theorem of graph theory):\n\n\n   2 * ecount(g)\n\n[1] 204\n\n\n\nThe average degree is just the sum of degrees divided by the number of nodes:\n\n\n   sum(d)/vcount(g)\n\n[1] 5.368421\n\n\n\nThe density is just the average degree divided by the number of nodes minus one, as explained here:\n\n\n   mean(d)/(vcount(g) - 1)\n\n[1] 0.1450925\n\n\nSome people also consider the degree variance of the graph as a measure of inequality of connectivity in the system. It is equal to the average sum of square deviations of each node’s degree from the average:\n\\[\n  \\mathcal{v}(G) = \\frac{\\sum_i (k_i - \\bar{k})^2}{n}\n\\]\n\n   sum((d - mean(d))^2)/vcount(g)\n\n[1] 22.96953\n\n\nThis tells us that there is a lot of inequality in the distribution of degrees in the graph (a graph with all nodes equal degree would have variance zero)."
  },
  {
    "objectID": "handout1.html#the-degree-distribution",
    "href": "handout1.html#the-degree-distribution",
    "title": "Basic Network Statistics",
    "section": "The Degree Distribution",
    "text": "The Degree Distribution\nAnother way of looking at inequalities of degrees in a graph is to examine its degree distribution. This gives us the probability of observing a node with a given degree k in the graph.\n\n   deg.dist <- degree_distribution(g)\n   deg.dist <- round(deg.dist, 3)\n   deg.dist\n\n [1] 0.000 0.053 0.158 0.237 0.158 0.132 0.053 0.026 0.053 0.000 0.026 0.026\n[13] 0.000 0.000 0.000 0.000 0.026 0.026 0.000 0.000 0.000 0.000 0.000 0.000\n[25] 0.000 0.026\n\n\nThe igraph function degree_distribution just returns a numeric vector of the same length as the maximum degree of the graph plus one. In this case that’s a vector of length 25 + 1 = 26. The first entry gives us the proportion of nodes with degree zero (isolates), the second the proportion of nodes of degree one, and so on up to the graph’s maximum degree.\nSince there are no isolates in the network, we can ignore the first element of this vector, to get the proportion of nodes of each degree in the Pulp Fiction network. To that, we fist create a two-column data.frame with the degrees in the first column and the proportions in the second:\n\n   degree <- c(1:max(d))\n   prop <- deg.dist\n   prop <- prop[-1]\n   deg.dist <- data.frame(degree, prop)\n   deg.dist\n\n   degree  prop\n1       1 0.053\n2       2 0.158\n3       3 0.237\n4       4 0.158\n5       5 0.132\n6       6 0.053\n7       7 0.026\n8       8 0.053\n9       9 0.000\n10     10 0.026\n11     11 0.026\n12     12 0.000\n13     13 0.000\n14     14 0.000\n15     15 0.000\n16     16 0.026\n17     17 0.026\n18     18 0.000\n19     19 0.000\n20     20 0.000\n21     21 0.000\n22     22 0.000\n23     23 0.000\n24     24 0.000\n25     25 0.026\n\n\nOf course, a better way to display the degree distribution of a graph is via some kind of data visualization, particularly for large networks where a long table of numbers is just not feasible. To do that, we can call on our good friend ggplot:\n\n   # install.packages(ggplot2)\n   library(ggplot2)\n   p <- ggplot(data = deg.dist, aes(x = degree, y = prop))\n   p <- p + geom_bar(stat = \"identity\", fill = \"red\", color = \"red\")\n   p <- p + theme_minimal()\n   p <- p + labs(x = \"\", y = \"Proportion\", \n                 title = \"Degree Distribution in Pulp Fiction Network\") \n   p <- p + geom_vline(xintercept = mean(d), \n                       linetype = 2, linewidth = 0.5, color = \"blue\")\n   p <- p + scale_x_continuous(breaks = c(1, 5, 10, 15, 20, 25))\n   p\n\n\n\n\nThe plot clearly shows that the Pulp Fiction network degree distribution is skewed with a small number of characters having a large degree \\(k \\geq 15\\) while most other characters in the movie have a small degree \\(k \\leq 5\\) indicating inequality of connectivity in the system."
  },
  {
    "objectID": "handout1.html#the-degree-correlation",
    "href": "handout1.html#the-degree-correlation",
    "title": "Basic Network Statistics",
    "section": "The Degree Correlation",
    "text": "The Degree Correlation\nAnother overall network statistic we may want to know is the degree correlation (Newman 2002). How do we compute it? Imagine taking each edge in the network and creating two degree vectors, one based on the degree of the node in one end and the degre of the node in another. Then the degree assortativity coefficient is just the Pearson product moment correlation between these two vectors.\nLet’s see how this would work for the Pulp Fiction network. First we need to extract an edge list from the graph:\n\n   g.el <- as_edgelist(g) #transforming graph to edgelist\n   head(g.el)\n\n     [,1]    [,2]       \n[1,] \"BRETT\" \"MARSELLUS\"\n[2,] \"BRETT\" \"MARVIN\"   \n[3,] \"BRETT\" \"ROGER\"    \n[4,] \"BRETT\" \"VINCENT\"  \n[5,] \"BUDDY\" \"MIA\"      \n[6,] \"BUDDY\" \"VINCENT\"  \n\n\nWe can see that the as_edgelist function takes the igraph network object as input and returns an \\(E \\times 2\\) matrix, with \\(E = 102\\) being the number of rows. Each column of the matrix records the name of the node on each end of the edge. So the first row of the edge list with entries “BRETT” and “MARSELLUS” tells us that there is an edge linking Brett and Marsellus, and so forth for each row.\nTo compute the correlation between the degrees of each node, all we need to do is attach the corresponding degrees to each name for each of the columns of the edge list, which can be done via data wrangling magic from the dplyr package (part of the tidyverse):\n\n   # install.packages(dplyr)\n   library(dplyr)\n   deg.dat <- data.frame(name1 = names(d), name2 = names(d), d)\n   el.temp <- data.frame(name2 = g.el[, 2]) %>% \n      left_join(deg.dat, by = \"name2\") %>% \n      dplyr::select(c(\"name2\", \"d\")) %>% \n      rename(d2 = d) \n   d.el <- data.frame(name1 = g.el[, 1]) %>% \n      left_join(deg.dat, by = \"name1\") %>% \n      dplyr::select(c(\"name1\", \"d\")) %>% \n      rename(d1 = d) %>% \n      cbind(el.temp)\nhead(d.el)\n\n  name1 d1     name2 d2\n1 BRETT  7 MARSELLUS 10\n2 BRETT  7    MARVIN  6\n3 BRETT  7     ROGER  6\n4 BRETT  7   VINCENT 25\n5 BUDDY  2       MIA 11\n6 BUDDY  2   VINCENT 25\n\n\nLine 3 creates a two-column data frame called “deg.dat” with as many rows as there are nodes in the network. The first two columns contain the names of each node (identically listed with different names) and the third columns contains the corresponding node’s degree.\nLines 4-7 use dplyr functions to create a new object “el.temp” joining the degree information to each of the node names listed in the second position in the original edge list “g.el,” and rename the imported column of degrees “d2.”\nLines 8-12 do the same for the nodes listed in the first position in the original edge list, renames the imported columns of degrees “d1,” and the binds the columns of the “el.temp” object to the new object “d.el.” The resulting object has four columns: Two for the names of the nodes incident to each edge on the edge list (columns 1 and 3), and two other ones corresponding to the degrees of the corresponding nodes (columns 2 and 4).\nWe can see from the output of the first few rows of the “d.el” object that indeed “BRETT” is assigned a degree of 7 in each row of the edge list, “BUDDY” a degree of 2, “MARSELLUS” a degree of 10, “VINCENT” a degree of 25 and so forth.\nNow to compute the degree correlation in the network all we need to do is call the native R function cor on the two columns from “d.el” that containing the degree information. Note that because each degree appears twice at the end of each edge in an undirected graph (as both “sender” and “receiver”), we need to double each column by appending the other degree column at the end. So the first degree column is the vector:\n\n   d1 <- c(d.el$d1, d.el$d2)\n\nAnd the second degree column is the vector:\n\n   d2 <- c(d.el$d2, d.el$d1)\n\nAnd the graph’s degree correlation (Newman 2003) is just the Pearson correlation between these two degree vectors:\n\n   cor(d1, d2)\n\n[1] -0.2896427\n\n\nThe result \\(r_{deg} = -0.29\\) tells us that there is anti-correlation by degree in the Pulp Fiction network. That is high-degree characters tend to appear with low degree characters, or conversely, high-degree characters (like Marsellus and Jules) don’t appear together very often.\nOf course, igraph has a function called assortativity_degree that does all the work for us:\n\n   assortativity_degree(g)\n\n[1] -0.2896427"
  },
  {
    "objectID": "handout1.html#the-average-shortest-path-length",
    "href": "handout1.html#the-average-shortest-path-length",
    "title": "Basic Network Statistics",
    "section": "The Average Shortest Path Length",
    "text": "The Average Shortest Path Length\nThe final statistic people use to characterize networks is the average shortest path length. In a network, even non-adjacent nodes, could be indirectly connected to other nodes via a path of some length (\\(l > 1\\)) So it is useful to know what the average of this quantity is across all dyads in the network.\nTo do that, we first need to compute the length of the shortest path \\(l\\) for each pair of nodes in the network (also known as the geodesic distance). Adjacent nodes get an automatic score of \\(l = 1\\). In igraph this is done as follows:\n\n   S <- distances(g)\n   S[1:7, 1:7]\n\n             BRETT BUDDY BUTCH CAPT KOONS ED SULLIVAN ENGLISH DAVE ESMARELDA\nBRETT            0     2     1          2           2            2         3\nBUDDY            2     0     2          2           2            2         4\nBUTCH            1     2     0          1           2            1         2\nCAPT KOONS       2     2     1          0           2            2         3\nED SULLIVAN      2     2     2          2           0            2         4\nENGLISH DAVE     2     2     1          2           2            0         3\nESMARELDA        3     4     2          3           4            3         0\n\n\nThe igraph function distances takes the network object as input and returns the desired shortest path matrix. So for instance, Brett is directly connected to Butch (they appear in a scene together) but indirectly connected to Buddy via a path of length two (they both appear in scenes with common neighbors even if they don’t appear together).\nThe maximum distance between two nodes in the graph (the longest shortest path to put it confusingly) is called the graph diameter. We can find this out simply by using the native R function for the maximum on the shortest paths matrix:\n\n   max(S)\n\n[1] 8\n\n\nThis means that in the Pulp Fiction network the maximum degree of separation between two characters is a path of length 8.\nOf course, we cann also call the igraph function diammeter:\n\n   diameter(g)\n\n[1] 8\n\n\nOnce we have the geodesic distance matrix, it is easy to calculate the average path length of the graph:\n\n   rs.S <- rowSums(S)\n   rm.S <- rs.S/(vcount(g) - 1)\n   mean(rm.S)\n\n[1] 2.769559\n\n\n\nFirst (line 1) we sum all the rows (or columns) of the geodesic distance matrix. This vector (of the same length as the number of nodes) gives us the sum of the geodesic distance of each node to each of the nodes (we will use this to compute closeness centrality later).\nThen (line 2) we divide this vector by the number of nodes minus one (to exclude the focal node) to create a vector of the average distance of each node to each of the other nodes.\nFinally (line 3) we take the average across all nodes of this average distance vector to get the graph’s average shortest path length, which in this case equals L = 2.8.\n\nThis means that, on average, each character in Pulp Fiction is separated by little less than three contacts in the co-appearance network (a fairly small world).\nOf course this can also be done in just one step on igraph:\n\n   mean_distance(g)\n\n[1] 2.769559"
  },
  {
    "objectID": "handout1.html#putting-it-all-together",
    "href": "handout1.html#putting-it-all-together",
    "title": "Basic Network Statistics",
    "section": "Putting it all Together",
    "text": "Putting it all Together\nNow we can put together all the basic network statistics that we have computed into some sort of summary table, like the ones here. We first create a vector with the names of each statistic:\n\n   Stat <- c(\"Nodes\", \"Edges\", \"Min. Degree\", \"Max. Degree\", \"Avg. Degree\", \"Degree Corr.\", \"Diameter\", \"Avg. Shortest Path Length\")\n\nThen we create a vector with the values:\n\n   Value <- c(vcount(g), ecount(g), min(d), max(d), round(mean(d), 2), round(assortativity_degree(g), 2), max(S), round(mean_distance(g), 2))\n\nWe can then put these two vector together into a data frame:\n\n   net.stats <- data.frame(Stat, Value)\n\nWe can then use the package kableExtra (a nice table maker) to create a nice html table:\n\n   # intall.packages(kableExtra)\n   library(kableExtra)\n   kbl(net.stats, format = \"pipe\", align = c(\"l\", \"c\"),\n       caption = \"Key Statistics for Pulp Fiction Network.\") %>% \n   kable_styling(bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\nKey Statistics for Pulp Fiction Network.\n \n  \n    Stat \n    Value \n  \n \n\n  \n    Nodes \n    38.00 \n  \n  \n    Edges \n    102.00 \n  \n  \n    Min. Degree \n    1.00 \n  \n  \n    Max. Degree \n    25.00 \n  \n  \n    Avg. Degree \n    5.37 \n  \n  \n    Degree Corr. \n    -0.29 \n  \n  \n    Diameter \n    8.00 \n  \n  \n    Avg. Shortest Path Length \n    2.77"
  },
  {
    "objectID": "handout1.html#appendix-loading-network-data-from-a-file",
    "href": "handout1.html#appendix-loading-network-data-from-a-file",
    "title": "Basic Network Statistics",
    "section": "Appendix: Loading Network Data from a File",
    "text": "Appendix: Loading Network Data from a File\nWhen get network data from an archival source, and it will be in the form of a matrix or an edge list, typically in some kind of comma separated value (csv) format. Here will show how to input that into R to create an igraph network object from an outside file.\nFirst we will write the Pulp Fiction data into an edge list and save it to disk. We already did that earlier with the “g.el” object. So all we have to do is save it to your local folder as a csv file:\n\n   #install.packages(here)\n   library(here)\n   write.csv(d.el[c(\"name1\", \"name2\")], here(\"pulp.csv\"))\n\nThe write.csv function just saves an R object into a .csv file. Here the R object is “g.el” and we asked it to save just the columns which contain the name of each character. This represents the adjacency relations in the network as an edge list. We use the package here to keep track of our working directory. See here (pun intended) for details.\nNow suppose that’s the network we want to work with and it’s saved in our hard drive. To load it, we just type:\n\n   g.el <- read.csv(here(\"pulp.csv\"), \n                    col.names = c(\"name1\", \"name2\"))\n   head(g.el)\n\n  name1     name2\n1 BRETT MARSELLUS\n2 BRETT    MARVIN\n3 BRETT     ROGER\n4 BRETT   VINCENT\n5 BUDDY       MIA\n6 BUDDY   VINCENT\n\n\nWhich gives us the edge list we want now saved into an R object of class data.frame. So all we need is to convert that into an igraph object. To do that we use one of the many graph_from... functions in the igraph package. In this case, we want graph_from_edgelist because our network is stored as an edge list:\n\n   g.el <- as.matrix(g.el)\n   g <- graph_from_edgelist(g.el, directed = FALSE)\n   V(g)\n\n+ 38/38 vertices, named, from 99e5caf:\n [1] BRETT           MARSELLUS       MARVIN          ROGER          \n [5] VINCENT         BUDDY           MIA             BUTCH          \n [9] CAPT KOONS      ESMARELDA       GAWKER #2       JULES          \n[13] PEDESTRIAN      SPORTSCASTER #1 ENGLISH DAVE    FABIENNE       \n[17] FOURTH MAN      HONEY BUNNY     MANAGER         JIMMIE         \n[21] JODY            PATRON          PUMPKIN         RAQUEL         \n[25] WINSTON         LANCE           MAYNARD         THE GIMP       \n[29] ZED             ED SULLIVAN     MOTHER          WOMAN          \n[33] PREACHER        SPORTSCASTER #2 THE WOLF        WAITRESS       \n[37] YOUNG MAN       YOUNG WOMAN    \n\n   E(g)\n\n+ 102/102 edges from 99e5caf (vertex names):\n [1] BRETT      --MARSELLUS       BRETT      --MARVIN         \n [3] BRETT      --ROGER           BRETT      --VINCENT        \n [5] BUDDY      --MIA             VINCENT    --BUDDY          \n [7] BRETT      --BUTCH           BUTCH      --CAPT KOONS     \n [9] BUTCH      --ESMARELDA       BUTCH      --GAWKER #2      \n[11] BUTCH      --JULES           MARSELLUS  --BUTCH          \n[13] BUTCH      --PEDESTRIAN      BUTCH      --SPORTSCASTER #1\n[15] BUTCH      --ENGLISH DAVE    BRETT      --FABIENNE       \n[17] BUTCH      --FABIENNE        JULES      --FABIENNE       \n[19] JULES      --FOURTH MAN      VINCENT    --FOURTH MAN     \n+ ... omitted several edges\n\n\nWhich gives us back the original igraph object we have been working with. Note that first we converted the data.frame object into a matrix object. We also specified that the graph is undirected by setting the option directed to false."
  },
  {
    "objectID": "handout2.html",
    "href": "handout2.html",
    "title": "Centrality",
    "section": "",
    "text": "In this handout we will go through the basic centrality metrics. Particularly, the “big three” according to Freeman (1979), namely, degree, closeness (in two flavors) and betweenness.\nWe first load our trusty Pulp Fiction data set from the networkdata package, which is an undirected graph of character scene co-appearances in the film:"
  },
  {
    "objectID": "handout2.html#degree-centrality",
    "href": "handout2.html#degree-centrality",
    "title": "Centrality",
    "section": "Degree Centrality",
    "text": "Degree Centrality\nDegree centrality is the simplest and most straightforward measure. In fact, we are already computed in handout 1 since it is the same as obtaining the graph’s degree sequence. So the igraph function degree would do it as we already saw.\nHere we follow a different approach using the row (or column) sums of the graph’s adjacency matrix:\n\n   A <- as_adjacency_matrix(g)\n   A <- as.matrix(A)\n   rowSums(A)\n\n          BRETT           BUDDY           BUTCH      CAPT KOONS     ED SULLIVAN \n              7               2              17               5               2 \n   ENGLISH DAVE       ESMARELDA        FABIENNE      FOURTH MAN       GAWKER #2 \n              4               1               3               2               3 \n    HONEY BUNNY          JIMMIE            JODY           JULES           LANCE \n              8               3               4              16               4 \n        MANAGER       MARSELLUS          MARVIN         MAYNARD             MIA \n              5              10               6               3              11 \n         MOTHER          PATRON      PEDESTRIAN        PREACHER         PUMPKIN \n              5               5               3               3               8 \n         RAQUEL           ROGER SPORTSCASTER #1 SPORTSCASTER #2        THE GIMP \n              3               6               2               1               2 \n       THE WOLF         VINCENT        WAITRESS         WINSTON           WOMAN \n              3              25               4               3               5 \n      YOUNG MAN     YOUNG WOMAN             ZED \n              4               4               2 \n\n\nThe igraph function as_adjancency_matrix doesn’t quite return a regular R matrix object, so we have to further coerce the resulting object into a numerical matrix containing zeroes and ones using the as.matrix function in line 2. Then we can apply the native rowSums function to obtain each node’s degree. Note that this is same output we got using the degree function before."
  },
  {
    "objectID": "handout2.html#indegree-and-outdegree",
    "href": "handout2.html#indegree-and-outdegree",
    "title": "Centrality",
    "section": "Indegree and Outdegree",
    "text": "Indegree and Outdegree\nThe movie network is based on the relationship of co-appearance in a scene which by nature lacks any natural directionality (it’s a symmetric relation) and can therefore be represented in an undirected graph. The concepts of in and outdegree, by contrast, are only applicable to directed relations. So to illustrate them, we need to switch to a different source of data.\nWe pick an advice network which is a classical directed kind of (asymmetric) relation. I can give advice to you, but that doesn’t necessarily mean you can give advice to me. The networkdata package contains one such data set collected in the late 80s early 1990s in a New England law firm (see the description here), called law_advice:\n\n   d.g <- law_advice\n   V(d.g)\n\n+ 71/71 vertices, from d1a9da7:\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50\n[51] 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n\n   vertex_attr(d.g)\n\n$status\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2\n[39] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n\n$gender\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 1 1 1 1 2 1 1 1 2\n[39] 2 1 1 1 2 2 1 2 1 2 1 1 2 1 1 1 1 1 2 1 2 2 2 1 1 2 1 1 2 1 2 1 2\n\n$office\n [1] 1 1 2 1 2 2 2 1 1 1 1 1 1 2 3 1 1 2 1 1 1 1 1 1 2 1 1 2 1 2 2 2 2 1 2 1 3 1\n[39] 1 1 1 1 1 3 1 2 3 1 1 2 2 1 1 1 1 1 1 2 2 1 1 1 2 1 1 1 1 1 1 1 1\n\n$seniority\n [1] 31 32 13 31 31 29 29 28 25 25 23 24 22  1 21 20 23 18 19 19 17  9 16 15 15\n[26] 15 13 11 10  7  8  8  8  8  8  5  5  7  6  6  5  4  5  5  3  3  3  1  4  3\n[51]  4  4 10  3  3  3  3  3  2  2  2  2  2  2  2  1  1  1  1  1  1\n\n$age\n [1] 64 62 67 59 59 55 63 53 53 53 50 52 57 56 48 46 50 45 46 49 43 49 45 44 43\n[26] 41 47 38 38 39 34 33 37 36 33 43 44 53 37 34 31 31 47 53 38 42 38 35 36 31\n[51] 29 29 38 29 34 38 33 33 30 31 34 32 29 45 28 43 35 26 38 31 26\n\n$practice\n [1] 1 2 1 2 1 1 2 1 2 2 1 2 1 2 2 2 2 1 2 1 1 1 1 1 2 1 1 2 2 1 1 1 1 2 2 1 2 1\n[39] 1 1 1 2 1 2 2 2 1 2 1 2 1 1 2 1 1 1 1 1 1 2 2 2 2 2 1 1 1 1 1 2 1\n\n$law_school\n [1] 1 1 1 3 2 1 3 3 1 3 1 2 2 1 3 1 1 2 1 1 2 3 2 2 2 3 1 2 3 3 2 3 3 2 3 3 3 2\n[39] 1 1 2 2 2 1 3 2 3 3 2 2 3 3 3 3 3 2 2 3 2 2 3 2 2 2 3 3 2 3 3 2 2\n\n\nWe can see that the graph has 71 vertices, and that there are various attributes associated with each vertex, like gender, age, seniority, status in the law firm, etc. We can query those attributes using the igraph function vertex_attr, which takes the graph object as input.\n\nSubsetting the Graph According to a Node Attribute\nTo keep things manageable, we will restrict our analysis to partners. To do that we need to select the subgraph that only includes the vertices with value of 1 in the “status” vertex attribute. From the data description, we know the first 36 nodes (with value of 1 in the status attribute) are the law firm’s partners (the rest are associates). In igraph we can do this as using the subgraph function:\n\n   d.g <- subgraph(d.g, 1:36)\n   V(d.g)\n\n+ 36/36 vertices, from 9c5215f:\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30 31 32 33 34 35 36\n\n   V(d.g)$status\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n\nThe first line just tells igraph to generate the subgraph containing the first 36 nodes (the partners). The subgraph function thus takes two main inputs: The graph object, and then a vector of node ids (or node labels) telling the function which nodes to select to create the node-induced subgraph.\nOf course we already knew from the data description that the first 36 nodes where the partners. But let’s say we have a large data set and we don’t know which nodes are the partners. A smarter way of selecting a subgraph based on a node attribute is as follows:\n\n   partners <- which(V(law_advice)$status == 1)\n   d.g <- subgraph(law_advice, partners)\n   V(d.g)\n\n+ 36/36 vertices, from 9c58e12:\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30 31 32 33 34 35 36\n\n\nThe first line using the native R vector function which allowing us to subset a vector based on a logical condition. The function takes a vector followed by a logical condition as input, and returns the position of the vector elements that meet that condition. In this case, we took the vector of values for the attribute of status and selected the node ids where status is equal to 1. We then fed that vector to the subgraph function in line 2.\nWe could do this with any other attribute:\n\n   older <- which(V(law_advice)$age > 50)\n   older\n\n [1]  1  2  3  4  5  6  7  8  9 10 12 13 14 38 44\n\n   og <- subgraph(law_advice, older)\n   V(og)\n\n+ 15/15 vertices, from 9c5df0c:\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15\n\n\nHere we selected the subgraph (called “og”, get it, get it) formed by the subset of nodes over the age of 50 at the firm. The values of the vector older tell us which of the 71 members meet the relevant condition.\n\n\nComputing in and outdegree\nOK, going back to the partners subgraph, we can now create our (asymmetric) adjacency matrix and compute the row and column sums:\n\n   d.A <- as_adjacency_matrix(d.g)\n   d.A <- as.matrix(d.A)\n   rowSums(d.A)\n\n [1]  3  7  7 17  4  0  4  2  3  7  5 18 11 13 10 19 17  5 21 10  9 12  9 16  8\n[26] 22 18 22 13 15 16  9 15  6 15  7\n\n   colSums(d.A)\n\n [1] 18 17  8 14 10 17  4  8 10  6 11 14 14 12 15 13 21  7  7 17 11 10  2 14  7\n[26] 20  2 14 12 11  8 13  2 16  8  2\n\n\nNote that in contrast to the undirected case the row and column sums give you two different sets of numbers. The row sums provide the directed graph’s outdegree set (number of outgoing links incident to each node), and the column sums provide the graph’s indegree set (number of incoming links incident to each node). So if you are high in the first vector, you are an advice giver (perhaps indicating informal status or experience) and if you are high in the second you are advice taker.\nOf course igraph has a dedicated function for this, which is just our old friend degree with an extra option mode, indicating whether you want the in or outdegrees:\n\n   d.o <- degree(d.g, mode = \"out\")\n   d.i <- degree(d.g, mode = \"in\")\n   d.o\n\n [1]  3  7  7 17  4  0  4  2  3  7  5 18 11 13 10 19 17  5 21 10  9 12  9 16  8\n[26] 22 18 22 13 15 16  9 15  6 15  7\n\n   d.i\n\n [1] 18 17  8 14 10 17  4  8 10  6 11 14 14 12 15 13 21  7  7 17 11 10  2 14  7\n[26] 20  2 14 12 11  8 13  2 16  8  2\n\n\nNote that the graph attributes are just vectors of values, and can be accessed from the graph object using the $ operator attached to the V() function as we did above.\nSo if we wanted to figure out the correlation between some vertex attribute and in or out degree centrality, all we need to do is correlate the two vectors:\n\n   r <- cor(d.o, V(d.g)$age)\n   round(r, 2)\n\n[1] -0.43\n\n\nWhich tells us that at least in this case, younger partners are more sought after as sources of advice than older partners."
  },
  {
    "objectID": "handout2.html#closeness-centrality",
    "href": "handout2.html#closeness-centrality",
    "title": "Centrality",
    "section": "Closeness Centrality",
    "text": "Closeness Centrality\nRecall that the closeness centrality is defined as the inverse of the sum of the lengths of shortest paths from each node to every other node. That means that to compute it, we first need to calculate the geodesic distance matrix. This is matrix in which each entry \\(g_{ij}\\) records the length of the shortest path(s) between row node \\(i\\) and column node \\(j\\). Then, we sum the rows (or columns) of this symmetric matrix and then we obtain the inverse to get the closeness of each node:\n\n   S <- distances(g) #length of shortest paths matrix\n   d.sum <- rowSums(S)\n   close1 <- round(1/d.sum, 4)\n   close1\n\n          BRETT           BUDDY           BUTCH      CAPT KOONS     ED SULLIVAN \n         0.0125          0.0108          0.0143          0.0125          0.0108 \n   ENGLISH DAVE       ESMARELDA        FABIENNE      FOURTH MAN       GAWKER #2 \n         0.0105          0.0070          0.0096          0.0104          0.0097 \n    HONEY BUNNY          JIMMIE            JODY           JULES           LANCE \n         0.0114          0.0093          0.0093          0.0132          0.0093 \n        MANAGER       MARSELLUS          MARVIN         MAYNARD             MIA \n         0.0111          0.0104          0.0103          0.0098          0.0115 \n         MOTHER          PATRON      PEDESTRIAN        PREACHER         PUMPKIN \n         0.0125          0.0111          0.0097          0.0097          0.0114 \n         RAQUEL           ROGER SPORTSCASTER #1 SPORTSCASTER #2        THE GIMP \n         0.0105          0.0125          0.0096          0.0057          0.0073 \n       THE WOLF         VINCENT        WAITRESS         WINSTON           WOMAN \n         0.0056          0.0139          0.0083          0.0105          0.0125 \n      YOUNG MAN     YOUNG WOMAN             ZED \n         0.0083          0.0083          0.0073 \n\n\nOf course, we could have just used the available function in igraph and computed the closeness centrality directly from the graph object using the function closeness:\n\n   close2 <- round(closeness(g), 4)\n   close2\n\n          BRETT           BUDDY           BUTCH      CAPT KOONS     ED SULLIVAN \n         0.0125          0.0108          0.0143          0.0125          0.0108 \n   ENGLISH DAVE       ESMARELDA        FABIENNE      FOURTH MAN       GAWKER #2 \n         0.0105          0.0070          0.0096          0.0104          0.0097 \n    HONEY BUNNY          JIMMIE            JODY           JULES           LANCE \n         0.0114          0.0093          0.0093          0.0132          0.0093 \n        MANAGER       MARSELLUS          MARVIN         MAYNARD             MIA \n         0.0111          0.0104          0.0103          0.0098          0.0115 \n         MOTHER          PATRON      PEDESTRIAN        PREACHER         PUMPKIN \n         0.0125          0.0111          0.0097          0.0097          0.0114 \n         RAQUEL           ROGER SPORTSCASTER #1 SPORTSCASTER #2        THE GIMP \n         0.0105          0.0125          0.0096          0.0057          0.0073 \n       THE WOLF         VINCENT        WAITRESS         WINSTON           WOMAN \n         0.0056          0.0139          0.0083          0.0105          0.0125 \n      YOUNG MAN     YOUNG WOMAN             ZED \n         0.0083          0.0083          0.0073 \n\n\nOnce we have the closeness centrality values, we are interested in who are the top nodes. The following code creates a table with the top five:\n\n   library(kableExtra)\n   close2 <- sort(close2, decreasing = TRUE)\n   close2 <- data.frame(close2[1:5])\n   kbl(close2, format = \"pipe\", align = c(\"l\", \"c\"),\n       col.names = c(\"Character\", \"Closeness\"),\n       caption = \"Top Five Closeness Characters in Pulp Fiction Network.\") %>% \n   kable_styling(bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\nTop Five Closeness Characters in Pulp Fiction Network.\n \n  \n    Character \n    Closeness \n  \n \n\n  \n    BUTCH \n    0.0143 \n  \n  \n    VINCENT \n    0.0139 \n  \n  \n    JULES \n    0.0132 \n  \n  \n    BRETT \n    0.0125 \n  \n  \n    CAPT KOONS \n    0.0125 \n  \n\n\n\n\n\nIt makes sense that the three main characters are also the ones that are at closest distances from everyone else!"
  },
  {
    "objectID": "handout2.html#edge-closeness",
    "href": "handout2.html#edge-closeness",
    "title": "Centrality",
    "section": "Edge Closeness",
    "text": "Edge Closeness\nBröhl and Lehnertz (2022) define the closeness of an edge as a function of the closeness of the two nodes incident to it. An edge \\(e_{jk}\\) linking vertex \\(v_j\\) to \\(v_k\\) has high closeness whenever vertices \\(v_j\\) and \\(v_k\\) also have high closeness.\nMore specifically, the closeness centrality of an edge is proportional to the ratio of the product of the closeness of the two nodes incident to it divided by their sum:\n\\[\nC(e_{jk}) = (E - 1)\\frac{C(v_j) \\times C(v_k)}{C(v_j)+C(v_k)}\n\\]\nNote that the equation normalizes the ratio of the product to the sum of the vertex closeness centralities by the number of edges minus one.\nTo compute edge closeness in a real network, we can use the same approach to data wrangling we used to compute the degree correlation in Handout 1. The goal is to create an edge list data frame containing five columns. The ids of the two nodes in the edge, the closeness centralities of the two nodes in the edge, and the closeness centrality of the edge calculated according to the above equation.\nIn the Pulp Fiction network this looks like:\n\n   library(dplyr)\n   g.el <- as_edgelist(g) #transforming graph to edgelist\n   c <- round(closeness(g), 3)  #closeness centrality vector\n   c.dat <- data.frame(name1 = names(c), name2 = names(c), c)\n   el.temp <- data.frame(name2 = g.el[, 2]) %>% \n      left_join(c.dat, by = \"name2\") %>% \n      dplyr::select(c(\"name2\", \"c\")) %>% \n      rename(c2 = c) \n   c.el <- data.frame(name1 = g.el[, 1]) %>% \n      left_join(c.dat, by = \"name1\") %>% \n      dplyr::select(c(\"name1\", \"c\")) %>% \n      rename(c1 = c) %>% \n      cbind(el.temp) %>% \n      mutate(e.clos = round((ecount(g)-1)*(c1*c2)/(c+c2), 3))\nhead(c.el)\n\n  name1    c1     name2    c2 e.clos\n1 BRETT 0.013 MARSELLUS 0.010  0.571\n2 BRETT 0.013    MARVIN 0.010  0.625\n3 BRETT 0.013     ROGER 0.013  0.632\n4 BRETT 0.013   VINCENT 0.014  0.681\n5 BUDDY 0.011       MIA 0.011  0.555\n6 BUDDY 0.011   VINCENT 0.014  0.622\n\n\nTo create a table of the top five closeness centrality edges, we just order the data frame by the last column and table it:\n\n   c.el <- c.el[order(c.el$e.clos, decreasing = TRUE), ] %>% \n      dplyr::select(c(\"name1\", \"name2\", \"e.clos\"))\n\n   kbl(c.el[1:5, ], format = \"pipe\", align = c(\"l\", \"l\", \"c\"),\n       col.names = c(\"i\", \"j\", \"Edge Clos.\"), row.names = FALSE,\n       caption = \"Edges Sorted by Closeness in the Pulp Fiction Network\") %>% \n   kable_styling(bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\nEdges Sorted by Closeness in the Pulp Fiction Network\n \n  \n    i \n    j \n    Edge Clos. \n  \n \n\n  \n    BUTCH \n    VINCENT \n    0.900 \n  \n  \n    BRETT \n    BUTCH \n    0.875 \n  \n  \n    MOTHER \n    VINCENT \n    0.875 \n  \n  \n    BUTCH \n    MIA \n    0.864 \n  \n  \n    JULES \n    PUMPKIN \n    0.850 \n  \n\n\n\n\n\nInterestingly, the top closeness edges tend to bring somewhat strange bedfellows together, characters that themselves don’t spend much time together in the film (e.g., the Butch/Vincent interaction is relatively brief and somewhat embarrassing for Vincent) but who themselves can reach other character clusters in the film via relatively short paths."
  },
  {
    "objectID": "handout2.html#closeness-centrality-in-directed-graphs",
    "href": "handout2.html#closeness-centrality-in-directed-graphs",
    "title": "Centrality",
    "section": "Closeness Centrality in Directed Graphs",
    "text": "Closeness Centrality in Directed Graphs\nWhat about closeness centrality for a directed network? Let us see how this works using a subgraph of the advice network, this time selecting just women under the age of forty:\n\n   women <- which(V(law_advice)$gender == 2)\n   wg <- subgraph(law_advice, women)\n   young <- which(V(wg)$age < 40)\n   wg <- subgraph(wg, young)\n   V(wg)\n\n+ 12/12 vertices, from 9ce9436:\n [1]  1  2  3  4  5  6  7  8  9 10 11 12\n\n\nThis network is small enough that a plot could be informative about its structure. Let us plot it using the package ggraph, a visualization package that follows the same principles as the ggplot grammar of graphics but for network graphs (see here).\n\n   #install.packages(\"ggraph\")\n   library(ggraph)\n    p <- ggraph(wg, layout = 'auto')\n    p <- p + geom_edge_parallel(color = \"steelblue\", edge_width = 0.5,\n                                arrow = arrow(length = unit(2.5, 'mm')),\n                                end_cap = circle(4, 'mm'), \n                                sep = unit(3, 'mm'))\n    p <- p + geom_node_point(aes(x = x, y = y), size = 8, color = \"tan2\") \n    p <- p + geom_node_text(aes(label = 1:vcount(wg)), size = 4, color = \"white\")\n    p <- p + theme_graph() \n    p\n\n\n\n\nWomen lawyers advice network\n\n\n\n\nNow a question we might ask is who has the greatest closeness centrality in this advice network. We could proceed as usual and compute the geodesic distances between actors:\n\n   S <- distances(wg)\n   S\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]\n [1,]    0    1    2    1    3    3    4    2    2     3     3     3\n [2,]    1    0    2    1    2    3    3    1    1     3     3     3\n [3,]    2    2    0    1    1    1    2    2    3     1     1     1\n [4,]    1    1    1    0    2    2    3    2    2     2     2     2\n [5,]    3    2    1    2    0    1    1    1    2     2     2     2\n [6,]    3    3    1    2    1    0    2    2    3     1     2     1\n [7,]    4    3    2    3    1    2    0    2    3     3     3     3\n [8,]    2    1    2    2    1    2    2    0    1     3     3     3\n [9,]    2    1    3    2    2    3    3    1    0     4     4     4\n[10,]    3    3    1    2    2    1    3    3    4     0     1     1\n[11,]    3    3    1    2    2    2    3    3    4     1     0     1\n[12,]    3    3    1    2    2    1    3    3    4     1     1     0\n\n\nNote that this is not quite right. In igraph the default settings of the distance function treats the graph as undirected. So it doesn’t use the strict directed paths, but it just treats them all as semi-paths ignoring direction. That is why, for instance, it counts node 1 as being “adjacent” to node 4 even though there is only one incoming link from 4 to 1 and why the whole matrix is symmetric, when we know from just eyeballing the network that there is a lot of asymmetry in terms of who can reach who via directed paths.\nTo get the actual directed distance matrix, we need to specify the “mode” option, asking whether we want in or out paths. Here, let’s select out-paths:\n\n   S <- distances(wg, mode = \"out\")\n   S\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]\n [1,]    0    1  Inf  Inf  Inf  Inf  Inf  Inf  Inf   Inf   Inf   Inf\n [2,]    1    0  Inf  Inf  Inf  Inf  Inf  Inf  Inf   Inf   Inf   Inf\n [3,]    2    2    0    1  Inf    1  Inf  Inf  Inf   Inf   Inf   Inf\n [4,]    1    1    1    0  Inf    2  Inf  Inf  Inf   Inf   Inf   Inf\n [5,]    3    2    1    2    0    1    1    1    2   Inf   Inf   Inf\n [6,]    3    3    1    2  Inf    0  Inf  Inf  Inf   Inf   Inf   Inf\n [7,]    4    3    2    3    1    2    0    2    3   Inf   Inf   Inf\n [8,]    2    1  Inf  Inf  Inf  Inf  Inf    0    1   Inf   Inf   Inf\n [9,]    2    1  Inf  Inf  Inf  Inf  Inf    1    0   Inf   Inf   Inf\n[10,]    3    3    1    2  Inf    1  Inf  Inf  Inf     0     1     2\n[11,]    3    3    1    2  Inf    2  Inf  Inf  Inf     1     0     1\n[12,]    3    3    1    2  Inf    1  Inf  Inf  Inf     1     1     0\n\n\nThis is better but introduces a problem. The directed graph is not strongly connected, so it means that some nodes cannot reach other ones via a directed path of any length. That means that the geodesic distances from a node to an unreachable node is coded as “infinite” (Inf). The problem with infinity is that it gets in the way of calculating sums of distances, a requirement for the closeness centrality.\n\n   S <- distances(wg, mode = \"out\")\n   rowSums(S)\n\n [1] Inf Inf Inf Inf Inf Inf Inf Inf Inf Inf Inf Inf\n\n\nAdding infinity to a number just returns infinity so all the rows with at least one “Inf” in the distance matrix get an Inf for the row sum. In this case that’s all of them. A bummer.\n\nHarmonic Centrality\nBut dont’ worry there’s a patch. It is called the harmonic centrality (Rochat 2009).1 This is a variation on the closeness centrality that works whether you are working with connected or disconnected graphs (or in the case of directed graphs regardless of whether the graph is strongly or weakly connected), and therefore regardless of whether the geodesic distance matrix contains Infs.2\nThe main difference between the harmonic and regular closeness centrality is that instead of calculating the inverse of the sum of the distances for each node, we calculate the sum of the inverses:\n\n   S <- distances(wg, mode = \"out\")\n   S = round(1/S, 2) \n   diag(S) <- 0 #setting diagonals to zero\n   S\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]\n [1,] 0.00 1.00  0.0 0.00    0  0.0    0  0.0 0.00     0     0   0.0\n [2,] 1.00 0.00  0.0 0.00    0  0.0    0  0.0 0.00     0     0   0.0\n [3,] 0.50 0.50  0.0 1.00    0  1.0    0  0.0 0.00     0     0   0.0\n [4,] 1.00 1.00  1.0 0.00    0  0.5    0  0.0 0.00     0     0   0.0\n [5,] 0.33 0.50  1.0 0.50    0  1.0    1  1.0 0.50     0     0   0.0\n [6,] 0.33 0.33  1.0 0.50    0  0.0    0  0.0 0.00     0     0   0.0\n [7,] 0.25 0.33  0.5 0.33    1  0.5    0  0.5 0.33     0     0   0.0\n [8,] 0.50 1.00  0.0 0.00    0  0.0    0  0.0 1.00     0     0   0.0\n [9,] 0.50 1.00  0.0 0.00    0  0.0    0  1.0 0.00     0     0   0.0\n[10,] 0.33 0.33  1.0 0.50    0  1.0    0  0.0 0.00     0     1   0.5\n[11,] 0.33 0.33  1.0 0.50    0  0.5    0  0.0 0.00     1     0   1.0\n[12,] 0.33 0.33  1.0 0.50    0  1.0    0  0.0 0.00     1     1   0.0\n\n\nNote that in this matrix of inverse distances, the closest (adjacent) nodes get the maximum score of one, and nodes farther apart when smaller scores (approaching zero). More importantly, those pesky Infs disappear (!) because unreachable directed pairs of nodes get the lowest score, corresponding to \\(1/\\infty = 0\\). Turns out the mathematics of infinity weren’t our enemy after all.\nAlso note that the reachability relation expressed in this matrix is asymmetric: So node 4 and reach node 1 (there is a directed tie from 4 to 1), but node 1 cannot reach 4. This is precisely what we want.\nOnce we have this matrix of inverse distances, we can then we can compute the harmonic centrality the same way as regular closeness by adding up the row scores for each node and dividing by the number of nodes minus one (to get the average):\n\n   d.harm <- rowSums(S)\n   d.harm <- d.harm/(vcount(wg) - 1)\n   d.harm <- round(d.harm, 2)\n   d.harm\n\n [1] 0.09 0.09 0.27 0.32 0.53 0.20 0.34 0.23 0.23 0.42 0.42 0.47\n\n\nWe can see that the highest harmonic closeness centrality node is 5, followed by 12. Here’s a plot of the network highlighting the highest harmonic (closeness) centrality node.\n\n   col <- rep(\"tan2\", vcount(wg)) #creating node color vector\n   col[which(d.harm == max(d.harm))] <- \"red\" #changing color of max centrality node to red\n   p <- p + geom_node_point(aes(x = x, y = y), size = 8, color = col)\n   p <- p + geom_node_text(aes(label = 1:vcount(wg)), size = 4, color = \"white\")\n   p\n\n\n\n\nWomen lawyers advice network with highest closeness centrality node in red\n\n\n\n\nOf course, igraph has a built in function to calculate the harmonic centrality called (you guessed it) harmonic_centrality:\n\n   d.harm <- harmonic_centrality(wg, normalized = TRUE)\n   d.harm <- round(d.harm, 2)\n   d.harm\n\n [1] 0.09 0.09 0.27 0.32 0.53 0.20 0.34 0.23 0.23 0.42 0.42 0.47\n\n\nWhich gives us the same results."
  },
  {
    "objectID": "handout2.html#betweenness",
    "href": "handout2.html#betweenness",
    "title": "Centrality",
    "section": "Betweenness",
    "text": "Betweenness\nWe finally come to betweenness centrality. Recall that the key conceptual distinction between closeness and betweenness according to Freeman (1979) is that between (pun intended) the capacity to reach others quickly (e.g., via the shortest paths) and the capacity to intermediate among those same paths. High betweenness nodes control the flow of information in the network between other nodes.\nThis is evident in the way betweenness is calculated. Recall that the betweenness of a node k relative to any pair of nodes i and j in the network is simply:\n\\[\n\\frac{\\sigma_{i(k)j}}{\\sigma_{ij}}\n\\]\nWhere the denominator of the fraction (\\(\\sigma_{ij}\\)) is a count of the total number of shortest paths that start and end with nodes i and j and the numerator of the fraction (\\(\\sigma_{i(k)j}\\)) is the subset of those paths that include node k as an inner node.\nAs Freeman (1979) also notes because this is a ratio, it can range from zero to one, with everything in between. As such the betweenness centrality of a node relative to any two others has an intuitive interpretation as a probability, namely the probability that if you send something from i to j it has to go through k. This probability is 1.0 if k stands in every shortest path between i and j and zero if they stand in none of the shortest paths indirectly connecting i and j.\nThe betweenness of a given node is just the sum all of these probabilities across every pair of nodes in the graph for each node:\n\\[\n\\sum_{i \\neq j, i \\neq n, j \\neq v} \\frac{\\sigma_{i(k)j}}{\\sigma_{ij}}\n\\]\nBelow we can see a point and line diagram of the undirectd Pulp Fiction network we have been working with.\n\n\n\n\n\nPulp Fiction character schene co-appearance network.\n\n\n\n\nWe should expect a character to have high betweenness in this network to the extent that they appear in scenes with characters who themselves don’t appear in any scenes together, thus inter-mediating between different parts of the story. Characters who only appear in one scene with some others (like The Wolf or The Gimp) are likely to be low in betweenness.\nLet’s create a top ten table of betweenness for the Pulp Fiction network. We use the igraph function betweenness to calculate the scores:\n\n   pulp.bet <- betweenness(g)\n   top.5.bet <- sort(pulp.bet, decreasing = TRUE)[1:10]\n   kbl(round(top.5.bet, 2), format = \"pipe\", align = c(\"l\", \"c\"),\n       col.names = c(\"Character\", \"Betweenness\"),\n       caption = \"Top Five Betweenness Characters in Pulp Fiction Network.\") %>% \n   kable_styling(bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\nTop Five Betweenness Characters in Pulp Fiction Network.\n \n  \n    Character \n    Betweenness \n  \n \n\n  \n    BUTCH \n    275.52 \n  \n  \n    VINCENT \n    230.19 \n  \n  \n    JULES \n    142.11 \n  \n  \n    MIA \n    76.68 \n  \n  \n    MAYNARD \n    70.00 \n  \n  \n    HONEY BUNNY \n    49.97 \n  \n  \n    PUMPKIN \n    49.97 \n  \n  \n    SPORTSCASTER #1 \n    36.00 \n  \n  \n    BRETT \n    29.85 \n  \n  \n    PREACHER \n    28.23 \n  \n\n\n\n\n\nUnsurprisingly, the top four characters are also the highest in betweenness. Somewhat surprisingly, the main antagonist of the story (the pawn shop owner) is also up there. After that we see a steep drop in the bottom five of the top ten.\nNow let us examine betweenness centrality in our directed women lawyers advice network:\n\n   w.bet <- betweenness(wg)\n   w.bet\n\n [1]  0.0000000  3.0000000 16.3333333 11.0000000  7.0000000  0.0000000\n [7]  0.0000000  5.0000000  0.0000000  0.3333333  1.0000000  0.3333333\n\n\nHere we see that node 3 is the highest in betweenness, pictured below:\n\n\n\n\n\nWomen lawyers advice network with highest closeness centrality node in blue and highest betweenness centrality node in red\n\n\n\n\nThis result makes sense. Node 3 intermediates all the connections linking the tightly knit group of nodes on the left side (6, 10, 11, 12) with the rest of the network. Also if nodes 5 and 7 need to pass something along to the rest, they have to use 3 at least half time. Node 4 also needs 3 to reach 6.\nThis result nicely illustrates the difference between closeness and betweenness."
  },
  {
    "objectID": "handout2.html#edge-betweenness",
    "href": "handout2.html#edge-betweenness",
    "title": "Centrality",
    "section": "Edge Betweenness",
    "text": "Edge Betweenness\nEdge betweenness is defined in similar fashion as node betweenness:\n\\[\n\\sum_{i \\neq j} \\frac{\\sigma_{i(e)j}}{\\sigma_{ij}}\n\\]\nWhere \\(\\sigma_{i(e)j}\\) is a count of the number of shortest paths between i and j that feature edge e as an intermediary link. This tells us that the betweenness of an edge e is the sum of the ratios of the number of times that edge appears in the middle of a shortest path connecting every pair of nodes in the graph i and j divided by the total number of shortest paths linking each pair of nodes.\nLike before, the edge betweenness with respect to a specific pair of nodes in the graph is a probability: Namely, that if you send something–using a shortest path–from any node i to any other node j it has to go through edge e. The resulting edge betweenness scores is the sum of these probabilities across every possible pair of nodes for each edge in the graph.\nFor this example, we will work with a simplified version of the women lawyers advice network, in which we transform it into an undirected graph. We use the igraph function as.undirected for that:\n\n   wg <- as.undirected(wg, mode = \"collapse\")\n\nThe “collapse” value in the “mode” argument tells as.undirected to link every connected dyad in the original directed graph using an undirected edge. It does that by removing the directional arrow of the single directed links and collapsing (hence the name) all the bi-directional links into a single undirected one.\nThe resulting undirected graph looks like this:\n\n\n\n\n\nLooking at this point and line plot of the women lawyers advice network, which edge do you think has the top betweenness?\nWell no need to figure that out via eyeballing! We can just use the igraph function edge_betweenness:\n\n   w.ebet <- edge_betweenness(wg)\n\nThe edge_betweenness function takes the igraph graph object as input and produces a vector of edge betweenness values of the same length as the number of edges in the graph, which happens to be 20 in this case.\nUsing this information, we can then create a table of the top ten edges ordered by betweenness:\n\n   edges <- as_edgelist(wg) #creating an edgelist\n   etab <- data.frame(edges, bet = round(w.ebet, 2)) #adding bet. scores to edgelist\n   etab <- etab[order(etab$bet, decreasing = TRUE), ] #ordering by bet.\n   kbl(etab[1:10, ], format = \"pipe\", align = c(\"l\", \"l\", \"c\"),\n       col.names = c(\"i\", \"j\", \"Edge Bet.\"), row.names = FALSE,\n       caption = \"Edges Sorted by Betweenness in the Women Lawyers Advice Network\") %>% \n   kable_styling(bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\nEdges Sorted by Betweenness in the Women Lawyers Advice Network\n \n  \n    i \n    j \n    Edge Bet. \n  \n \n\n  \n    3 \n    4 \n    19.17 \n  \n  \n    5 \n    8 \n    15.83 \n  \n  \n    3 \n    5 \n    13.67 \n  \n  \n    5 \n    7 \n    11.00 \n  \n  \n    2 \n    4 \n    9.17 \n  \n  \n    3 \n    11 \n    8.33 \n  \n  \n    5 \n    6 \n    8.17 \n  \n  \n    1 \n    4 \n    7.00 \n  \n  \n    2 \n    8 \n    6.50 \n  \n  \n    8 \n    9 \n    6.33 \n  \n\n\n\n\n\nNot surprisingly, the top edges are the ones linking nodes 3 and 4 and nodes 5 and 8.\n\nDisconnecting a Graph Via Bridge Removal\nHigh betweenness edges are likely to function as bridges being the only point of indirect connectivity between most nodes in the social structure. That means that an easy way to disconnect a connected graph is to remove the bridges (Girvan and Newman 2002).\nIn igraph we can produce an edge deleted subgraph of an original graph using the “minus” operator, along with the edge function like this:\n\n   del.g <- wg - edge(\"3|4\")\n   del.g <- del.g - edge(\"5|8\")\n\nThe first line creates a new graph object (a subgraph) which equals the original graph minus the edge linking nodes 3 and 4. The second line takes this last subgraph and further deletes the edge linking nodes 5 and 8.\nThe resulting subgraph, minus the top two high-betweenness edges, looks like:\n\n\n\n\n\nWhich is indeed disconnected!"
  },
  {
    "objectID": "handout2.html#induced-betweenness",
    "href": "handout2.html#induced-betweenness",
    "title": "Centrality",
    "section": "Induced Betweenness",
    "text": "Induced Betweenness\nIn the assigned handbook chapter reading, Borgatti and Everett argue that another way of thinking about centrality of a node (or edge) is to calculate the difference that removing that node makes for some graph property in the network. They further suggest that the sum of the centrality scores of each node is just such a property, proposing that betweenness is particularly interesting in this regard. Let’s see how this works.\nWe will use the undirected version of the women lawyers advice network for this example. Let’s say we are interested in the difference that node 10 makes for the betweenness centralities of everyone else. In that case we would proceed as follows:\n\n   bet <- betweenness(wg) #original centrality scores\n   Sbet <- sum(bet) #sum of original centrality scores\n   wg.d <- wg - vertex(\"10\") #removing vertex 10 from the graph\n   bet.d <- betweenness(wg.d) #centrality scores of node deleted subgraph\n   Sbet.d <- sum(bet.d) #sum of centrality scores of node deleted subgraph\n   total.c <- Sbet - Sbet.d #total centrality\n   indirect.c <- total.c - bet[10] #indirect centrality\n   indirect.c\n\n[1] 12.66667\n\n\nLine 1 just calculates the regular betweenness centrality vector for the graph. Line 2 sums up all of the entries of this vector. Line 3 creates a node deleted subgraph by removing node 10. This is done using the “minus” operator and the igraph function vertex, which works just like the edge function we used earlier to create an edge deleted subgraph, except it takes a node id or name as input.\nLines 4-5 just recalculate the sum of betweenness centralities in the subgraph that excludes node 10. Then in line 6 we subtract the sum of centralities of the node deleted subgraph from the sum of centralities of the original graph. If this number, which Borgatti and Everett call the “total” centrality, is large and positive then that means that node 10 makes a difference for the centrality of others.\nHowever, part of that difference is node 10’s own “direct” centrality, so to get a more accurate sense of node 10’s impact on other people’s centrality we need to subtract node 10’s direct centrality from the total number, which we do in line 7 to get node 10’s “indirect” centrality. The result is shown in the last line, which indicates that node 10 has a pretty big impact on other people’s betweenness centralities, net of their own (which is pretty small).\nNow all we need to do is do the same for each node to create a vector of indirect betweenness centralities. So we incorporate the code above into a short loop through all vertices:\n\n   total.c <- 0 #empty vector\n   indirect.c <- 0 #empty vector\n   for (i in 1:vcount(wg)) {\n      wg.d <- wg - vertex(i)\n      bet.d <- betweenness(wg.d) #centrality scores of node deleted subgraph\n      Sbet.d <- sum(bet.d) #sum of centrality scores of node deleted subgraph\n      total.c[i] <- Sbet - Sbet.d #total centrality\n   indirect.c[i] <- total.c[i] - bet[i] #total minus direct\n   }\n\nWe can now list the total, direct, and indirect betweenness centralities for the women lawyers graph using a nice table:\n\n   i.bet <- data.frame(n = 1:vcount(wg), total.c, round(betweenness(wg), 1), round(indirect.c, 1))\n   kbl(i.bet, format = \"pipe\", align = c(\"l\", \"c\", \"c\", \"c\"),\n       col.names = c(\"Node\", \"Total\", \"Direct\", \"Indirect\"), row.names = FALSE,\n       caption = \"Induced Betweenness Scores in the Women Lawyers Advice Network\") %>% \n   kable_styling(bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\nInduced Betweenness Scores in the Women Lawyers Advice Network\n \n  \n    Node \n    Total \n    Direct \n    Indirect \n  \n \n\n  \n    1 \n    16 \n    0.0 \n    16.0 \n  \n  \n    2 \n    4 \n    6.7 \n    -2.7 \n  \n  \n    3 \n    -24 \n    23.2 \n    -47.2 \n  \n  \n    4 \n    -4 \n    12.2 \n    -16.2 \n  \n  \n    5 \n    19 \n    18.8 \n    0.2 \n  \n  \n    6 \n    10 \n    3.7 \n    6.3 \n  \n  \n    7 \n    18 \n    0.0 \n    18.0 \n  \n  \n    8 \n    4 \n    8.8 \n    -4.8 \n  \n  \n    9 \n    18 \n    0.0 \n    18.0 \n  \n  \n    10 \n    13 \n    0.3 \n    12.7 \n  \n  \n    11 \n    14 \n    0.0 \n    14.0 \n  \n  \n    12 \n    13 \n    0.3 \n    12.7 \n  \n\n\n\n\n\nThis approach to decomposing betweenness centrality provides a new way to categorize actors in a network:\n\nOn the one hand, we have actors like nodes 3 and 4 who “hog” centrality from others. Perhaps these are the prototypical high betweenness actors who monopolize the flow through the network. Their own direct centrality is high, but their indirect centrality is negative, suggesting that others become more central when they are removed from the graph as they can now become intermediaries themselves.\nIn contrast, we also have actors like node 5 who are high centrality themselves, but who’s removal from the network does not affect anyone else’s centrality. These actors are high betweenness but themselves don’t monopolize the flow of information in the network.\nThen we have actors (like nodes 9-12) who have low centrality, but whose removal from the network makes a positive difference for other people’s centrality, which overall decreases when they are removed from the network.\nFinally, we have actors line nodes 2 and 8, who are not particularly central, but who also hog centrality from others, in that removing them from the network also increases other people’s centrality (although not such an extent as the hogs)."
  },
  {
    "objectID": "handout2.html#generalized-harmonic-centrality",
    "href": "handout2.html#generalized-harmonic-centrality",
    "title": "Centrality",
    "section": "Generalized Harmonic Centrality",
    "text": "Generalized Harmonic Centrality\nAgneessens, Borgatti, and Everett (2017) propose a “generalized” version of the harmonic centrality that yields plain old degree centrality and the regular harmonic centrality as special cases. The key is to introduce a parameter \\(\\delta\\) governing how much weight we give to shortest paths based on distance. Let’s see how this works.\nRecall that the harmonic centrality we defined earlier is given by:\n\\[\n\\frac{\\sum_{j \\neq i} (g_{ij})^{-1}}{n-1}\n\\]\nFor any node \\(i\\), where \\(g_{ij}\\) is the geodesic distance between \\(i\\) and every other node in the graph \\(j\\), which could be “infinite” if there is no path linking them.\nAgneessens et al’s tweak is to instead compute:\n\\[\n\\frac{\\sum_{j \\neq i} (g_{ij})^{-\\delta}}{n-1}\n\\]\nWhere \\(\\delta\\) is a free parameter chosen by the researcher with the restriction that \\(\\delta \\geq 0\\) (if you want to calculate a closeness measure as we will see below).\nWhen \\(\\delta = \\infty\\) the numerator element \\(1/(g_{ij})^{\\infty} = 1\\) only when nodes are adjacent and \\(g_{ij} = 1\\) (because \\(1^{\\infty} = 1\\)); otherwise, for \\(g_{ij} > 1\\) then \\(1/(g_{ij})^{\\infty} = 0\\), and therefore the generalized harmonic centrality just becomes a (normalized) version of degree centrality. Alternatively, when \\(\\delta = 1\\) we just get the plain old harmonic centrality we defined earlier.\nThe interesting cases come from \\(1 > \\delta < \\infty\\) and \\(0 > \\delta < 1\\). In the first case, nodes at shorter distances are weighted more (like in the standard harmonic centrality measure) as \\(\\delta\\) becomes bigger and bigger then the generalized harmonic centrality approximates degree. For values below one, as \\(\\delta\\) approaches zero, then indirect connections to nodes of greater length are discounted less, and thus count for “more” in defining your generalized harmonic centrality score.\nLet us see a real-world example of the generalized harmonic centrality in action:\nFirst, we create a custom function to compute the generalized harmonic centrality:\n\n   g.harm <- function(x, d) {\n      library(igraph)\n      S <- distances(x) #get distances from graph object\n      S <- 1/S^d #matrix of generalized inverse distances\n      diag(S) <- 0 #set diagonals to zero\n      c <- rowSums(S)/(vcount(x) - 1) #summing and averaging\n      return(c)\n   }\n\nSecond, we compute three versions of the harmonic centrality, with \\(\\delta = 5\\), \\(\\delta = 0.05\\), and \\(\\delta = -5\\), using the full (unrestricted by age) subgraph of the law_advice network composed of the women lawyers at the firm, with relations constrained to be undirected:\n\n   women <- which(V(law_advice)$gender == 2)\n   wg <- subgraph(law_advice, women)\n   wg <- as.undirected(wg)\n   c1 <- g.harm(wg, d = 5)\n   c2 <- g.harm(wg, d = 0.05)\n   c3 <- g.harm(wg, d = -5)\n\n\nThe first version of the harmonic centrality in line 5, with a positive value of \\(\\delta\\) above zero, will compute centrality scores emphasizing direct (one-step) connections, thus coming closer to degree.\nThe second version, in line 6, with a value of \\(\\delta\\) close to zero, will give comparatively more emphasis to indirect connections weighing longer paths almost as much as shorter paths (but always a little less), thus being more similar to closeness centrality.\nFinally, the last version, in line 7, with \\(\\delta < 0\\), will weigh longer paths more than shorter ones, serving as a measure of eccentricity (farness from others) not closeness.\n\n\n\n\n\n\nFull women lawyers advice network\n\n\n\n\nAbove is a plot of the women lawyers network showing the top node for each of the centralities:\n\nIn red we have node 3 who has the largest degree (\\(k(3) = 8\\)) and thus comes out on top using the generalized harmonic centrality version emphasizing direct connections (\\(\\delta > 1\\)).\nThen in blue we have node 9 who can reach the most others via the shortest paths, and thus comes out on top when the generalized harmonic centrality emphasizes indirect connectivity.\nFinally, in purple we have node 12, which is farthest from everyone else, and thus comes out on “top” when longer indirect connections count for more (\\(\\delta < 0)\\).\n\nAs we said earlier, both regular harmonic centrality and degree are special cases of the generalized measure. We can check this by setting \\(\\delta\\) to either one or infinity.\nWhen we set \\(\\delta=1\\) the generalized harmonic centrality is the same as (normalized by number of nodes minus one) the regular harmonic centrality:\n\n   g.harm(wg, d = 1)\n\n [1] 0.5980392 0.5343137 0.7058824 0.5980392 0.6568627 0.6274510 0.4264706\n [8] 0.5392157 0.6960784 0.6470588 0.6666667 0.4068627 0.5882353 0.5196078\n[15] 0.5833333 0.6029412 0.5588235 0.5441176\n\n   harmonic_centrality(wg, normalized = TRUE)\n\n [1] 0.5980392 0.5343137 0.7058824 0.5980392 0.6568627 0.6274510 0.4264706\n [8] 0.5392157 0.6960784 0.6470588 0.6666667 0.4068627 0.5882353 0.5196078\n[15] 0.5833333 0.6029412 0.5588235 0.5441176\n\n\nWhen we set \\(\\delta=\\infty\\) the generalized harmonic centrality is the same as (normalized by number of nodes minus one) degree centrality:\n\n   g.harm(wg, d = Inf)\n\n [1] 0.23529412 0.17647059 0.47058824 0.23529412 0.35294118 0.29411765\n [7] 0.05882353 0.17647059 0.41176471 0.35294118 0.41176471 0.05882353\n[13] 0.23529412 0.17647059 0.23529412 0.35294118 0.23529412 0.23529412\n\n   degree(wg, normalized = TRUE)\n\n [1] 0.23529412 0.17647059 0.47058824 0.23529412 0.35294118 0.29411765\n [7] 0.05882353 0.17647059 0.41176471 0.35294118 0.41176471 0.05882353\n[13] 0.23529412 0.17647059 0.23529412 0.35294118 0.23529412 0.23529412"
  },
  {
    "objectID": "handout2.html#appendix-selecting-a-subgraph-based-on-edge-conditions",
    "href": "handout2.html#appendix-selecting-a-subgraph-based-on-edge-conditions",
    "title": "Centrality",
    "section": "Appendix: Selecting a Subgraph Based on Edge Conditions",
    "text": "Appendix: Selecting a Subgraph Based on Edge Conditions\nA great question asked in class goes as follows: What if I want to create a subgraph based on selecting a subset of a nodes, and then the other nodes in the graph that are that set of node’s in-neighbors?\nLet’s see how that would work.\nFirst, we create a vector with the node ids of our focal nodes, which will be women under 40 in the law_advice network.\n\n   yw <- which(V(law_advice)$gender == 2 & V(law_advice)$age < 40)\n   yw\n\n [1] 29 34 39 48 51 57 59 60 61 67 69 71\n\n\nSecond, we need to collect the node ids of the people who point to these set of nodes; that is, each of these node’s in-neighbors. For that, we use the igraph function neighbors:\n\n   yw_in <- list() #creating empty list\n   k <- 1 #counter for list position\n   for (i in yw) {\n      nei <- neighbors(law_advice, i, mode = \"in\") \n      nei <- as.vector(nei) #vector of in-neighbors ids\n      yw_in[[k]] <- nei #adding to list\n      k <- k + 1 #incrementing counter\n   }\n\nLine one creates an (empty) list object in R. The beauty of a list object is that it is an object that can hold other objects (vectors, matrices, igraph graph objects, etc.) as members (it can also have other lists as members, with lists all the way down). For a primer on how to work with R lists see here\nThe idea is to populate this initially empty list with the vectors of the in-neighbor ids of each node listed in the vector yw. Lines 2-8 do that using a simple for loop starring the igraph command neighbors, a function which takes two inputs: an igraph graph object, and a node id. The argument “mode” (“in” or “out” for directed graphs), tells it which kid of neighbors you want (not necessary for undirected graphs). Here we want the in-neighbors, so mode = “in”.\nNow we have a list object in R of length equal to the number of younger women (12 in this case) with each entry equal to the ids of those women’s in-neighbors.\n\n   length(yw_in)\n\n[1] 12\n\n   head(yw_in)\n\n[[1]]\n [1]  4 10 12 15 16 17 19 26 27 28 34 36 41 42 45 48 70\n\n[[2]]\n [1]  7 10 12 13 14 15 16 17 19 22 26 27 28 29 33 36 42 44 45 46 48 56 60 61 64\n\n[[3]]\n [1] 13 17 30 40 41 42 48 51 52 55 56 57 65 66 67 69 71\n\n[[4]]\n[1] 16 17 39 42\n\n[[5]]\n[1] 58 59\n\n[[6]]\n [1] 27 39 41 51 55 56 62 65 66 67 71\n\n\nNow we need to create a vector of the unique ids of these nodes. To do this, we just to “unlist” all of the node ids to create a simple vector from the list object.\nThe unlist native R function does that for us, taking a list as input and returning all of the elements inside each of the separate objects stored in the list as output. Here we wrap that call in the unique native R function to eliminate repeats (common in-neighbors across women):\n\n   yw_in <- unique(unlist(yw_in))\n   yw_in\n\n [1]  4 10 12 15 16 17 19 26 27 28 34 36 41 42 45 48 70  7 13 14 22 29 33 44 46\n[26] 56 60 61 64 30 40 51 52 55 57 65 66 67 69 71 39 58 59 62 35\n\n\nOf course, because the younger women are their own in-neighbors, they are included in this vector, so we need to get rid of them:\n\n   yw_in <- setdiff(yw_in, yw)\n   yw_in\n\n [1]  4 10 12 15 16 17 19 26 27 28 36 41 42 45 70  7 13 14 22 33 44 46 56 64 30\n[26] 40 52 55 65 66 58 62 35\n\n\nWe use the native command setdiff to find the elements in vector yw_in that are not contained in the vector of young women ids yw or the difference between the set of nodes ids stored in yw_in and the set of node ids stored in yw.\nNow that we have the vector of ids of the focal nodes and the vector of ids of their in-neighbors, we are ready to create our subgraph! All we need to do is specify we want both the younger law firm women and their in-neighbors in our node-induced subgraph:\n\n   g <- subgraph(law_advice, c(yw, yw_in))\n\nWe can even specify a new vertex attribute, differentiating the focal network from the in-neighbor network.\n\n   V(g)$net_status <- c(rep(1, length(yw)), rep(2, length(c(yw, yw_in)) - length(yw)))\n   vertex_attr(g)\n\n$status\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[39] 2 2 2 2 2 2 2\n\n$gender\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 1 1 2 1 1 2 1 1 1 2 1 2 2 2 1 1 1 2 1 2 2 2 1\n[39] 2 1 1 2 2 1 2\n\n$office\n [1] 1 2 1 1 1 2 3 1 1 1 1 1 1 2 1 2 2 1 2 1 1 1 1 1 3 1 2 1 2 1 1 1 1 2 2 1 1 1\n[39] 1 1 1 1 1 1 1\n\n$seniority\n [1] 31 29 25 24 22  1 21 20 23 19  9 15 13 11 10  7  8  8  8  5  6  6  5  4  5\n[26]  3  3  1  4  4  3  3  3  3  2  2  2  2  2  2  1  1  1  1  1\n\n$age\n [1] 59 63 53 52 57 56 48 46 50 46 49 41 47 38 38 39 37 36 33 43 37 34 31 31 53\n[26] 38 42 35 29 29 34 38 33 33 30 31 34 32 45 28 43 35 38 31 26\n\n$practice\n [1] 2 2 2 2 1 2 2 2 2 2 1 1 1 2 2 1 1 2 2 1 1 1 1 2 2 2 2 2 1 1 1 1 1 1 1 2 2 2\n[39] 2 1 1 1 1 2 1\n\n$law_school\n [1] 3 3 3 2 2 1 3 1 1 1 3 3 1 2 3 3 3 2 3 3 1 1 2 2 1 3 2 3 3 3 3 2 2 3 2 2 3 2\n[39] 2 3 3 2 3 2 2\n\n$net_status\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[39] 2 2 2 2 2 2 2\n\n\nFinally, we create an edge deleted subgraph including only the incoming advice edges from nodes who are not younger women in the firm to younger women and deleting everything other link:\n\n   e.rem <- E(g)[V(g)[net_status==1] %->% V(g)[net_status==1]] \n   #selecting edges from younger women to younger women\n   g.r <- delete_edges(g, e.rem) #removing edges\n   e.rem <- E(g.r)[V(g.r)[net_status==1] %->% V(g.r)[net_status==2]] \n   #selecting edges from younger women to non-younger women\n   g.r <- delete_edges(g.r, e.rem) #removing edges\n   e.rem <- E(g.r)[V(g.r)[net_status==2] %->% V(g.r)[net_status==2]] \n   #selecting edges from non-younger women to non-younger women\n   g.r <- delete_edges(g.r, e.rem) #removing edges\n   Iso = which(degree(g.r)==0) #selecting isolates\n   g.r <- delete_vertices(g.r, Iso) #removing isolates\n\nHere we can see both the delete_edges and delete_vertices functions from igraph in action. Both take some graph object as input followed by either an edge sequence (in this case produced by E(g))or a vector of node ids respectively. In both cases those particular edges or nodes are removed from the graph.\nThe other neat functionality we see on display here is the igraph %->% operator for directed graph edges (the equivalent for undirected graphs is the double dash %–%). This allows us to select a set of edges according to a vertex condition (e.g., homophilous (same group) edges or edges that link a member from group a to a member from group b).\nSo the code chunk:\n\n   E(g)[V(g)[net_status==1] %->% V(g)[net_status==1]] \n\n+ 69/436 edges from 9f7341a:\n [1]  1-> 4  1-> 5  1-> 6  1-> 9  1->10  1->11  1->12  2-> 3  3-> 8  3-> 9\n[11]  3->12  4-> 1  4-> 3  4-> 5  4-> 6  4-> 7  4-> 8  4-> 9  4->10  4->12\n[21]  5-> 4  5-> 7  5-> 8  5-> 9  5->12  6-> 1  6-> 4  6-> 7  6-> 8  6-> 9\n[31]  7-> 5  7-> 6  7-> 8  7->11  7->12  8-> 1  8-> 2  8-> 3  8-> 4  8-> 6\n[41]  8-> 7  8-> 9  8->10  8->12  9-> 1  9-> 4  9-> 6  9-> 8  9->11  9->12\n[51] 10-> 1 10-> 4 10-> 5 10-> 7 10-> 9 10->11 10->12 11-> 1 11-> 7 11-> 8\n[61] 11-> 9 11->10 12-> 1 12-> 4 12-> 5 12-> 7 12-> 8 12-> 9 12->11\n\n\nTakes the edge set of the graph g (E(g)) and gives us the subset of edges that go from a vertex with net_status equal to one to another vertex that also has net_status equal to one (in this case edges directed from one of our focal nodes to another one of our focal nodes). This, of course, happens to be all the directed edges linking nodes one through twelve in the network. The same go for the other calls to the same function using different logical combinations of values of net_status between nodes.\nFINALLY, can now plot the incoming advice network to younger women (in red):\n\n\n\n\n\nWomen lawyers advice network showing incoming links from outside the group (blue nodes) to younger women (red nodes)"
  },
  {
    "objectID": "handout2.html#note-handling-graph-objects-in-lists",
    "href": "handout2.html#note-handling-graph-objects-in-lists",
    "title": "Centrality",
    "section": "Note: Handling Graph Objects in Lists",
    "text": "Note: Handling Graph Objects in Lists\nSpeaking of lists, sometimes network data comes pre-stored as an R list. This is typical if you have a network with multiple kinds of ties recorded on the same set of actors (and thus multiple networks), or longitudinal network data, where we collect multiple “snapshots” of the same system (containing the same or more typically a different set of actors per time slice).\nThe networkdata package contains one such data set called atp. It’s a network of Tennis players who played in grand slam or official matches of the Association of Tennis Professionals (hence ATP) covering the years 1968-2021 (Radicchi 2011).\nIn the directed graph representing each network, a tie goes from the loser to the winner of each match. Accordingly, it can be interpreted as a directed “deference” network (it would be a dominance network if it was the other way around), where actor i “defers” to actor j by getting their ass kicked by them.\nLet’s see how this list of networks works:\n\n   g <- atp\n   head(g)\n\n[[1]]\nIGRAPH 08a202a DNW- 497 1213 -- ATP Season 1968\n+ attr: name (g/c), name (v/c), age (v/n), hand (v/c), country (v/c),\n| surface (e/c), weight (e/n)\n+ edges from 08a202a (vertex names):\n [1] U Unknown    ->Jose Mandarino     Alfredo Acuna->Alan Fox          \n [3] Andres Gimeno->Ken Rosewall       Andres Gimeno->Raymond Moore     \n [5] Juan Gisbert ->Tom Okker          Juan Gisbert ->Zeljko Franulovic \n [7] Onny Parun   ->Jan Kodes          Peter Curtis ->Tom Okker         \n [9] Premjit Lall ->Clark Graebner     Rod Laver    ->Ken Rosewall      \n[11] Thomas Lejus ->Nicola Pietrangeli Tom Okker    ->Arthur Ashe       \n[13] U Unknown    ->Jaidip Mukherjea   U Unknown    ->Jose Luis Arillla \n+ ... omitted several edges\n\n[[2]]\nIGRAPH c895c57 DNW- 446 1418 -- ATP Season 1969\n+ attr: name (g/c), name (v/c), age (v/n), hand (v/c), country (v/c),\n| surface (e/c), weight (e/n)\n+ edges from c895c57 (vertex names):\n [1] U Unknown           ->U Unknown        \n [2] Alejandro Olmedo    ->Ron Holmberg     \n [3] Arthur Ashe         ->Rod Laver        \n [4] Bob Carmichael      ->Jim Osborne      \n [5] Cliff Richey        ->Zeljko Franulovic\n [6] Francois Jauffret   ->Martin Mulligan  \n [7] Fred Stolle         ->John Newcombe    \n+ ... omitted several edges\n\n[[3]]\nIGRAPH d6709af DNW- 451 1650 -- ATP Season 1970\n+ attr: name (g/c), name (v/c), age (v/n), hand (v/c), country (v/c),\n| surface (e/c), weight (e/n)\n+ edges from d6709af (vertex names):\n [1] Mark Cox            ->Jan Kodes        \n [2] Charlie Pasarell    ->Stan Smith       \n [3] Cliff Richey        ->Arthur Ashe      \n [4] Francois Jauffret   ->Manuel Orantes   \n [5] Georges Goven       ->Jan Kodes        \n [6] Harald Elschenbroich->Zeljko Franulovic\n [7] Ilie Nastase        ->Zeljko Franulovic\n+ ... omitted several edges\n\n[[4]]\nIGRAPH a73a020 DNW- 459 2580 -- ATP Season 1971\n+ attr: name (g/c), name (v/c), age (v/n), hand (v/c), country (v/c),\n| surface (e/c), weight (e/n)\n+ edges from a73a020 (vertex names):\n [1] Andres Gimeno    ->Ken Rosewall   Arthur Ashe      ->Rod Laver     \n [3] Charlie Pasarell ->Cliff Drysdale Frank Froehling  ->Clark Graebner\n [5] Joaquin Loyo Mayo->Thomaz Koch    John Alexander   ->John Newcombe \n [7] John Newcombe    ->Marty Riessen  Nikola Pilic     ->Cliff Drysdale\n [9] Owen Davidson    ->Cliff Drysdale Robert Maud      ->Cliff Drysdale\n[11] Roger Taylor     ->Marty Riessen  Roy Emerson      ->Rod Laver     \n[13] Tom Okker        ->John Newcombe  Allan Stone      ->Bob Carmichael\n+ ... omitted several edges\n\n[[5]]\nIGRAPH 761ed05 DNW- 504 2767 -- ATP Season 1972\n+ attr: name (g/c), name (v/c), age (v/n), hand (v/c), country (v/c),\n| surface (e/c), weight (e/n)\n+ edges from 761ed05 (vertex names):\n [1] Jean Loup Rouyer->Stan Smith     Marty Riessen   ->Cliff Drysdale\n [3] Roy Emerson     ->Arthur Ashe    Roy Emerson     ->Arthur Ashe   \n [5] Tom Leonard     ->John Newcombe  Tom Okker       ->Arthur Ashe   \n [7] Tom Okker       ->Rod Laver      Adriano Panatta ->Andres Gimeno \n [9] Adriano Panatta ->Ilie Nastase   Allan Stone     ->John Alexander\n[11] Allan Stone     ->Marty Riessen  Andres Gimeno   ->Jan Kodes     \n[13] Andres Gimeno   ->Stan Smith     Andrew Pattison ->Ilie Nastase  \n+ ... omitted several edges\n\n[[6]]\nIGRAPH 92ff576 DNW- 592 3653 -- ATP Season 1973\n+ attr: name (g/c), name (v/c), age (v/n), hand (v/c), country (v/c),\n| surface (e/c), weight (e/n)\n+ edges from 92ff576 (vertex names):\n [1] Harold Solomon    ->Stan Smith       John Alexander    ->Stan Smith      \n [3] Patrice Dominguez ->Paolo Bertolucci Paul Gerken       ->Jimmy Connors   \n [5] Roy Emerson       ->Rod Laver        Adriano Panatta   ->Ilie Nastase    \n [7] Bjorn Borg        ->Adriano Panatta  Brian Gottfried   ->Cliff Richey    \n [9] Charlie Pasarell  ->John Alexander   Cliff Richey      ->Stan Smith      \n[11] Corrado Barazzutti->Bjorn Borg       Francois Jauffret ->Ilie Nastase    \n[13] Georges Goven     ->Manuel Orantes   Manuel Orantes    ->Ilie Nastase    \n+ ... omitted several edges\n\n\nWe create a graph object and then examine its contents, which we can see is a set of graph objects. In unnamed R lists each of the objects inside is indexed by a number in double brackets. So [[6]] just means the sixth network in the list object (corresponding to the year 1973).\nNow let’s say we wanted to compute a network statistic like density. One way to proceed would be:\n\n   edge_density(g)\n\nError in `ensure_igraph()`:\n! Must provide a graph object (provided wrong object type).\n\n\nWhich gives us a weird error about the wrong object type. The reason is that edge_density expects an igraph graph object as input, but g is not a graph object it is a list of such objects. For it to work you have to reference a particular element inside the list not the whole list.\nTo do that, we use the double bracket notation:\n\n   edge_density(g[[6]])\n\n[1] 0.01044096\n\n\nWhich gives us the density for the 1973 network.\n\nLooping Through Lists\nBut what if we wanted a table of network statistics for all the years or some subset of years? Of course, we could just type a million versions of the edge_density command or whatever, but that would be tedious. We could also write a for loop or something like that (less tedious). Even less tedious is to use the many apply functions in R that are designed to work with lists, which is a subject onto itself in R programming.\nBut here we can just use the simple version. Let’s say we wanted a vector of densities (or any other whole network statistic) for the whole 54 years. In that case, our friend sapply can do the job:\n\n   sapply(g, edge_density)\n\n [1] 0.004920653 0.007144657 0.008130081 0.012272740 0.010914671 0.010440961\n [7] 0.010567864 0.013315132 0.012088214 0.014019237 0.014135328 0.011649909\n[13] 0.011172821 0.011261426 0.012703925 0.012177336 0.012648755 0.012445937\n[19] 0.012034362 0.012351377 0.010174271 0.009772014 0.019526953 0.012236462\n[25] 0.014050245 0.015054181 0.013872832 0.014727924 0.014329906 0.013935502\n[31] 0.013962809 0.013870042 0.013665097 0.013818887 0.012551113 0.011571679\n[37] 0.012329090 0.012923683 0.011402945 0.012677988 0.012256963 0.013512884\n[43] 0.012543025 0.013661748 0.013786518 0.013679697 0.015052857 0.015075622\n[49] 0.015081206 0.014346468 0.015764351 0.020169225 0.011889114 0.016935400\n\n\nsapply is kind of a “meta” function that takes two inputs: A list, and the name of a function (which could be native, a package, or user defined); sapply then “applies” that function to each element inside the list. Here we asked R to apply the function edge_density to each element of the list of networks g and it obliged, creating a vector of length 54 containing the info.\nWe could use any igraph function, like number of nodes in the graph:\n\n   sapply(g, vcount)\n\n [1] 497 446 451 459 504 592 595 535 553 524 509 572 582 573 554 532 495 513 510\n[20] 523 596 597 405 542 509 498 520 496 502 499 497 480 486 479 497 517 505 492\n[39] 524 488 493 464 482 459 457 453 428 430 431 438 419 364 345 393\n\n\nWe could also select subset of elements inside the list. For instance this counts the number of nodes for the first five years:\n\n   sapply(g[1:5], vcount)\n\n[1] 497 446 451 459 504\n\n\nOr for years 2, 6, 8, and 12:\n\n   sapply(g[c(2, 6, 8, 12)], vcount)\n\n[1] 446 592 535 572\n\n\nNote the single bracket notation here to refer to subsets of elements in the list. Inside the brackets we could put any arbitrary vector, as long as the numbers in the vector do no exceed the length of the list.\nOf course, sometimes the functions we apply to elements of the list don’t return single numbers but vectors or other igraph objects. In that case it would be better to use lapply which is just like sapply but returns another list with the set of answers inside it.\nFor instance, let’s say we wanted the top five players for each year. In this deference network, a “top” player is one who beats many others, which means they have high indegree (lots of losers pointing at them).\nFirst we create a custom function to compute the indegree and return an ordered named vector of top 5 players:\n\n   top5 <- function(x) {\n      library(igraph)\n      t <- degree(x, mode = \"in\")\n      t <- sort(t, decreasing = TRUE)[1:5]\n      return(t)\n   }\n\nNow, we can just feed that function to lapply:\n\n   top.list <- lapply(g, top5)\n   head(top.list)\n\n[[1]]\n   Arthur Ashe      Rod Laver Clark Graebner   Ken Rosewall      Tom Okker \n            33             27             25             23             22 \n\n[[2]]\nJohn Newcombe     Tom Okker     Rod Laver    Tony Roche   Arthur Ashe \n           45            41            40            40            33 \n\n[[3]]\n      Arthur Ashe      Cliff Richey         Rod Laver        Stan Smith \n               51                49                48                45 \nZeljko Franulovic \n               42 \n\n[[4]]\n     Ilie Nastase         Tom Okker     Marty Riessen        Stan Smith \n               69                63                61                61 \nZeljko Franulovic \n               60 \n\n[[5]]\n  Ilie Nastase     Stan Smith Manuel Orantes  Jimmy Connors    Arthur Ashe \n            99             72             68             65             55 \n\n[[6]]\n Ilie Nastase     Tom Okker Jimmy Connors   Arthur Ashe    Stan Smith \n           96            81            68            63            63 \n\n\nWhich is a list of named vectors containing the number of victories of the top five players each year.\nBecause the object top.list is just a list, we can subset it just like before. Let’s say we wanted to see the top players for more recent years:\n\n   top.list[49:54]\n\n[[1]]\n   Andy Murray  Dominic Thiem  Kei Nishikori Novak Djokovic   David Goffin \n            63             55             53             50             47 \n\n[[2]]\n         Rafael Nadal          David Goffin      Alexander Zverev \n                   58                    55                    52 \nRoberto Bautista Agut         Dominic Thiem \n                   45                    43 \n\n[[3]]\n   Dominic Thiem Alexander Zverev   Novak Djokovic    Fabio Fognini \n              51               50               46               45 \n   Roger Federer \n              44 \n\n[[4]]\n   Daniil Medvedev     Novak Djokovic       Rafael Nadal Stefanos Tsitsipas \n                55                 52                 52                 49 \n     Roger Federer \n                47 \n\n[[5]]\n     Andrey Rublev     Novak Djokovic Stefanos Tsitsipas       Rafael Nadal \n                40                 36                 27                 26 \n   Daniil Medvedev \n                24 \n\n[[6]]\n   Daniil Medvedev Stefanos Tsitsipas        Casper Ruud   Alexander Zverev \n                54                 52                 52                 51 \n    Novak Djokovic \n                49 \n\n\nA series of names which make sense to you if you follow Tennis.\n\n\nNaming Lists\nFinally, sometimes it useful to name the elements of a list. In this case, for instance, having the year number would be easier to remember what’s what. For this, you can use the names command, which works via standard R assignment:\n\n   names(g) <- c(1968:2021)\n   head(g)\n\n$`1968`\nIGRAPH 08a202a DNW- 497 1213 -- ATP Season 1968\n+ attr: name (g/c), name (v/c), age (v/n), hand (v/c), country (v/c),\n| surface (e/c), weight (e/n)\n+ edges from 08a202a (vertex names):\n [1] U Unknown    ->Jose Mandarino     Alfredo Acuna->Alan Fox          \n [3] Andres Gimeno->Ken Rosewall       Andres Gimeno->Raymond Moore     \n [5] Juan Gisbert ->Tom Okker          Juan Gisbert ->Zeljko Franulovic \n [7] Onny Parun   ->Jan Kodes          Peter Curtis ->Tom Okker         \n [9] Premjit Lall ->Clark Graebner     Rod Laver    ->Ken Rosewall      \n[11] Thomas Lejus ->Nicola Pietrangeli Tom Okker    ->Arthur Ashe       \n[13] U Unknown    ->Jaidip Mukherjea   U Unknown    ->Jose Luis Arillla \n+ ... omitted several edges\n\n$`1969`\nIGRAPH c895c57 DNW- 446 1418 -- ATP Season 1969\n+ attr: name (g/c), name (v/c), age (v/n), hand (v/c), country (v/c),\n| surface (e/c), weight (e/n)\n+ edges from c895c57 (vertex names):\n [1] U Unknown           ->U Unknown        \n [2] Alejandro Olmedo    ->Ron Holmberg     \n [3] Arthur Ashe         ->Rod Laver        \n [4] Bob Carmichael      ->Jim Osborne      \n [5] Cliff Richey        ->Zeljko Franulovic\n [6] Francois Jauffret   ->Martin Mulligan  \n [7] Fred Stolle         ->John Newcombe    \n+ ... omitted several edges\n\n$`1970`\nIGRAPH d6709af DNW- 451 1650 -- ATP Season 1970\n+ attr: name (g/c), name (v/c), age (v/n), hand (v/c), country (v/c),\n| surface (e/c), weight (e/n)\n+ edges from d6709af (vertex names):\n [1] Mark Cox            ->Jan Kodes        \n [2] Charlie Pasarell    ->Stan Smith       \n [3] Cliff Richey        ->Arthur Ashe      \n [4] Francois Jauffret   ->Manuel Orantes   \n [5] Georges Goven       ->Jan Kodes        \n [6] Harald Elschenbroich->Zeljko Franulovic\n [7] Ilie Nastase        ->Zeljko Franulovic\n+ ... omitted several edges\n\n$`1971`\nIGRAPH a73a020 DNW- 459 2580 -- ATP Season 1971\n+ attr: name (g/c), name (v/c), age (v/n), hand (v/c), country (v/c),\n| surface (e/c), weight (e/n)\n+ edges from a73a020 (vertex names):\n [1] Andres Gimeno    ->Ken Rosewall   Arthur Ashe      ->Rod Laver     \n [3] Charlie Pasarell ->Cliff Drysdale Frank Froehling  ->Clark Graebner\n [5] Joaquin Loyo Mayo->Thomaz Koch    John Alexander   ->John Newcombe \n [7] John Newcombe    ->Marty Riessen  Nikola Pilic     ->Cliff Drysdale\n [9] Owen Davidson    ->Cliff Drysdale Robert Maud      ->Cliff Drysdale\n[11] Roger Taylor     ->Marty Riessen  Roy Emerson      ->Rod Laver     \n[13] Tom Okker        ->John Newcombe  Allan Stone      ->Bob Carmichael\n+ ... omitted several edges\n\n$`1972`\nIGRAPH 761ed05 DNW- 504 2767 -- ATP Season 1972\n+ attr: name (g/c), name (v/c), age (v/n), hand (v/c), country (v/c),\n| surface (e/c), weight (e/n)\n+ edges from 761ed05 (vertex names):\n [1] Jean Loup Rouyer->Stan Smith     Marty Riessen   ->Cliff Drysdale\n [3] Roy Emerson     ->Arthur Ashe    Roy Emerson     ->Arthur Ashe   \n [5] Tom Leonard     ->John Newcombe  Tom Okker       ->Arthur Ashe   \n [7] Tom Okker       ->Rod Laver      Adriano Panatta ->Andres Gimeno \n [9] Adriano Panatta ->Ilie Nastase   Allan Stone     ->John Alexander\n[11] Allan Stone     ->Marty Riessen  Andres Gimeno   ->Jan Kodes     \n[13] Andres Gimeno   ->Stan Smith     Andrew Pattison ->Ilie Nastase  \n+ ... omitted several edges\n\n$`1973`\nIGRAPH 92ff576 DNW- 592 3653 -- ATP Season 1973\n+ attr: name (g/c), name (v/c), age (v/n), hand (v/c), country (v/c),\n| surface (e/c), weight (e/n)\n+ edges from 92ff576 (vertex names):\n [1] Harold Solomon    ->Stan Smith       John Alexander    ->Stan Smith      \n [3] Patrice Dominguez ->Paolo Bertolucci Paul Gerken       ->Jimmy Connors   \n [5] Roy Emerson       ->Rod Laver        Adriano Panatta   ->Ilie Nastase    \n [7] Bjorn Borg        ->Adriano Panatta  Brian Gottfried   ->Cliff Richey    \n [9] Charlie Pasarell  ->John Alexander   Cliff Richey      ->Stan Smith      \n[11] Corrado Barazzutti->Bjorn Borg       Francois Jauffret ->Ilie Nastase    \n[13] Georges Goven     ->Manuel Orantes   Manuel Orantes    ->Ilie Nastase    \n+ ... omitted several edges\n\n\nNow instead of the useless one, two, three, etc. names, we have the actual year numbers as the names of the elements on each list.\nSo if we wanted to know the top five players for 1988 we could just type:\n\n   top5(g[[\"1988\"]])\n\n   Stefan Edberg     Andre Agassi     Boris Becker    Mats Wilander \n              63               59               52               49 \nAaron Krickstein \n              48 \n\n\nNote the double bracket notation and the fact that the name of the list is a character not a number (hence the scare quotes).\nIf we don’t want to remember the bracket business, we could also use the $ operator to refer to particular list elements:\n\n   top5(g$\"1988\")\n\n   Stefan Edberg     Andre Agassi     Boris Becker    Mats Wilander \n              63               59               52               49 \nAaron Krickstein \n              48 \n\n\nOf course, we can also use the names to subset the list. Let’s say we wanted the top five players for 1970, 1980, 1990, 2000, 2010, and 2020.\nAll we have to do is type:\n\n   decades <- c(\"1970\", \"1980\", \"1990\", \"2000\", \"2010\", \"2020\")\n   lapply(g[decades], top5)\n\n$`1970`\n      Arthur Ashe      Cliff Richey         Rod Laver        Stan Smith \n               51                49                48                45 \nZeljko Franulovic \n               42 \n\n$`1980`\n     Ivan Lendl    John Mcenroe Brian Gottfried      Bjorn Borg Eliot Teltscher \n             97              76              63              62              62 \n\n$`1990`\n  Boris Becker  Stefan Edberg     Ivan Lendl   Pete Sampras Emilio Sanchez \n            62             57             50             47             44 \n\n$`2000`\nYevgeny Kafelnikov        Marat Safin    Gustavo Kuerten      Magnus Norman \n                63                 61                 59                 58 \n    Lleyton Hewitt \n                53 \n\n$`2010`\n   Rafael Nadal   Roger Federer    David Ferrer Robin Soderling   Jurgen Melzer \n             63              54              53              53              51 \n\n$`2020`\n     Andrey Rublev     Novak Djokovic Stefanos Tsitsipas       Rafael Nadal \n                40                 36                 27                 26 \n   Daniil Medvedev \n                24 \n\n\nNote that we are back to the single bracket notation.\nWith a bit of practice, lists will become your friends!"
  },
  {
    "objectID": "handout3.html",
    "href": "handout3.html",
    "title": "Status and Prestige",
    "section": "",
    "text": "In the last handout, we saw how to compute the most popular centrality measures. Freeman’s “big three” have strong graph-theoretic foundation and do a good job of formalizing and quantifying the idea that a node is central if it is “well-placed” in the network, where being well-placed resolves into either being able to reach others (directly as with degree or indirectly as with closeness) or being able to intermediate between others (as with betweenness)."
  },
  {
    "objectID": "handout3.html#networks-as-prisms",
    "href": "handout3.html#networks-as-prisms",
    "title": "Status and Prestige",
    "section": "Networks as Prisms",
    "text": "Networks as Prisms\nThere is, however, another strong and well-motivated intuition as to what it means to be “well-placed” in a network. Here the ties in the network are seen less as “pipes” that transmit stuff and more like “prisms” that reflect on you (Podolny 2001).\nOne way to think about this second version of well-placedness is that what is transmitted through the network is the network itself, or more accurately, the importance, status, and prestige of the people you are connected to, preferably flowing from them (high status people) to you.\nUnder this interpretation, actors get status and prestige in the network from being connected to prestigious and high status others. Those others, in turn, get their status from being connected to high status others, and so ad infinitum.\nOne way of quantifying this idea goes like this. If \\(\\mathbf{x}\\) is a vector containing the desired status scores, then the status of actor \\(i\\) should be equal to:\n\\[\n   x_i = \\sum_{j} a_{ij}x_j\n\\tag{1}\\]\nWhere \\(a_{ij} = 1\\) if \\(i\\) is adjacent to \\(j\\) in the network. Note that this formula just sums up the status scores of all the others each actor is connected to.\nIn matrix notation, if \\(\\mathbf{x}\\) is a column vector of status scores then:\n\\[\n   \\mathbf{x} = A\\mathbf{x}\n\\]\nBecause \\(\\mathbf{x}\\) is an \\(n \\times n\\) matrix and \\(\\mathbf{x}\\) is \\(n \\times 1\\) column vector, the multiplication \\(A\\mathbf{x}\\) will return another column vector of dimensions \\(n \\times 1\\), in this case \\(\\mathbf{x}\\) itself!\nNote the problem that this formulation poses: \\(\\mathbf{x}\\) appears on both sides of the equation, which means that in order to know the status of any one node we would need to know the status of the others, but calculating the status of the others depends on knowing the status of the focal node, and so on. There’s a chicken and the egg problem here.\nNow, there is an obvious (to the math majors) mathematical solution to this problem, because there’s a class of solvable (under some mild conditions imposed on the matrix \\(\\mathbf{A}\\)) linear algebra problems that take the form:\n\\[\n   \\lambda\\mathbf{x} = A\\mathbf{x}\n\\]\nWhere \\(\\lambda\\) is just a plain old number (a scalar). Once again conditional of the aforementioned mild conditions being met, we can iteratively search for a value \\(\\lambda\\), fix it, then fill up the \\(\\mathbf{x}\\) vector with another set of values, fix those, search for a new \\(\\lambda\\), and continue until we have values of \\(\\lambda\\) and \\(\\mathbf{x}\\) that make the above equality true.\nWhen we do that successfully, we say that the value of \\(\\lambda\\) we hit upon is an eigenvalue of the matrix \\(\\mathbf{A}\\) and the values of the vector \\(\\mathbf{x}\\) we came up with are an eigenvector of the same matrix (technically in the above equation a right eigenvector).\nEigenvalues and eigenvectors, like Don Quixote and Sancho Panza, come in pairs, because you need a unique combination of both to solve the equation. Typically, a given matrix (like an adjacency matrix) will have multiple \\(\\lambda/\\mathbf{x}\\) pairs that will solve the equation. Together the whole set \\(\\lambda/\\mathbf{x}\\) pairs that make the equation true are the eigenvalues and eigenvectors of the matrix."
  },
  {
    "objectID": "handout3.html#distributing-centrality-to-others",
    "href": "handout3.html#distributing-centrality-to-others",
    "title": "Status and Prestige",
    "section": "Distributing Centrality to Others",
    "text": "Distributing Centrality to Others\nLet’s start with the simplest model of how people can get their status from the status of others in a network. It is the simplest because it is based on degree.\nImagine everyone has the same “quantum” of status to begin with (this can be stored in a vector containing the same number of length equals to number of actors in the network). Then, at each step, people “send” the same amount of status to all their alters in the network. At the end of each step, we compute people’s status scores using Equation 1. We stop doing this after the status scores of people stop changing across each iteration.\nLet us see a real-life example at work.\nWe will use a data set collected by David Krackhardt on the friendships of 21 managers in a high tech company in the West coast (see the description here). The data are reported as directed ties (\\(i\\) nominates \\(j\\) as a friend) but we will constrain ties to be undirected:\n\n   library(networkdata)\n   library(igraph)\n   g <- as.undirected(ht_friends, mode = \"collapse\")\n\nThis is what the network looks like:\n\n\n\n\n\nKrackhardt’s Manager Data.\n\n\n\n\nWe extract the adjacency matrix corresponding to this network:\n\n   A <- as.matrix(as_adjacency_matrix(g))\n\nAnd here’s a simple custom function using a while loop that exemplifies the process of status distribution through the network we talked about earlier:\n\n   status1 <- function(A) {\n      n <- nrow(A) #number of actors\n      x <- rep(1, n) #initial status vector set to all ones\n      w <- 1 \n      k <- 0 #initializing counter\n      while (w > 0.0001) {\n          o.x <- x #old status scores\n          x <- A %*% x #new scores a function of old scores and adjacency matrix\n          x <- x/norm(x, type = \"E\") #normalizing new status scores\n          w <- abs(sum(abs(x) - abs(o.x))) #diff. between new and old scores\n          k <- k + 1 #incrementing while counter\n      }\n   return(as.vector(x))\n   }\n\nLines 1-5 initialize various quantities, most importantly the initial status vector for each node to just a series of ones:\n\n   rep(1, nrow(A))\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n\nThen lines 6-12 implement a little loop of how status is distributed through the network, with the most important piece of code being line 8 where the current status scores for each node are just the sum of the status scores of its neighbors computed one iteration earlier. The program stops when the difference between the old and the new scores is negligible (\\(\\delta < 0.0001\\)) as checked in line 10.\nNote the normalization step on line 9, which is necessary to prevent the sum of status scores from getting bigger and bigger indefinitely (in mathese, this is referred to as the sum “diverging”). In base R, the type = \"E\" normalization implements the euclidean vector norm (also sometimes confusingly called the Frobenieus norm), by which we divide each value of the status scores by after each update.1\nAnd here’s the resulting (row) vector of status scores for each node:\n\n   s <- status1(A)\n   s <- s/max(s) #normalizing by maximum\n   round(s, 3)\n\n [1] 0.619 0.635 0.446 0.489 0.629 0.430 0.205 0.380 0.444 0.468 0.814 0.549\n[13] 0.162 0.401 0.613 0.360 1.000 0.247 0.680 0.360 0.392\n\n\nWhat if I told you that this vector is the same as that given by the leading (first) eigenvector of the adjacency matrix?\n\n   s <- abs(eigen(A)$vector[, 1])#computing the first eigenvector\n   s <- s/max(s) #normalizing by maximum\n   round(s, 3)\n\n [1] 0.619 0.635 0.446 0.489 0.629 0.430 0.205 0.380 0.444 0.468 0.814 0.549\n[13] 0.162 0.401 0.613 0.360 1.000 0.247 0.680 0.360 0.392\n\n\nWhich is of course what is computed by the eigen_centrality function in igraph:\n\n   round(eigen_centrality(g)$vector, 3) #igraph automatically normalizes the scores\n\n [1] 0.619 0.635 0.446 0.489 0.629 0.430 0.205 0.380 0.444 0.468 0.814 0.549\n[13] 0.162 0.401 0.613 0.360 1.000 0.247 0.680 0.360 0.392\n\n\nSo, the “eigenvector centralities” are just the limit scores produced by the status distribution process implemented in the status1 function!\nWhen treated as a structural index of connectivity in a graph (aka a centrality measure) the eigenvector status scores induce an ordering of the nodes which we may be interested in looking at:\n\n   nodes <- 1:vcount(g)\n   eig.cent <- round(status1(A), 3)\n   eig.dat <- data.frame(Nodes = nodes, Eigen.Cent = eig.cent, Deg.Cent = degree(g))\n   eig.dat <- eig.dat[order(eig.dat$Eigen.Cent, decreasing = TRUE), ]\n   library(kableExtra)\n   kbl(eig.dat[1:10, ], \n       format = \"html\", align = \"c\", row.names = FALSE,\n       caption = \"Top Ten Eigenvector Scores.\") %>%    kable_styling(bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\nTop Ten Eigenvector Scores.\n \n  \n    Nodes \n    Eigen.Cent \n    Deg.Cent \n  \n \n\n  \n    17 \n    0.413 \n    18 \n  \n  \n    11 \n    0.336 \n    14 \n  \n  \n    19 \n    0.281 \n    10 \n  \n  \n    2 \n    0.262 \n    10 \n  \n  \n    5 \n    0.260 \n    10 \n  \n  \n    1 \n    0.256 \n    9 \n  \n  \n    15 \n    0.253 \n    9 \n  \n  \n    12 \n    0.227 \n    8 \n  \n  \n    4 \n    0.202 \n    7 \n  \n  \n    10 \n    0.193 \n    8 \n  \n\n\n\n\n\nMost other measures of status in networks are constructed using similar principles. What changes is the model of how status is distributed in the system. That’s why scary and non-intuitive stuff about eigenvectors or whatever is misleading.\nOther measures are designed such that they either change the quantum of status that is distributed through the network by making it dependent on some node characteristic (like degree) or differentiate between different routes of distribution in directed graphs, by for instance, differentiating status derived from outgoing links from that derived from incoming links.\nLet’s see some examples of these alternative cases."
  },
  {
    "objectID": "handout3.html#a-degree-dependent-model-of-status-aka-pagerank",
    "href": "handout3.html#a-degree-dependent-model-of-status-aka-pagerank",
    "title": "Status and Prestige",
    "section": "A Degree-Dependent Model of Status (AKA PageRank)",
    "text": "A Degree-Dependent Model of Status (AKA PageRank)\nNote that the model of status distribution implied by the Bonacich’s Eigenvector Centrality just reviewed implies that each node distributes the same amount of status independently of the number of connection it has; status just replicates indefinitely. Thus, a node with a 100 friends has 100 status units to distribute to each of them and a node with a 10 friends has 10 units.\nThis is why the eigenvector idea rewards nodes who are connected to popular others more. Even though everyone begins with a single unit of status, well-connected nodes by degree end up having more of it to distribute.\nBut what if status dissipated proportionately to the number of connections one had? For instance, if someone has 100 friends and they only had so much time or energy, they would only have a fraction of status to distribute to others than a person with 10 friends.\nIn that case, the node with a hundred friend would only have 1/100 of status unites to distribute to each of their connections while the node with 10 friends would have 1/10 units. Under this formulation, being connected to discerning others, that is people who only connect to a few, is better than being connected to others who connect to everyone else indiscriminately.\nHow would we implement this model? First, let’s create a variation of the undirected friendship nomination adjacency matrix called the \\(\\mathbf{P}\\) matrix:\n\n   g <- as.undirected(ht_friends, mode = \"collapse\")\n   A <- as.matrix(as_adjacency_matrix(g))\n   P <- A/rowSums(A)\n\nSo this is the original adjacency matrix, with each entry \\(a_{ij}\\) divided by the sum of the corresponding row, which, as you may recall, is equivalent to the degree of node \\(i\\).\nHere are the first 10 rows and columns of the new matrix:\n\n   round(P[1:10, 1:10], 2)\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n [1,] 0.00 0.11 0.00 0.11 0.00 0.00 0.00 0.11 0.00  0.00\n [2,] 0.10 0.00 0.00 0.10 0.10 0.10 0.00 0.00 0.00  0.00\n [3,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.17\n [4,] 0.14 0.14 0.00 0.00 0.00 0.00 0.00 0.14 0.00  0.00\n [5,] 0.00 0.10 0.00 0.00 0.00 0.00 0.00 0.00 0.10  0.10\n [6,] 0.00 0.14 0.00 0.00 0.00 0.00 0.14 0.00 0.14  0.00\n [7,] 0.00 0.00 0.00 0.00 0.00 0.33 0.00 0.00 0.00  0.00\n [8,] 0.20 0.00 0.00 0.20 0.00 0.00 0.00 0.00 0.00  0.20\n [9,] 0.00 0.00 0.00 0.00 0.17 0.17 0.00 0.00 0.00  0.17\n[10,] 0.00 0.00 0.12 0.00 0.12 0.00 0.00 0.12 0.12  0.00\n\n\nNote that the entries are now numbers between zero and one and the matrix is asymmetric that is \\(p_{ij}\\) is not necessarily equal to \\(p_{ji}\\). In fact \\(p_{ij}\\) will only be equal to \\(p_{ji}\\) when \\(k_i = k_j\\) (nodes have the same degree).\nMoreover the rows of \\(\\mathbf{P}\\) sum to one:\n\n   rowSums(P)\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n\nWhich means that the \\(\\mathbf{P}\\) matrix is row stochastic. That is the “outdegree” of each node in the matrix is forced to sum to a fixed number (which means that it is a useless quantity). However, the indegree is not:\n\n   round(colSums(P), 2)\n\n [1] 1.11 1.34 0.63 0.86 1.56 1.06 0.37 0.51 0.61 1.21 2.33 0.92 0.17 0.87 1.08\n[16] 0.53 2.73 0.54 1.21 0.60 0.77\n\n\nWhich means that inequalities in the system will be tied to the indegree of each node in the \\(\\mathbf{P}\\) matrix, which is given by either the column sums of the matrix (as we just saw) or the row sums of the transpose of the same matrix:\n\n   round(rowSums(t(P)), 2)\n\n [1] 1.11 1.34 0.63 0.86 1.56 1.06 0.37 0.51 0.61 1.21 2.33 0.92 0.17 0.87 1.08\n[16] 0.53 2.73 0.54 1.21 0.60 0.77\n\n\nThis will come in handy in a second.\nThe \\(\\mathbf{P}\\) matrix has many interpretations, but here it just quantifies the idea that the amount of centrality each node can distribute is proportional to their degree, and that the larger the degree, the less there is to distribute (the smaller each cell \\(p_{ij}\\) will be). Meanwhile, it is clear that nodes that are pointed to by many other nodes who themselves don’t point to many others have a large indegree in \\(\\mathbf{P}\\).\nNow we can just adapt the the model of status distribution we used for eigenvector centrality but this time using the \\(\\mathbf{P}\\) rather than the \\(\\mathbf{A}\\) matrix. Note that because we are interested in the status that comes into each node we use the transpose of \\(\\mathbf{P}\\) rather than \\(\\mathbf{P}\\).\nSo at each step the status of a node is equivalent to the sum of the status scores of their in-neighbors, with more discerning in-neighbors passing along more status than less discerning ones:\n\n   s2 <- round(status1(t(P)), 3)\n   s2 <- s2/max(s2)\n   round(s2, 3)\n\n [1] 0.500 0.555 0.333 0.388 0.555 0.388 0.167 0.278 0.333 0.445 0.778 0.445\n[13] 0.110 0.333 0.500 0.278 1.000 0.222 0.555 0.278 0.333\n\n\nWhat if I told you that these numbers are the same as the leading eigenvector of \\(\\mathbf{P}^T\\)?\n\n   s2 <- abs(eigen(t(P))$vector[, 1])\n   s2 <- s2/max(s2)\n   round(s2, 3) \n\n [1] 0.500 0.556 0.333 0.389 0.556 0.389 0.167 0.278 0.333 0.444 0.778 0.444\n[13] 0.111 0.333 0.500 0.278 1.000 0.222 0.556 0.278 0.333\n\n\nAnd, of course, the (normalized) scores produced by this approach are identical to those computed by the page_rank function in igraph:\n\n   pr <- page_rank(g, damping = 1, algo = \"arpack\")$vector\n   round(pr/max(pr), 3)\n\n [1] 0.500 0.556 0.333 0.389 0.556 0.389 0.167 0.278 0.333 0.444 0.778 0.444\n[13] 0.111 0.333 0.500 0.278 1.000 0.222 0.556 0.278 0.333\n\n\nSo the distributional model of status is the same one implemented in the PageRank algorithm!\nPageRank of course was designed to deal with directed graphs (like the World Wide Web). So let’s load up the version of the Krackhardt’s Managers data that contains the advice network which is an unambiguously directed relation.\n\n   g <- ht_advice\n\nWe then compute the \\(\\mathbf{P}\\) matrix:\n\n   A <- as.matrix(as_adjacency_matrix(g))\n   P <- A/rowSums(A)\n   round(P[1:10, 1:10], 2)\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n [1,] 0.00 0.17 0.00 0.17 0.00 0.00 0.00 0.17 0.00  0.00\n [2,] 0.00 0.00 0.00 0.00 0.00 0.33 0.33 0.00 0.00  0.00\n [3,] 0.07 0.07 0.00 0.07 0.00 0.07 0.07 0.07 0.07  0.07\n [4,] 0.08 0.08 0.00 0.00 0.00 0.08 0.00 0.08 0.00  0.08\n [5,] 0.07 0.07 0.00 0.00 0.00 0.07 0.07 0.07 0.00  0.07\n [6,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.00\n [7,] 0.00 0.12 0.00 0.00 0.00 0.12 0.00 0.00 0.00  0.00\n [8,] 0.00 0.12 0.00 0.12 0.00 0.12 0.12 0.00 0.00  0.12\n [9,] 0.08 0.08 0.00 0.00 0.00 0.08 0.08 0.08 0.00  0.08\n[10,] 0.07 0.07 0.07 0.07 0.07 0.00 0.00 0.07 0.00  0.00\n\n\nRemember how we said earlier that there are multiple ways of thinking about \\(\\mathbf{P}\\)? Another way of thinking about the \\(\\mathbf{P}\\) matrix is as characterizing the behavior of a random walker in the directed graph. At any time point \\(t\\) the walker (a piece of information, a virus, or status itself) sits on a node and the with probability \\(p_{ij}\\) it jumps to one of that node’s out-neighbors. The probabilities are stored in the matrix \\(\\mathbf{P}\\).\nOne issue that arises is that there could be nodes with no out-neighbors (so-called sink nodes) or like node 6 above, who has just one out-neighbor, in which case the probability is 1.0 that if the random walker is at node 6 it will go to node 21.\nTo avoid this issue the original designers of the PageRank algorithm (Brin and Page 1998) added a “fudge” factor: That is, with probability \\(\\alpha\\) the random walker should hop from node to node following the directed links in the graph. But once in a while with probability \\(1-\\alpha\\) the walker should decide to “teleport” (with uniform probability) to any node in the graph whether it is an out-neighbor of the current node or not.\nHow do we do that? Well we need to “fix” the \\(\\mathbf{P}\\) matrix to allow for such behavior. So instead of \\(\\mathbf{P}\\) we estimate our distributive status model on the matrix \\(\\mathbf{G}\\) (yes, for Google):\n\\[\n   \\mathbf{G} = \\alpha \\mathbf{P} + (1 - \\alpha) \\mathbf{E}\n\\]\nWhere \\(\\mathbf{E}\\) is a matrix of the same dimensions as \\(\\mathbf{P}\\) but containing \\(1/n\\) in every cell indicating that every node has an equal chance of being “teleported” to.\nSo fixing \\(\\alpha = 0.85\\) our \\(\\mathbf{G}\\) matrix would be:\n\n   n <- vcount(g)\n   E <- matrix(1/n, n, n)\n   G <- (0.85 * P) + (0.15 * E)\n\nAnd then we just play our status distribution game on the transpose of \\(\\mathbf{G}\\):\n\n   s3 <- round(status1(t(G)), 3)\n   round(s3/max(s3), 3)\n\n [1] 0.302 0.573 0.165 0.282 0.100 0.437 0.635 0.255 0.092 0.180 0.237 0.242\n[13] 0.087 0.268 0.097 0.170 0.258 0.440 0.087 0.200 1.000\n\n\nWhich is the same answer you would get from the igraph function page_rank by setting the “damping” parameter to 0.85:\n\n   pr <- page_rank(g, damping = 0.85, algo = \"arpack\")$vector\n   round(pr/max(pr), 3)\n\n [1] 0.302 0.573 0.165 0.281 0.100 0.437 0.635 0.256 0.091 0.180 0.237 0.242\n[13] 0.086 0.269 0.097 0.169 0.258 0.440 0.086 0.200 1.000"
  },
  {
    "objectID": "handout3.html#hubs-and-authorities",
    "href": "handout3.html#hubs-and-authorities",
    "title": "Status and Prestige",
    "section": "Hubs and Authorities",
    "text": "Hubs and Authorities\nRecall from previous discussions that everything doubles (sometimes quadruples like degree correlations) in directed graphs. The same goes for status as reflected in a distributive model through the network.\nConsider two ways of showing your status in a system governed by directed relations (like advice). You can be highly sought after by others (be an “authority”), or you can be discerning in who you seek advice from, preferably seeking out people who are also sought after (e.g., be a “hub” pointing to high-quality others).\nThese two forms of status are mutually defining. The top authorities are those who are sought after by the top hubs, and the top hubs are the ones who seek the top authorities!\nSo this leads to a doubling of Equation 1:\n\\[  \n   x^h_i = \\sum_j a_{ij} x^a_j\n\\]\n\\[\n   x^a_i = \\sum_i a_{ij} x^h_i\n\\]\nWhich says that the hub score \\(x^h\\) of a node is the sum of the authority scores \\(x^a\\) of the nodes they point to (sum over \\(j\\); the outdegree), and the authority score of a node is the sum of the hub scores of the nodes that point to it (sum over \\(i\\); the indegree).\nSo we need to make our status game a bit more complicated (but not too much) to account for this duality:\n\n   status2 <- function(A) {\n     n <- nrow(A)\n     a <- rep(1, n)  #initializing authority scores\n     diff.a <- 1 #initializing diff.\n     while (diff.a >= 0.0001) {\n         o.a <- a #old authority scores\n         h <- A %*% o.a #new hub scores a function of previous authority scores of their out-neighbors\n         h <- h/norm(h, type = \"E\")\n         a <- t(A) %*% h #new authority scores a function of current hub scores of their in-neighbors\n         a <- a/norm(a, type = \"E\")\n         diff.a <- abs(sum(abs(o.a) - abs(a))) #diff. between old and new authority scores\n         }\n   return(list(h = as.vector(h), a = as.vector(a)))\n   }\n\nEverything is like our previous status1 function except now we are keeping track of two mutually defining scores a and h. As you may have guessed this is just an implementation of the “HITS” algorithm developed by Kleinberg (1999).2\nThe results for the Krackhardt advice network are:\n\n   round(status2(A)$a/max(status2(A)$a), 3)\n\n [1] 0.782 1.000 0.356 0.496 0.330 0.644 0.684 0.711 0.290 0.615 0.769 0.498\n[13] 0.323 0.677 0.267 0.570 0.645 0.871 0.323 0.589 0.776\n\n   round(status2(A)$h/max(status2(A)$h), 3)\n\n [1] 0.370 0.176 0.841 0.709 0.835 0.065 0.492 0.490 0.773 0.672 0.206 0.122\n[13] 0.331 0.279 1.000 0.274 0.313 0.800 0.581 0.687 0.600\n\n\nWhich are equivalent to using the igraph functions authority_score and hub_score:\n\n   round(authority_score(g, scale = TRUE)$vector, 3)\n\n [1] 0.782 1.000 0.356 0.496 0.330 0.644 0.684 0.711 0.290 0.615 0.769 0.498\n[13] 0.323 0.677 0.267 0.570 0.645 0.871 0.323 0.589 0.776\n\n   round(hub_score(g, scale = TRUE)$vector, 3)\n\n [1] 0.370 0.176 0.841 0.709 0.835 0.065 0.492 0.490 0.773 0.672 0.206 0.122\n[13] 0.331 0.279 1.000 0.274 0.313 0.800 0.581 0.687 0.600\n\n\nRecall that both the eigenvector and PageRank status scores computed via the network status distribution game routine ended up being equivalent to the eigenvectors of a network proximity matrix (the adjacency matrix \\(\\mathbf{A}\\) and the probability matrix \\(\\mathbf{P}\\) respectively). It would be surprising if the same wasn’t true of the hub and authority status scores.\nLet’s find out which ones!\nConsider the matrices:\n\\[\n\\mathbf{M} = \\mathbf{A}\\mathbf{A}^T\n\\]\n\\[\n\\mathbf{N} = \\mathbf{A}^T\\mathbf{A}\n\\]\nLet’s see what they look like in the Krackhardt manager’s network:\n\n   M = A %*% t(A)\n   N = t(A) %*% A\n   M[1:10, 1:10]\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n [1,]    6    1    5    5    5    1    3    4    5     5\n [2,]    1    3    3    2    3    1    2    3    3     0\n [3,]    5    3   15   11   12    1    8    8   12     8\n [4,]    5    2   11   12   11    1    7    6   11     8\n [5,]    5    3   12   11   15    1    7    7   12    10\n [6,]    1    1    1    1    1    1    1    1    1     0\n [7,]    3    2    8    7    7    1    8    5    8     4\n [8,]    4    3    8    6    7    1    5    8    7     4\n [9,]    5    3   12   11   12    1    8    7   13     7\n[10,]    5    0    8    8   10    0    4    4    7    14\n\n   N[1:10, 1:10]\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n [1,]   13   13    4    5    5    6    8    8    4     8\n [2,]   13   18    5    8    5    9   11   10    4     9\n [3,]    4    5    5    4    4    2    4    4    2     3\n [4,]    5    8    4    8    3    4    6    6    3     4\n [5,]    5    5    4    3    5    1    3    3    3     3\n [6,]    6    9    2    4    1   10    7    7    2     6\n [7,]    8   11    4    6    3    7   13    6    3     7\n [8,]    8   10    4    6    3    7    6   10    3     6\n [9,]    4    4    2    3    3    2    3    3    4     3\n[10,]    8    9    3    4    3    6    7    6    3     9\n\n\nWhat’s in these matrices? Well let’s look at \\(\\mathbf{M}\\). The diagonals will look familiar because they happen to be the outdegree of each node:\n\n   degree(g, mode = \"out\")[1:10]\n\n [1]  6  3 15 12 15  1  8  8 13 14\n\n\nYou may have guessed that the diagonals of matrix \\(\\mathbf{N}\\) contain the indegrees:\n\n   degree(g, mode = \"in\")[1:10]\n\n [1] 13 18  5  8  5 10 13 10  4  9\n\n\nWhich means that the off-diagonals cells of each matrix \\(m_{ij}\\) and \\(n_{ij}\\), contain the common out-neighbors and common in-neighbors shared by nodes \\(i\\) and \\(j\\) in the graph, respectively.\nAs you may already be suspecting, the hub and authorities scores are the leading eigenvectors of the \\(\\mathbf{M}\\) and \\(\\mathbf{N}\\) matrices (Kleinberg 1999):\n\n   h <- eigen(M)$vector[,1] * -1\n   a <- eigen(N)$vector[,1] * -1\n   round(h/max(h), 3)\n\n [1] 0.370 0.176 0.841 0.709 0.835 0.065 0.492 0.490 0.773 0.672 0.206 0.122\n[13] 0.331 0.279 1.000 0.274 0.313 0.800 0.581 0.687 0.600\n\n   round(a/max(a), 3)\n\n [1] 0.782 1.000 0.356 0.496 0.330 0.644 0.684 0.711 0.290 0.615 0.769 0.498\n[13] 0.323 0.677 0.267 0.570 0.645 0.871 0.323 0.589 0.776\n\n\nOnce again demonstrating the equivalence between eigenvectors of proximity matrices in networks and our prismatic status distribution game!"
  },
  {
    "objectID": "handout4.html",
    "href": "handout4.html",
    "title": "Role Equivalence and Structural Similarity",
    "section": "",
    "text": "One of the earlier “proofs of concept” of the power of social network analysis came from demonstrating that you could formalize the fuzzy idea of “role” central to functionalist sociology and British social anthropology using the combined tools of graph theoretical and matrix representations of networks (White, Boorman, and Breiger 1976).\nThis and other contemporaneous work (Breiger, Boorman, and Arabie 1975) set off an entire sub-tradition of data analysis of networks focused on the idea that one could partition the set of vertices in a graph into meaningful classes based on some mathematical (e.g., graph theoretic) criterion.\nThese classes would in turn would be isomorphic with the concept of role as social position and the classes thereby derived as indicating the number of such positions in the social structure under investigation as well as which actors belonged to which positions."
  },
  {
    "objectID": "handout4.html#structural-equivalence",
    "href": "handout4.html#structural-equivalence",
    "title": "Role Equivalence and Structural Similarity",
    "section": "Structural Equivalence",
    "text": "Structural Equivalence\nThe earliest work pursued simultaneously by analysts at Harvard (White, Boorman, and Breiger 1976) and Chicago (Burt 1976) relied on the idea of structural equivalence.\nIn a graph \\(G = \\{E, V\\}\\) two nodes \\(v_i, v_j\\) are structurally equivalent if they are connected to the same others in the network; that is, if \\(N(v_i)\\) is the set of nodes adjacent to node \\(v_i\\) and \\(N(v_j)\\) is the set of nodes adjacent to node \\(v_j\\), then:\n\\[\n   v_i \\equiv v_j \\iff N(v_i) = N(v_j)\n\\]\nIn a graph, an equivalence class \\(C\\) is just a set of nodes that are structurally equivalent, such that if \\(v_i \\in C_i\\) and \\(v_j \\in C_i\\) then \\(v_i \\equiv v_j\\) for all pairs \\((v_i, v_j) \\in C_i\\).\nThe partitioning of the vertex set into a set of equivalence classes \\(\\{C_1, C_2 \\ldots C_k\\}\\) as well as the adjacency relations between nodes in the same class and nodes in different classes defines the role structure of the network."
  },
  {
    "objectID": "handout4.html#structural-equivalence-in-an-ideal-world",
    "href": "handout4.html#structural-equivalence-in-an-ideal-world",
    "title": "Role Equivalence and Structural Similarity",
    "section": "Structural Equivalence in an Ideal World",
    "text": "Structural Equivalence in an Ideal World\nLet us illustrate these concepts. Consider the following toy graph:\n\n\n\n\n\nFigure 1: A toy graph demonstrating structural equivalence.\n\n\n\n\nWith associated adjacency matrix:\n\n\n\n\n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n  \n \n\n  \n    A \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    B \n    1 \n    0 \n    0 \n    1 \n    1 \n    1 \n    1 \n    0 \n    0 \n  \n  \n    C \n    1 \n    0 \n    0 \n    1 \n    1 \n    1 \n    1 \n    0 \n    0 \n  \n  \n    D \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n  \n  \n    E \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n  \n  \n    F \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n  \n  \n    G \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    H \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n    0 \n    0 \n    1 \n  \n  \n    I \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n    0 \n    1 \n    0 \n  \n\n\n\n\n\nA simple function to check for structural equivalence in the graph, relying on the native R function setequal would be:\n\n   check.equiv <- function(x) {\n      n <- vcount(x)\n      v <- V(x)$name\n      E <- matrix(0, n, n)\n      for (i in v) {\n         for (j in v) {\n            if (i != j & E[which(v == j), which(v == i)] != 1) {\n               N.i <- neighbors(x, i)\n               N.j <- neighbors(x, j)\n               if (are_adjacent(x, i, j) == TRUE) {\n                  N.i <- c(names(N.i), i)\n                  N.j <- c(names(N.j), j)\n                  } #end sub-if\n               if (setequal(N.i, N.j) == TRUE) {\n                  E[which(v == i), which(v == j)] <- 1\n                  E[which(v == j), which(v == i)] <- 1\n                  } #end sub-if\n               } #end main if\n            } #end j loop\n         } #end i loop\n      rownames(E) <- v\n      colnames(E) <- v\n   return(E)\n   }\n\nThis function creates an empty “equivalence” matrix \\(\\mathbf{E}\\) in line 4, loops through each pair of nodes in the graph in lines 5-20. The main condition restricts the checking to nodes that are not the same or have not yet to be found to be equivalent (line 7). Lines 8-9 extract the node neighborhoods using the igraph function neighbors.\nLines 10-13 check to see if the pair of nodes that are being checked for equivalence are themselves adjacent. If they are indeed adjacent (the conditional in line 10 is TRUE) then we need to use the so-called closed neighborhood of \\(v_i\\) and \\(v_j\\), written \\(N[v_i], N[v_j]\\), to do the equivalence check, or otherwise we get the wrong answer.1\nThe equivalence check is done in line 14 using the native R function setequal. This function takes two inputs (e.g., two vectors) and will return a value of TRUE if the elements in the first vector are the same as the elements in the second vector. In that case we update the matrix \\(\\mathbf{E}\\) accordingly.\nAfter writing our function, we can then type:\n\n   Equiv <- check.equiv(g)\n\nAnd the resulting equivalence matrix \\(\\mathbf{E}\\) corresponding to the graph in Figure 1 is:\n\n\n\n\n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n  \n \n\n  \n    A \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    B \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    C \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    D \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    E \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    F \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    G \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    H \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n  \n  \n    I \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n  \n\n\n\n\n\nIn this matrix, there is a 1 in the corresponding cell if the row node is structurally equivalent to the column node.\nOne thing we can do with this matrix is re-order the rows and columns, so that rows(columns) corresponding to nodes that are “adjacent” in the equivalence relation appear next to one another in the matrix.\nTo do that we can use the corrMatOrder function from the corrplot package, designed to work with correlation matrices, but works with any matrix of values:\n\n   #install.packages(\"corrplot\")\n   library(corrplot)\n   SE.ord <- corrMatOrder(Equiv, order = \"hclust\", hclust.method = \"ward.D2\")\n   SE.ord\n\n[1] 6 4 5 8 9 1 7 2 3\n\n\nThe corrplot function corrMatorder takes a matrix as input and returns a vector of reordered values of the rows(columns) as output. We use a hierarchical clustering algorithm using Ward’s method to do the job.\nWe can see that the new re-ordered vector has the previous row(column) 6 in fist position, 4 at second, five at third, 8 at fourth, and so forth.;\nWe can then re-order rows and columns of the old equivalence matrix using this new ordering by typing:\n\n   Equiv <- Equiv[SE.ord, SE.ord]\n\nThe resulting re-ordered matrix looks like:\n\n\n\n\n \n  \n      \n    F \n    D \n    E \n    H \n    I \n    A \n    G \n    B \n    C \n  \n \n\n  \n    F \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    D \n    1 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    E \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    H \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    I \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    A \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    G \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    B \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n  \n  \n    C \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n  \n\n\n\n\n\nOnce the equivalence matrix is re-ordered we can see that sets of structurally equivalent nodes in Figure 1, appear clustered along the diagonals. This type of re-arranged matrix is said to be in block-diagonal form (e.g., non-zero entries clustered along the diagonals).\nEven more interestingly, we can do the same re-arranging on the original adjacency matrix, to reveal:\n\n\n\n\n \n  \n      \n    F \n    D \n    E \n    H \n    I \n    A \n    G \n    B \n    C \n  \n \n\n  \n    F \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    1 \n    1 \n  \n  \n    D \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    1 \n    1 \n  \n  \n    E \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    1 \n    1 \n  \n  \n    H \n    1 \n    1 \n    1 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    I \n    1 \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    A \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n  \n  \n    G \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    1 \n  \n  \n    B \n    1 \n    1 \n    1 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n  \n  \n    C \n    1 \n    1 \n    1 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n  \n\n\n\n\n\nThis is called a blocked adjacency matrix. As you can see, once the structural equivalence relations in the network are revealed by permuting the rows and columns, the adjacency matrix shows an orderly pattern.\nThe way to interpret the blocked adjacency matrix is as follows:\n\nThe block diagonals of the matrix reveal the intra-block relations between sets of structurally equivalent nodes. If the block diagonal is empty–called a zero block–it means that set of structurally equivalent nodes does not connect with one another directly. If it has ones–called a one block–it it means that members of that set of structurally equivalent nodes are also neighbors.\nThe off diagonal blocks reveals the inter-block adjacency relations between different clusters of structurally equivalent nodes. If an off-diagonal block is a one-block, it means that members of block \\(C_i\\) send ties to members of block \\(C_j\\). If and off diagonal block is a zero-block, it means that members of block \\(C_i\\) avoid associating with members of block \\(C_j\\).\n\nSo if:\n\\[\nC_1 = \\{D, E, F\\}\n\\]\n\\[\nC_2 = \\{H, I\\}\n\\]\n\\[\nC_3 = \\{A, G\\}\n\\]\n\\[\nC_4 = \\{B, C\\}\n\\]\nThen we can see that:\n\nMembers of \\(C_1\\) connect with members of \\(C_2\\) and \\(C_4\\) but not among themselves.\nMembers of \\(C_2\\) connect among themselves and with \\(C_1\\).\nMembers of \\(C_3\\) connect among themselves and with \\(C_4\\).\nMembers of \\(C_4\\) connect with \\(C_1\\) and \\(C_3\\) but avoid associating with their own block.\n\nThese intra and inter-block relations can then be represented in the reduced image matrix:\n\n\n\n\n \n  \n      \n    C_1 \n    C_2 \n    C_3 \n    C_4 \n  \n \n\n  \n    C_1 \n    0 \n    1 \n    0 \n    1 \n  \n  \n    C_2 \n    1 \n    1 \n    0 \n    0 \n  \n  \n    C_3 \n    0 \n    0 \n    1 \n    1 \n  \n  \n    C_4 \n    1 \n    0 \n    1 \n    0 \n  \n\n\n\n\n\nWhich reveals a more economical representation of the system based on structural equivalence."
  },
  {
    "objectID": "handout4.html#structural-equivalence-in-the-real-world",
    "href": "handout4.html#structural-equivalence-in-the-real-world",
    "title": "Role Equivalence and Structural Similarity",
    "section": "Structural Equivalence in the Real World",
    "text": "Structural Equivalence in the Real World\nOf course, in real data, it is very unlikely that two nodes will meet the exact mathematical criterion of structural equivalence. They might have three out of five, or eight out of nine common neighbors but would still get a big zero in the \\(\\mathbf{E}\\) defined using our strict function.\nSo, a lot of role analysis in real networks follows instead searches for a near cousin to structural equivalence. This leads us to the large class of distance and similarity metrics, and the task is to pick one such that structural equivalence falls off as a special case of the given metric.\nFor random reasons, early work in social network analysis focused on distance metrics, while more recent work inspired by network science focuses on similarity metrics. The end goal is the same though; to cluster nodes in a graph such that those in the same class are the most structurally similar to one another.\nLet us, therefore, begin with the distance approach. Here the goal is simply to pick a distance metric \\(d\\) with a well defined minimum \\(d_{min}\\) or maximum value \\(d_{max}\\), such that:\n\\[\n   v_i \\equiv v_j \\iff d(v_i, v_j) = d_{min} \\lor d(v_i, v_j) = d_{max}\n\\]\nWhere whether we pick the maximum or minimum value depends on the particularities of the measure \\(d\\).\nWe then populate the \\(\\mathbf{E}\\) matrix with the values of \\(d\\) for each pair of nodes \\((v_i, v_j)\\), do some kind of clustering on the matrix, and use our clusters assignments to re-arrange the original adjacency matrix to find our blocks, and so forth.\nA very obvious candidate for \\(d\\) is the Euclidean Distance (Burt 1976):\n\\[\n   d_{i,j} = \\sqrt{\\sum_{k \\neq i,j} (a_{ik} - a_{jk})^2}\n\\]\nWhere \\(a_{ik}\\) and \\(a_{jk}\\) are the corresponding entries in the graph’s adjacency matrix \\(\\mathbf{A}\\). The minimum for this measure is \\(d_{min} = 0\\), so this is the value we should find for structurally equivalent nodes.\nA function that does this for any graph is:\n\n   d.euclid <- function(x) {\n      A <- as.matrix(as_adjacency_matrix(x))\n      n <- nrow(A)\n      E <- matrix(0, n, n)\n      for (i in 1:n) {\n         for (j in 1:n) {\n            if (i < j & i != j) {\n               d.ij <- 0\n               for (k in 1:n) {\n                  if (k != i & k != j) {\n                     d.ij <- d.ij + (A[i,k] - A[j,k])^2\n                     }\n                  }\n               E[i,j] <- sqrt(d.ij)\n               E[j,i] <- sqrt(d.ij)\n            }\n         }\n      }\n   rownames(E) <- rownames(A)\n   colnames(E) <- colnames(A)\n   return(E)\n   }\n\nAnd we can try it out with our toy graph:\n\n   E <- d.euclid(g)\n   round(E, 1)\n\n    A   B   C   D   E   F   G   H   I\nA 0.0 2.0 2.0 1.7 1.7 1.7 0.0 2.6 2.6\nB 2.0 0.0 0.0 2.6 2.6 2.6 2.0 1.7 1.7\nC 2.0 0.0 0.0 2.6 2.6 2.6 2.0 1.7 1.7\nD 1.7 2.6 2.6 0.0 0.0 0.0 1.7 2.0 2.0\nE 1.7 2.6 2.6 0.0 0.0 0.0 1.7 2.0 2.0\nF 1.7 2.6 2.6 0.0 0.0 0.0 1.7 2.0 2.0\nG 0.0 2.0 2.0 1.7 1.7 1.7 0.0 2.6 2.6\nH 2.6 1.7 1.7 2.0 2.0 2.0 2.6 0.0 0.0\nI 2.6 1.7 1.7 2.0 2.0 2.0 2.6 0.0 0.0\n\n\nAnd it looks like indeed it detected the structurally equivalent nodes in the graph. We can see it clearly by re-ordering the rows and columns according to our known ordering:\n\n   E <- E[SE.ord, SE.ord]\n   round(E, 1)\n\n    F   D   E   H   I   A   G   B   C\nF 0.0 0.0 0.0 2.0 2.0 1.7 1.7 2.6 2.6\nD 0.0 0.0 0.0 2.0 2.0 1.7 1.7 2.6 2.6\nE 0.0 0.0 0.0 2.0 2.0 1.7 1.7 2.6 2.6\nH 2.0 2.0 2.0 0.0 0.0 2.6 2.6 1.7 1.7\nI 2.0 2.0 2.0 0.0 0.0 2.6 2.6 1.7 1.7\nA 1.7 1.7 1.7 2.6 2.6 0.0 0.0 2.0 2.0\nG 1.7 1.7 1.7 2.6 2.6 0.0 0.0 2.0 2.0\nB 2.6 2.6 2.6 1.7 1.7 2.0 2.0 0.0 0.0\nC 2.6 2.6 2.6 1.7 1.7 2.0 2.0 0.0 0.0\n\n\nHere the block-diagonals of the matrix contain zeroes because the \\(d\\) is a distance function with a minimum of zero. If we wanted it to contain ones instead we would normalize:\n\\[\n   d^* = 1-\\left(\\frac{d}{max(d)}\\right)\n\\]\nWhich would give us:\n\n   E.norm <- 1 - E/max(E)\n   round(E.norm, 1)\n\n    F   D   E   H   I   A   G   B   C\nF 1.0 1.0 1.0 0.2 0.2 0.3 0.3 0.0 0.0\nD 1.0 1.0 1.0 0.2 0.2 0.3 0.3 0.0 0.0\nE 1.0 1.0 1.0 0.2 0.2 0.3 0.3 0.0 0.0\nH 0.2 0.2 0.2 1.0 1.0 0.0 0.0 0.3 0.3\nI 0.2 0.2 0.2 1.0 1.0 0.0 0.0 0.3 0.3\nA 0.3 0.3 0.3 0.0 0.0 1.0 1.0 0.2 0.2\nG 0.3 0.3 0.3 0.0 0.0 1.0 1.0 0.2 0.2\nB 0.0 0.0 0.0 0.3 0.3 0.2 0.2 1.0 1.0\nC 0.0 0.0 0.0 0.3 0.3 0.2 0.2 1.0 1.0\n\n\nAs noted, a distance function defines a clustering on the nodes, and the results of the clustering generate our blocks. In R we can use the functions dist and hclust to do the job:\n\n   E <- dist(E) #transforming E to a dist object\n   h.res <- hclust(E, method = \"ward.D2\")\n   plot(h.res)\n\n\n\n\nWhich are our original clusters of structurally equivalent nodes!\nLet’s see how this would work in real data. Let’s take the Flintstones (film) co-appearance network as an example:\n\n   library(networkdata)\n   g.flint <- movie_267\n   g.flint <- delete_vertices(g.flint, degree(g.flint) <= 3)\n   E <- d.euclid(g.flint)\n   E <- dist(E) #transforming E to a dist object\n   h.res <- hclust(E, method = \"ward.D2\") #hierarchical clustering\n   #install.packages(\"dendextend\")\n   library(dendextend) #nice dendrogram plotting\n   dend <- as.dendrogram(h.res) %>% \n      color_branches(k = 8) %>% \n      color_labels(k = 8) %>% \n      set(\"labels_cex\", 0.65) %>% \n      set(\"branches_lwd\", 2) %>% \n      plot\n\n\n\n\nThe Flintstones is a film based on a family, and families are the prototypes of social roles in networks, so if structural equivalence gets at roles, then it should recover known kin roles here, and indeed it does to some extent. One cluster is the focal parents separated into “moms” and “dads” roles, and another has the kids.\nAfter we have our clustering, we may wish to extract the structurally equivalent blocks from the hierarchical clustering results. To do that, we need to cut the dendrogram at a height that will produce a given number of clusters. Of course because hierarchical clustering is agglomerative, it begins with all nodes in the same cluster and ends with all nodes in a single cluster. So a reasonable solution is some (relatively) small number of clusters \\(k\\) such that \\(1 > k < n\\) that is, some number larger than one but smaller than the number of nodes in the graph.\nChoosing the number of clusters after a hierarchical clustering is not a well-defined problem, so you have to use a combination of pragmatic and domain-specific knowledge criteria to decide. Here, it looks like four blocks provides enough resolution and substantive interpretability so let’s do that. To do the job we use a function from the dendextend package we used above to draw our pretty colored dendrogram data viz, called cutree which as its name implies cuts the dendrogram at a height that produces the required number of classes:\n\n   blocks  <- cutree(h.res, k = 4)\n   blocks\n\n        BAM-BAM          BARNEY           BETTY        FELDSPAR            FRED \n              1               2               2               3               2 \n   HEADMISTRESS      HERDMASTER            LAVA           LEACH          MORRIS \n              4               3               2               1               4 \n      MRS SLATE         PEBBLES PEBBLES BAM-BAM        PILTDOWN      POINDEXTER \n              4               1               1               3               4 \n         PYRITE           SLATE           WILMA \n              3               2               2 \n\n\nNote that the result is a named vector with the node labels as the names and a value of \\(k\\) for each node, where \\(k\\) indicates the class of that node. For instance, \\(\\{Barney, Betty, Fred, Lava, Slate, Wilma\\}\\) all belong to \\(k = 2\\). Remember this is a purely nominal classification so the order of the numbers doesn’t matter."
  },
  {
    "objectID": "handout4.html#concor",
    "href": "handout4.html#concor",
    "title": "Role Equivalence and Structural Similarity",
    "section": "CONCOR",
    "text": "CONCOR\nThe other (perhaps less obvious) way of defining a distance between nodes in the network based on their connectivity patterns to other nodes is the correlation distance:\n\\[\n    d_{i,j} =\n    \\frac{\n    \\sum_{i \\neq k}\n    (a_{ki} - \\overline{a}_{i}) \\times\n    \\sum_{j \\neq k}\n    (a_{kj} - \\overline{a}_{j})\n    }\n    {\n    \\sqrt{\n    \\sum_{i \\neq k}\n    (a_{ki} - \\overline{a}_{i})^2 \\times\n    \\sum_{j \\neq k}\n    (a_{kj} - \\overline{a}_{j})^2\n        }\n    }\n\\]\nA more involved but still meaningful formula compared of the Euclidean distance. Here \\(\\overline{a}_{i}\\) is the column mean of the entries of node \\(i\\) in the affiliation matrix.\nSo the correlation distance is the ratio of the covariance of the column vectors corresponding to each node in the adjacency matrix and the product of their standard deviations.\nThe correlation distance between nodes in our toy network is given by simply typing:\n\n   m <- as.matrix(as_adjacency_matrix(g))\n   C <- cor(m)\n   round(C, 2)\n\n      A     B     C     D     E     F     G     H     I\nA  1.00 -0.32 -0.32  0.32  0.32  0.32  0.50 -0.63 -0.63\nB -0.32  1.00  1.00 -1.00 -1.00 -1.00 -0.32  0.35  0.35\nC -0.32  1.00  1.00 -1.00 -1.00 -1.00 -0.32  0.35  0.35\nD  0.32 -1.00 -1.00  1.00  1.00  1.00  0.32 -0.35 -0.35\nE  0.32 -1.00 -1.00  1.00  1.00  1.00  0.32 -0.35 -0.35\nF  0.32 -1.00 -1.00  1.00  1.00  1.00  0.32 -0.35 -0.35\nG  0.50 -0.32 -0.32  0.32  0.32  0.32  1.00 -0.63 -0.63\nH -0.63  0.35  0.35 -0.35 -0.35 -0.35 -0.63  1.00  0.55\nI -0.63  0.35  0.35 -0.35 -0.35 -0.35 -0.63  0.55  1.00\n\n\nWhich gives the Pearson product moment correlation of each pair of columns in the adjacency matrix.\nThe key thing that was noticed by Breiger, Boorman, and Arabie (1975) is that we can iterate this process, and compute correlation distances of correlation distances between nodes in the graph. If we do this for our toy network a few (e.g., three) times we get:\n\n   C1 <- cor(C)\n   C2 <- cor(C1)\n   C3 <- cor(C2)\n   round(C3, 2)\n\n   A  B  C  D  E  F  G  H  I\nA  1 -1 -1  1  1  1  1 -1 -1\nB -1  1  1 -1 -1 -1 -1  1  1\nC -1  1  1 -1 -1 -1 -1  1  1\nD  1 -1 -1  1  1  1  1 -1 -1\nE  1 -1 -1  1  1  1  1 -1 -1\nF  1 -1 -1  1  1  1  1 -1 -1\nG  1 -1 -1  1  1  1  1 -1 -1\nH -1  1  1 -1 -1 -1 -1  1  1\nI -1  1  1 -1 -1 -1 -1  1  1\n\n\nInterestingly the positive correlations converge to 1.0 and the negative correlations converge to -1.0!\nIf we sort the rows and columns of the new matrix according to these values, we get:\n\n   C.ord <- corrMatOrder(C3, order = \"hclust\", hclust.method = \"ward.D2\")\n   C3 <- C3[C.ord, C.ord]\n   round(C3, 2)\n\n   B  C  H  I  F  D  E  A  G\nB  1  1  1  1 -1 -1 -1 -1 -1\nC  1  1  1  1 -1 -1 -1 -1 -1\nH  1  1  1  1 -1 -1 -1 -1 -1\nI  1  1  1  1 -1 -1 -1 -1 -1\nF -1 -1 -1 -1  1  1  1  1  1\nD -1 -1 -1 -1  1  1  1  1  1\nE -1 -1 -1 -1  1  1  1  1  1\nA -1 -1 -1 -1  1  1  1  1  1\nG -1 -1 -1 -1  1  1  1  1  1\n\n\nAha! The iterated correlations seems to have split the matrix into two blocks \\(C_1 = \\{G, F, D, A, G\\}\\) and \\(C_2 = \\{I, H, B, C\\}\\). Each of the blocks is composed of two sub-blocks that we know are structurally equivalent from our previous analysis.\nA function implementing this method of iterated correlation distances until convergence looks like:\n\n   con.cor <- function(x) {\n      C <- x\n      while (mean(abs(C)) != 1) {\n         C <- cor(C)\n         }\n      b1 <- C[, 1] > 0\n      b2 <- !b1\n      return(list(x[, b1, drop = FALSE], x[, b2, drop = FALSE]))\n      }\n\nThis function takes a graph’s adjacency matrix as input, creates a copy of the adjacency matrix in line 2 (to be put through the iterated correlations meat grinder). The three-line (3-5) while loop goes through the iterated correlations (stopping when the matrix is full of ones). Then the resulting two blocks are returned as the columns of a couple of matrices stored in a list in line 8.\nFor instance, to use our example above:\n\n   con.cor(m)\n\n[[1]]\n  A D E F G\nA 0 0 0 0 1\nB 1 1 1 1 1\nC 1 1 1 1 1\nD 0 0 0 0 0\nE 0 0 0 0 0\nF 0 0 0 0 0\nG 1 0 0 0 0\nH 0 1 1 1 0\nI 0 1 1 1 0\n\n[[2]]\n  B C H I\nA 1 1 0 0\nB 0 0 0 0\nC 0 0 0 0\nD 1 1 1 1\nE 1 1 1 1\nF 1 1 1 1\nG 1 1 0 0\nH 0 0 0 1\nI 0 0 1 0\n\n\nThe columns of these two matrices are the two blocks we found before. Of course, to implement this method as a divisive clustering algorithm, what we want is to split these two blocks into two finer grained blocks, by iterative correlations of the columns of these two sub-matrices (to reveal two further sub-matrices each) and thus find our original four structurally equivalent groups.\nThe following function–simplified and adapted from Adam Slez’s work—which includes the con.cor function shown earlier inside of it, will do it:\n\n  blocks <- function(g, s = 2) {\n     A <- as.matrix(as_adjacency_matrix(g))\n     B <- list(A)\n     for(i in 1:s) {\n       B <- unlist(lapply(B, con.cor), recursive = FALSE)\n       }\n     return(lapply(B, colnames))\n   }\n\nThis function takes the graph as input and returns a list of column names containing the structurally equivalent blocks as output:\n\n   blocks(g)\n\n[[1]]\n[1] \"A\" \"G\"\n\n[[2]]\n[1] \"D\" \"E\" \"F\"\n\n[[3]]\n[1] \"B\" \"C\"\n\n[[4]]\n[1] \"H\" \"I\"\n\n\nWhich are the original structurally equivalent classes. The argument s controls the number of splits. When it is equal to one, the function produces two blocks, and when it is equal to two it produces four blocks, when it is equal to three, six blocks, and so on.\nThis is the algorithm called CONCOR (Breiger, Boorman, and Arabie 1975), short for convergence of iterate correlations, and can be used to cluster the rows(columns) of any valued square data matrix.\nFor instance, if we wanted to split the Flintstones network into four blocks we would proceed as follows:\n\n   g.flint <- movie_267\n   g.flint <- delete_vertices(g.flint, degree(g.flint) <= 3)\n   blocks(g.flint)\n\n[[1]]\n[1] \"BAM-BAM\"    \"BETTY\"      \"PEBBLES\"    \"POINDEXTER\" \"WILMA\"     \n\n[[2]]\n[1] \"FELDSPAR\"   \"HERDMASTER\" \"LAVA\"       \"PILTDOWN\"   \"PYRITE\"    \n[6] \"SLATE\"     \n\n[[3]]\n[1] \"BARNEY\"          \"FRED\"            \"LEACH\"           \"PEBBLES BAM-BAM\"\n\n[[4]]\n[1] \"HEADMISTRESS\" \"MORRIS\"       \"MRS SLATE\"   \n\n\nWhich is similar to the results we got from the Euclidean distance method, except that now the children are put in the same blocks as the moms.\nWe could then visualize the results as follows:\n\n   #install.packages(\"ggcorrplot\")\n   library(ggcorrplot)\n   A <- as.matrix(as_adjacency_matrix(g.flint))\n   ord <- unlist(blocks(g.flint, 2))\n   A <- A[ord, ord]\n   p <- ggcorrplot(A, colors = c(\"white\", \"white\", \"black\")) \n   p <- p + theme(legend.position = \"none\", \n                  axis.text.y = element_text(size = 8),\n                  axis.text.x = element_text(size = 8),\n                  )\n   p <- p + geom_hline(yintercept = 5.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_vline(xintercept = 5.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_hline(yintercept = 11.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_vline(xintercept = 11.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_hline(yintercept = 15.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_vline(xintercept = 15.5, linewidth = 2, color = \"blue\")\n   p\n\n\n\n\nWhich is just a tile plot of the original adjacency matrix (adjacent cells in black) with rows and columns re-ordered according to the four-block solution and ggplot vertical and horizontal lines highlighting the boundaries of the blocked matrix.\nAs you can see, characters with similar patterns of scene co-appearances (like Barney and Fred) are drawn next to one another, revealing the larger “roles” in the network."
  },
  {
    "objectID": "handout4.html#structural-equivalence-in-directed-graphs",
    "href": "handout4.html#structural-equivalence-in-directed-graphs",
    "title": "Role Equivalence and Structural Similarity",
    "section": "Structural Equivalence in Directed Graphs",
    "text": "Structural Equivalence in Directed Graphs\nLike before, the main complication introduced by the directed case is the “doubling” of the relations considered.\nIn the Euclidean distance case, we have to decide whether we want to compute two set of distances between nodes, one based on the in-distance vectors and the other on the out-distance vectors, and the two sets of hierarchical clustering partitions.\nAnother approach is simply to combine both according to the formula:\n\\[\n   d_{i,j} = \\sqrt{\n                  \\sum_{k \\neq i,j} (a_{ik} - a_{jk})^2 +\n                  \\sum_{k \\neq i,j} (a_{ki} - a_{kj})^2\n                  }\n\\]\nWhich just computes the Euclidean distances between nodes using both in and out neighbors. Here nodes would be structurally equivalent only if they have the same set of in and out-neighbors. This would mean changing line 11 in the function d.euclid above with the following:\n\n   d.ij <- d.ij + ((A[i,k] - A[j,k])^2 + (A[k,i] - A[k,j])^2)\n\nThis way, distances are computed on both the row and columns of the directed graph’s adjacency matrix.\nIf we are using the correlation distance approach in a directed graph, then the main trick is to stack the original adjacency matrix against its transpose, and then compute the correlation distance on the columns of the stacked matrices, which by definition combines information in incoming and outgoing ties.\nLet’s see a brief example. Let’s load up the Krackhardt’s high-tech managers data on advice relations and look at the adjacency matrix:\n\n   g <- ht_advice\n   A <- as.matrix(as_adjacency_matrix(g))\n   A\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]\n [1,]    0    1    0    1    0    0    0    1    0     0     0     0     0\n [2,]    0    0    0    0    0    1    1    0    0     0     0     0     0\n [3,]    1    1    0    1    0    1    1    1    1     1     1     1     0\n [4,]    1    1    0    0    0    1    0    1    0     1     1     1     0\n [5,]    1    1    0    0    0    1    1    1    0     1     1     0     1\n [6,]    0    0    0    0    0    0    0    0    0     0     0     0     0\n [7,]    0    1    0    0    0    1    0    0    0     0     1     1     0\n [8,]    0    1    0    1    0    1    1    0    0     1     1     0     0\n [9,]    1    1    0    0    0    1    1    1    0     1     1     1     0\n[10,]    1    1    1    1    1    0    0    1    0     0     1     0     1\n[11,]    1    1    0    0    0    0    1    0    0     0     0     0     0\n[12,]    0    0    0    0    0    0    1    0    0     0     0     0     0\n[13,]    1    1    0    0    1    0    0    0    1     0     0     0     0\n[14,]    0    1    0    0    0    0    1    0    0     0     0     0     0\n[15,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[16,]    1    1    0    0    0    0    0    0    0     1     0     0     0\n[17,]    1    1    0    1    0    0    1    0    0     0     0     0     0\n[18,]    1    1    1    1    1    0    1    1    1     1     1     0     1\n[19,]    1    1    1    0    1    0    1    0    0     1     1     0     0\n[20,]    1    1    0    0    0    1    0    1    0     0     1     1     0\n[21,]    0    1    1    1    0    1    1    1    0     0     0     1     0\n      [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21]\n [1,]     0     0     1     0     1     0     0     1\n [2,]     0     0     0     0     0     0     0     1\n [3,]     1     0     0     1     1     0     1     1\n [4,]     0     0     1     1     1     0     1     1\n [5,]     1     0     1     1     1     1     1     1\n [6,]     0     0     0     0     0     0     0     1\n [7,]     1     0     0     1     1     0     0     1\n [8,]     0     0     0     0     1     0     0     1\n [9,]     1     0     1     1     1     0     0     1\n[10,]     0     1     1     1     1     1     1     0\n[11,]     0     0     0     0     0     0     0     0\n[12,]     0     0     0     0     0     0     0     1\n[13,]     1     0     0     0     1     0     0     0\n[14,]     0     0     0     0     1     0     0     1\n[15,]     1     0     1     1     1     1     1     1\n[16,]     0     0     0     0     1     0     0     0\n[17,]     0     0     0     0     0     0     0     1\n[18,]     1     1     1     0     0     1     1     1\n[19,]     1     1     0     0     1     0     1     0\n[20,]     1     1     1     1     1     0     0     1\n[21,]     1     0     0     1     1     0     1     0\n\n\nRecall that in these data a tie goes from an advice seeker to an advisor. So the standard correlation distance on the columns computes the in-correlation, or structural equivalence based on incoming ties (two managers are equivalent if they are nominated as advisors by the same others).\nWe may also be interested in the out-correlation, that is structural equivalence based on out-going ties. Here two managers are structurally equivalent is they seek advice from the same others. This information is contained in the transpose of the original adjacency matrix:\n\n   A.t <- t(A)\n   A.t\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]\n [1,]    0    0    1    1    1    0    0    0    1     1     1     0     1\n [2,]    1    0    1    1    1    0    1    1    1     1     1     0     1\n [3,]    0    0    0    0    0    0    0    0    0     1     0     0     0\n [4,]    1    0    1    0    0    0    0    1    0     1     0     0     0\n [5,]    0    0    0    0    0    0    0    0    0     1     0     0     1\n [6,]    0    1    1    1    1    0    1    1    1     0     0     0     0\n [7,]    0    1    1    0    1    0    0    1    1     0     1     1     0\n [8,]    1    0    1    1    1    0    0    0    1     1     0     0     0\n [9,]    0    0    1    0    0    0    0    0    0     0     0     0     1\n[10,]    0    0    1    1    1    0    0    1    1     0     0     0     0\n[11,]    0    0    1    1    1    0    1    1    1     1     0     0     0\n[12,]    0    0    1    1    0    0    1    0    1     0     0     0     0\n[13,]    0    0    0    0    1    0    0    0    0     1     0     0     0\n[14,]    0    0    1    0    1    0    1    0    1     0     0     0     1\n[15,]    0    0    0    0    0    0    0    0    0     1     0     0     0\n[16,]    1    0    0    1    1    0    0    0    1     1     0     0     0\n[17,]    0    0    1    1    1    0    1    0    1     1     0     0     0\n[18,]    1    0    1    1    1    0    1    1    1     1     0     0     1\n[19,]    0    0    0    0    1    0    0    0    0     1     0     0     0\n[20,]    0    0    1    1    1    0    0    0    0     1     0     0     0\n[21,]    1    1    1    1    1    1    1    1    1     0     0     1     0\n      [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21]\n [1,]     0     1     1     1     1     1     1     0\n [2,]     1     1     1     1     1     1     1     1\n [3,]     0     1     0     0     1     1     0     1\n [4,]     0     1     0     1     1     0     0     1\n [5,]     0     1     0     0     1     1     0     0\n [6,]     0     1     0     0     0     0     1     1\n [7,]     1     1     0     1     1     1     0     1\n [8,]     0     1     0     0     1     0     1     1\n [9,]     0     1     0     0     1     0     0     0\n[10,]     0     1     1     0     1     1     0     0\n[11,]     0     1     0     0     1     1     1     0\n[12,]     0     1     0     0     0     0     1     1\n[13,]     0     1     0     0     1     0     0     0\n[14,]     0     1     0     0     1     1     1     1\n[15,]     0     0     0     0     1     1     1     0\n[16,]     0     1     0     0     1     0     1     0\n[17,]     0     1     0     0     0     0     1     1\n[18,]     1     1     1     0     0     1     1     1\n[19,]     0     1     0     0     1     0     0     0\n[20,]     0     1     0     0     1     1     0     1\n[21,]     1     1     0     1     1     0     1     0\n\n\nCorrelating the columns of this matrix would thus give us the out-correlation distance based on advice seeking relations.\n“Stacking” is a way to combine both in and out-going ties and compute a single distance based on both. It just means that we literally bind the rows of the firs matrix and its transpose:\n\n   A.stack <- rbind(A, A.t)\n   A.stack\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]\n [1,]    0    1    0    1    0    0    0    1    0     0     0     0     0\n [2,]    0    0    0    0    0    1    1    0    0     0     0     0     0\n [3,]    1    1    0    1    0    1    1    1    1     1     1     1     0\n [4,]    1    1    0    0    0    1    0    1    0     1     1     1     0\n [5,]    1    1    0    0    0    1    1    1    0     1     1     0     1\n [6,]    0    0    0    0    0    0    0    0    0     0     0     0     0\n [7,]    0    1    0    0    0    1    0    0    0     0     1     1     0\n [8,]    0    1    0    1    0    1    1    0    0     1     1     0     0\n [9,]    1    1    0    0    0    1    1    1    0     1     1     1     0\n[10,]    1    1    1    1    1    0    0    1    0     0     1     0     1\n[11,]    1    1    0    0    0    0    1    0    0     0     0     0     0\n[12,]    0    0    0    0    0    0    1    0    0     0     0     0     0\n[13,]    1    1    0    0    1    0    0    0    1     0     0     0     0\n[14,]    0    1    0    0    0    0    1    0    0     0     0     0     0\n[15,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[16,]    1    1    0    0    0    0    0    0    0     1     0     0     0\n[17,]    1    1    0    1    0    0    1    0    0     0     0     0     0\n[18,]    1    1    1    1    1    0    1    1    1     1     1     0     1\n[19,]    1    1    1    0    1    0    1    0    0     1     1     0     0\n[20,]    1    1    0    0    0    1    0    1    0     0     1     1     0\n[21,]    0    1    1    1    0    1    1    1    0     0     0     1     0\n[22,]    0    0    1    1    1    0    0    0    1     1     1     0     1\n[23,]    1    0    1    1    1    0    1    1    1     1     1     0     1\n[24,]    0    0    0    0    0    0    0    0    0     1     0     0     0\n[25,]    1    0    1    0    0    0    0    1    0     1     0     0     0\n[26,]    0    0    0    0    0    0    0    0    0     1     0     0     1\n[27,]    0    1    1    1    1    0    1    1    1     0     0     0     0\n[28,]    0    1    1    0    1    0    0    1    1     0     1     1     0\n[29,]    1    0    1    1    1    0    0    0    1     1     0     0     0\n[30,]    0    0    1    0    0    0    0    0    0     0     0     0     1\n[31,]    0    0    1    1    1    0    0    1    1     0     0     0     0\n[32,]    0    0    1    1    1    0    1    1    1     1     0     0     0\n[33,]    0    0    1    1    0    0    1    0    1     0     0     0     0\n[34,]    0    0    0    0    1    0    0    0    0     1     0     0     0\n[35,]    0    0    1    0    1    0    1    0    1     0     0     0     1\n[36,]    0    0    0    0    0    0    0    0    0     1     0     0     0\n[37,]    1    0    0    1    1    0    0    0    1     1     0     0     0\n[38,]    0    0    1    1    1    0    1    0    1     1     0     0     0\n[39,]    1    0    1    1    1    0    1    1    1     1     0     0     1\n[40,]    0    0    0    0    1    0    0    0    0     1     0     0     0\n[41,]    0    0    1    1    1    0    0    0    0     1     0     0     0\n[42,]    1    1    1    1    1    1    1    1    1     0     0     1     0\n      [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21]\n [1,]     0     0     1     0     1     0     0     1\n [2,]     0     0     0     0     0     0     0     1\n [3,]     1     0     0     1     1     0     1     1\n [4,]     0     0     1     1     1     0     1     1\n [5,]     1     0     1     1     1     1     1     1\n [6,]     0     0     0     0     0     0     0     1\n [7,]     1     0     0     1     1     0     0     1\n [8,]     0     0     0     0     1     0     0     1\n [9,]     1     0     1     1     1     0     0     1\n[10,]     0     1     1     1     1     1     1     0\n[11,]     0     0     0     0     0     0     0     0\n[12,]     0     0     0     0     0     0     0     1\n[13,]     1     0     0     0     1     0     0     0\n[14,]     0     0     0     0     1     0     0     1\n[15,]     1     0     1     1     1     1     1     1\n[16,]     0     0     0     0     1     0     0     0\n[17,]     0     0     0     0     0     0     0     1\n[18,]     1     1     1     0     0     1     1     1\n[19,]     1     1     0     0     1     0     1     0\n[20,]     1     1     1     1     1     0     0     1\n[21,]     1     0     0     1     1     0     1     0\n[22,]     0     1     1     1     1     1     1     0\n[23,]     1     1     1     1     1     1     1     1\n[24,]     0     1     0     0     1     1     0     1\n[25,]     0     1     0     1     1     0     0     1\n[26,]     0     1     0     0     1     1     0     0\n[27,]     0     1     0     0     0     0     1     1\n[28,]     1     1     0     1     1     1     0     1\n[29,]     0     1     0     0     1     0     1     1\n[30,]     0     1     0     0     1     0     0     0\n[31,]     0     1     1     0     1     1     0     0\n[32,]     0     1     0     0     1     1     1     0\n[33,]     0     1     0     0     0     0     1     1\n[34,]     0     1     0     0     1     0     0     0\n[35,]     0     1     0     0     1     1     1     1\n[36,]     0     0     0     0     1     1     1     0\n[37,]     0     1     0     0     1     0     1     0\n[38,]     0     1     0     0     0     0     1     1\n[39,]     1     1     1     0     0     1     1     1\n[40,]     0     1     0     0     1     0     0     0\n[41,]     0     1     0     0     1     1     0     1\n[42,]     1     1     0     1     1     0     1     0\n\n\nNote that this matrix has the same number of columns as the original adjacency matrix and double the number of rows. This doesn’t matter since the correlation distance works on the columns, meaning that it will return a matrix of the same dimensions as the original:\n\n   round(cor(A.stack), 2)\n\n       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7] [,8]  [,9] [,10] [,11] [,12]\n [1,]  1.00  0.43  0.00  0.09  0.09  0.22  0.14 0.37  0.13  0.25  0.37  0.22\n [2,]  0.43  1.00 -0.19  0.00 -0.19  0.49  0.24 0.38 -0.15 -0.24  0.51  0.52\n [3,]  0.00 -0.19  1.00  0.52  0.62 -0.24  0.19 0.33  0.57  0.00  0.03 -0.03\n [4,]  0.09  0.00  0.52  1.00  0.43 -0.03  0.29 0.33  0.57  0.10  0.03 -0.03\n [5,]  0.09 -0.19  0.62  0.43  1.00 -0.35  0.00 0.14  0.67  0.20  0.03 -0.15\n [6,]  0.22  0.49 -0.24 -0.03 -0.35  1.00  0.27 0.36 -0.16  0.00  0.50  0.74\n [7,]  0.14  0.24  0.19  0.29  0.00  0.27  1.00 0.19  0.24 -0.05  0.10  0.06\n [8,]  0.37  0.38  0.33  0.33  0.14  0.36  0.19 1.00  0.27  0.01  0.41  0.49\n [9,]  0.13 -0.15  0.57  0.57  0.67 -0.16  0.24 0.27  1.00  0.07  0.03  0.04\n[10,]  0.25 -0.24  0.00  0.10  0.20  0.00 -0.05 0.01  0.07  1.00  0.24 -0.11\n[11,]  0.37  0.51  0.03  0.03  0.03  0.50  0.10 0.41  0.03  0.24  1.00  0.49\n[12,]  0.22  0.52 -0.03 -0.03 -0.15  0.74  0.06 0.49  0.04 -0.11  0.49  1.00\n[13,]  0.17 -0.11  0.36  0.14  0.25 -0.08  0.11 0.19  0.22  0.17  0.32 -0.16\n[14,]  0.47  0.51  0.13  0.03  0.13  0.50  0.30 0.51  0.24  0.03  0.57  0.62\n[15,] -0.08 -0.48  0.63  0.25  0.63 -0.47 -0.19 0.07  0.42  0.18 -0.10 -0.25\n[16,]  0.38  0.21  0.14  0.24  0.14  0.22  0.00 0.62  0.12  0.15  0.56  0.18\n[17,]  0.37  0.40  0.13  0.03 -0.07  0.61  0.00 0.61  0.03  0.03  0.68  0.74\n[18,]  0.06  0.11 -0.03 -0.14  0.09  0.21 -0.45 0.15 -0.11  0.28  0.28  0.29\n[19,] -0.08 -0.25  0.38  0.18  0.38 -0.22 -0.05 0.26  0.30  0.28  0.21 -0.15\n[20,]  0.28  0.00  0.52  0.52  0.43  0.08  0.38 0.33  0.57  0.29  0.24  0.08\n[21,]  0.02  0.10 -0.04  0.06 -0.23  0.24  0.29 0.18  0.05 -0.02  0.24  0.17\n      [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21]\n [1,]  0.17  0.47 -0.08  0.38  0.37  0.06 -0.08  0.28  0.02\n [2,] -0.11  0.51 -0.48  0.21  0.40  0.11 -0.25  0.00  0.10\n [3,]  0.36  0.13  0.63  0.14  0.13 -0.03  0.38  0.52 -0.04\n [4,]  0.14  0.03  0.25  0.24  0.03 -0.14  0.18  0.52  0.06\n [5,]  0.25  0.13  0.63  0.14 -0.07  0.09  0.38  0.43 -0.23\n [6,] -0.08  0.50 -0.47  0.22  0.61  0.21 -0.22  0.08  0.24\n [7,]  0.11  0.30 -0.19  0.00  0.00 -0.45 -0.05  0.38  0.29\n [8,]  0.19  0.51  0.07  0.62  0.61  0.15  0.26  0.33  0.18\n [9,]  0.22  0.24  0.42  0.12  0.03 -0.11  0.30  0.57  0.05\n[10,]  0.17  0.03  0.18  0.15  0.03  0.28  0.28  0.29 -0.02\n[11,]  0.32  0.57 -0.10  0.56  0.68  0.28  0.21  0.24  0.24\n[12,] -0.16  0.62 -0.25  0.18  0.74  0.29 -0.15  0.08  0.17\n[13,]  1.00  0.20  0.26  0.51  0.20  0.05  0.63  0.36 -0.02\n[14,]  0.20  1.00 -0.10  0.34  0.57  0.16  0.11  0.24  0.14\n[15,]  0.26 -0.10  1.00  0.02 -0.10  0.08  0.34  0.25 -0.18\n[16,]  0.51  0.34  0.02  1.00  0.45  0.11  0.41  0.24  0.17\n[17,]  0.20  0.57 -0.10  0.45  1.00  0.40  0.11  0.24  0.14\n[18,]  0.05  0.16  0.08  0.11  0.40  1.00  0.18 -0.03 -0.32\n[19,]  0.63  0.11  0.34  0.41  0.11  0.18  1.00  0.28 -0.03\n[20,]  0.36  0.24  0.25  0.24  0.24 -0.03  0.28  1.00 -0.04\n[21,] -0.02  0.14 -0.18  0.17  0.14 -0.32 -0.03 -0.04  1.00\n\n\nAnd we would then do a blockmodel based on these distances:\n\n   blocks2 <- function(A, s = 2) {\n     colnames(A) <- 1:ncol(A) #use only if the original matrix has no names\n     B <- list(A)\n     for(i in 1:s) {\n       B <- unlist(lapply(B, con.cor), recursive = FALSE)\n       }\n     return(lapply(B, colnames))\n   }\n   blocks2(A.stack)\n\n[[1]]\n [1] \"1\"  \"2\"  \"6\"  \"8\"  \"11\" \"12\" \"14\" \"16\" \"17\" \"18\"\n\n[[2]]\n[1] \"7\"  \"21\"\n\n[[3]]\n[1] \"3\"  \"4\"  \"5\"  \"9\"  \"15\" \"20\"\n\n[[4]]\n[1] \"10\" \"13\" \"19\""
  },
  {
    "objectID": "handout4.html#structuraly-equivalence-in-multigraphs",
    "href": "handout4.html#structuraly-equivalence-in-multigraphs",
    "title": "Role Equivalence and Structural Similarity",
    "section": "Structuraly Equivalence in Multigraphs",
    "text": "Structuraly Equivalence in Multigraphs\nNote that we would apply the same trick if we wanted to do a blockmodel based on multiple relations like friendship and advice. Here’s a blockmodel based on the stacked matrices of incoming ties of both types:\n\n   A.f <- as.matrix(as_adjacency_matrix(ht_friends))\n   A.stack <- rbind(A, A.f)\n   blocks2(A.stack)\n\n[[1]]\n[1] \"1\"  \"3\"  \"5\"  \"14\" \"15\" \"20\"\n\n[[2]]\n[1] \"4\"  \"9\"  \"13\" \"19\"\n\n[[3]]\n[1] \"2\"  \"8\"  \"12\" \"16\" \"17\" \"18\"\n\n[[4]]\n[1] \"6\"  \"7\"  \"10\" \"11\" \"21\"\n\n\nAnd one combining incoming and outgoing friendship and advice ties:\n\n   A.stack <- rbind(A, A.t, A.f, t(A.f))\n   blocks2(A.stack)\n\n[[1]]\n[1] \"1\"  \"2\"  \"8\"  \"12\" \"16\" \"21\"\n\n[[2]]\n[1] \"6\"  \"11\" \"14\" \"17\"\n\n[[3]]\n[1] \"3\"  \"4\"  \"5\"  \"7\"  \"9\"  \"20\"\n\n[[4]]\n[1] \"10\" \"13\" \"15\" \"18\" \"19\"\n\n\nNote that here the stacked matrix has four sub-matrices: (1) Incoming advice, (2) outgoing advice, (3) incoming friendship, and (4) Outgoing friendship."
  },
  {
    "objectID": "handout5.html",
    "href": "handout5.html",
    "title": "Vertex Similarity",
    "section": "",
    "text": "As we noted in the role equivalence handout, for odd reasons, classical approaches to structural similarity in networks used distance based approaches but did not measure similarity directly. More recent approaches from network and information science prefer to define vertex similarity using direct similarity measures based on local structural characteristics, like node neighborhoods.\nMathematically, similarity is a less stringent (but also less well-defined) relation between pairs of nodes in a graph than distance, so it can be easier to work with in most applications.\nFor instance, similarity is required to be symmetric (\\(s_{ij} = s_{ji}\\) for all \\(i\\) and \\(j\\)) and most metrics have reasonable bounds (e.g., 0 for least similar and 1.0 for maximally similar). Given such a bounded similarity we can get to dissimilarity by subtracting one: \\(d_{ij} = 1 - s_{ij}\\)"
  },
  {
    "objectID": "handout5.html#basic-ingredients-of-vertex-similarity-metrics",
    "href": "handout5.html#basic-ingredients-of-vertex-similarity-metrics",
    "title": "Vertex Similarity",
    "section": "Basic Ingredients of Vertex Similarity Metrics",
    "text": "Basic Ingredients of Vertex Similarity Metrics\nConsider two nodes and the set of nodes that are the immediate neighbors to each. In this case, most vertex similarity measures will make use of three pieces of information:\n\nThe number of common neighbors \\(p\\).\nThe number of actors \\(q\\) who are connected to node \\(i\\) but not to node \\(j\\).\nThe number of actors \\(r\\) who are connected to node \\(j\\) but not to node \\(i\\).\n\nIn the simplest case of the binary undirected graph then these are given by:\n\\[\n   p = \\sum_{k = 1}^{n} a_{ik} a_{jk}\n\\]\n\\[\n   q = \\sum_{k = 1}^{n} a_{ik} (1 - a_{jk})\n\\]\n\\[\n   r = \\sum_{k = 1}^{n} (1- a_{ik}) a_{jk}\n\\]\nIn matrix form:\n\\[\n   \\mathbf{A}(p) = \\mathbf{A} \\mathbf{A} = \\mathbf{A}^2\n\\]\n\\[\n   \\mathbf{A}(q) = \\mathbf{A} (1 - \\mathbf{A})\n\\]\n\\[\n   \\mathbf{A}(r) = (1 - \\mathbf{A}) \\mathbf{A}\n\\]\nLet’s look at an example:\n\n   library(networkdata)\n   library(igraph)\n   g.flint <- movie_267\n   g.flint <- delete_vertices(g.flint, degree(g.flint) <= 3)\n   A <- as.matrix(as_adjacency_matrix(g.flint))\n   A.p <- A %*% A #common neighbors matrix\n   A.q <- A %*% (1 - A) #neighbors of i not connected to j\n   A.r <- (1 - A) %*% A #neighbors of j not connected to i\n   A.p[1:10, 1:10]\n\n             BAM-BAM BARNEY BETTY FELDSPAR FRED HEADMISTRESS HERDMASTER LAVA\nBAM-BAM            6      4     5        2    5            2          2    5\nBARNEY             4     13     9        1   11            5          3    7\nBETTY              5      9    13        2    9            3          4    8\nFELDSPAR           2      1     2        2    1            0          2    2\nFRED               5     11     9        1   14            4          3    9\nHEADMISTRESS       2      5     3        0    4            5          0    3\nHERDMASTER         2      3     4        2    3            0          4    4\nLAVA               5      7     8        2    9            3          4   11\nLEACH              2      3     3        1    2            2          1    2\nMORRIS             2      3     2        0    2            3          0    2\n             LEACH MORRIS\nBAM-BAM          2      2\nBARNEY           3      3\nBETTY            3      2\nFELDSPAR         1      0\nFRED             2      2\nHEADMISTRESS     2      3\nHERDMASTER       1      0\nLAVA             2      2\nLEACH            3      1\nMORRIS           1      3\n\n   A.q[1:10, 1:10]\n\n             BAM-BAM BARNEY BETTY FELDSPAR FRED HEADMISTRESS HERDMASTER LAVA\nBAM-BAM            0      2     1        4    1            4          4    1\nBARNEY             9      0     4       12    2            8         10    6\nBETTY              8      4     0       11    4           10          9    5\nFELDSPAR           0      1     0        0    1            2          0    0\nFRED               9      3     5       13    0           10         11    5\nHEADMISTRESS       3      0     2        5    1            0          5    2\nHERDMASTER         2      1     0        2    1            4          0    0\nLAVA               6      4     3        9    2            8          7    0\nLEACH              1      0     0        2    1            1          2    1\nMORRIS             1      0     1        3    1            0          3    1\n             LEACH MORRIS\nBAM-BAM          4      4\nBARNEY          10     10\nBETTY           10     11\nFELDSPAR         1      2\nFRED            12     12\nHEADMISTRESS     3      2\nHERDMASTER       3      4\nLAVA             9      9\nLEACH            0      2\nMORRIS           2      0\n\n   A.r[1:10, 1:10]\n\n             BAM-BAM BARNEY BETTY FELDSPAR FRED HEADMISTRESS HERDMASTER LAVA\nBAM-BAM            0      9     8        0    9            3          2    6\nBARNEY             2      0     4        1    3            0          1    4\nBETTY              1      4     0        0    5            2          0    3\nFELDSPAR           4     12    11        0   13            5          2    9\nFRED               1      2     4        1    0            1          1    2\nHEADMISTRESS       4      8    10        2   10            0          4    8\nHERDMASTER         4     10     9        0   11            5          0    7\nLAVA               1      6     5        0    5            2          0    0\nLEACH              4     10    10        1   12            3          3    9\nMORRIS             4     10    11        2   12            2          4    9\n             LEACH MORRIS\nBAM-BAM          1      1\nBARNEY           0      0\nBETTY            0      1\nFELDSPAR         2      3\nFRED             1      1\nHEADMISTRESS     1      0\nHERDMASTER       2      3\nLAVA             1      1\nLEACH            0      2\nMORRIS           2      0\n\n\nNote that while \\(\\mathbf{A}(p)\\) is necessarily symmetric, neither \\(q\\) nor \\(r\\) have to be. Barney has many more neighbors that Bam-Bam is not connected to than vice versa. Also note that the \\(\\mathbf{A}(r)\\) matrix is just the transpose of the \\(\\mathbf{A}(q)\\) matrix in the undirected case.\nSo the most obvious measure of similarity between two nodes is simply the number of common neighbors (Leicht, Holme, and Newman 2006):\n\\[\n   s_{ij} = p_{ij}\n\\]\nWe have already seen a version of this in the directed case when talking about the HITS algorithm (Kleinberg 1999), which computes a spectral (eigenvector-based) ranking based on the matrices of common in and out-neighbors in a directed graph.\n\\[\n   p^{in}_{ij} = \\sum_{k = 1}^{n} a_{ki} a_{kj}\n\\]\n\\[\n   p^{out}_{ij} = \\sum_{k = 1}^{n} a_{ik} a_{jk}\n\\]\nWhich in matrix form is:\n\\[\n   \\mathbf{A}(p^{out}) = \\mathbf{A}^T \\mathbf{A}\n\\]\n\\[\n   \\mathbf{A}(p^{in}) = \\mathbf{A} \\mathbf{A}^T\n\\]\nIn this case, similarity can be measured either by the number of common in-neighbors or the number of common out-neighbors.\nIf the network under consideration is a (directed) citation network with nodes being papers and links between papers defined as a citation from paper \\(i\\) to paper \\(j\\), then the number of common in-neighbors between two papers is their co-citation similarity (the number of other papers that cite both papers), and the number of common out-neighbors is their bibliographic coupling similarity (the overlap in their list of references).\nOne problem with using unbounded quantities like the sheer number of common (in or out) neighbors to define node similarity is that they are only limited by the number of nodes in the network (Leicht, Holme, and Newman 2006). Thus, an actor with many neighbors will end up having lots of other neighbors in common with lots of other nodes, which will mean we would count them as “similar” to almost everyone."
  },
  {
    "objectID": "handout5.html#normalized-similarity-metrics",
    "href": "handout5.html#normalized-similarity-metrics",
    "title": "Vertex Similarity",
    "section": "Normalized Similarity Metrics",
    "text": "Normalized Similarity Metrics\nNormalized similarity metrics deal with this issue by adjusting the raw similarity based on \\(p\\) using the number of non-shared neighbors \\(q\\) and \\(r\\).\nThe two most popular versions of normalized vertex similarity scores are the Jaccard index and the cosine similarity (sometimes also referred to as the Salton Index).\nThe Jacccard index is given by:\n\\[\n   s_{ij} = \\frac{p}{p + q + r}\n\\]\nWhich is the ratio of the size of the intersection of the neighborhoods of the two nodes (number of common neighbors) divided by the size of the union of the two neighborhoods.\nIn our example, this would be:\n\n   J <- A.p / (A.p + A.q + A.r)\n   round(J[1:10, 1:10], 2)\n\n             BAM-BAM BARNEY BETTY FELDSPAR FRED HEADMISTRESS HERDMASTER LAVA\nBAM-BAM         1.00   0.27  0.36     0.33 0.33         0.22       0.25 0.42\nBARNEY          0.27   1.00  0.53     0.07 0.69         0.38       0.21 0.41\nBETTY           0.36   0.53  1.00     0.15 0.50         0.20       0.31 0.50\nFELDSPAR        0.33   0.07  0.15     1.00 0.07         0.00       0.50 0.18\nFRED            0.33   0.69  0.50     0.07 1.00         0.27       0.20 0.56\nHEADMISTRESS    0.22   0.38  0.20     0.00 0.27         1.00       0.00 0.23\nHERDMASTER      0.25   0.21  0.31     0.50 0.20         0.00       1.00 0.36\nLAVA            0.42   0.41  0.50     0.18 0.56         0.23       0.36 1.00\nLEACH           0.29   0.23  0.23     0.25 0.13         0.33       0.17 0.17\nMORRIS          0.29   0.23  0.14     0.00 0.13         0.60       0.00 0.17\n             LEACH MORRIS\nBAM-BAM       0.29   0.29\nBARNEY        0.23   0.23\nBETTY         0.23   0.14\nFELDSPAR      0.25   0.00\nFRED          0.13   0.13\nHEADMISTRESS  0.33   0.60\nHERDMASTER    0.17   0.00\nLAVA          0.17   0.17\nLEACH         1.00   0.20\nMORRIS        0.20   1.00\n\n\nHere showing that Barney is more similar to Fred and Betty than he is to Bam-Bam.\nThe cosine similarity is given by:\n\\[\n   s_{ij} = \\frac{p}{\\sqrt{p + q} \\sqrt{p + r}}\n\\]\nWhich is the ratio of the number of common neighbors divided by the product of the square root of the degrees of each node (or the square root of the product which is the same thing), since \\(p\\) + \\(q\\) is the degree of node \\(i\\) and \\(p\\) + \\(r\\) is the degree of node \\(j\\).\nIn our example, this would be:\n\n   C <- A.p / (sqrt(A.p + A.q) * sqrt(A.p + A.r))\n   round(C[1:10, 1:10], 2)\n\n             BAM-BAM BARNEY BETTY FELDSPAR FRED HEADMISTRESS HERDMASTER LAVA\nBAM-BAM         1.00   0.45  0.57     0.58 0.55         0.37       0.41 0.62\nBARNEY          0.45   1.00  0.69     0.20 0.82         0.62       0.42 0.59\nBETTY           0.57   0.69  1.00     0.39 0.67         0.37       0.55 0.67\nFELDSPAR        0.58   0.20  0.39     1.00 0.19         0.00       0.71 0.43\nFRED            0.55   0.82  0.67     0.19 1.00         0.48       0.40 0.73\nHEADMISTRESS    0.37   0.62  0.37     0.00 0.48         1.00       0.00 0.40\nHERDMASTER      0.41   0.42  0.55     0.71 0.40         0.00       1.00 0.60\nLAVA            0.62   0.59  0.67     0.43 0.73         0.40       0.60 1.00\nLEACH           0.47   0.48  0.48     0.41 0.31         0.52       0.29 0.35\nMORRIS          0.47   0.48  0.32     0.00 0.31         0.77       0.00 0.35\n             LEACH MORRIS\nBAM-BAM       0.47   0.47\nBARNEY        0.48   0.48\nBETTY         0.48   0.32\nFELDSPAR      0.41   0.00\nFRED          0.31   0.31\nHEADMISTRESS  0.52   0.77\nHERDMASTER    0.29   0.00\nLAVA          0.35   0.35\nLEACH         1.00   0.33\nMORRIS        0.33   1.00\n\n\nShowing results similar (pun intended) to those obtained using the Jaccard index.\nA less commonly used option is the Dice Coefficient (sometimes called the Sorensen Index) given by:\n\\[\n   s_{ij} = \\frac{2p}{2p + q + r}\n\\]\nWhich is given by the ratio of twice the number of common neighbors divided by twice the same quantity plus the sum of the non-shared neighbors (and thus a variation of the Jaccard measure).\nIn our example, this would be:\n\n   D <- (2 * A.p) / ((2 * A.p) + A.q + A.r)\n   round(D[1:10, 1:10], 2)\n\n             BAM-BAM BARNEY BETTY FELDSPAR FRED HEADMISTRESS HERDMASTER LAVA\nBAM-BAM         1.00   0.42  0.53     0.50 0.50         0.36       0.40 0.59\nBARNEY          0.42   1.00  0.69     0.13 0.81         0.56       0.35 0.58\nBETTY           0.53   0.69  1.00     0.27 0.67         0.33       0.47 0.67\nFELDSPAR        0.50   0.13  0.27     1.00 0.12         0.00       0.67 0.31\nFRED            0.50   0.81  0.67     0.12 1.00         0.42       0.33 0.72\nHEADMISTRESS    0.36   0.56  0.33     0.00 0.42         1.00       0.00 0.38\nHERDMASTER      0.40   0.35  0.47     0.67 0.33         0.00       1.00 0.53\nLAVA            0.59   0.58  0.67     0.31 0.72         0.38       0.53 1.00\nLEACH           0.44   0.38  0.38     0.40 0.24         0.50       0.29 0.29\nMORRIS          0.44   0.38  0.25     0.00 0.24         0.75       0.00 0.29\n             LEACH MORRIS\nBAM-BAM       0.44   0.44\nBARNEY        0.38   0.38\nBETTY         0.38   0.25\nFELDSPAR      0.40   0.00\nFRED          0.24   0.24\nHEADMISTRESS  0.50   0.75\nHERDMASTER    0.29   0.00\nLAVA          0.29   0.29\nLEACH         1.00   0.33\nMORRIS        0.33   1.00\n\n\nOnce again, showing results comparable to the previous.\nNote, that all three of these pairwise measures of similarity are bounded between zero and one, with nodes being maximally similar to themselves and with pairs of distinct nodes being maximally similar when they have the same set of neighbors (e.g., they are structurally equivalent).\nLeicht, Holme, and Newman (2006) introduce a variation on the theme of normalized structural similarity scores. Their point is that maybe we should care about nodes that are surprisingly similar given some suitable null model. They propose the configuration model as such a null model. This model takes a graph with the same degree distribution as the original but with connections formed at random as reference.\nThe LHN similarity index (for Leicht, Holme, and Newman) is then given by:\n\\[\ns_{ij} = \\frac{p}{(p + q)(p + r)}\n\\]\nWhich can be seen as a variation of the cosine similarity defined earlier.\nIn our example, this would be:\n\n   D <- A.p / ((A.p + A.q) * (A.p + A.r))\n   round(D[1:10, 1:10], 2)\n\n             BAM-BAM BARNEY BETTY FELDSPAR FRED HEADMISTRESS HERDMASTER LAVA\nBAM-BAM         0.17   0.05  0.06     0.17 0.06         0.07       0.08 0.08\nBARNEY          0.05   0.08  0.05     0.04 0.06         0.08       0.06 0.05\nBETTY           0.06   0.05  0.08     0.08 0.05         0.05       0.08 0.06\nFELDSPAR        0.17   0.04  0.08     0.50 0.04         0.00       0.25 0.09\nFRED            0.06   0.06  0.05     0.04 0.07         0.06       0.05 0.06\nHEADMISTRESS    0.07   0.08  0.05     0.00 0.06         0.20       0.00 0.05\nHERDMASTER      0.08   0.06  0.08     0.25 0.05         0.00       0.25 0.09\nLAVA            0.08   0.05  0.06     0.09 0.06         0.05       0.09 0.09\nLEACH           0.11   0.08  0.08     0.17 0.05         0.13       0.08 0.06\nMORRIS          0.11   0.08  0.05     0.00 0.05         0.20       0.00 0.06\n             LEACH MORRIS\nBAM-BAM       0.11   0.11\nBARNEY        0.08   0.08\nBETTY         0.08   0.05\nFELDSPAR      0.17   0.00\nFRED          0.05   0.05\nHEADMISTRESS  0.13   0.20\nHERDMASTER    0.08   0.00\nLAVA          0.06   0.06\nLEACH         0.33   0.11\nMORRIS        0.11   0.33\n\n\nWhich, once again, produces similar results to what we found before. Note, however, that the LHN is not naturally maximal for self-similar nodes."
  },
  {
    "objectID": "handout5.html#similarity-and-structural-equivalence",
    "href": "handout5.html#similarity-and-structural-equivalence",
    "title": "Vertex Similarity",
    "section": "Similarity and Structural Equivalence",
    "text": "Similarity and Structural Equivalence\nAll normalized similarity measures bounded between zero and one (like Jaccard, Cosine, and Dice) also define a distance on each pair of nodes which is equal to one minus the similarity. So the cosine distance between two nodes is one minus the cosine similarity, and so on for the Jaccard and Dice indexes.\nBecause they define distances, this also means that these approaches can be used to find approximately structurally equivalent classes of nodes in a graph just like we did with the Euclidean and correlation distances.\nFor instance, consider our toy graph from before with four structurally equivalent sets of nodes:\n\n\n\n\n\nA toy graph demonstrating structural equivalence.\n\n\n\n\nThe cosine similarity matrix for this graph is:\n\n   A <- as.matrix(as_adjacency_matrix(g))\n   A.p <- A %*% A \n   A.q <- A %*% (1 - A) \n   A.r <- (1 - A) %*% A \n   C <- A.p / (sqrt(A.p + A.q) * sqrt(A.p + A.r))\n   round(C, 2)\n\n     A    B    C    D    E    F    G    H    I\nA 1.00 0.26 0.26 0.58 0.58 0.58 0.67 0.00 0.00\nB 0.26 1.00 1.00 0.00 0.00 0.00 0.26 0.67 0.67\nC 0.26 1.00 1.00 0.00 0.00 0.00 0.26 0.67 0.67\nD 0.58 0.00 0.00 1.00 1.00 1.00 0.58 0.25 0.25\nE 0.58 0.00 0.00 1.00 1.00 1.00 0.58 0.25 0.25\nF 0.58 0.00 0.00 1.00 1.00 1.00 0.58 0.25 0.25\nG 0.67 0.26 0.26 0.58 0.58 0.58 1.00 0.00 0.00\nH 0.00 0.67 0.67 0.25 0.25 0.25 0.00 1.00 0.75\nI 0.00 0.67 0.67 0.25 0.25 0.25 0.00 0.75 1.00\n\n\nNote that structurally equivalent nodes have similarity scores equal to 1.0. In this case, the distance matrix is given by:\n\n   D <- 1 - C\n\nAnd a hierarchical clustering on this matrix reveals the structurally equivalent classes:\n\n   D <- dist(D) \n   h.res <- hclust(D, method = \"ward.D2\")\n   plot(h.res)\n\n\n\n\nWe can package all that we said before into a handy function that takes a graph as input and returns all four normalized similarity metrics as output:\n\n   vertex.sim <- function(x) {\n      A <- as.matrix(as_adjacency_matrix(x))\n      A.p <- A %*% A \n      A.q <- A %*% (1 - A) \n      A.r <- (1 - A) %*% A \n      J <- A.p / (A.p + A.q + A.r)\n      C <- A.p / (sqrt(A.p + A.q) * sqrt(A.p + A.r))\n      D <- (2 * A.p) / ((2 * A.p) + A.q + A.r)\n      L <- A.p / ((A.p + A.q) * (A.p + A.r))\n      return(list(J = J, C = C, D = D, L = L))\n      }\n\nIn the Flintstones network, we could then find structurally equivalent blocks from the similarity analysis as follows (using Cosine):\n\n   g.flint <- movie_267\n   g.flint <- delete_vertices(g.flint, degree(g.flint) <= 3)\n   D <- dist(1- vertex.sim(g.flint)$C)\n   h.res <- hclust(D, method = \"ward.D2\") #hierarchical clustering\n   plot(h.res)\n\n\n\n\nWe can then extract our blocks just like we did with the Euclidean distance hierarchical clustering results in the previous handout:\n\n   blocks  <- cutree(h.res, k = 6)\n   blocks\n\n        BAM-BAM          BARNEY           BETTY        FELDSPAR            FRED \n              1               2               2               3               2 \n   HEADMISTRESS      HERDMASTER            LAVA           LEACH          MORRIS \n              4               3               2               5               4 \n      MRS SLATE         PEBBLES PEBBLES BAM-BAM        PILTDOWN      POINDEXTER \n              2               1               5               6               5 \n         PYRITE           SLATE           WILMA \n              6               2               2 \n\n\nThis analysis now puts \\(\\{Barney, Betty, Fred, Lava, Mrs. Slate, Slate, Wilma\\}\\) in the second block of equivalent actors (shown as the right-most cluster of actors in the dendrogram)."
  },
  {
    "objectID": "handout5.html#generalized-similarities",
    "href": "handout5.html#generalized-similarities",
    "title": "Vertex Similarity",
    "section": "Generalized Similarities",
    "text": "Generalized Similarities\nSo far, we have defined distances and similarities mainly as a function of the number of shared neighbors between two nodes. Structural equivalence is an idealized version of this, obtaining whenever people share exactly the same set of neighbors.\nYet, similarities based on local neighborhood information only have been criticized (e.g., by Borgatti and Everett (1992)) for not quite capturing the sociological intuition behind the idea of a role which is usually what they are deployed for. That is, two doctors don’t occupy the same role because they treat the same set of patients (in fact, this would be weird); instead, the set of doctors occupy the same role to the extent they treat a set of actors in the same (or similar) role: Namely, patients, regardless of whether those actors are literally the same or not.\nThis worry led social network analysts to try to generalize the concept of structural equivalence using less stringent algebraic definition, resulting in something of a proliferation of different equivalences such as automorphic or regular equivalence (Borgatti and Everett 1992).\nUnfortunately, none of this work led (unlike the work on structural equivalence) to useful or actionable tools for data analysis, with some even concluding that some definitions of equivalence (like regular equivalence) are unlikely to be found in any interesting substantive setting.\nA better approach here is to use the more relaxed notion of similarity like we did above to come up with a more general definition that can capture our role-based intuitions. Note that all the similarity notions studied earlier have structural equivalence as the limiting case of maximum similarity. So they are still based on the idea that nodes are similar to the extent they connect to the same others.\nWe can generalize this idea (to deal with the doctor/patient role problem) in the following way: nodes are similar to the extent they connect to similar others, with the restriction that we can only use endogenous (structural connectivity) information—like with structural equivalence or common-neighbor approaches—to define everyone’s similarity (no exogenous attribute stuff).\nAs Jeh and Widom (2002) note, this approach does lead to a coherent generalization of the idea of similarity for nodes in graphs because we can just define similarity recursively and iterate through the graph until the similarity scores stop changing.1 More specifically, they propose to measure the similarity between two nodes \\(i\\) and \\(j\\) at each time-step in the iteration using the formula:\n\\[\\begin{equation}\n   s_{ij} = \\frac{\\alpha}{d_i d_j} \\sum_{k \\in N(i)} \\sum_{l \\in N(j)} s_{kl}\n\\end{equation}\\]\nSo the similarity between two nodes is just the sum of the pairwise similarities between each of their neighbors (computed in the previous step), weighted by the ratio of a free parameter \\(\\alpha\\) (a number between zero and one) to the product of their degrees (to take a weighted average).\nThis measure nicely captures the idea that nodes are similar to the extent they both connect to people who are themselves similar. Note that it doesn’t matter whether these neighbors are shared between the two nodes (the summation occurs over each pair of nodes in \\(p\\) + \\(q\\) versus \\(p\\) + \\(r\\) as defined earlier) or whether the set of neighbors are themselves neighbors, which deals with the doctor/patient problem.\nA function that implements this idea looks like:\n\n   SimRank.in <- function(A, C = 0.8, iter = 10) {\n      d <- colSums(A)\n      n <- nrow(A)\n      S <- diag(1, n, n)\n      rownames(S) <- rownames(A)\n      colnames(S) <- colnames(A)\n      m <- 1\n      while(m < iter) {\n          for(i in 1:n) {\n               for(j in 1:n) {\n                    if (i < j) {\n                        a <- names(which(A[, i] == 1)) \n                        b <- names(which(A[, j] == 1)) \n                        Sij <- 0\n                        for (k in a) {\n                           for (l in b) {\n                              Sij <- Sij + S[k, l] #i's similarity to j\n                           }\n                        }\n                        S[i, j] <- C/(d[i] * d[j]) * Sij\n                        S[j, i] <- C/(d[i] * d[j]) * Sij\n                    }\n               }\n          }\n         m <- m + 1\n      }\n   return(S)\n}\n\nNote that this function calculates SimRank using each node’s in-neighbors (this doesn’t matter if the graph is undirected).\nLet’s try it out in the Flintstones graph using \\(\\alpha = 0.95\\):\n\n   A <- as.matrix(as_adjacency_matrix(g.flint))\n   S <- SimRank.in(A, 0.95)\n   round(S[, 1:5], 2)\n\n                BAM-BAM BARNEY BETTY FELDSPAR FRED\nBAM-BAM            1.00   0.46  0.47     0.52 0.47\nBARNEY             0.46   1.00  0.47     0.45 0.48\nBETTY              0.47   0.47  1.00     0.48 0.47\nFELDSPAR           0.52   0.45  0.48     1.00 0.46\nFRED               0.47   0.48  0.47     0.46 1.00\nHEADMISTRESS       0.47   0.48  0.46     0.44 0.48\nHERDMASTER         0.48   0.47  0.48     0.57 0.48\nLAVA               0.48   0.47  0.47     0.49 0.48\nLEACH              0.49   0.48  0.48     0.53 0.47\nMORRIS             0.49   0.48  0.47     0.44 0.47\nMRS SLATE          0.47   0.46  0.47     0.48 0.47\nPEBBLES            0.51   0.47  0.48     0.53 0.48\nPEBBLES BAM-BAM    0.49   0.48  0.48     0.48 0.48\nPILTDOWN           0.47   0.48  0.47     0.52 0.48\nPOINDEXTER         0.47   0.47  0.48     0.51 0.47\nPYRITE             0.47   0.48  0.47     0.52 0.48\nSLATE              0.47   0.47  0.48     0.50 0.48\nWILMA              0.47   0.47  0.48     0.48 0.47\n\n\nWe can transform the generalized similarities to distances and plot:\n\n   D <- dist(1- S)\n   h.res <- hclust(D, method = \"ward.D2\") #hierarchical clustering\n   plot(h.res)\n\n\n\n   blocks  <- cutree(h.res, k = 6)\n   blocks\n\n        BAM-BAM          BARNEY           BETTY        FELDSPAR            FRED \n              1               2               3               4               2 \n   HEADMISTRESS      HERDMASTER            LAVA           LEACH          MORRIS \n              5               4               3               6               5 \n      MRS SLATE         PEBBLES PEBBLES BAM-BAM        PILTDOWN      POINDEXTER \n              3               1               6               4               6 \n         PYRITE           SLATE           WILMA \n              4               3               3 \n\n\nWhich returns a somewhat different role partition than the metrics relying on structural equivalence. In the SimRank equivalence partition, \\(\\{Fred, Barney\\}\\) are in their own standalone class, while \\(\\{Betty, Wilma, Slate, Mrs. Slate, Lava\\}\\) appear as a separate cluster."
  },
  {
    "objectID": "handout5.html#global-similarity-indices",
    "href": "handout5.html#global-similarity-indices",
    "title": "Vertex Similarity",
    "section": "Global Similarity Indices",
    "text": "Global Similarity Indices\nAs we saw earlier, the most important ingredient of structural similarity measures between pairs of nodes is the number of common of neighbors (followed by the degrees of each node), and this quantity is given by the square of the adjacency matrix \\(\\mathbf{A}^2\\). So we can say that this matrix gives us a basic similarity measure between nodes, namely, the common neighbors similarity:\n\\[\n\\mathbf{S} = \\mathbf{A}^2\n\\]\nAnother way of seeing this is that a common neighbor defines a path of length two between a pair of nodes. So the number of common neighbors between two nodes is equivalent to the number of paths of length two between them. We are thus saying that the similarity between two nodes increases as the number of paths of length two between them increases, and that info is also recorded in the \\(\\mathbf{A}^2\\) matrix.\nBut if the similarity between node pairs increases in the number of paths of length two between them, wouldn’t nodes be even more similar if they also have a bunch of paths of length three between them?\nLü, Jin, and Zhou (2009) asked themselves the same question and proposed the following as a similarity metric based on paths:\n\\[\\begin{equation}\n\\mathbf{S} = \\mathbf{A}^2 + \\alpha \\mathbf{A}^3\n\\end{equation}\\]\nThis is the so-called local path similarity index (Lü, Jin, and Zhou 2009). Obviously, structurally equivalent nodes will be counted as similar by this metric (lots of paths of length two between them) but also nodes indirectly connected by many paths of length three, but to a lesser extent given the discount parameter \\(\\alpha\\) (a number between zero and one).\nA function that does this is:\n\n   local.path <- function(A, alpha = 0.5) {\n      A2 <- A %*% A\n      S <- A2 + alpha*(A2 %*% A)\n   return(S)\n   }\n\nHere’s how the local path similarity looks in the Flintstones network:\n\n   A <- as.matrix(as_adjacency_matrix(g.flint))\n   S <- local.path(A)\n   S[1:10, 1:10]\n\n             BAM-BAM BARNEY BETTY FELDSPAR FRED HEADMISTRESS HERDMASTER LAVA\nBAM-BAM         20.0   29.5  30.5      6.5 31.5         12.5        9.5 25.0\nBARNEY          29.5   50.0  51.0     13.0 52.0         21.0       21.0 46.5\nBETTY           30.5   51.0  52.0     11.0 53.0         24.0       18.0 46.0\nFELDSPAR         6.5   13.0  11.0      3.0 13.5          4.5        5.0 10.0\nFRED            31.5   52.0  53.0     13.5 55.0         21.5       21.5 47.5\nHEADMISTRESS    12.5   21.0  24.0      4.5 21.5         13.0        6.5 21.5\nHERDMASTER       9.5   21.0  18.0      5.0 21.5          6.5       10.0 17.0\nLAVA            25.0   46.5  46.0     10.0 47.5         21.5       17.0 42.0\nLEACH            9.5   15.5  16.0      3.5 17.5          7.5        5.5 15.5\nMORRIS           8.5   13.0  15.5      2.5 13.0          8.0        3.5 12.5\n             LEACH MORRIS\nBAM-BAM        9.5    8.5\nBARNEY        15.5   13.0\nBETTY         16.0   15.5\nFELDSPAR       3.5    2.5\nFRED          17.5   13.0\nHEADMISTRESS   7.5    8.0\nHERDMASTER     5.5    3.5\nLAVA          15.5   12.5\nLEACH          6.0    4.0\nMORRIS         4.0    6.0\n\n\nOf course as \\(\\alpha\\) approaches zero, then the local path measure reduces to the number of common neighbors, while numbers closer to one count paths of length three more.\nAnother thing people may wonder if why not keep going and add paths of length four:\n\\[\\begin{equation}\n\\mathbf{S} = \\mathbf{A}^2 + \\alpha \\mathbf{A}^3 + \\alpha^2 \\mathbf{A}^4\n\\end{equation}\\]\nOr paths of length whatever:\n\\[\n\\mathbf{S} = \\mathbf{A}^2 + \\alpha \\mathbf{A}^3 + \\alpha^2 \\mathbf{A}^4 ... + \\alpha^{k-2} \\mathbf{A}^k\n\\]\nWhere \\(k\\) is the length of the maximum path considered. Lü, Jin, and Zhou (2009) argue that these higher order paths don’t matter. Another issue is that there was already an all-paths similarity measure in existence, one developed by the mathematical social scientist Leo Katz (1953) in the 1950s (!).\nThe basic idea was to use linear algebra tricks to solve:\n\\[\n\\mathbf{S} = \\sum_{k=1}^{\\infty} \\alpha^k A^k\n\\]\nWhich would theoretically count the paths of all possible lengths between two nodes while discounting the contribution of the longer paths in proportion to their length (as \\(k\\) gets bigger, with \\(\\alpha\\) a number between zero and one, \\(\\alpha^k\\) gets smaller and smaller).\nLinear algebra hocus-pocus (non-technical explainer here) turns the above infinite sum into the more tractable:\n\\[\n\\mathbf{S} = (\\mathbf{I} - \\alpha A)^{-1}\n\\]\nWhere \\(\\mathbf{I}\\) is the identity matrix (a matrix of all zeros except that it has the number one in each diagonal cell) of the same dimensions as the original. Raising the result of the subtraction in parentheses to minus one indicates the matrix inverse operation (most matrices are invertible, unless they are weird).\nA function to compute the Katz similarity between all node pairs is:\n\n   katz.sim <- function(A, min.alpha = 0.05) {\n      I <- diag(nrow(A)) #creating identity matrix\n      alpha <- runif(1, min = min.alpha, max = 1/eigen(A)$values[1])\n      S <- solve(I - alpha * A) \n      return(S)\n   }\n\nFor technical reasons (e.g., guarantee that the infinite sum converges) we need to choose \\(\\alpha\\) to be a number larger than zero but smaller than the reciprocal of the first eigenvalue of the matrix. Hence we just pick a random value in that interval in line 3 (set the seed to get reproducible answers). Line 4 computes the actual Katz similarity using the native R function solve to find the relevant matrix inverse.2\nIn the Flintstones network the Katz similarity looks like:\n\n   set.seed(456)\n   S <- katz.sim(A)\n   round(S[1:10, 1:10], 2)\n\n             BAM-BAM BARNEY BETTY FELDSPAR FRED HEADMISTRESS HERDMASTER LAVA\nBAM-BAM         1.03   0.08  0.09     0.01 0.09         0.01       0.01 0.03\nBARNEY          0.08   1.07  0.11     0.07 0.12         0.03       0.08 0.10\nBETTY           0.09   0.11  1.07     0.01 0.11         0.08       0.02 0.11\nFELDSPAR        0.01   0.07  0.01     1.01 0.07         0.00       0.01 0.01\nFRED            0.09   0.12  0.11     0.07 1.07         0.02       0.08 0.11\nHEADMISTRESS    0.01   0.03  0.08     0.00 0.02         1.02       0.00 0.08\nHERDMASTER      0.01   0.08  0.02     0.01 0.08         0.00       1.02 0.02\nLAVA            0.03   0.10  0.11     0.01 0.11         0.08       0.02 1.06\nLEACH           0.01   0.02  0.02     0.00 0.07         0.01       0.01 0.07\nMORRIS          0.01   0.02  0.07     0.00 0.01         0.01       0.00 0.01\n             LEACH MORRIS\nBAM-BAM       0.01   0.01\nBARNEY        0.02   0.02\nBETTY         0.02   0.07\nFELDSPAR      0.00   0.00\nFRED          0.07   0.01\nHEADMISTRESS  0.01   0.01\nHERDMASTER    0.01   0.00\nLAVA          0.07   0.01\nLEACH         1.01   0.01\nMORRIS        0.01   1.01\n\n\nLeicht, Holme, and Newman (2006) argue that the Katz approach is fine and dandy as a similarity measure, but note that is an unweighted index (like the raw number of common neighbors). This means that nodes with large degree will end up being “similar” to a buch of other nodes in the graph, just because they have lots of paths of length whatever between them and those nodes.\nLeicht, Holme, and Newman (2006) propose a “fix” for this weakness in the Katz similarity, resulting in the matrix linear algebra equivalent of a degree-normalized similarity measure like the Jaccard or Cosine.\nSo instead of Katz they suggest we compute:\n\\[\n\\mathbf{S} = \\mathbf{D}^{-1} \\left( \\frac{\\alpha A}{\\lambda_1} \\right)^{-1} \\mathbf{D}^{-1}\n\\]\nHere \\(\\mathbf{D}\\) is a matrix containing the degrees of each node along the diagonal. The inverse of this matrix \\(\\mathbf{D}^{-1}\\) will contain the reciprocal of each degree \\(1/k_i\\) along the diagonals. \\(\\lambda_1\\), on the other hand, is just the first eigenvalue of the adjacency matrix.\nSo, the LHN Similarity is just the Katz similarity weighted by the degree of the sender and receiver node along each path, further discounting paths featuring high-degree nodes at either or both ends.\nWhich leads to the function:\n\n   LHN.sim <- function(A, alpha = 0.9) {\n      D <- solve(diag(rowSums(A))) #inverse of degree matrix\n      lambda <- eigen(A)$values[1] #first eigenvalue of adjacency matrix\n      S <- D %*% solve((alpha * A)/lambda) %*% D #LHN index\n      rownames(S) <- rownames(A)\n      colnames(S) <- colnames(A)\n      return(S)\n   }\n\nAnd the Flintstones result:\n\n   S <- LHN.sim(A)\n   round(S[, 1:5], 2)\n\n                BAM-BAM BARNEY BETTY FELDSPAR  FRED\nBAM-BAM           -0.09   0.01  0.01    -0.16 -0.01\nBARNEY             0.01  -0.02  0.00     0.11  0.01\nBETTY              0.01   0.00 -0.04    -0.48  0.00\nFELDSPAR          -0.16   0.11 -0.48    -2.55  0.25\nFRED              -0.01   0.01  0.00     0.25 -0.01\nHEADMISTRESS       0.20   0.05 -0.13    -2.96 -0.05\nHERDMASTER         0.16   0.08  0.10     0.07 -0.07\nLAVA               0.00  -0.02 -0.02    -0.42  0.02\nLEACH             -0.27  -0.10  0.04     2.01  0.09\nMORRIS            -0.25  -0.03  0.12     1.90  0.03\nMRS SLATE         -0.05   0.01  0.04     0.74 -0.01\nPEBBLES            0.19   0.01  0.01    -0.16 -0.01\nPEBBLES BAM-BAM    0.03   0.00  0.03     0.16  0.00\nPILTDOWN           0.00   0.00  0.00    -0.35  0.00\nPOINDEXTER         0.01   0.03  0.04     0.66 -0.02\nPYRITE             0.00   0.00  0.00    -0.35  0.00\nSLATE             -0.08  -0.01  0.04     0.91  0.01\nWILMA              0.01   0.00  0.02     0.09  0.00\n\n\nNote that as Leicht, Holme, and Newman (2006) discuss, the entries in the LHN version of the similarity \\(\\mathbf{S}\\) can be either positive or negative. Negative entries are nodes that are surprisingly dissimilar given their degrees, and positive numbers indicating node pairs that are surprisingly similar. Numbers closer to zero are nodes that are neither similar nor dissimilar given their degrees.\nHere we can see that Barney and Fred are actually not that similar to one another (after we take into account their very high degree) and that Barney is actually most similar to Feldspar (and Fred even more so).\nBecause the LHN similarities have negative and positive values, they already define a distance between nodes in the graph, like the correlation distance. So if we wanted to find blocks of actors using this similarity criterion, all we need to do is:\n\n   D <- dist(S)\n   h.res <- hclust(D, method = \"ward.D2\")\n   plot(h.res)\n\n\n\n   blocks <- cutree(h.res, k = 8)\n   blocks\n\n        BAM-BAM          BARNEY           BETTY        FELDSPAR            FRED \n              1               2               1               3               2 \n   HEADMISTRESS      HERDMASTER            LAVA           LEACH          MORRIS \n              4               5               1               6               7 \n      MRS SLATE         PEBBLES PEBBLES BAM-BAM        PILTDOWN      POINDEXTER \n              8               1               2               1               8 \n         PYRITE           SLATE           WILMA \n              1               8               2 \n\n\nIt seems like this approach is geared towards finding smaller, more fine-grained clusters of similar actors in the data."
  },
  {
    "objectID": "handout5.html#appendix",
    "href": "handout5.html#appendix",
    "title": "Vertex Similarity",
    "section": "Appendix",
    "text": "Appendix\nA more general version of the function to compute SimRank looks like this (partly based on Fouss, Saerens, and Shimbo 2016, 84, algorithm 2.4):\n\n   SimRank <- function(A, sigma = 0.0001, alpha = 0.8) {\n      n <- nrow(A)\n      d <- as.numeric(colSums(A) > 0)\n      e <- matrix(1, n, 1)\n      Q <- diag(as.vector(t(e) %*% A), n, n)\n      diag(Q) <- 1/diag(Q)\n      Q <- A %*% Q\n      Q[is.nan(Q)] <- 0\n      diff <- 1\n      k <- 1\n      K <- diag(1, n, n)\n      while(diff > sigma) {\n         K.old <- K\n         K.p <- alpha * t(Q) %*% K %*% Q\n         K <- K.p - diag(as.vector(diag(K.p)), n, n) + diag(d, n, n)\n         diff <- abs(sum(abs(K.old)) - sum(abs(K)))\n         k <- k + 1\n      }\n      rownames(K) <- rownames(A)\n      colnames(K) <- colnames(A)\n      return(K)\n   }"
  },
  {
    "objectID": "handout6.html",
    "href": "handout6.html",
    "title": "Community Structure",
    "section": "",
    "text": "What are communities? In networks, communities are subset of nodes that have more interactions or connectivity within themselves than they do outside of themselves (these are sometimes called “modules” outside of sociology). Communities thus exemplify the sociological concept of a group.\nA network has community structure if it contains many such subsets or groups of nodes that interact preferentially among themselves. Not all networks have to have community structure; a network in which all nodes interact with equal propensity doesn’t have communities.\nSo whether a network has community structure, and whether a given guess as to what these communities are yields actual communities (cluster of nodes that interact more among themselves than they do with outsiders) is an empirical question than needs to be answered with data.\nBut first, we need to develop a criterion for whether a given partition of the network into mutually exclusive node subsets is actually producing communities as we defined them earlier. This criterion should be independent of particular methods and algorithms that claim to find communities, so that way we can compare them with one another and see whether the partitions they recommend yield actual communities.\nMark Newman (2006b), who has done the most influential work in this area, proposed such a criterion and called it the modularity of a partition (e.g., the extent to which a partition has identified the “modules” or subsets of the network)."
  },
  {
    "objectID": "handout6.html#a-bag-of-links",
    "href": "handout6.html#a-bag-of-links",
    "title": "Community Structure",
    "section": "A bag of links",
    "text": "A bag of links\nAn intuitive way to understand the modularity is as follows. Imagine we have an idea of what the communities in a network are. This could be given by some special community partition method, our intuition, or some exogenous coloring on the nodes (e.g., as given by a node attribute like race, gender, position in the organization, etc.). The membership of each node in each community is thus stored in a vector, \\(C_i = k\\) if node \\(i\\) belongs to community \\(k\\).\nOur job is to decide whether that community partition is a good one. One way to proceed is to imagine that we take the network in question, and we throw each observed link into a bag. Then our modularity measure should give us an idea of the probability that if we drew a link at random, the nodes at the end of each link belong to same community (or have the same color if the communities are defined by exogenous attributes). If the probability is high, then the network has community structure. If the probability is no better than we would expect given a null model that says there is nothing going on in the network connectivity-wise except random chance, then the modularity should be low (and we decide that the partition we chose actually does not divide the network into meaningful communities).\nLet’s assume our bag of links is full of directed links and we draw a bunch of them at random. Let’s call the node at the starting end of the link \\(s\\) and the node at the destination end of the link \\(d\\). Then we can measure the modularity—let’s call it \\(Q\\)—of a given partition of the network into \\(m\\) clusters as follows:\n\\[\nQ = \\sum_{k = 1}^m \\left[P(s \\in k \\land d \\in k) - P(s \\in k) \\times P(d \\in k) \\right]\n\\]\nIn this equation, \\(P(s \\in k \\land d \\in k)\\) is the probability that a link drawn at random has a source and a destination node that belong to the same community \\(k\\); \\(P(s \\in k)\\) is just the probability of drawing a link that has a starting node in community \\(k\\) (regardless of the membership of the destination node) and \\(P(d \\in k)\\) is the probability of drawing a link that has a destination node that belongs to community \\(k\\) (regardless of the membership fo the source node).\nIf you remember your elementary probability theory, you know that the joint probability of two events that are assumed to be independent of one another is just the product of their individual probabilities. So that means that \\(P(s \\in k) \\times P(d \\in k)\\) is the expected probability of finding that both the source and destination node are in the same community \\(k\\) in a network in which communities don’t matter for link formation (because the two probabilities are assumed to be independent).\nSo the formula for the modularity subtracts the observed probability of finding links with two nodes in the same community from what we would expect if communities didn’t matter. So it measures the extent to which we find within-community links in the network beyond what we would expect by chance. Obviously, the higher this number, the higher the deviation from chance is, and the more community structure there is in the network.\nLet’s move to a real example. Below are two plots of the advice network from the law_friends data from the networkdata package. The first shows the nodes colored by status in the firm (partners versus associates) and the other shows the nodes colored by gender. It is pretty evident that there is more community by status than by gender; that is, friendship nominations tend to go from people of a given rank to people of the same rank, which makes sense. So a measure of modularity should be higher when using status as our partition than when using gender.\n\n\n\n\n\n\nFigure 1: Law Firm Friendship Network by Status\n\n\n\n\n\n\n\nFigure 2: Law Firm Friendship Network by Gender\n\n\n\n\n\nLet’s see an example of the modularity computed using our “bag of links” framework. Below is a quick function that uses dplyr code to take a graph as input and return and edge list data frame containing the “community membership” of each node by some characteristic given by the second input into the function.\n\n   link.bag <- function(x, c) {\n      library(dplyr)\n      g.el <- data.frame(as_edgelist(x))\n      names(g.el) <- c(\"vi\", \"vj\")\n      comm.dat <- data.frame(vi = as.vector(V(x)), \n                             vj = as.vector(V(x)), \n                             c = as.vector(c))\n      el.temp <- data.frame(vj = g.el[, 2]) %>% \n         left_join(comm.dat, by = \"vj\") %>% \n         dplyr::select(c(\"vj\", \"c\")) %>% \n         rename(c2 = c) \n      d.el <- data.frame(vi = g.el[, 1]) %>% \n         left_join(comm.dat, by = \"vi\") %>% \n         dplyr::select(c(\"vi\", \"c\")) %>% \n         rename(c1 = c) %>% \n         cbind(el.temp)\n   return(d.el)\n   }\n\nSo if we wanted an edge list data frame containing each node’s status membership in the law_advice network, we would just type:\n\n   link.dat <- link.bag(g, V(g)$status)\n   head(link.dat)\n\n  vi c1 vj c2\n1  1  1  2  1\n2  1  1  4  1\n3  1  1  8  1\n4  1  1 17  1\n5  2  1 16  1\n6  2  1 17  1\n\n\nNote than an edge list is already a “bag of links” so we can obtain the three probabilities we need to compute the modularity of a partition directly from the edge list data frame. Here’s a function that does this:\n\n   mod.Q1 <- function(x, c1 = 2, c2 = 4) {\n      Q <- 0\n      comms <- unique(x[, c1])\n      for (k in comms) {\n         e.same <- x[x[, c1] == k & x[, c2] == k, ]\n         e.sour <- x[x[, c1] == k, ]\n         e.dest <- x[x[, c2] == k, ]\n         e.total <- nrow(x)\n         p.same <- nrow(e.same)/e.total\n         p.sour <- nrow(e.sour)/e.total\n         p.dest <- nrow(e.dest)/e.total\n         Q <- Q + (p.same - (p.sour * p.dest))\n      }\n   return(Q)\n   }\n\nThis function takes the edge list data frame as input. Optionally, you can specify the two columns containing community membership info for the source and destination node in each link (in this case this happens to be the second and fourth columns). The Function works like this:\n\nThe probability of a link drawn randomly from the bag is just the number of links where the source and destination links belong to the same community (e.same, computed in line 5) divided by the total number of links (which is the number of rows in the edge list data frame), this ratio is computed in line 9 (p.same).\nThe overall probability of a link containing a source node in community \\(k\\) is computed in line 10 (p.sour), and the overall probability of a link containing a destination node in community \\(k\\) is computed in line 11 (p.dest).\nThe actual modularity is computed step by step by summation across levels of the community indicator variable in line 12 which is the sum of the difference between p.same and the product of p.sour times p.dest across all communities \\(m\\) (in this case \\(m = 2\\) as there are only two levels of status).\n\nSo if wanted to check if status was more powerful in structuring the community organization of the law_friends network than gender, we would just type:\n\n   mod.Q1(link.bag(g, V(g)$status))\n\n[1] 0.2715221\n\n   mod.Q1( link.bag(g, V(g)$gender))\n\n[1] 0.07495538\n\n\nWhich indeed confirms that status is a more powerful group formation principle than gender in this network."
  },
  {
    "objectID": "handout6.html#modularity-from-the-adjancency-matrix",
    "href": "handout6.html#modularity-from-the-adjancency-matrix",
    "title": "Community Structure",
    "section": "Modularity from the Adjancency Matrix",
    "text": "Modularity from the Adjancency Matrix\nNote that while the “bag of links” idea is good for showing the basic probabilistic principle behind the modularity in a network, we can compute directly from the adjacency matrix without going through the edge list data frame construction step.\nA function that does this looks like:\n\n   mod.Q2 <- function(x, c) {\n   A <- as.matrix(as_adjacency_matrix(x))\n   vol <- sum(A)\n   Q <- 0\n   for (k in unique(c)) {\n      A.sub <- A[which(c == k), which(c == k)]\n      vol.k <- sum(A.sub)\n      A.i <- A[which(c == k), ]\n      A.j <- A[, which(c == k)]\n      Q <- Q + ((vol.k/vol) - ((sum(A.i) * sum(A.j))/vol^2))\n      }\n   return(Q)\n   }\n\nThis function just takes a graph and a vector indicating the community membership of each node and returns the modularity as output. Like before the modularity is the difference in the probability of observing a within-community link between two nodes (given by the ratio of the number of links in the sub-adjacency matrix containing only within community-nodes—obtained in line 7—and the overall number of links in the adjacency matrix, computed in line 3) and the expected probability of a link between a source node with community membership \\(k\\) and a destination node with the same community membership.\nThis last quantity is given by the product of the sum links in a the sub-adjacency matrix with row nodes that belong to community \\(k\\) (the source nodes) and the number of links in the sub adjacency matrix with the column nodes belonging to community \\(k\\) (the destination nodes) divided by the square of the total number of links observed the network.\nIn formulese:\n\\[\\begin{equation}\nQ = \\sum_{k=1}^m \\left[\\frac{\\sum_{i \\in k} \\sum_{j \\in k} a_{ij}}{\\sum_i \\sum_j a_{ij}}  - \\frac{(\\sum_{i \\in k} \\sum_j a_{ij})(\\sum_i \\sum_{j \\in k} a_{ij})}{(\\sum_i \\sum_j a_{ij})^2} \\right]\n\\end{equation}\\]\nWhere \\(\\sum_i \\sum_ja_{ij}\\) is the sum of all the entries in the network adjacency matrix, \\(\\sum_{i \\in k} \\sum_{j \\in k} a_{ij}\\) is the sum of all the entries of the sub-adjacency matrix where both the row and column nodes come from community \\(k\\), \\(\\sum_{i \\in k} \\sum_j a_{ij}\\) is the sum of all the entries in the sub-adjacency matrix where only the row nodes come from community \\(k\\), and \\(\\sum_i \\sum_{j \\in k} a_{ij}\\) is the sum of all the entries in the sub-adjacency matrix where only the column nodes come from community \\(k\\).\nWe can easily see that this approach gives us the same answer as the bag of links version:\n\n   mod.Q2(g, V(g)$status)\n\n[1] 0.2715221\n\n   mod.Q2(g, V(g)$gender)\n\n[1] 0.07495538\n\n\nOf course, you don’t even have to use a custom function like the above, because igraph has one, called (you guessed it) modularity:\n\n   modularity(g, V(g)$status)\n\n[1] 0.2715221\n\n   modularity(g, V(g)$gender)\n\n[1] 0.07495538\n\n\nBut at least now you know what’s going on inside of it!"
  },
  {
    "objectID": "handout6.html#the-modularity-matrix",
    "href": "handout6.html#the-modularity-matrix",
    "title": "Community Structure",
    "section": "The Modularity Matrix",
    "text": "The Modularity Matrix\nObviously, the modularity wouldn’t be a famous method if it was just a way of measuring the goodness of a community partition produced by other methods. It itself can be used to find community partitions by using a method that somehow produces node partition that find the largest values that it can take in a graph.\nA useful tool in this quest is the modularity matrix \\(\\mathbf{B}\\) (Newman 2006b), which is defined as a variation on the adjacency matrix \\(\\mathbf{A}\\), each cell of the modularity matrix \\(b_{ij}\\) takes the value:\n\\[\nb_{ij} = a_{ij} - \\frac{k^{out}_ik^{in}_j}{\\sum_i\\sum_j a_{ij}}\n\\]\nWhere \\(k^{out}_i\\) is node \\(i\\)’s outdegree and \\(k^{in}_j\\) is node j’s indegree. Note that the modularity matrix has the same “observed minus expected” structure as the formulas for the modularity. In this case we compare whether we see a link from \\(i\\) to \\(j\\) as given by \\(a_{ij}\\) against the chances of observing a link in a graph in which nodes connect at random with probability proportional to their degrees (as given by the right-hand side fraction).\nNote that if the graph is undirected, the modularity matrix is just:\n\\[\nb_{ij} = a_{ij} - \\frac{k_ik_j}{\\sum_i\\sum_j a_{ij}}\n\\]\nA function to compute the modularity matrix by looping through every element of the adjacency matrix for a directed graph looks like:\n\n   mod.mat <- function(x){\n      A <- as.matrix(as_adjacency_matrix(x))\n      od <- rowSums(A) #outdegrees\n      id <- colSums(A) #indegrees\n      vol <- sum(A)\n      n <- nrow(A)\n      B <- matrix(0, n, n)\n      for (i in 1:n){\n         for (j in 1:n) {\n            B[i,j] <- A[i,j] - ((od[i]*id[j])/vol)\n         }\n      }\n   return(B)\n   }\n\nPeeking inside the resulting matrix:\n\n   round(mod.mat(g)[1:10, 1:10], 3)\n\n        [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]   [,8]   [,9]  [,10]\n [1,] -0.035  0.930 -0.028  0.902 -0.035 -0.014 -0.014  0.951 -0.098 -0.028\n [2,] -0.035 -0.070 -0.028 -0.098 -0.035 -0.014 -0.014 -0.049 -0.098 -0.028\n [3,]  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n [4,] -0.131  0.739  0.895 -0.366 -0.131 -0.052 -0.052 -0.183  0.634 -0.105\n [5,] -0.026 -0.052 -0.021 -0.073 -0.026 -0.010  0.990 -0.037 -0.073 -0.021\n [6,]  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n [7,] -0.017 -0.035  0.986 -0.049  0.983 -0.007 -0.007 -0.024 -0.049 -0.014\n [8,] -0.009 -0.017 -0.007 -0.024 -0.009 -0.003 -0.003 -0.012 -0.024 -0.007\n [9,] -0.052 -0.105 -0.042  0.854 -0.052 -0.021 -0.021 -0.073 -0.146 -0.042\n[10,] -0.122  0.756 -0.098 -0.341 -0.122 -0.049 -0.049  0.829  0.659 -0.098\n\n\nInterestingly, the matrix has some negative entries, some positive entries and some close to or actually zero. We interpret these as follows: If an entry is negative it means that a link between the row and column nodes is much less likely to happen than expected given the each node’s degrees, a positive entry indicates the opposite; a larger than expected chance of a link forming. Entries close to zero indicate those nodes have odds close to random chance of forming a tie.\nOf course igraph has a function, called modularity_matrix, which gives us the same result:\n\n   B <- modularity_matrix(g)\n   round(B[1:10, 1:10], 3)\n\n        [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]   [,8]   [,9]  [,10]\n [1,] -0.035  0.930 -0.028  0.902 -0.035 -0.014 -0.014  0.951 -0.098 -0.028\n [2,] -0.035 -0.070 -0.028 -0.098 -0.035 -0.014 -0.014 -0.049 -0.098 -0.028\n [3,]  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n [4,] -0.131  0.739  0.895 -0.366 -0.131 -0.052 -0.052 -0.183  0.634 -0.105\n [5,] -0.026 -0.052 -0.021 -0.073 -0.026 -0.010  0.990 -0.037 -0.073 -0.021\n [6,]  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n [7,] -0.017 -0.035  0.986 -0.049  0.983 -0.007 -0.007 -0.024 -0.049 -0.014\n [8,] -0.009 -0.017 -0.007 -0.024 -0.009 -0.003 -0.003 -0.012 -0.024 -0.007\n [9,] -0.052 -0.105 -0.042  0.854 -0.052 -0.021 -0.021 -0.073 -0.146 -0.042\n[10,] -0.122  0.756 -0.098 -0.341 -0.122 -0.049 -0.049  0.829  0.659 -0.098\n\n\nIn the case of an undirected graph, computing the modularity is even simpler:\n\n   mod.mat.u <- function(x){\n      A <- as.matrix(as_adjacency_matrix(x))\n      d <- rowSums(A) #degrees\n      vol <- sum(A)\n      n <- nrow(A)\n      B <- A - (d %*% t(d)/vol)\n   return(B)\n   }\n\nLet’s see the function in action:\n\n   ug <- as_undirected(g, mode = \"collapse\")\n   Bu <- mod.mat.u(ug)\n   round(Bu[1:10, 1:10], 2)\n\n       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10]\n [1,] -0.08  0.90 -0.04  0.78 -0.06 -0.02 -0.03  0.93 -0.14 -0.14\n [2,]  0.90 -0.13 -0.05  0.72 -0.08 -0.03 -0.04 -0.09 -0.18  0.82\n [3,] -0.04 -0.05 -0.02  0.89 -0.03 -0.01  0.98 -0.04 -0.07 -0.07\n [4,]  0.78  0.72  0.89 -0.61 -0.17 -0.06 -0.08 -0.19  0.61 -0.39\n [5,] -0.06 -0.08 -0.03 -0.17 -0.05 -0.02  0.98 -0.05 -0.11 -0.11\n [6,] -0.02 -0.03 -0.01 -0.06 -0.02 -0.01 -0.01 -0.02 -0.04 -0.04\n [7,] -0.03 -0.04  0.98 -0.08  0.98 -0.01 -0.01 -0.03 -0.05 -0.05\n [8,]  0.93 -0.09 -0.04 -0.19 -0.05 -0.02 -0.03 -0.06 -0.12  0.88\n [9,] -0.14 -0.18 -0.07  0.61 -0.11 -0.04 -0.05 -0.12 -0.25  0.75\n[10,] -0.14  0.82 -0.07 -0.39 -0.11 -0.04 -0.05  0.88  0.75 -0.25\n\n\nWhich gives us the same results as using igraph:\n\n   Bu <- modularity_matrix(ug, directed = FALSE)\n   round(Bu[1:10, 1:10], 2)\n\n       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10]\n [1,] -0.08  0.90 -0.04  0.78 -0.06 -0.02 -0.03  0.93 -0.14 -0.14\n [2,]  0.90 -0.13 -0.05  0.72 -0.08 -0.03 -0.04 -0.09 -0.18  0.82\n [3,] -0.04 -0.05 -0.02  0.89 -0.03 -0.01  0.98 -0.04 -0.07 -0.07\n [4,]  0.78  0.72  0.89 -0.61 -0.17 -0.06 -0.08 -0.19  0.61 -0.39\n [5,] -0.06 -0.08 -0.03 -0.17 -0.05 -0.02  0.98 -0.05 -0.11 -0.11\n [6,] -0.02 -0.03 -0.01 -0.06 -0.02 -0.01 -0.01 -0.02 -0.04 -0.04\n [7,] -0.03 -0.04  0.98 -0.08  0.98 -0.01 -0.01 -0.03 -0.05 -0.05\n [8,]  0.93 -0.09 -0.04 -0.19 -0.05 -0.02 -0.03 -0.06 -0.12  0.88\n [9,] -0.14 -0.18 -0.07  0.61 -0.11 -0.04 -0.05 -0.12 -0.25  0.75\n[10,] -0.14  0.82 -0.07 -0.39 -0.11 -0.04 -0.05  0.88  0.75 -0.25\n\n\nThe modularity matrix has some interesting properties. For instance, both its rows and columns sum to zero:\n\n   round(rowSums(Bu), 2)\n\n [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[39] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n\n   round(colSums(Bu), 2)\n\n [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[39] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n\n\nWhich makes \\(\\mathbf{B}\\) doubly centered a neat but so far useless property."
  },
  {
    "objectID": "handout6.html#using-the-modularity-matrix-to-find-the-modularity-of-a-partition",
    "href": "handout6.html#using-the-modularity-matrix-to-find-the-modularity-of-a-partition",
    "title": "Community Structure",
    "section": "Using the Modularity Matrix to Find the Modularity of a Partition",
    "text": "Using the Modularity Matrix to Find the Modularity of a Partition\nA more interesting property of the modularity matrix is that we can use it to compute the actual modularity of a given binary partition of the network into clusters. Let’s take status in the law_friends network as an example.\nFirst we create a new vector \\(\\mathbf{s}\\) that equals one when node \\(i\\) is in the first group (partners) and minus one when node \\(i\\) is in the second group (associates):\n\n   s <- V(g)$status\n   s[which(s==1)] <- 1\n   s[which(s==2)] <- -1\n   s\n\n [1]  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n[26]  1  1  1  1  1  1  1  1  1  1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n[51] -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n\n\nOnce we have this vector, the modularity is just:\n\\[\nQ = \\frac{\\mathbf{s}^T\\mathbf{B}\\mathbf{s}}{2\\sum_i\\sum_ja_{ij}}\n\\]\nWhich we can readily check in R:\n\n   A <- as.matrix(as_adjacency_matrix(g))\n   as.numeric(t(s) %*% B %*% s)/(2 * sum(A))\n\n[1] 0.2715221\n\n\nYou can think of the \\(\\mathbf{s}\\) vector as identifying the binary community membership via contrast coding.\nWe can also use the modularity matrix to find the modularity of a partition via dummy coding. To do this, we create two vectors for each level of the group membership variable with \\(u^{(1)}_i = 1\\) when when node \\(i\\) is a partner (otherwise \\(u^{(1)}_i = 0\\)), and \\(u^{(2)}_i = 1\\) when when node \\(i\\) is an associate (otherwise \\(u^{(2)}_i = 0\\)).\nIn R we can do this as follows:\n\n   u1 <- rep(0, length(V(g)$status))\n   u2 <- rep(0, length(V(g)$status))\n   u1[which(V(g)$status==1)] <- 1\n   u2[which(V(g)$status==2)] <- 1\n\nWe then join the two vectors into a matrix \\(\\mathbf{U}\\) of dimensions \\(n \\times 2\\):\n\n   U <- cbind(u1, u2)\n   head(U)\n\n     u1 u2\n[1,]  1  0\n[2,]  1  0\n[3,]  1  0\n[4,]  1  0\n[5,]  1  0\n[6,]  1  0\n\n   tail(U)\n\n      u1 u2\n[63,]  0  1\n[64,]  0  1\n[65,]  0  1\n[66,]  0  1\n[67,]  0  1\n[68,]  0  1\n\n\nOnce we have this matrix, the modularity of the partition is given by:\n\\[\n\\frac{tr(U^TBU)}{\\sum_i\\sum_ja_{ij}}\n\\]\nWhere \\(tr\\) denotes the trace operation in a matrix, namely, taking the sum of its diagonal elements.\nWe can check that this gives us the correct solution in R as follows:\n\n   sum(diag(t(U) %*% B %*% U))/sum(A)\n\n[1] 0.2715221\n\n\nNeat! It makes sense (given the name of the matrix) that we can compute the modularity from \\(\\mathbf{B}\\), and we can do it in multiple ways."
  },
  {
    "objectID": "handout6.html#using-the-modularity-matrix-to-find-communities",
    "href": "handout6.html#using-the-modularity-matrix-to-find-communities",
    "title": "Community Structure",
    "section": "Using the Modularity Matrix to Find Communities",
    "text": "Using the Modularity Matrix to Find Communities\nHere’s an even more useful property of the modularity matrix. Remember the network distribution game we played to defined our various reflective status measure in Handout 3?\n\n   status1 <- function(A) {\n      n <- nrow(A) #number of actors\n      x <- rep(1, n) #initial status vector set to all ones\n      w <- 1 \n      k <- 0 #initializing counter\n      while (w > 0.0001) {\n          o.x <- x #old status scores\n          x <- A %*% x #new scores a function of old scores and adjacency matrix\n          x <- x/norm(x, type = \"E\") #normalizing new status scores\n          w <- abs(sum(abs(x) - abs(o.x))) #diff. between new and old scores\n          k <- k + 1 #incrementing while counter\n      }\n   return(as.vector(x))\n   }\n\nWhat if we played it with the modularity matrix?\n\n   s <- status1(Bu)\n   round(s,2)\n\n [1] -0.10 -0.14 -0.02 -0.21 -0.03  0.00  0.01 -0.05 -0.21 -0.18 -0.16 -0.24\n[13] -0.13 -0.06 -0.04 -0.12 -0.24  0.08 -0.03 -0.17 -0.18 -0.15 -0.09 -0.14\n[25] -0.11 -0.17 -0.19  0.04 -0.10 -0.02  0.11  0.06  0.06 -0.06  0.10  0.01\n[37] -0.05  0.04 -0.05  0.12  0.19 -0.02  0.07  0.05  0.02  0.08  0.08  0.09\n[49]  0.17  0.00  0.20  0.06  0.20  0.13  0.20  0.09  0.08  0.05  0.06  0.02\n[61]  0.24  0.14  0.17  0.06  0.13  0.09  0.09  0.09\n\n\nThis seems more interesting. The resulting “status” vector has both negative and positive entries. What if I told you that this vector is actual a partition of the original graph into two communities?\nNot only that, I have even better news. This is the partition that maximizes the modularity in the graph, the best two-group partition that has the most edges within groups and the least edges going across groups.\nLet’s see some evidence for these claims in real data.\nFirst, let’s turn the “status” vector into a community indicator vector. We assign all nodes with scores larger than zero to one community and nodes with scores equal to zero or less to the other one:\n\n   C <- rep(0, length(s))\n   C[which(s<=0)] <- 1\n   C[which(s >0)] <- 2\n   names(C) <- 1:length(C)\n   C\n\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \n 1  1  1  1  1  2  2  1  1  1  1  1  1  1  1  1  1  2  1  1  1  1  1  1  1  1 \n27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 \n 1  2  1  1  2  2  2  1  2  2  1  2  1  2  2  1  2  2  2  2  2  2  2  2  2  2 \n53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 \n 2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2 \n\n\nAnd for the big check:\n\n   modularity(ug, C)\n\n[1] 0.2935153\n\n\nWhich is a pretty big score, at least larger than we obtained earlier using the exogenous indicator of status in the law firm as the basis for a binary partition. Figure 3 shows visual evidence that this is indeed an optimal binary partition of the law_friends network.\nOf course, you may also remember from the Status and Prestige handout that the little status game we played above is also a method (called the “power” method) of computing the leading eigenvector (the one associated with the largest eigenvalue) of a matrix. So turns out that the modularity maximizing bi-partition of a graph is just given by this vector:\n\\[\n\\mathbf{B}\\mathbf{v} = \\lambda\\mathbf{v}\n\\]\nSolving for \\(\\mathbf{v}\\) in the equation above will give us the community memberships of the nodes. In R you can do this using the native eigen function like we did for the status scores:\n\n   v <- eigen(Bu)$vectors[, 1] \n   round(v, 2)\n\n [1] -0.10 -0.14 -0.02 -0.21 -0.03  0.00  0.01 -0.05 -0.21 -0.18 -0.16 -0.24\n[13] -0.13 -0.06 -0.04 -0.12 -0.24  0.08 -0.03 -0.17 -0.18 -0.15 -0.09 -0.14\n[25] -0.11 -0.17 -0.19  0.04 -0.10 -0.02  0.11  0.06  0.06 -0.06  0.10  0.01\n[37] -0.05  0.04 -0.05  0.12  0.19 -0.02  0.07  0.05  0.02  0.08  0.08  0.09\n[49]  0.17  0.00  0.20  0.06  0.20  0.13  0.20  0.09  0.08  0.05  0.06  0.02\n[61]  0.24  0.14  0.17  0.06  0.13  0.09  0.09  0.09\n\n\nWhich look like the same values we obtained earlier.\nWe can package all of the steps above into a simple function:\n\n   split.two <- function(x) {\n      e <- eigen(x)$vectors[, 1]\n      v <- rep(0, length(e))\n      v[e<= 0] <- 1\n      v[e > 0] <- 2\n      names(v) <- 1:length(v)\n      c1 <- rep(0, length(v))\n      c2 <- rep(0, length(v))\n      c1[which(v == 1)] <- 1\n      c2[which(v == 2)] <- 1\n      dummy <- cbind(c1, c2)\n   return(list(v = v, dummy = dummy))\n   }\n\nAnd check to see if it works:\n\n   split.two(Bu)$v\n\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \n 1  1  1  1  1  2  2  1  1  1  1  1  1  1  1  1  1  2  1  1  1  1  1  1  1  1 \n27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 \n 1  2  1  1  2  2  2  1  2  2  1  2  1  2  2  1  2  2  2  2  2  2  2  2  2  2 \n53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 \n 2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2 \n\n\nIndeed it splits the nodes in a graph into two modularity maximizing communities.\nIt is clear that this approach hints at a divisive partitioning strategy, where we begin with two communities and then try to split one (or both) of those communities into two more like with CONCOR.\nHowever, unlike CONCOR, We don’t want to be unprincipled here because splitting a well-defined community into two just for the hell of it, can actually lead to worse modularity than keeping the original larger communities.\nSo we need an approach that tries out splitting one of the original two communities into two smaller ones, checks the new modularity, compares it with the old, and only goes ahead with the partition if the new modularity is larger than the old one.\nHere’s one way of doing that:\n\n   split.mult <- function(B, vol, u, v, s = 0.01) {\n      split <- list()\n      delta.Q <- rep(0, max(v))\n      for (i in 1:max(v)) {\n         old.Q <- t(u[, i]) %*% B %*% u[, i] #Using dummy coding to compute Q\n         sub <- which(u[, i] == 1)\n         new.B <- B[sub, sub]\n         new.u <- split.two(new.B)$dummy\n         new.v <- split.two(new.B)$v\n         Q1 <- t(new.u[, 1]) %*% new.B %*% new.u[, 1] #Using dummy coding to compute Q\n         Q2 <- t(new.u[, 2]) %*% new.B %*% new.u[, 2] #Using dummy coding to compute Q\n         new.Q <- Q1 + Q2\n         delta.Q[i] <- (new.Q - old.Q) * (1/vol)\n         if (delta.Q[i] > s) {\n            split[[i]] <- new.v\n            names(split[[i]]) <- which(u[, i] == 1)\n            }\n         else if (delta.Q[i] <= s) {\n            split[[i]] <- NULL\n            }\n      }\n   return(list(delta.Q = round(delta.Q, 3), v = v, split = split))\n   }\n\nThis function uses the output of the split.two function to check if any of the two sub-communities should be split in two further ones by comparing the modularities pre and post second partition.\nHere are the results applied to the law_friends network:\n\n   s <- split.mult(B = Bu, \n                   vol = sum(as.matrix(as_adjacency_matrix(ug))), \n                   u = split.two(Bu)$dummy, \n                   v = split.two(Bu)$v)\n   s\n\n$delta.Q\n[1] 0.000 0.048\n\n$v\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \n 1  1  1  1  1  2  2  1  1  1  1  1  1  1  1  1  1  2  1  1  1  1  1  1  1  1 \n27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 \n 1  2  1  1  2  2  2  1  2  2  1  2  1  2  2  1  2  2  2  2  2  2  2  2  2  2 \n53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 \n 2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2 \n\n$split\n$split[[1]]\nNULL\n\n$split[[2]]\n 6  7 18 28 31 32 33 35 36 38 40 41 43 44 45 46 47 48 49 50 51 52 53 54 55 56 \n 2  2  2  2  2  2  2  2  2  1  1  1  1  1  1  1  2  2  1  1  1  1  1  1  2  2 \n57 58 59 60 61 62 63 64 65 66 67 68 \n 2  1  1  2  1  1  1  1  1  1  1  1 \n\n\nThe delta.Q vector stores the results of our modularity checks. In this case, the test in the first community failed; the modularity didn’t change (the difference between the old and the new is zero) when we tried to split into two smaller communities. This is usually indicative that the original community was a well-defined group with lots of within-cluster ties so that splitting it makes no difference (that’s the orange nodes in the figure above).\nNevertheless, the test in the second community returned a positive result, indicating we should split it in two. Because we left one of the original communities alone, the resulting network will have three communities after splitting the second one in two.\nNote that the function returns the original two-split named vector of communities assignments v, a NULL result for the first split, and a new vector assigning nodes in the second original split to two other communities (split[[2]]).\nWe now create a new three-community vector from these results:\n\n   s$v[names(s$split[[2]])] <- s$split[[2]] + 1\n   s$v\n\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \n 1  1  1  1  1  3  3  1  1  1  1  1  1  1  1  1  1  3  1  1  1  1  1  1  1  1 \n27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 \n 1  3  1  1  3  3  3  1  3  3  1  2  1  2  2  1  2  2  2  2  3  3  2  2  2  2 \n53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 \n 2  2  3  3  3  2  2  3  2  2  2  2  2  2  2  2 \n\n\nFigure 4 shows the results of the three-community partition.\n\n\n\n\n\n\nFigure 3: Law Firm Friendship Network by The Best Binary Partition Using the Leading Eigenvector of the Modularity Matrix Approach\n\n\n\n\n\n\n\nFigure 4: Law Firm Friendship Network by The Best Three-Community Partition Using the Leading Eigenvector of the Modularity Matrix Approach\n\n\n\n\n\nNow let’s say we wanted to check if we should further split any of these three communities into two. All we need to do is create “dummy” variables for the new three community partition and feed these (and the three community membership vector) to the split.mult function, while keeping the original modularity matrix (Newman 2006a):\n\n   c1 <- rep(0, length(s$v))\n   c2 <- rep(0, length(s$v))\n   c3 <- rep(0, length(s$v))\n   c1[which(s$v==1)] <- 1\n   c2[which(s$v==2)] <- 1\n   c3[which(s$v==3)] <- 1\n   dummies <- cbind(c1, c2, c3)\n   s <- split.mult(B = Bu, \n                   vol = sum(as.matrix(as_adjacency_matrix(ug))), \n                   u = dummies, \n                   v = s$v)\n   s\n\n$delta.Q\n[1]  0.000  0.000 -0.002\n\n$v\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \n 1  1  1  1  1  3  3  1  1  1  1  1  1  1  1  1  1  3  1  1  1  1  1  1  1  1 \n27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 \n 1  3  1  1  3  3  3  1  3  3  1  2  1  2  2  1  2  2  2  2  3  3  2  2  2  2 \n53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 \n 2  2  3  3  3  2  2  3  2  2  2  2  2  2  2  2 \n\n$split\nlist()\n\n\nHere, the the three modularity split checks failed, as given by the delta.Q vector. So no further splits were made (hence the split object is an empty list). This tells us that according to this method, three communities is the optimal partition.\nOf course, if you are working on your project, you don’t have to mess with all of the functions above, because igraph has implemented Newman’s leading eigenvector (of the modularity matrix) community detection method (Newman 2006a) in a single function called cluster_leading_eigen.\nTo use it, just feed it the relevant graph:\n\n   le.res <- cluster_leading_eigen(ug)\n\nAnd examine the membership object stored in the results list:\n\n   names(le.res$membership) <- 1:length(le.res$membership)\n   le.res$membership\n\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \n 1  1  1  1  1  2  2  1  1  1  1  1  1  1  1  1  1  2  1  1  1  1  1  1  1  1 \n27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 \n 1  2  1  1  2  2  2  1  2  2  1  3  1  3  3  1  3  2  3  3  2  2  3  3  3  3 \n53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 \n 3  3  2  2  2  3  3  2  3  3  3  3  3  3  3  3 \n\n\nWhich returns identical community assignments as above (but with the labels between communities 2 and 3 flipped)."
  },
  {
    "objectID": "handout6.html#communities-versus-exogeneous-attributes",
    "href": "handout6.html#communities-versus-exogeneous-attributes",
    "title": "Community Structure",
    "section": "Communities versus Exogeneous Attributes",
    "text": "Communities versus Exogeneous Attributes\nRemember we began this handout using an exogenous criterion (law firm status as partner or associate) to examine the community structure of the network (which does pretty well according to the modularity). But now we have also used a pure link-based approach to finding communities. How do they compare? Here’s a way to find out:\n\n   library(kableExtra)\n   t <- table(V(ug)$status, le.res$membership)\n   kbl(t, row.names = TRUE, align = \"c\", format = \"html\") %>%\n      column_spec(1, bold = TRUE) %>% \n      kable_styling(full_width = TRUE,\n                     bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\n \n  \n      \n    1 \n    2 \n    3 \n  \n \n\n  \n    1 \n    27 \n    9 \n    0 \n  \n  \n    2 \n    3 \n    7 \n    22 \n  \n\n\n\n\n\nThis is just a contingency table listing status in the rows (1 = partner) and the three leading eigenvector communities in the columns.\nWe find that community one (the most coherent; in orange) is composed of 89% of partners. So it is a high status clique. Community three (in blue) on the other hand is composed of 100% of associates, a less dense, but completely (low) status homogeneous clique. Finally community two (green) is a status-mixed clique, composed of nine partners and seven associates.\nSo in this network, both status and endogenous group dynamics seem to be at play."
  },
  {
    "objectID": "handout6.html#an-agglomerative-approach-based-on-the-modularity",
    "href": "handout6.html#an-agglomerative-approach-based-on-the-modularity",
    "title": "Community Structure",
    "section": "An Agglomerative Approach Based on the Modularity",
    "text": "An Agglomerative Approach Based on the Modularity\nThe leading eigenvector approach discussed above is a divisive clustering algorithm. All nodes begin in one community, then we split in two, then we try to split one of those two into two other ones (if the modularity increases) and so forth.\nBut as we noted in the previous handout, another way to cluster is bottom up. All nodes begin in their own singleton cluster, and we join nodes if they maximize some criterion, in this case, the modularity. We then join new nodes to that group if they increase the modularity, otherwise we try other joins, merging communities as we go along, until we can’t increase the modularity any longer.\nThis agglomerative approach to clustering is not guaranteed to reach some kind of global maximum, but it can work in most applied settings even at the risk of getting stuck in a local optimum.\nNewman (2004) developed an agglomerative algorithm that optimizes the modularity, a more recent version of which is included in an igraph function called cluster_fast_greedy (Clauset, Newman, and Moore 2004).\nLet’s try it out to see how it works:\n\n   fg.res <- cluster_fast_greedy(ug)\n\nThe resulting list object has various sub-objects as components. One of them is called merges which gives a history of the various merges that resulted in the final communities. This means that we can view the results as a dendrogram just like we did with structural similarity analyses based on hierarchical clustering of a distance matrix.\nTo do this we just transform the resulting object into an hclust object and plot:\n\n   hc <- as.hclust(fg.res)\n   par(cex = 0.5) #smaller labels\n   plot(hc)\n\n\n\n\nWhich gives us the sense that the network is divided into three communities like we saw earlier. If wanted to see which nodes are in which, we could just cut the dendrogram at \\(k = 3\\):\n\n   library(dendextend)\n   cutree(hc, k = 3)\n\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \n 1  1  2  1  2  2  2  1  1  1  1  1  1  2  1  1  1  2  1  1  1  1  1  1  2  1 \n27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 \n 1  2  1  2  2  2  2  1  2  2  1  3  1  3  3  3  3  3  3  3  2  2  3  1  3  3 \n53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 \n 3  3  3  2  3  3  3  2  3  3  3  3  3  3  3  3 \n\n\nWhich provides the relevant assignment to communities (if we wanted more communities, we would cut the tree at a lower height).\nOf course, this information was already stored in our results in the membership object:\n\n   names(fg.res$membership) <- 1:68\n   fg.res$membership\n\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \n 1  1  2  1  2  2  2  1  1  1  1  1  1  2  1  1  1  2  1  1  1  1  1  1  2  1 \n27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 \n 1  2  1  2  2  2  2  1  2  2  1  3  1  3  3  3  3  3  3  3  2  2  3  1  3  3 \n53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 \n 3  3  3  2  3  3  3  2  3  3  3  3  3  3  3  3 \n\n\nFigure 5 visualizes the resulting three-community partition, which is pretty close to what we got using the leading eigenvector method."
  },
  {
    "objectID": "handout6.html#community-detection-using-edge-betweenness",
    "href": "handout6.html#community-detection-using-edge-betweenness",
    "title": "Community Structure",
    "section": "Community Detection Using Edge Betweenness",
    "text": "Community Detection Using Edge Betweenness\nA final approach to community detection with good sociological bona fides relies on edge betweenness, a concept we covered on the centrality handout. And edge has high betweenness if many other pairs of nodes need to use to reach one another via shortest paths.\nIt follows that if we iteratively identify high betweenness edges, removing the top one, recalculate the edge betweenness on the resulting edge-deleted subgraph, and remove that top one (or flip a coin if two are tied) we have an algorithm for identifying communities as those subgraphs that end up being disconnected after removing the bridges (Girvan and Newman 2002).\nNote that like the leading eigenvector method, this approach is divisive with all nodes starting in a single cluster, then that cluster splitting in two, and so forth. This algorithm is implemented in igraph in the function cluster_edge_betweenness.\nLet’s see how it works:\n\n   eb.res <- cluster_edge_betweenness(ug, directed = FALSE)\n\nInterestingly, even though this is a divisive algorithm, the results can be presented as a dendrogram, but storing top down divisions rather than bottom up merges:\n\n   hc <- as.hclust(eb.res)\n   par(cex = 0.5) #smaller labels\n   plot(hc)\n\n\n\n\nNote that the edge betweenness provides a different picture of the community structure of the network. This is not surprising because it is based on graph theoretic principles and not on null model principles like the modularity: The picture here is of two broad communities (in the middle) surrounded by many smaller islands, including some singletons.\nWe can, of course, combine the modularity and edge-betweenness criteria by trying out dendrogram cuts until we find one with the highest modularity.\nA simple function that does that looks like:\n\n   top.mod <- function(x, g, m) {\n      k <- 1:m\n      Q <- rep(0, m)\n      tab <- cbind(k, Q)\n      for (i in 2:m) {\n         c <- cutree(x, i)\n         tab[i, 2] <- round(modularity(g, c, directed = FALSE), 3)\n      }\n   return(tab)\n   }\n   top.mod(hc, ug, 25)\n\n       k     Q\n [1,]  1 0.000\n [2,]  2 0.000\n [3,]  3 0.067\n [4,]  4 0.069\n [5,]  5 0.073\n [6,]  6 0.073\n [7,]  7 0.072\n [8,]  8 0.071\n [9,]  9 0.070\n[10,] 10 0.167\n[11,] 11 0.166\n[12,] 12 0.181\n[13,] 13 0.178\n[14,] 14 0.183\n[15,] 15 0.179\n[16,] 16 0.183\n[17,] 17 0.182\n[18,] 18 0.188\n[19,] 19 0.193\n[20,] 20 0.194\n[21,] 21 0.192\n[22,] 22 0.188\n[23,] 23 0.208\n[24,] 24 0.201\n[25,] 25 0.195\n\n\nWhich seems to recommend we divide the graph into 23 communities! This is, of course, what is stored in the membership object:\n\n   names(eb.res$membership) <- 1:68\n   eb.res$membership\n\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \n 1  1  2  1  3  4  2  1  1  1  1  1  1  5  6  1  1  3  7  1  1  1  1  1  1  1 \n27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 \n 1  3  1  8  1  3  3  1  3  1  1  1  1  1  1  1  9  7 10 11  3  3 12 13 14 15 \n53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 \n16 17  3 18  7 19 20 21 22  1 23 23 23 23 23 23 \n\n\nAnother thing we can do with the edge_betweenness function output is that we can highlight the edges identified as bridges during the divisive process, as this information is stored in the object bridges. We can use this vector of edge ids to create an edge color vector that will highlight the bridges in red.\n\n\n\n\n   set.seed(123)\n   V(ug)$color <- fg.res$membership\n   plot(ug, \n     vertex.size=6, vertex.frame.color=\"lightgray\", \n     vertex.label = NA, edge.arrow.size = 0.25,\n     vertex.label.dist=1, edge.curved=0.2)   \n   \n   set.seed(123)\n   V(ug)$color <- c25[eb.res$membership]\n   e.col <- rep(\"lightblue\", ecount(ug))\n   e.col[eb.res$bridges] <- \"red\"\n   E(ug)$color <- e.col\n   plot(ug, \n     vertex.size=6, vertex.frame.color=\"lightgray\", \n     vertex.label = NA, edge.arrow.size = 0.25,\n     vertex.label.dist=1, edge.curved=0.2)\n\n\n\n\n\n\nFigure 5: Law Firm Friendship Network Partitioned According to Agglomerative Clustering Based on Optimizing the Modularity\n\n\n\n\n\n\n\nFigure 6: Law Firm Friendship Network Partitioned According to the Edge Betweenness Algorithm with Number of Communities that Maximize the Modularity (Bridge Edges in Red)\n\n\n\n\n\n\nThe resulting twenty-three-community partition based on the edge betweenness is shown in Figure 6."
  },
  {
    "objectID": "handout7.html",
    "href": "handout7.html",
    "title": "Analyzing Two-Mode Networks",
    "section": "",
    "text": "This handout deals with the network analysis of two-mode networks. Note that in the literature there is some terminological slippage. Two-mode networks are a type of social network. By definition two-mode networks can be represented using rectangular adjacency matrices (sometimes called affiliation matrices in sociology).\nIn this case, two-mode networks fall under the general category of “two-mode data.” Any data set that has information on two types of objects (e.g., people and variables) is two-mode data so two-mode networks are just a special case of two-mode data.\nIn this sense, a useful distinction, due to Borgatti & Everett, is useful. This is that between the “modes” and the “ways” of a data matrix. So most data matrices are two-ways, in that they have at least two dimensions (e.g., the row and column dimensions).\nBut some data matrices (like the usual adjacency matrix in regular network data) only collect information on a single type of entity, so they are “one mode, two ways.” But sometimes we have network data on two sets of objects, in which case, we use a data matrix that has “two-modes” (sets of nodes) and “two ways” (rows and columns).\nSo what makes a network a “two-mode network”? Well, a two-mode network is different from a regular network, because it has two sets of nodes not just one. So instead of \\(V\\) now we have \\(V_1\\) and \\(V_2\\). Moreover, the edges in a two-mode network only go from nodes in one set to nodes in the other set; there are no within-node-set edges."
  },
  {
    "objectID": "handout7.html#bipartite-graphs",
    "href": "handout7.html#bipartite-graphs",
    "title": "Analyzing Two-Mode Networks",
    "section": "Bipartite Graphs",
    "text": "Bipartite Graphs\nThis restriction makes the graph that represents a two-mode network a special kind of graph called a bipartite graph. A graph is bipartite if the set of nodes in the graph can be divided into two groups, such that relations go from nodes in one set to nodes in the other set.\nNote that bipartite graphs can be be used to represent both two-mode and regular one mode networks, as long as the above condition holds. For instance, a dating network with 100% heterosexual people in it will yield a bipartite graph based on the dating relation, with men in one set and women on the other node set, even though it’s a one-mode network.\nSo whether or not a graph is bipartite is something you can check for.\nLet’s see how that works. Let us load the most famous two-mode network data set (kind of the Drosophila of two-mode network analysis; one of the most repeatedly analyzed social structures in history: For a classic sampling of such analyses see here) a network composed of eighteen women from the social elite of a tiny town in the south in the 1930s who attended fourteen social events (Breiger 1974):\n\n   library(igraph)\n   library(networkdata)\n   g <- southern_women\n\nNow we already know this is a bipartite graph. However, let’s say you are new and you’ve never heard of these data. You can check whether the graph you loaded up is bipartite or not by using the igraph function is_bipartite:\n\n   is_bipartite(g)\n\n[1] TRUE\n\n\nWhich returns TRUE as an answer. Had we loaded up any old non-bipartite graph, the answer would have been:\n\n   g.whatever <- movie_45\n   is_bipartite(g.whatever)\n\n[1] FALSE\n\n\nWhich makes sense because that’s just a regular old graph.\nNote that if we check the bipartite graph object, it looks like any other igraph object:\n\n   g\n\nIGRAPH 1074643 UN-B 32 89 -- \n+ attr: type (v/l), name (v/c)\n+ edges from 1074643 (vertex names):\n [1] EVELYN   --6/27 EVELYN   --3/2  EVELYN   --4/12 EVELYN   --9/26\n [5] EVELYN   --2/25 EVELYN   --5/19 EVELYN   --9/16 EVELYN   --4/8 \n [9] LAURA    --6/27 LAURA    --3/2  LAURA    --4/12 LAURA    --2/25\n[13] LAURA    --5/19 LAURA    --3/15 LAURA    --9/16 THERESA  --3/2 \n[17] THERESA  --4/12 THERESA  --9/26 THERESA  --2/25 THERESA  --5/19\n[21] THERESA  --3/15 THERESA  --9/16 THERESA  --4/8  BRENDA   --6/27\n[25] BRENDA   --4/12 BRENDA   --9/26 BRENDA   --2/25 BRENDA   --5/19\n[29] BRENDA   --3/15 BRENDA   --9/16 CHARLOTTE--4/12 CHARLOTTE--9/26\n+ ... omitted several edges\n\n\nBut we can tell that the graph is a two-mode network because we have links starting with people with old lady names from the 1930s (which are also the names of a bunch of kids in middle school in 2024) and ending with events that have dates in them. So the (undirected) edge is \\(person-event\\).\nThe graph is undirected because the “membership” or “attendance” relation between a person and an organization/event doesn’t have a natural directionality.\nAnother way of checking the “bipartiteness” of a graph in igraph is by using the bipartite_mapping function.\nLet’s see what it does:\n\n   bipartite_mapping(g)\n\n$res\n[1] TRUE\n\n$type\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n    FALSE     FALSE     FALSE     FALSE     FALSE     FALSE     FALSE     FALSE \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n    FALSE     FALSE     FALSE     FALSE     FALSE     FALSE     FALSE     FALSE \n   OLIVIA     FLORA      6/27       3/2      4/12      9/26      2/25      5/19 \n    FALSE     FALSE      TRUE      TRUE      TRUE      TRUE      TRUE      TRUE \n     3/15      9/16       4/8      6/10      2/23       4/7     11/21       8/3 \n     TRUE      TRUE      TRUE      TRUE      TRUE      TRUE      TRUE      TRUE \n\n\nThis function takes the candidate bipartite graph as input and returns to objects: res is just a check to see if the graph is actually bipartite (TRUE in this case), type is a logical vector of dimensions \\(M + N\\) (where \\(M\\) is the number of nodes in the person set and \\(N\\) is the number of nodes in the event set) dividing the nodes into two groups. Here people get FALSE and events get TRUE, but this designations are arbitrary (a kind of dummy coding; FALSE = 0 and TRUE = 1).\nWe can add this as a node attribute to our graph so that way we know which node is in which set:\n\n   V(g)$type <- bipartite_mapping(g)$type"
  },
  {
    "objectID": "handout7.html#the-affiliation-matrix",
    "href": "handout7.html#the-affiliation-matrix",
    "title": "Two Mode Networks",
    "section": "The Affiliation Matrix",
    "text": "The Affiliation Matrix\nOnce you have your bipartite graph loaded up, you may want (if the graph is small enough) to check out the graph’s affiliation matrix \\(A\\).\nThis works just like before, except that now we use the as_biadjacency_matrix function:\n\n   A <- as.matrix(as_biadjacency_matrix(g))\n   A\n\n          6/27 3/2 4/12 9/26 2/25 5/19 3/15 9/16 4/8 6/10 2/23 4/7 11/21 8/3\nEVELYN       1   1    1    1    1    1    0    1   1    0    0   0     0   0\nLAURA        1   1    1    0    1    1    1    1   0    0    0   0     0   0\nTHERESA      0   1    1    1    1    1    1    1   1    0    0   0     0   0\nBRENDA       1   0    1    1    1    1    1    1   0    0    0   0     0   0\nCHARLOTTE    0   0    1    1    1    0    1    0   0    0    0   0     0   0\nFRANCES      0   0    1    0    1    1    0    1   0    0    0   0     0   0\nELEANOR      0   0    0    0    1    1    1    1   0    0    0   0     0   0\nPEARL        0   0    0    0    0    1    0    1   1    0    0   0     0   0\nRUTH         0   0    0    0    1    0    1    1   1    0    0   0     0   0\nVERNE        0   0    0    0    0    0    1    1   1    0    0   1     0   0\nMYRNA        0   0    0    0    0    0    0    1   1    1    0   1     0   0\nKATHERINE    0   0    0    0    0    0    0    1   1    1    0   1     1   1\nSYLVIA       0   0    0    0    0    0    1    1   1    1    0   1     1   1\nNORA         0   0    0    0    0    1    1    0   1    1    1   1     1   1\nHELEN        0   0    0    0    0    0    1    1   0    1    1   1     0   0\nDOROTHY      0   0    0    0    0    0    0    1   1    0    0   0     0   0\nOLIVIA       0   0    0    0    0    0    0    0   1    0    1   0     0   0\nFLORA        0   0    0    0    0    0    0    0   1    0    1   0     0   0\n\n\nIn this matrix we list one set of nodes in the rows and the other set is in the columns. Each cell \\(a_{ij} = 1\\) if row node \\(i\\) is affiliated with column node \\(j\\), otherwise \\(a_{ij} = 0\\).\nNote that if we were to use the regular as_adjacency_matrix function on a bipartite graph, we get a curious version of the adjacency matrix:\n\n   B <- as.matrix(as_adjacency_matrix(g))\n   B\n\n          EVELYN LAURA THERESA BRENDA CHARLOTTE FRANCES ELEANOR PEARL RUTH\nEVELYN         0     0       0      0         0       0       0     0    0\nLAURA          0     0       0      0         0       0       0     0    0\nTHERESA        0     0       0      0         0       0       0     0    0\nBRENDA         0     0       0      0         0       0       0     0    0\nCHARLOTTE      0     0       0      0         0       0       0     0    0\nFRANCES        0     0       0      0         0       0       0     0    0\nELEANOR        0     0       0      0         0       0       0     0    0\nPEARL          0     0       0      0         0       0       0     0    0\nRUTH           0     0       0      0         0       0       0     0    0\nVERNE          0     0       0      0         0       0       0     0    0\nMYRNA          0     0       0      0         0       0       0     0    0\nKATHERINE      0     0       0      0         0       0       0     0    0\nSYLVIA         0     0       0      0         0       0       0     0    0\nNORA           0     0       0      0         0       0       0     0    0\nHELEN          0     0       0      0         0       0       0     0    0\nDOROTHY        0     0       0      0         0       0       0     0    0\nOLIVIA         0     0       0      0         0       0       0     0    0\nFLORA          0     0       0      0         0       0       0     0    0\n6/27           1     1       0      1         0       0       0     0    0\n3/2            1     1       1      0         0       0       0     0    0\n4/12           1     1       1      1         1       1       0     0    0\n9/26           1     0       1      1         1       0       0     0    0\n2/25           1     1       1      1         1       1       1     0    1\n5/19           1     1       1      1         0       1       1     1    0\n3/15           0     1       1      1         1       0       1     0    1\n9/16           1     1       1      1         0       1       1     1    1\n4/8            1     0       1      0         0       0       0     1    1\n6/10           0     0       0      0         0       0       0     0    0\n2/23           0     0       0      0         0       0       0     0    0\n4/7            0     0       0      0         0       0       0     0    0\n11/21          0     0       0      0         0       0       0     0    0\n8/3            0     0       0      0         0       0       0     0    0\n          VERNE MYRNA KATHERINE SYLVIA NORA HELEN DOROTHY OLIVIA FLORA 6/27 3/2\nEVELYN        0     0         0      0    0     0       0      0     0    1   1\nLAURA         0     0         0      0    0     0       0      0     0    1   1\nTHERESA       0     0         0      0    0     0       0      0     0    0   1\nBRENDA        0     0         0      0    0     0       0      0     0    1   0\nCHARLOTTE     0     0         0      0    0     0       0      0     0    0   0\nFRANCES       0     0         0      0    0     0       0      0     0    0   0\nELEANOR       0     0         0      0    0     0       0      0     0    0   0\nPEARL         0     0         0      0    0     0       0      0     0    0   0\nRUTH          0     0         0      0    0     0       0      0     0    0   0\nVERNE         0     0         0      0    0     0       0      0     0    0   0\nMYRNA         0     0         0      0    0     0       0      0     0    0   0\nKATHERINE     0     0         0      0    0     0       0      0     0    0   0\nSYLVIA        0     0         0      0    0     0       0      0     0    0   0\nNORA          0     0         0      0    0     0       0      0     0    0   0\nHELEN         0     0         0      0    0     0       0      0     0    0   0\nDOROTHY       0     0         0      0    0     0       0      0     0    0   0\nOLIVIA        0     0         0      0    0     0       0      0     0    0   0\nFLORA         0     0         0      0    0     0       0      0     0    0   0\n6/27          0     0         0      0    0     0       0      0     0    0   0\n3/2           0     0         0      0    0     0       0      0     0    0   0\n4/12          0     0         0      0    0     0       0      0     0    0   0\n9/26          0     0         0      0    0     0       0      0     0    0   0\n2/25          0     0         0      0    0     0       0      0     0    0   0\n5/19          0     0         0      0    1     0       0      0     0    0   0\n3/15          1     0         0      1    1     1       0      0     0    0   0\n9/16          1     1         1      1    0     1       1      0     0    0   0\n4/8           1     1         1      1    1     0       1      1     1    0   0\n6/10          0     1         1      1    1     1       0      0     0    0   0\n2/23          0     0         0      0    1     1       0      1     1    0   0\n4/7           1     1         1      1    1     1       0      0     0    0   0\n11/21         0     0         1      1    1     0       0      0     0    0   0\n8/3           0     0         1      1    1     0       0      0     0    0   0\n          4/12 9/26 2/25 5/19 3/15 9/16 4/8 6/10 2/23 4/7 11/21 8/3\nEVELYN       1    1    1    1    0    1   1    0    0   0     0   0\nLAURA        1    0    1    1    1    1   0    0    0   0     0   0\nTHERESA      1    1    1    1    1    1   1    0    0   0     0   0\nBRENDA       1    1    1    1    1    1   0    0    0   0     0   0\nCHARLOTTE    1    1    1    0    1    0   0    0    0   0     0   0\nFRANCES      1    0    1    1    0    1   0    0    0   0     0   0\nELEANOR      0    0    1    1    1    1   0    0    0   0     0   0\nPEARL        0    0    0    1    0    1   1    0    0   0     0   0\nRUTH         0    0    1    0    1    1   1    0    0   0     0   0\nVERNE        0    0    0    0    1    1   1    0    0   1     0   0\nMYRNA        0    0    0    0    0    1   1    1    0   1     0   0\nKATHERINE    0    0    0    0    0    1   1    1    0   1     1   1\nSYLVIA       0    0    0    0    1    1   1    1    0   1     1   1\nNORA         0    0    0    1    1    0   1    1    1   1     1   1\nHELEN        0    0    0    0    1    1   0    1    1   1     0   0\nDOROTHY      0    0    0    0    0    1   1    0    0   0     0   0\nOLIVIA       0    0    0    0    0    0   1    0    1   0     0   0\nFLORA        0    0    0    0    0    0   1    0    1   0     0   0\n6/27         0    0    0    0    0    0   0    0    0   0     0   0\n3/2          0    0    0    0    0    0   0    0    0   0     0   0\n4/12         0    0    0    0    0    0   0    0    0   0     0   0\n9/26         0    0    0    0    0    0   0    0    0   0     0   0\n2/25         0    0    0    0    0    0   0    0    0   0     0   0\n5/19         0    0    0    0    0    0   0    0    0   0     0   0\n3/15         0    0    0    0    0    0   0    0    0   0     0   0\n9/16         0    0    0    0    0    0   0    0    0   0     0   0\n4/8          0    0    0    0    0    0   0    0    0   0     0   0\n6/10         0    0    0    0    0    0   0    0    0   0     0   0\n2/23         0    0    0    0    0    0   0    0    0   0     0   0\n4/7          0    0    0    0    0    0   0    0    0   0     0   0\n11/21        0    0    0    0    0    0   0    0    0   0     0   0\n8/3          0    0    0    0    0    0   0    0    0   0     0   0\n\n\nThis matrix is of dimensions \\((M + N) \\times (M + N)\\), which is \\((18 + 14) \\times (18 + 14) = 32 \\times 32\\) in the Southern Women data.\nThe adjacency matrix of a bipartite graph necessarily has to big “blocks” of zeroes in it corresponding to where the links between nodes in the same set would be (but aren’t because this is a two-mode network).\nSo the regular adjacency matrix of a bipartite graph \\(\\mathbf{B}\\) has the following form (Fouss, Saerens, and Shimbo 2016, 12):\n\\[\n\\mathbf{B} = \\left[\n\\begin{matrix}\n\\mathbf{O}_{M \\times M} & \\mathbf{A}_{M \\times N} \\\\\n\\mathbf{A}^T_{N \\times M} & \\mathbf{O}_{N \\times N}\n\\end{matrix}\n\\right]\n\\]\nWhere \\(\\mathbf{O}\\) is just the all zeros matrix of the relevant dimensions, and \\(\\mathbf{A}\\) is the bi-adjacency (affiliation) matrix as defined earlier."
  },
  {
    "objectID": "handout7.html#basic-two-mode-network-statistics",
    "href": "handout7.html#basic-two-mode-network-statistics",
    "title": "Analyzing Two-Mode Networks",
    "section": "Basic Two-Mode Network Statistics",
    "text": "Basic Two-Mode Network Statistics\nWe can calculate some basic network statistics from the affiliation (bi-adjacency) matrix. We have two number of nodes to calculate, but only one quantity for the number of edges.\nThe number of nodes on the people side \\(N\\) is just the number of rows of \\(A\\):\n\n   nrow(A)\n\n[1] 18\n\n\nAnd the number of events/groups \\(M\\) is just the number of columns:\n\n   ncol(A)\n\n[1] 14\n\n\nFinally, the number of edges \\(E\\) is just the sum of all the entries of \\(A\\):\n\n   sum(A)\n\n[1] 89\n\n\nNote that if you were to use the igraph function vcount on the original graph object, you get the wrong answer:\n\n   vcount(g)\n\n[1] 32\n\n\nThat’s because vcount is working with the \\(32 \\times 32\\) regular adjacency matrix, not the bi-adjacency matrix. Here, vcount is returning the total number of nodes in the graph summing across the two sets, which is \\(M + N\\).\nIf you wanted to get the right answer for each set of edges from the regular igraph graph object, you could use the type node attribute we defined earlier along with the subgraph function:\n\n   vcount(subgraph(g, V(g)$type == FALSE))\n\n[1] 18\n\n\nWhich gives us the number of women. For the events we do the same thing:\n\n   vcount(subgraph(g, V(g)$type == TRUE))\n\n[1] 14\n\n\nHowever, because there’s only one set of edges, ecount still gives us the right answer:\n\n   ecount(g)\n\n[1] 89\n\n\nWhich is the same as:\n\n   sum(A)\n\n[1] 89\n\n\n\nDegree Statistics\nBecause we have two sets of degrees, all the basic degree statistics in the network double up. So we have two mean degrees, two maximum degrees, and two minimum degree to take care of:\n\n   mean.d.p <- mean(rowSums(A))\n   mean.d.g <- mean(colSums(A))\n   max.d.p <- max(rowSums(A))\n   max.d.g <- max(colSums(A))\n   min.d.p <- min(rowSums(A))\n   min.d.g <- min(colSums(A))\n\nSo we have:\n\n   round(mean.d.p, 1)\n\n[1] 4.9\n\n   round(mean.d.g, 1)\n\n[1] 6.4\n\n   max.d.p\n\n[1] 8\n\n   max.d.g\n\n[1] 14\n\n   min.d.p\n\n[1] 2\n\n   min.d.g\n\n[1] 3\n\n\nHowever, note that because there’s only one set of undirected edges, the total number of edges incident to each node in each of the two sets is always going to be the same.\nThat means that there’s only one sum of degrees. So the sum of degrees for people:\n\n   sum(rowSums(A))\n\n[1] 89\n\n\nIs the same as the sum of degrees of events:\n\n   sum(colSums(A))\n\n[1] 89\n\n\nNote that in a bipartite graph, therefore, the sum of degrees of nodes in each node set is equal to the \\(|E|\\), the number of edges in the graph!\n\n\nDensity\nAs we saw in the case of one-mode networks, one of the most basic network statistics that can be derived from the above quantities is the density (observed number of edges divided by maximum possible number of edges in the graph).\nIn a two-mode network, density is given by:\n\\[\nd = \\frac{|E|}{N \\times M}\n\\]\nWhere \\(|E|\\) is the number of edges in the network. In our case we can compute the density as follows:\n\n   d <- sum(A)/(nrow(A) * ncol(A))\n   d\n\n[1] 0.3531746"
  },
  {
    "objectID": "handout7.html#degree-centrality",
    "href": "handout7.html#degree-centrality",
    "title": "Analyzing Two-Mode Networks",
    "section": "Degree Centrality",
    "text": "Degree Centrality\nIn a two-mode network, there are two degree sets, each corresponding to one set of nodes. For the people, in this case, their degree (centrality) is just the number of events they attend, and for the groups, it’s just the number of people that attend each event.\nAs we have already seen, we can get each from the affiliation matrix. The degree of the people are just the row sums:\n\n   rowSums(A)\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n        8         7         8         7         4         4         4         3 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n        4         4         4         6         7         8         5         2 \n   OLIVIA     FLORA \n        2         2 \n\n\nAnd the degree of the events are just the column sums:\n\n   colSums(A)\n\n 6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10  2/23   4/7 11/21 \n    3     3     6     4     8     8    10    14    12     5     4     6     3 \n  8/3 \n    3 \n\n\nThe igraph function degree will also give us the right answer, but in the form of a single vector including both people and events:\n\n   degree(g)\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n        8         7         8         7         4         4         4         3 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n        4         4         4         6         7         8         5         2 \n   OLIVIA     FLORA      6/27       3/2      4/12      9/26      2/25      5/19 \n        2         2         3         3         6         4         8         8 \n     3/15      9/16       4/8      6/10      2/23       4/7     11/21       8/3 \n       10        14        12         5         4         6         3         3 \n\n\nAs Borgatti and Everett (1997) note, if we want normalized degree centrality measures, we need to divide by either \\(M\\) (for people) or \\(N\\) (for events). That is, for people we use the number of events as the norm (as this is the theoretical maximum) and for events the number of people.\nSo for people, normalized degree is:\n\n   round(rowSums(A)/ncol(A), 3)\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n    0.571     0.500     0.571     0.500     0.286     0.286     0.286     0.214 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n    0.286     0.286     0.286     0.429     0.500     0.571     0.357     0.143 \n   OLIVIA     FLORA \n    0.143     0.143 \n\n\nAnd for events:\n\n   round(colSums(A)/nrow(A), 3)\n\n 6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10  2/23   4/7 11/21 \n0.167 0.167 0.333 0.222 0.444 0.444 0.556 0.778 0.667 0.278 0.222 0.333 0.167 \n  8/3 \n0.167 \n\n\nOr with igraph:\n\n   round(degree(g)/c(rep(14, 18), rep(18, 14)), 3)\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n    0.571     0.500     0.571     0.500     0.286     0.286     0.286     0.214 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n    0.286     0.286     0.286     0.429     0.500     0.571     0.357     0.143 \n   OLIVIA     FLORA      6/27       3/2      4/12      9/26      2/25      5/19 \n    0.143     0.143     0.167     0.167     0.333     0.222     0.444     0.444 \n     3/15      9/16       4/8      6/10      2/23       4/7     11/21       8/3 \n    0.556     0.778     0.667     0.278     0.222     0.333     0.167     0.167"
  },
  {
    "objectID": "handout7.html#geodesic-distances",
    "href": "handout7.html#geodesic-distances",
    "title": "Analyzing Two-Mode Networks",
    "section": "Geodesic Distances",
    "text": "Geodesic Distances\nGeodesic distances work a bit different in two-mode networks because of the only between-node-sets edges restriction.\nFor instance, the minimum geodesic distance \\(g_{ii'}\\) between two people is two (a person cannot be adjacent to another person), but it is one between a person and a group (if the person is a member of the group).\nIn the same way, a group \\(g\\) cannot be at geodesic distance less than three from a person \\(p*\\) who is not a member, because the shortest path is \\(g-p-g^*-p^*\\).\nThat is, there has to be some other group \\(g^*\\) shared between a member \\(p\\) of the focal group \\(g\\) and another person \\(p^*\\) for the shortest path between \\(g\\) and the non-member \\(p^*\\) to exist, and that involves three links at minimum: \\(g-p\\), \\(p-g^*\\), and \\(g^*-p^*\\). This means that the links in paths in two-mode networks always alternate between persons and group nodes.\nBeyond that geodesic distances work the same way. In igraph when we use the distances function on a bipartite graph, we get:\n\n   D.pg <- distances(g)\n   head(D.pg)\n\n          EVELYN LAURA THERESA BRENDA CHARLOTTE FRANCES ELEANOR PEARL RUTH\nEVELYN         0     2       2      2         2       2       2     2    2\nLAURA          2     0       2      2         2       2       2     2    2\nTHERESA        2     2       0      2         2       2       2     2    2\nBRENDA         2     2       2      0         2       2       2     2    2\nCHARLOTTE      2     2       2      2         0       2       2     4    2\nFRANCES        2     2       2      2         2       0       2     2    2\n          VERNE MYRNA KATHERINE SYLVIA NORA HELEN DOROTHY OLIVIA FLORA 6/27 3/2\nEVELYN        2     2         2      2    2     2       2      2     2    1   1\nLAURA         2     2         2      2    2     2       2      4     4    1   1\nTHERESA       2     2         2      2    2     2       2      2     2    3   1\nBRENDA        2     2         2      2    2     2       2      4     4    1   3\nCHARLOTTE     2     4         4      2    2     2       4      4     4    3   3\nFRANCES       2     2         2      2    2     2       2      4     4    3   3\n          4/12 9/26 2/25 5/19 3/15 9/16 4/8 6/10 2/23 4/7 11/21 8/3\nEVELYN       1    1    1    1    3    1   1    3    3   3     3   3\nLAURA        1    3    1    1    1    1   3    3    3   3     3   3\nTHERESA      1    1    1    1    1    1   1    3    3   3     3   3\nBRENDA       1    1    1    1    1    1   3    3    3   3     3   3\nCHARLOTTE    1    1    1    3    1    3   3    3    3   3     3   3\nFRANCES      1    3    1    1    3    1   3    3    3   3     3   3\n\n   tail(D.pg)\n\n      EVELYN LAURA THERESA BRENDA CHARLOTTE FRANCES ELEANOR PEARL RUTH VERNE\n4/8        1     3       1      3         3       3       3     1    1     1\n6/10       3     3       3      3         3       3       3     3    3     3\n2/23       3     3       3      3         3       3       3     3    3     3\n4/7        3     3       3      3         3       3       3     3    3     1\n11/21      3     3       3      3         3       3       3     3    3     3\n8/3        3     3       3      3         3       3       3     3    3     3\n      MYRNA KATHERINE SYLVIA NORA HELEN DOROTHY OLIVIA FLORA 6/27 3/2 4/12 9/26\n4/8       1         1      1    1     3       1      1     1    2   2    2    2\n6/10      1         1      1    1     1       3      3     3    4   4    4    4\n2/23      3         3      3    1     1       3      1     1    4   4    4    4\n4/7       1         1      1    1     1       3      3     3    4   4    4    4\n11/21     3         1      1    1     3       3      3     3    4   4    4    4\n8/3       3         1      1    1     3       3      3     3    4   4    4    4\n      2/25 5/19 3/15 9/16 4/8 6/10 2/23 4/7 11/21 8/3\n4/8      2    2    2    2   0    2    2   2     2   2\n6/10     4    2    2    2   2    0    2   2     2   2\n2/23     4    2    2    2   2    2    0   2     2   2\n4/7      4    2    2    2   2    2    2   0     2   2\n11/21    4    2    2    2   2    2    2   2     0   2\n8/3      4    2    2    2   2    2    2   2     2   0\n\n\nWhich is a square matrix of dimensions \\((M + N) \\times (M + N)\\); that’s \\((18 + 14) \\times (18 + 14) = 32 \\times 32\\) in our case.\nWe can check in R:\n\n   dim(D.pg)\n\n[1] 32 32\n\n\nAs we can see in the distance matrix, distances between nodes in the same set are even \\(g_{ii'|jj'} = \\{2, 4, \\ldots\\}\\) but distances in nodes in different sets are odd \\(g_{ij|ji} = \\{1, 3, \\ldots\\}\\). Beyond this hiccup, distances can be interpreted in the same way as one-mode networks."
  },
  {
    "objectID": "handout7.html#closeness-centrality-in-two-mode-networks",
    "href": "handout7.html#closeness-centrality-in-two-mode-networks",
    "title": "Analyzing Two-Mode Networks",
    "section": "Closeness Centrality in two-mode Networks",
    "text": "Closeness Centrality in two-mode Networks\nThis means that (unnormalized) closeness centrality works the same way as it does in regular networks:\n\n   round(closeness(g), 3)\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n    0.017     0.015     0.017     0.015     0.013     0.014     0.014     0.014 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n    0.015     0.015     0.014     0.015     0.016     0.017     0.015     0.014 \n   OLIVIA     FLORA      6/27       3/2      4/12      9/26      2/25      5/19 \n    0.012     0.012     0.012     0.012     0.013     0.012     0.014     0.016 \n     3/15      9/16       4/8      6/10      2/23       4/7     11/21       8/3 \n    0.017     0.019     0.018     0.013     0.012     0.013     0.012     0.012 \n\n\nWhich is just the inverse of the sums of the distances matrix for people and groups counting their geodesic distances to nodes of both sets.\nHowever, as Borgatti and Everett (1997) note, if we want normalized closeness centralities, we can’t use the off-the-shelf normalization for one-mode networks in igraph (\\(n-1\\)) as it will give us non-sense results because now we have two sets of nodes.\nInstead, we need to normalize the closeness score for each node set by its theoretical maximum for each node set.\nFor people, this is:\n\\[\nN + 2(M - 1)\n\\]\nAnd for groups/events this same quantity is:\n\\[\nM + 2(N - 1)\n\\]\nThe basic idea is that nodes can be at minimum geodesic distance \\(g = 1\\) from nodes of the other set (for people, groups; for groups, people) and at minimum distance \\(g = 2\\) from nodes of their own set, with their own presence eliminated by subtraction (Borgatti and Everett 1997).\nIn our case, we create a normalization vector with these quantities of length \\(M + N\\):\n\n   M <- nrow(A)\n   N <- ncol(A)\n   n.p <- N + 2 * (M - 1)\n   n.e <- M + 2 * (N - 1)\n   norm.vec <- c(rep(n.p, M), rep(n.e, N))\n\nAnd normalized closeness is:\n\n   round(norm.vec/rowSums(D.pg), 3)\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n    0.800     0.727     0.800     0.727     0.600     0.667     0.667     0.667 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n    0.706     0.706     0.686     0.727     0.774     0.800     0.727     0.649 \n   OLIVIA     FLORA      6/27       3/2      4/12      9/26      2/25      5/19 \n    0.585     0.585     0.524     0.524     0.564     0.537     0.595     0.688 \n     3/15      9/16       4/8      6/10      2/23       4/7     11/21       8/3 \n    0.733     0.846     0.786     0.550     0.537     0.564     0.524     0.524 \n\n\nWhich are the same numbers in Borgatti and Everett (1997, table 1, column 6)."
  },
  {
    "objectID": "handout7.html#betweenness-centrality-in-two-mode-networks",
    "href": "handout7.html#betweenness-centrality-in-two-mode-networks",
    "title": "Analyzing Two-Mode Networks",
    "section": "Betweenness Centrality in two-mode Networks",
    "text": "Betweenness Centrality in two-mode Networks\nAs Borgatti and Everett (1997) also note, the normalizations for betweenness centrality in the two-mode case are a bit more involved. This is because they depend on which node set is larger than the other.\nFor the larger node set, which in our case is the people, the normalization is:\n\\[\n2(M-1)(N-1)\n\\]\nFor the smaller node set, which in our case is the groups/events, the normalization is:\n\\[\n\\frac{1}{2}(N)(N-1)+\\frac{1}{2}(M-1)(M-2)+(M-1)(N-1)\n\\]\nRemember that you have to switch this around if you are analyzing a network with more groups than people.\nCreating the relevant vectors:\n\n   n.p <- 2*(M-1)*(N-1)\n   n.e <- (1/2)*(N*(N-1))+(1/2)*(M-1)*(M-2)+(M-1)*(N-1)\n   norm.vec <- c(rep(n.p, M), rep(n.e, N))\n\nAnd normalized betweenness is:\n\n   round(betweenness(g)/norm.vec, 4)*100\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n     9.72      5.17      8.82      4.98      1.07      1.08      0.95      0.68 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n     1.69      1.58      1.65      4.77      7.22     11.42      4.27      0.20 \n   OLIVIA     FLORA      6/27       3/2      4/12      9/26      2/25      5/19 \n     0.51      0.51      0.22      0.21      1.84      0.78      3.80      6.56 \n     3/15      9/16       4/8      6/10      2/23       4/7     11/21       8/3 \n    13.07     24.60     22.75      1.15      1.98      1.83      0.23      0.23 \n\n\nWhich are (with some slight differences and rounding errors) the same numbers in Borgatti and Everett (1997, table 2, column 3)."
  },
  {
    "objectID": "handout7.html#the-duality-of-persons-and-groups",
    "href": "handout7.html#the-duality-of-persons-and-groups",
    "title": "Analyzing Two-Mode Networks",
    "section": "The Duality of Persons and Groups",
    "text": "The Duality of Persons and Groups\nRemember that in the one-mode case, multiplying the adjacency matrix times its transpose yields the common neighbors matrix \\(\\mathbf{M}\\):\n\\[\n\\mathbf{M} = \\mathbf{A}\\mathbf{A}^T\n\\]\nAs famously noted by Breiger (1974), doing the same for the affiliation matrix of a two-mode network also returns the common-neighbors matrix, but because objects in one mode can only connect to objects in another mode, this also reveals the duality of persons and groups: The connections between people are made up of the groups they share, and the connections between groups are revealed by the groups they share.\nThus, computing the common neighbors matrix for both persons and groups (also called the projection of the two-mode network into each of its modes) produces a one-mode similarity matrix between people and groups, where the similarities are defined by the number of objects in the other mode that they share.\nSo for the people the relevant projection is:\n\\[\n\\mathbf{P} = \\mathbf{A}\\mathbf{A}^T\n\\]\nAnd for the groups:\n\\[\n\\mathbf{G} = \\mathbf{A}^T\\mathbf{A}\n\\]\nWhich in our case yields:\n\n   P <- A %*% t(A)\n   P\n\n          EVELYN LAURA THERESA BRENDA CHARLOTTE FRANCES ELEANOR PEARL RUTH\nEVELYN         8     6       7      6         3       4       3     3    3\nLAURA          6     7       6      6         3       4       4     2    3\nTHERESA        7     6       8      6         4       4       4     3    4\nBRENDA         6     6       6      7         4       4       4     2    3\nCHARLOTTE      3     3       4      4         4       2       2     0    2\nFRANCES        4     4       4      4         2       4       3     2    2\nELEANOR        3     4       4      4         2       3       4     2    3\nPEARL          3     2       3      2         0       2       2     3    2\nRUTH           3     3       4      3         2       2       3     2    4\nVERNE          2     2       3      2         1       1       2     2    3\nMYRNA          2     1       2      1         0       1       1     2    2\nKATHERINE      2     1       2      1         0       1       1     2    2\nSYLVIA         2     2       3      2         1       1       2     2    3\nNORA           2     2       3      2         1       1       2     2    2\nHELEN          1     2       2      2         1       1       2     1    2\nDOROTHY        2     1       2      1         0       1       1     2    2\nOLIVIA         1     0       1      0         0       0       0     1    1\nFLORA          1     0       1      0         0       0       0     1    1\n          VERNE MYRNA KATHERINE SYLVIA NORA HELEN DOROTHY OLIVIA FLORA\nEVELYN        2     2         2      2    2     1       2      1     1\nLAURA         2     1         1      2    2     2       1      0     0\nTHERESA       3     2         2      3    3     2       2      1     1\nBRENDA        2     1         1      2    2     2       1      0     0\nCHARLOTTE     1     0         0      1    1     1       0      0     0\nFRANCES       1     1         1      1    1     1       1      0     0\nELEANOR       2     1         1      2    2     2       1      0     0\nPEARL         2     2         2      2    2     1       2      1     1\nRUTH          3     2         2      3    2     2       2      1     1\nVERNE         4     3         3      4    3     3       2      1     1\nMYRNA         3     4         4      4    3     3       2      1     1\nKATHERINE     3     4         6      6    5     3       2      1     1\nSYLVIA        4     4         6      7    6     4       2      1     1\nNORA          3     3         5      6    8     4       1      2     2\nHELEN         3     3         3      4    4     5       1      1     1\nDOROTHY       2     2         2      2    1     1       2      1     1\nOLIVIA        1     1         1      1    2     1       1      2     2\nFLORA         1     1         1      1    2     1       1      2     2\n\n   G <- t(A) %*% A\n   G\n\n      6/27 3/2 4/12 9/26 2/25 5/19 3/15 9/16 4/8 6/10 2/23 4/7 11/21 8/3\n6/27     3   2    3    2    3    3    2    3   1    0    0   0     0   0\n3/2      2   3    3    2    3    3    2    3   2    0    0   0     0   0\n4/12     3   3    6    4    6    5    4    5   2    0    0   0     0   0\n9/26     2   2    4    4    4    3    3    3   2    0    0   0     0   0\n2/25     3   3    6    4    8    6    6    7   3    0    0   0     0   0\n5/19     3   3    5    3    6    8    5    7   4    1    1   1     1   1\n3/15     2   2    4    3    6    5   10    8   5    3    2   4     2   2\n9/16     3   3    5    3    7    7    8   14   9    4    1   5     2   2\n4/8      1   2    2    2    3    4    5    9  12    4    3   5     3   3\n6/10     0   0    0    0    0    1    3    4   4    5    2   5     3   3\n2/23     0   0    0    0    0    1    2    1   3    2    4   2     1   1\n4/7      0   0    0    0    0    1    4    5   5    5    2   6     3   3\n11/21    0   0    0    0    0    1    2    2   3    3    1   3     3   3\n8/3      0   0    0    0    0    1    2    2   3    3    1   3     3   3\n\n\nThe off-diagonal entries of these square person by person (group by group) matrices is the number of groups (people) shared by each person (group) and the diagonals are the number of memberships of each person (the size of each group/event).\nIn igraph the relevant function is called bipartite_projection. It takes a graph as an input and returns a list containing igraph graph objects of both projections by default:\n\n   Proj <- bipartite_projection(g)\n   G.p <- Proj[[1]]\n   G.g <- Proj[[2]]\n\n\n   set.seed(123)\n   plot(G.p, \n     vertex.size=8, vertex.frame.color=\"lightgray\", \n     vertex.label.dist=2, edge.curved=0.2, \n     vertex.label.cex = 1.25, edge.color = \"lightgray\",\n     edge.width = E(G.p)$weight)\n\n\n\n\nOne mode projection of people.\n\n\n\n\n\n   set.seed(123)\n   plot(G.g, \n     vertex.size=8, vertex.frame.color=\"lightgray\", \n     vertex.label.dist=2, edge.curved=0.2, \n     vertex.label.cex = 1.25, edge.color = \"lightgray\",\n     edge.width = E(G.g)$weight)\n\n\n\n\nOne mode projection of groups\n\n\n\n\nWhich produces a binarized undirected graph of each projection.\nThe actual shared memberships and shared members are stored as an attribute of each edge called weight used in the plotting code above to set the edge.width:\n\n   edge_attr(G.p)\n\n$weight\n  [1] 6 6 7 3 4 3 3 3 2 2 2 2 2 1 2 1 1 6 6 3 4 4 3 2 2 2 2 2 1 1 1 6 4 4 4 4 3\n [38] 3 3 3 2 2 2 2 1 1 4 4 4 3 2 2 2 2 2 1 1 1 2 2 2 1 1 1 1 3 2 2 1 1 1 1 1 1\n [75] 1 3 2 2 2 2 2 1 1 1 2 2 2 2 2 2 1 2 1 1 3 3 2 2 2 2 2 1 1 4 3 3 3 3 2 1 1\n[112] 4 4 3 2 3 1 1 6 3 2 5 1 1 6 4 2 1 1 4 1 2 2 1 1 1 1 1 2\n\n   edge_attr(G.g)\n\n$weight\n [1] 2 3 2 3 3 3 1 2 3 2 3 3 3 2 2 4 6 5 5 2 4 4 3 3 2 3 6 7 3 6 7 4 5 1 1 1 1 1\n[39] 8 5 4 3 2 2 2 9 5 4 2 2 1 5 4 3 3 3 5 3 3 2 2 1 1 3 3 3\n\n\nSo to get the weighted projection matrix, we need to type:\n\n   as.matrix(as_adjacency_matrix(G.p, attr = \"weight\"))\n\n          EVELYN LAURA THERESA BRENDA CHARLOTTE FRANCES ELEANOR PEARL RUTH\nEVELYN         0     6       7      6         3       4       3     3    3\nLAURA          6     0       6      6         3       4       4     2    3\nTHERESA        7     6       0      6         4       4       4     3    4\nBRENDA         6     6       6      0         4       4       4     2    3\nCHARLOTTE      3     3       4      4         0       2       2     0    2\nFRANCES        4     4       4      4         2       0       3     2    2\nELEANOR        3     4       4      4         2       3       0     2    3\nPEARL          3     2       3      2         0       2       2     0    2\nRUTH           3     3       4      3         2       2       3     2    0\nVERNE          2     2       3      2         1       1       2     2    3\nMYRNA          2     1       2      1         0       1       1     2    2\nKATHERINE      2     1       2      1         0       1       1     2    2\nSYLVIA         2     2       3      2         1       1       2     2    3\nNORA           2     2       3      2         1       1       2     2    2\nHELEN          1     2       2      2         1       1       2     1    2\nDOROTHY        2     1       2      1         0       1       1     2    2\nOLIVIA         1     0       1      0         0       0       0     1    1\nFLORA          1     0       1      0         0       0       0     1    1\n          VERNE MYRNA KATHERINE SYLVIA NORA HELEN DOROTHY OLIVIA FLORA\nEVELYN        2     2         2      2    2     1       2      1     1\nLAURA         2     1         1      2    2     2       1      0     0\nTHERESA       3     2         2      3    3     2       2      1     1\nBRENDA        2     1         1      2    2     2       1      0     0\nCHARLOTTE     1     0         0      1    1     1       0      0     0\nFRANCES       1     1         1      1    1     1       1      0     0\nELEANOR       2     1         1      2    2     2       1      0     0\nPEARL         2     2         2      2    2     1       2      1     1\nRUTH          3     2         2      3    2     2       2      1     1\nVERNE         0     3         3      4    3     3       2      1     1\nMYRNA         3     0         4      4    3     3       2      1     1\nKATHERINE     3     4         0      6    5     3       2      1     1\nSYLVIA        4     4         6      0    6     4       2      1     1\nNORA          3     3         5      6    0     4       1      2     2\nHELEN         3     3         3      4    4     0       1      1     1\nDOROTHY       2     2         2      2    1     1       0      1     1\nOLIVIA        1     1         1      1    2     1       1      0     2\nFLORA         1     1         1      1    2     1       1      2     0\n\n\n\n   as.matrix(as_adjacency_matrix(G.g, attr = \"weight\"))\n\n      6/27 3/2 4/12 9/26 2/25 5/19 3/15 9/16 4/8 6/10 2/23 4/7 11/21 8/3\n6/27     0   2    3    2    3    3    2    3   1    0    0   0     0   0\n3/2      2   0    3    2    3    3    2    3   2    0    0   0     0   0\n4/12     3   3    0    4    6    5    4    5   2    0    0   0     0   0\n9/26     2   2    4    0    4    3    3    3   2    0    0   0     0   0\n2/25     3   3    6    4    0    6    6    7   3    0    0   0     0   0\n5/19     3   3    5    3    6    0    5    7   4    1    1   1     1   1\n3/15     2   2    4    3    6    5    0    8   5    3    2   4     2   2\n9/16     3   3    5    3    7    7    8    0   9    4    1   5     2   2\n4/8      1   2    2    2    3    4    5    9   0    4    3   5     3   3\n6/10     0   0    0    0    0    1    3    4   4    0    2   5     3   3\n2/23     0   0    0    0    0    1    2    1   3    2    0   2     1   1\n4/7      0   0    0    0    0    1    4    5   5    5    2   0     3   3\n11/21    0   0    0    0    0    1    2    2   3    3    1   3     0   3\n8/3      0   0    0    0    0    1    2    2   3    3    1   3     3   0\n\n\nNote that because both G.p and G.g are weighted graphs we can calculate the weighted version of degree for both persons and groups from them (sometimes called the vertex strength).\nIn igraph we can do this as follows:\n\n   strength(G.p)\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n       50        45        57        46        24        32        36        31 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n       40        38        33        37        46        43        34        24 \n   OLIVIA     FLORA \n       14        14 \n\n   strength(G.g)\n\n 6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10  2/23   4/7 11/21 \n   19    20    32    23    38    41    48    59    46    25    13    28    18 \n  8/3 \n   18 \n\n\nInterestingly, as noted by Faust (1997, 167), there is a mathematical connection between the strength of each vertex in the weighted projection and the centrality of the nodes from the other set they are connected to:\n\nFor people, the vertex strength is equal to the sum of the sizes of the groups they belong to minus their own degree.\nFor groups, the vertex strength is equal to the sum of the memberships of the people that belong to them, minus their own size.\n\nWe can verify this relationship for \\(EVELYN\\):\n\n   sum.size.evelyn <- sum(A[\"EVELYN\", ] * degree(g)[which(V(g)$type == TRUE)]) #sum of the sizes of the groups Evelyn belongs to\n   sum.size.evelyn - degree(g)[which(V(g)$name == \"EVELYN\")]\n\nEVELYN \n    50 \n\n\nWhich is indeed Evelyn’s vertex strength.\nDually, the same relation applies to groups:\n\n   sum.mem.6.27 <- sum(A[, \"6/27\"] * degree(g)[which(V(g)$type == FALSE)]) #sum of the memberships of people in the first group\n   sum.mem.6.27 - degree(g)[which(V(g)$name == \"6/27\")]\n\n6/27 \n  19 \n\n\nWhich is indeed the vertex strength of the event held on 6/27."
  },
  {
    "objectID": "handout7.html#normalized-vertex-similarity-metrics",
    "href": "handout7.html#normalized-vertex-similarity-metrics",
    "title": "Analyzing Two-Mode Networks",
    "section": "Normalized Vertex Similarity Metrics",
    "text": "Normalized Vertex Similarity Metrics\nNote that the one-mode projections are unnormalized similarity matrices just like in the case of regular networks. That means that if we have the degrees of nodes in each mode, we can transform this matrix into any of the normalized vertex similarity metrics we discussed before, including Jaccard, Cosine, Dice, LHN, and so on.\nThus repackaging our vertex similarity function for the two-mode case, we have:\n\n   vertex.sim <- function(x) {\n      A <- as.matrix(as_biadjacency_matrix(x))\n      M <- nrow(A) #number of persons\n      N <- ncol(A) #number of groups\n      p.d <- rowSums(A) #person degrees\n      g.d <- colSums(A) #group degrees\n      P <- A %*% t(A) #person projection\n      G <- t(A) %*% A #group projection\n      J.p <- diag(1, M, M)\n      J.g <- diag(1, N, N)\n      C.p <- diag(1, M, M)\n      C.g <- diag(1, N, N)\n      D.p <- diag(1, M, M)\n      D.g <- diag(1, N, N)\n      L.p <- diag(1, M, M)\n      L.g <- diag(1, N, N)\n      for (i in 1:M) {\n         for (j in 1:M) {\n            if (i < j) {\n               J.p[i,j] <- P[i,j]/(P[i,j] + p.d[i] + p.d[j])\n               J.p[j,i] <- P[i,j]/(P[i,j] + p.d[i] + p.d[j])\n               C.p[i,j] <- P[i,j]/(sqrt(p.d[i] * p.d[j]))\n               C.p[j,i] <- P[i,j]/(sqrt(p.d[i] * p.d[j]))\n               D.p[i,j] <- (2*P[i,j])/(2*P[i,j] + p.d[i] + p.d[j])\n               D.p[j,i] <- (2*P[i,j])/(2*P[i,j] + p.d[i] + p.d[j])\n               L.p[i,j] <- P[i,j]/(p.d[i] * p.d[j])\n               L.p[j,i] <- P[i,j]/(p.d[i] * p.d[j])\n               }\n            }\n         }\n      for (i in 1:N) {\n         for (j in 1:N) {\n            if (i < j) {\n               J.g[i,j] <- G[i,j]/(G[i,j] + g.d[i] + g.d[j])\n               J.g[j,i] <- G[i,j]/(G[i,j] + g.d[i] + g.d[j])\n               C.g[i,j] <- G[i,j]/(sqrt(g.d[i] * g.d[j]))\n               C.g[j,i] <- G[i,j]/(sqrt(g.d[i] * g.d[j]))\n               D.g[i,j] <- (2*G[i,j])/(2*G[i,j] + g.d[i] + g.d[j])\n               D.g[j,i] <- (2*G[i,j])/(2*G[i,j] + g.d[i] + g.d[j])\n               L.g[i,j] <- G[i,j]/(g.d[i] * g.d[j])\n               L.g[j,i] <- G[i,j]/(g.d[i] * g.d[j])\n               }\n            }\n         }\n      return(list(J.p = J.p, C.p = C.p, D.p = D.p, L.p = L.p,\n                  J.g = J.g, C.g = C.g, D.g = D.g, L.g = L.g))\n      }\n\nUsing this function to compute the Jaccard similarity between people yields:\n\n   J.p <- vertex.sim(g)$J.p\n   rownames(J.p) <- rownames(A)\n   colnames(J.p) <- rownames(A)\n   round(J.p, 2)\n\n          EVELYN LAURA THERESA BRENDA CHARLOTTE FRANCES ELEANOR PEARL RUTH\nEVELYN      1.00  0.29    0.30   0.29      0.20    0.25    0.20  0.21 0.20\nLAURA       0.29  1.00    0.29   0.30      0.21    0.27    0.27  0.17 0.21\nTHERESA     0.30  0.29    1.00   0.29      0.25    0.25    0.25  0.21 0.25\nBRENDA      0.29  0.30    0.29   1.00      0.27    0.27    0.27  0.17 0.21\nCHARLOTTE   0.20  0.21    0.25   0.27      1.00    0.20    0.20  0.00 0.20\nFRANCES     0.25  0.27    0.25   0.27      0.20    1.00    0.27  0.22 0.20\nELEANOR     0.20  0.27    0.25   0.27      0.20    0.27    1.00  0.22 0.27\nPEARL       0.21  0.17    0.21   0.17      0.00    0.22    0.22  1.00 0.22\nRUTH        0.20  0.21    0.25   0.21      0.20    0.20    0.27  0.22 1.00\nVERNE       0.14  0.15    0.20   0.15      0.11    0.11    0.20  0.22 0.27\nMYRNA       0.14  0.08    0.14   0.08      0.00    0.11    0.11  0.22 0.20\nKATHERINE   0.12  0.07    0.12   0.07      0.00    0.09    0.09  0.18 0.17\nSYLVIA      0.12  0.12    0.17   0.12      0.08    0.08    0.15  0.17 0.21\nNORA        0.11  0.12    0.16   0.12      0.08    0.08    0.14  0.15 0.14\nHELEN       0.07  0.14    0.13   0.14      0.10    0.10    0.18  0.11 0.18\nDOROTHY     0.17  0.10    0.17   0.10      0.00    0.14    0.14  0.29 0.25\nOLIVIA      0.09  0.00    0.09   0.00      0.00    0.00    0.00  0.17 0.14\nFLORA       0.09  0.00    0.09   0.00      0.00    0.00    0.00  0.17 0.14\n          VERNE MYRNA KATHERINE SYLVIA NORA HELEN DOROTHY OLIVIA FLORA\nEVELYN     0.14  0.14      0.12   0.12 0.11  0.07    0.17   0.09  0.09\nLAURA      0.15  0.08      0.07   0.12 0.12  0.14    0.10   0.00  0.00\nTHERESA    0.20  0.14      0.12   0.17 0.16  0.13    0.17   0.09  0.09\nBRENDA     0.15  0.08      0.07   0.12 0.12  0.14    0.10   0.00  0.00\nCHARLOTTE  0.11  0.00      0.00   0.08 0.08  0.10    0.00   0.00  0.00\nFRANCES    0.11  0.11      0.09   0.08 0.08  0.10    0.14   0.00  0.00\nELEANOR    0.20  0.11      0.09   0.15 0.14  0.18    0.14   0.00  0.00\nPEARL      0.22  0.22      0.18   0.17 0.15  0.11    0.29   0.17  0.17\nRUTH       0.27  0.20      0.17   0.21 0.14  0.18    0.25   0.14  0.14\nVERNE      1.00  0.27      0.23   0.27 0.20  0.25    0.25   0.14  0.14\nMYRNA      0.27  1.00      0.29   0.27 0.20  0.25    0.25   0.14  0.14\nKATHERINE  0.23  0.29      1.00   0.32 0.26  0.21    0.20   0.11  0.11\nSYLVIA     0.27  0.27      0.32   1.00 0.29  0.25    0.18   0.10  0.10\nNORA       0.20  0.20      0.26   0.29 1.00  0.24    0.09   0.17  0.17\nHELEN      0.25  0.25      0.21   0.25 0.24  1.00    0.12   0.12  0.12\nDOROTHY    0.25  0.25      0.20   0.18 0.09  0.12    1.00   0.20  0.20\nOLIVIA     0.14  0.14      0.11   0.10 0.17  0.12    0.20   1.00  0.33\nFLORA      0.14  0.14      0.11   0.10 0.17  0.12    0.20   0.33  1.00"
  },
  {
    "objectID": "handout7.html#eigenvector-status",
    "href": "handout7.html#eigenvector-status",
    "title": "Analyzing Two-Mode Networks",
    "section": "Eigenvector Status",
    "text": "Eigenvector Status\nMeasures of status and prestige are particularly applicable to two-mode networks. The reason is that the reflective principle behind these measures interacts nicely with the duality principle.\nFor instance, when it comes to eigenvector-style measures, the neat idea that people are central if they belong to central groups and groups and central if their members are central people (with people centrality defined by membership in central groups) can be effectively captured by these metrics (Bonacich 1991).\nThus if \\(x\\) are the status scores for people, and \\(y\\) are the status scores for groups, then the \\(x\\) scores should be given by the sum of the \\(y\\) scores of the groups each person belongs, and the \\(y\\) scores should be given by the sum of the \\(x\\) scores of their members.\nIn mathese:\n\\[\nx = \\mathbf{A}^Ty\n\\]\n\\[\ny = \\mathbf{A}x\n\\]\nOnce again, producing another instance of a cat chasing its own tail (we need to know the values of \\(y\\) to figure out the values of \\(x\\) and we need to know the values of \\(x\\) to figure out the values of \\(y\\)).\nHow do we proceed? Well, let’s bring back our trusty status distribution game:\n\n   status1 <- function(A) {\n      n <- nrow(A) #number of actors\n      x <- rep(1, n) #initial status vector set to all ones\n      w <- 1 \n      k <- 0 #initializing counter\n      while (w > 0.0001) {\n          o.x <- x #old status scores\n          x <- A %*% x #new scores a function of old scores and adjacency matrix\n          x <- x/norm(x, type = \"E\") #normalizing new status scores\n          w <- abs(sum(abs(x) - abs(o.x))) #diff. between new and old scores\n          k <- k + 1 #incrementing while counter\n      }\n   return(as.vector(x))\n   }\n\nThen the main question is over what matrix will the status game be played for both people and groups?\nAs Bonacich (1991) noted, the projection matrices of Breiger (1974) are natural candidates for this task. Let’s try it out.\nFor people this would be:\n\n   p.s <- status1(P)\n   names(p.s) <- rownames(P)\n   round(p.s, 3)\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n    0.335     0.309     0.371     0.313     0.168     0.209     0.228     0.180 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n    0.236     0.218     0.187     0.220     0.277     0.264     0.201     0.131 \n   OLIVIA     FLORA \n    0.070     0.070 \n\n\nAnd for groups:\n\n   g.s <- status1(G)\n   names(g.s) <- colnames(A)\n   round(g.s, 3)\n\n 6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10  2/23   4/7 11/21 \n0.142 0.150 0.253 0.176 0.322 0.328 0.384 0.507 0.380 0.170 0.090 0.203 0.113 \n  8/3 \n0.113 \n\n\nLo and behold, these are the status scores we seek. It turns out they can be computed by figuring out the leading eigenvector (what our status game does for any matrix) of the Breiger projection matrices (Bonacich 1991):\n\\[\n\\lambda x = (\\mathbf{A}\\mathbf{A}^T)x\n\\]\n\\[\n\\lambda y = (\\mathbf{A}^T\\mathbf{A})y\n\\]\nIn R we can do this using the eigen function:\n\n   eig.p <- eigen(P)\n   eig.g <- eigen(G)\n   p.s <- eig.p$vector[, 1] * -1\n   g.s <- eig.g$vector[, 1] * -1\n   names(p.s) <- rownames(A)\n   names(g.s) <- colnames(A)\n   round(p.s, 3)\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n    0.335     0.309     0.371     0.313     0.168     0.209     0.228     0.180 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n    0.236     0.218     0.187     0.220     0.277     0.264     0.201     0.131 \n   OLIVIA     FLORA \n    0.070     0.070 \n\n   round(g.s, 3)\n\n 6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10  2/23   4/7 11/21 \n0.142 0.150 0.253 0.176 0.322 0.328 0.384 0.507 0.379 0.170 0.090 0.203 0.113 \n  8/3 \n0.113 \n\n\nNeat! These are the same scores we obtained by playing our status game. The scores are also readily interpretable: The most central people belong to the most central (largest membership) groups and the most central groups are the ones that attract the most central (highest activity) members.\nAnother way of thinking of the eigenvector centrality of each node in this context is as a weighted sum1 of the eigenvector centralities on the nodes in the other mode they are connected to (Faust 1997, 170).\nSo for any person, let’s say \\(EVELYN\\), their eigenvector centrality is equal to:\n\n   sum(A[\"EVELYN\", ] * g.s) * 1/sqrt(eig.p$values[1])\n\n[1] 0.334734\n\n\nWhich is indeed Evelyn’s Eigenvector score.\nThe same goes for the eigenvector score of groups, which are just a weighted sum of the Eigenvector centralities of the people who belong to them:\n\n   sum(A[, \"6/27\"] * p.s) *1/sqrt(eig.p$values[1])\n\n[1] 0.1419431\n\n\nWhich is indeed the eigenvector score for the event held on 6/27. Duality at work!\nFinally, here are the Southern Women dual Eigenvector Centralities in tabular form:\n\n   library(kableExtra)\n   p.dat <- data.frame(People = rownames(A), Eig.Cent = round(p.s, 3))\n   p.dat <- p.dat[order(p.dat$Eig.Cent, decreasing = TRUE), ]\n   kbl(p.dat, format = \"html\", , align = c(\"l\", \"c\"), row.names = FALSE) %>% \n      column_spec(1, bold = TRUE) %>% \n      kable_styling(full_width = TRUE,\n                     bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\n \n  \n    People \n    Eig.Cent \n  \n \n\n  \n    THERESA \n    0.371 \n  \n  \n    EVELYN \n    0.335 \n  \n  \n    BRENDA \n    0.313 \n  \n  \n    LAURA \n    0.309 \n  \n  \n    SYLVIA \n    0.277 \n  \n  \n    NORA \n    0.264 \n  \n  \n    RUTH \n    0.236 \n  \n  \n    ELEANOR \n    0.228 \n  \n  \n    KATHERINE \n    0.220 \n  \n  \n    VERNE \n    0.218 \n  \n  \n    FRANCES \n    0.209 \n  \n  \n    HELEN \n    0.201 \n  \n  \n    MYRNA \n    0.187 \n  \n  \n    PEARL \n    0.180 \n  \n  \n    CHARLOTTE \n    0.168 \n  \n  \n    DOROTHY \n    0.131 \n  \n  \n    OLIVIA \n    0.070 \n  \n  \n    FLORA \n    0.070 \n  \n\n\n\n\n\n\n   g.dat <- data.frame(Groups = colnames(A), Eig.Cent = round(g.s, 3))\n   g.dat <- g.dat[order(g.dat$Eig.Cent, decreasing = TRUE), ]\n   kbl(g.dat, format = \"html\", align = c(\"l\", \"c\"), row.names = FALSE) %>% \n      column_spec(1, bold = TRUE) %>% \n      kable_styling(full_width = TRUE,\n                     bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\n \n  \n    Groups \n    Eig.Cent \n  \n \n\n  \n    9/16 \n    0.507 \n  \n  \n    3/15 \n    0.384 \n  \n  \n    4/8 \n    0.379 \n  \n  \n    5/19 \n    0.328 \n  \n  \n    2/25 \n    0.322 \n  \n  \n    4/12 \n    0.253 \n  \n  \n    4/7 \n    0.203 \n  \n  \n    9/26 \n    0.176 \n  \n  \n    6/10 \n    0.170 \n  \n  \n    3/2 \n    0.150 \n  \n  \n    6/27 \n    0.142 \n  \n  \n    11/21 \n    0.113 \n  \n  \n    8/3 \n    0.113 \n  \n  \n    2/23 \n    0.090"
  },
  {
    "objectID": "handout7.html#core-and-periphery",
    "href": "handout7.html#core-and-periphery",
    "title": "Analyzing Two-Mode Networks",
    "section": "Core and Periphery",
    "text": "Core and Periphery\nAs noted by Borgatti and Everett (2000), the Bonacich Eigenvector scores are also a model of a core-periphery partition in the two-mode network. This is already evident in the definition of the dual centralities: Popular actors (who participate in many events) make the events they participate in more central, and central events (that have many actors) become central when they attract the popular kids. The most central actors and most central events will thus form a clique in the network separating them from the rest.\nThis means the Eigenvector scores can be used to partition any two-mode network into a core (popular kids, popular events) and a periphery (less popular kids, less popular events). All we need to do to see the partition is to re-order the rows and columns of the affiliation matrix according to the value of the magnitude of the Eigenvector scores for people and events:\n\n   p <- ggcorrplot(t(A[order(p.s), order(g.s)]), \n                   colors = c(\"white\", \"white\", \"red\")) \n   p <- p + theme(legend.position = \"none\", \n                  axis.text.y = element_text(size = 8),\n                  axis.text.x = element_text(size = 8, angle = 0),\n                  )\n   p <- p + scale_x_discrete(position = \"top\") \n   p <- p + geom_hline(yintercept = 10.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_vline(xintercept = 8.5, linewidth = 2, color = \"blue\")\n   p\n\n\n\n\nHere the upper-right block reveals the core actors and events in the network; namely, the events almost universally attended by the most active participants (Everett and Borgatti 2013, table 2)."
  },
  {
    "objectID": "handout7.html#pagerank-status-for-two-mode-networks",
    "href": "handout7.html#pagerank-status-for-two-mode-networks",
    "title": "Two Mode Networks",
    "section": "PageRank Status for Two Mode Networks",
    "text": "PageRank Status for Two Mode Networks\nWe can, of course, also play our status game with degree-normalized versions of the affiliation matrix (a.k.a. PageRank).\nFirst we need to create row stochastic versions of \\(\\mathbf{A}\\) and \\(\\mathbf{A}^T\\). Recall that a matrix is row stochastic if their rows sum to one.\nFor the people, we can do this by taking the original affiliation matrix, and pre-multiplying it by a diagonal square matrix \\(\\mathbf{D}_P^{-1}\\) of dimensions \\(M \\times M\\) containing the inverse of the degrees of each person in the affiliation network along the diagonals and zeros everywhere else, yielding the row-stochastic matrix \\(\\mathbf{P}_P\\) of dimensions \\(M \\times N\\):\n\\[\n\\mathbf{P}_P = \\mathbf{D}_P^{-1}\\mathbf{A}\n\\]\nAnd we can do the same with the groups, except that we pre-multiply the transpose of the original affiliation matrix by \\(\\mathbf{D}_G^{-1}\\) which is an \\(N \\times N\\) matrix containing the inverse of the size of each group along the diagonals and zero everywhere else, this yields the matrix \\(\\mathbf{P}_G\\) of dimensions \\(N \\times M\\):\n\\[\n\\mathbf{P}_G = \\mathbf{D}_G^{-1}\\mathbf{A}^T\n\\]\nIn R can compute \\(\\mathbf{P}_P\\) and \\(\\mathbf{P}_G\\) as follows:\n\n   D.p <- diag(1/rowSums(A))\n   P.p <- D.p %*% A\n   D.g <- diag(1/colSums(A))\n   P.g <- D.g %*% t(A)\n\nAnd we can check that both P.p (for people) and P.g (groups) are row stochastic:\n\n   rowSums(P.p)\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n   rowSums(P.g)\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n\nAnd that they are of the predicted dimensions:\n\n   dim(P.p)\n\n[1] 18 14\n\n   dim(P.g)\n\n[1] 14 18\n\n\nGreat! Now, we can obtain the degree-normalized projections for people by multiplying \\(\\mathbf{P}_P\\) times \\(\\mathbf{P}_G\\):\n\\[\n\\mathbf{P}_{PP} = \\mathbf{P}_P\\mathbf{P}_G\n\\]\nWhich produces the matrix \\(\\mathbf{P}_{PP}\\) a square \\(M \\times M\\) matrix containing the degree-normalized similarities between each pair of people.\nWe then do the same for groups:\n\\[\n\\mathbf{P}_{GG} = \\mathbf{P}_G\\mathbf{P}_P\n\\]\nWhich produces the matrix \\(\\mathbf{P}_{GG}\\) a square \\(N \\times N\\) matrix containing the degree-normalized similarities between each pair of groups.\nIn R we obtain these matrices as follows:\n\n   P.pp <- P.p %*% P.g\n   P.gg <- P.g %*% P.p\n   rownames(P.pp) <- colnames(P.pp)\n   rownames(P.gg) <- colnames(P.gg)\n\nWhich are still row stochastic–but now square–matrices:\n\n   rowSums(P.pp)\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n        1         1         1         1         1         1         1         1 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n        1         1         1         1         1         1         1         1 \n   OLIVIA     FLORA \n        1         1 \n\n   rowSums(P.gg)\n\n 6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10  2/23   4/7 11/21 \n    1     1     1     1     1     1     1     1     1     1     1     1     1 \n  8/3 \n    1 \n\n   dim(P.pp)\n\n[1] 18 18\n\n   dim(P.gg)\n\n[1] 14 14\n\n\nLet’s peek inside one of these matrices:\n\n   round(P.pp[1:10, 1:10], 2)\n\n          EVELYN LAURA THERESA BRENDA CHARLOTTE FRANCES ELEANOR PEARL RUTH\nEVELYN      0.19  0.14    0.14   0.13      0.07    0.06    0.04  0.03 0.03\nLAURA       0.16  0.18    0.13   0.13      0.06    0.07    0.06  0.03 0.04\nTHERESA     0.14  0.12    0.16   0.10      0.08    0.06    0.05  0.03 0.05\nBRENDA      0.15  0.13    0.12   0.17      0.09    0.07    0.06  0.03 0.04\nCHARLOTTE   0.14  0.10    0.16   0.16      0.16    0.07    0.06  0.00 0.06\nFRANCES     0.12  0.12    0.12   0.12      0.07    0.12    0.08  0.05 0.05\nELEANOR     0.08  0.11    0.11   0.11      0.06    0.08    0.11  0.05 0.07\nPEARL       0.09  0.07    0.09   0.07      0.00    0.07    0.07  0.09 0.05\nRUTH        0.07  0.07    0.09   0.07      0.06    0.05    0.07  0.04 0.09\nVERNE       0.04  0.04    0.06   0.04      0.03    0.02    0.04  0.04 0.06\n          VERNE\nEVELYN     0.02\nLAURA      0.02\nTHERESA    0.03\nBRENDA     0.02\nCHARLOTTE  0.03\nFRANCES    0.02\nELEANOR    0.04\nPEARL      0.05\nRUTH       0.06\nVERNE      0.11\n\n\nWhat are these numbers? Well, they can be interpreted as probabilities that a random walker would start at the row node and, following any \\(\\{person-group-person'-group'-person''-group''(\\ldots)person^X\\}\\) sequence of hops will end pass through the column person. Thus, higher values indicate an affinity or proximity between the people (and the groups in the corresponding matrix).\nWe can now play the PageRank status game on the transpose of \\(\\mathbf{P}_{PP}\\) and \\(\\mathbf{P}_{GG}\\), just like we did in the one-mode case, to get the scores we want:\n\n   pr.p <- status1(t(P.pp))\n   pr.g <- status1(t(P.gg))\n\nWhich leads to the following PageRank centrality rankings for persons and groups:\n\n   p.dat <- data.frame(People = rownames(A), PageRank.Cent = round(pr.p, 3))\n   p.dat <- p.dat[order(p.dat$PageRank.Cent, decreasing = TRUE), ]\n   kbl(p.dat, format = \"html\", , align = c(\"l\", \"c\"), row.names = FALSE) %>% \n      column_spec(1, bold = TRUE) %>% \n      kable_styling(full_width = TRUE,\n                     bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\n \n  \n    People \n    PageRank.Cent \n  \n \n\n  \n    EVELYN \n    0.352 \n  \n  \n    THERESA \n    0.352 \n  \n  \n    NORA \n    0.352 \n  \n  \n    LAURA \n    0.308 \n  \n  \n    BRENDA \n    0.308 \n  \n  \n    SYLVIA \n    0.308 \n  \n  \n    KATHERINE \n    0.264 \n  \n  \n    HELEN \n    0.220 \n  \n  \n    CHARLOTTE \n    0.176 \n  \n  \n    FRANCES \n    0.176 \n  \n  \n    ELEANOR \n    0.176 \n  \n  \n    RUTH \n    0.176 \n  \n  \n    VERNE \n    0.176 \n  \n  \n    MYRNA \n    0.176 \n  \n  \n    PEARL \n    0.132 \n  \n  \n    DOROTHY \n    0.088 \n  \n  \n    OLIVIA \n    0.088 \n  \n  \n    FLORA \n    0.088 \n  \n\n\n\n\n\n\n   g.dat <- data.frame(Groups = colnames(A), PageRank.Cent = round(pr.g, 3))\n   g.dat <- g.dat[order(g.dat$PageRank.Cent, decreasing = TRUE), ]\n   kbl(g.dat, format = \"html\", align = c(\"l\", \"c\"), row.names = FALSE) %>% \n      column_spec(1, bold = TRUE) %>% \n      kable_styling(full_width = TRUE,\n                     bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\n \n  \n    Groups \n    PageRank.Cent \n  \n \n\n  \n    9/16 \n    0.517 \n  \n  \n    4/8 \n    0.444 \n  \n  \n    3/15 \n    0.369 \n  \n  \n    2/25 \n    0.295 \n  \n  \n    5/19 \n    0.295 \n  \n  \n    4/7 \n    0.222 \n  \n  \n    4/12 \n    0.221 \n  \n  \n    6/10 \n    0.185 \n  \n  \n    2/23 \n    0.148 \n  \n  \n    9/26 \n    0.147 \n  \n  \n    6/27 \n    0.111 \n  \n  \n    3/2 \n    0.111 \n  \n  \n    11/21 \n    0.111 \n  \n  \n    8/3 \n    0.111 \n  \n\n\n\n\n\nThe PageRank two-mode scores lead to a different ranking of the nodes, because now people are central if they belong to selective (not just large) groups and groups are central if their members are discerning (not just very active) people.\nJust like before we can use the PageRank scores to reveal a pattern in the affiliation matrix:\n\n   p <- ggcorrplot(t(A[order(pr.p), order(-pr.g)]), \n                   colors = c(\"white\", \"white\", \"red\")) \n   p <- p + theme(legend.position = \"none\", \n                  axis.text.y = element_text(size = 8),\n                  axis.text.x = element_text(size = 8, angle = 0),\n                  )\n   p <- p + scale_x_discrete(position = \"top\") \n   p <- p + geom_abline(intercept = 0, slope = 1.26, linewidth = 2, color = \"blue\")\n   p\n\n\n\n\nIn the PageRank status score setup, events are ordered (from left to right) by popularity (most popular events on the left) and actors are ordered by activity (from top to bottom; active actors on top), creating a purely triangular pattern."
  },
  {
    "objectID": "handout7.html#correspondence-analysis",
    "href": "handout7.html#correspondence-analysis",
    "title": "Two-Mode Networks",
    "section": "Correspondence Analysis",
    "text": "Correspondence Analysis\nCorrespondence Analysis (CA) a relatively simple way to analyze and visualize two-mode data. In fact, we have already computed most of what we need to perform a CA of the two-mode network when figuring out the PageRank Status scores. However, there are a few additional computational details to discuss.\n\nThe Eigendecomposition of a Square Matrix\nFirst, let us review the idea of an eigendecomposition of a square matrix. Let’s say we have the following matrix \\(\\mathbf{B}\\) of dimensions \\(3 \\times 3\\):\n\n   set.seed(567)\n   B <- matrix(round(runif(9), 2), nrow = 3, ncol = 3)\n   B\n\n     [,1] [,2] [,3]\n[1,] 0.74 0.49 0.07\n[2,] 0.88 0.26 0.51\n[3,] 0.63 0.24 0.59\n\n\nMost matrices like this can be decomposed into two other matrices \\(\\mathbf{U}\\) and \\(\\mathbf{\\lambda}\\), such that the following matrix multiplication equation is true:\n\\[\n\\mathbf{B} = \\mathbf{U}\\mathbf{\\lambda}\\mathbf{U}^{-1}\n\\]\nBoth \\(\\mathbf{U}\\) and \\(\\mathbf{\\lambda}\\) are of the same dimensions as the original, with \\(\\mathbf{U}\\) having numbers in each cell and \\(\\mathbf{\\lambda}\\) being a matrix with values along the diagonals and zeros everywhere else.\nThe column values of \\(\\mathbf{U}\\) are called the eigenvectors of \\(\\mathbf{B}\\) and the diagonal values of \\(\\mathbf{\\lambda}\\) are called the eigenvalues of \\(\\mathbf{B}\\).\nIn R you can find the values that yield the eigendecomposition of any square matrix (if one exists) using the function eigen.\nSo in our case this would be:\n\n   eig.res <- eigen(B)\n   eig.res\n\neigen() decomposition\n$values\n[1]  1.4256541  0.3195604 -0.1552145\n\n$vectors\n           [,1]       [,2]       [,3]\n[1,] -0.5148954 -0.4669390  0.4838633\n[2,] -0.6388257  0.2808667 -0.8653808\n[3,] -0.5716507  0.8384997 -0.1303551\n\n\nThe function eigen returns a list with two components, one called values are the diagonal values of \\(\\mathbf{\\lambda}\\), and the other one called vectors is the eigenvector matrix \\(\\mathbf{U}\\).\nWe can check that these two elements can help us reconstruct the original matrix as follows:\n\n   lambda <- diag(eig.res$values)\n   U <- eig.res$vectors\n   B.rec <- U %*% lambda %*% solve(U)\n   B.rec\n\n     [,1] [,2] [,3]\n[1,] 0.74 0.49 0.07\n[2,] 0.88 0.26 0.51\n[3,] 0.63 0.24 0.59\n\n\nWhich are indeed the original values of \\(\\mathbf{B}\\)!\nNow, the idea is that we can perform this eigendecomposition with any matrix, including a network adjacency matrix or a proximity matrix derived from it like the ones we used to calculate the PageRank Status scores earlier.\nIn fact, we have already done that in part many times before, because the status scores compute the first column (leading eigenvector) of the \\(\\mathbf{U}\\) matrix for any proximity or adjacency matrix you feed into it.\nThe key point is that once you have the eigendecomposition of the matrix, and the full set of eigenvectors stored in \\(\\mathbf{U}\\), you can always choose the first few columns of \\(\\mathbf{U}\\), which gives us the best low dimensional approximation of the original matrix.\n\n\nThe CA Matrix\nSo which matrix should be use for the CA? Let’s see:\nFirst we need to create row stochastic versions of \\(\\mathbf{A}\\) and \\(\\mathbf{A}^T\\). Recall that a matrix is row stochastic if their rows sum to one.\nFor the people, we can do this by taking the original affiliation matrix, and pre-multiplying it by a diagonal square matrix \\(\\mathbf{D}_P^{-1}\\) of dimensions \\(M \\times M\\) containing the inverse of the degrees of each person in the affiliation network along the diagonals and zeros everywhere else, yielding the row-stochastic matrix \\(\\mathbf{P}_P\\) of dimensions \\(M \\times N\\):\n\\[\n\\mathbf{P}_P = \\mathbf{D}_P^{-1}\\mathbf{A}\n\\]\nAnd we can do the same with the groups, except that we pre-multiply the transpose of the original affiliation matrix by \\(\\mathbf{D}_G^{-1}\\) which is an \\(N \\times N\\) matrix containing the inverse of the size of each group along the diagonals and zero everywhere else, this yields the matrix \\(\\mathbf{P}_G\\) of dimensions \\(N \\times M\\):\n\\[\n\\mathbf{P}_G = \\mathbf{D}_G^{-1}\\mathbf{A}^T\n\\]\nIn R can compute \\(\\mathbf{P}_P\\) and \\(\\mathbf{P}_G\\) as follows:\n\n   D.p <- diag(1/rowSums(A))\n   P.p <- D.p %*% A\n   rownames(P.p) <- rownames(A)\n   D.g <- diag(1/colSums(A))\n   P.g <- D.g %*% t(A)\n   rownames(P.g) <- colnames(A)\n\nAnd we can check that both P.p (for people) and P.g (groups) are row stochastic:\n\n   rowSums(P.p)\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n        1         1         1         1         1         1         1         1 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n        1         1         1         1         1         1         1         1 \n   OLIVIA     FLORA \n        1         1 \n\n   rowSums(P.g)\n\n 6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10  2/23   4/7 11/21 \n    1     1     1     1     1     1     1     1     1     1     1     1     1 \n  8/3 \n    1 \n\n\nAnd that they are of the predicted dimensions:\n\n   dim(P.p)\n\n[1] 18 14\n\n   dim(P.g)\n\n[1] 14 18\n\n\nGreat! Now, we can obtain the degree-normalized projections for people by multiplying \\(\\mathbf{P}_P\\) times \\(\\mathbf{P}_G\\):\n\\[\n\\mathbf{P}_{PP} = \\mathbf{P}_P\\mathbf{P}_G\n\\]\nWhich produces the matrix \\(\\mathbf{P}_{PP}\\) a square \\(M \\times M\\) matrix containing the degree-normalized similarities between each pair of people.\nWe then do the same for groups:\n\\[\n\\mathbf{P}_{GG} = \\mathbf{P}_G\\mathbf{P}_P\n\\]\nWhich produces the matrix \\(\\mathbf{P}_{GG}\\) a square \\(N \\times N\\) matrix containing the degree-normalized similarities between each pair of groups.\nIn R we obtain these matrices as follows:\n\n   P.pp <- P.p %*% P.g\n   P.gg <- P.g %*% P.p\n\nWhich are still row stochastic–but now square–matrices:\n\n   rowSums(P.pp)\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n        1         1         1         1         1         1         1         1 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n        1         1         1         1         1         1         1         1 \n   OLIVIA     FLORA \n        1         1 \n\n   rowSums(P.gg)\n\n 6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10  2/23   4/7 11/21 \n    1     1     1     1     1     1     1     1     1     1     1     1     1 \n  8/3 \n    1 \n\n   dim(P.pp)\n\n[1] 18 18\n\n   dim(P.gg)\n\n[1] 14 14\n\n\nLet’s peek inside one of these matrices:\n\n   round(P.pp[1:10, 1:10], 2)\n\n          EVELYN LAURA THERESA BRENDA CHARLOTTE FRANCES ELEANOR PEARL RUTH\nEVELYN      0.19  0.14    0.14   0.13      0.07    0.06    0.04  0.03 0.03\nLAURA       0.16  0.18    0.13   0.13      0.06    0.07    0.06  0.03 0.04\nTHERESA     0.14  0.12    0.16   0.10      0.08    0.06    0.05  0.03 0.05\nBRENDA      0.15  0.13    0.12   0.17      0.09    0.07    0.06  0.03 0.04\nCHARLOTTE   0.14  0.10    0.16   0.16      0.16    0.07    0.06  0.00 0.06\nFRANCES     0.12  0.12    0.12   0.12      0.07    0.12    0.08  0.05 0.05\nELEANOR     0.08  0.11    0.11   0.11      0.06    0.08    0.11  0.05 0.07\nPEARL       0.09  0.07    0.09   0.07      0.00    0.07    0.07  0.09 0.05\nRUTH        0.07  0.07    0.09   0.07      0.06    0.05    0.07  0.04 0.09\nVERNE       0.04  0.04    0.06   0.04      0.03    0.02    0.04  0.04 0.06\n          VERNE\nEVELYN     0.02\nLAURA      0.02\nTHERESA    0.03\nBRENDA     0.02\nCHARLOTTE  0.03\nFRANCES    0.02\nELEANOR    0.04\nPEARL      0.05\nRUTH       0.06\nVERNE      0.11\n\n\nWhat are these numbers? Well, they can be interpreted as probabilities that a random walker starting at the row node and, following any sequence of \\(person-group-person'-group'\\) hops, will reach the column person. Thus, higher values indicate an affinity or proximity between the people (and the groups in the corresponding matrix).\n\n\nThe Duality of CA Scores Between Persons and Groups\nWe went through all these steps because CA works with the eigendecomposition of the two last square matrices we obtained, namely, \\(\\mathbf{P_{PP}}\\) and \\(\\mathbf{P_{GG}}\\):\n\n   CA.p <- eigen(P.pp)\n   CA.g <- eigen(P.gg)\n\nLet’s see what we have here:\n\n   round(CA.p$values, 2)\n\n [1] 1.00 0.63 0.32 0.18 0.14 0.11 0.10 0.06 0.04 0.04 0.02 0.01 0.01 0.00 0.00\n[16] 0.00 0.00 0.00\n\n   round(CA.g$values, 2)\n\n [1] 1.00 0.63 0.32 0.18 0.14 0.11 0.10 0.06 0.04 0.04 0.02 0.01 0.01 0.00\n\n\nSo the two matrices have identical eigenvalues, and the first one is 1.0. Let’s check out the first three eigenvectors:\n\n   rownames(CA.p$vectors) <- rownames(A)\n   rownames(CA.g$vectors) <- colnames(A)\n   round(CA.p$vectors[, 1:3], 2)\n\n           [,1]  [,2]  [,3]\nEVELYN    -0.24 -0.24  0.03\nLAURA     -0.24 -0.25 -0.01\nTHERESA   -0.24 -0.20  0.02\nBRENDA    -0.24 -0.26 -0.01\nCHARLOTTE -0.24 -0.29 -0.01\nFRANCES   -0.24 -0.24 -0.02\nELEANOR   -0.24 -0.15 -0.03\nPEARL     -0.24 -0.01  0.06\nRUTH      -0.24 -0.05  0.03\nVERNE     -0.24  0.13 -0.04\nMYRNA     -0.24  0.25 -0.10\nKATHERINE -0.24  0.31 -0.22\nSYLVIA    -0.24  0.26 -0.20\nNORA      -0.24  0.26 -0.03\nHELEN     -0.24  0.24  0.07\nDOROTHY   -0.24  0.09  0.09\nOLIVIA    -0.24  0.33  0.66\nFLORA     -0.24  0.33  0.66\n\n   round(CA.g$vectors[, 1:3], 2)\n\n      [,1]  [,2]  [,3]\n6/27  0.27 -0.30 -0.01\n3/2   0.27 -0.28 -0.04\n4/12  0.27 -0.30  0.00\n9/26  0.27 -0.30 -0.02\n2/25  0.27 -0.25  0.00\n5/19  0.27 -0.16  0.00\n3/15  0.27 -0.04  0.05\n9/16  0.27 -0.01  0.05\n4/8   0.27  0.15 -0.19\n6/10  0.27  0.32  0.22\n2/23  0.27  0.35 -0.79\n4/7   0.27  0.29  0.20\n11/21 0.27  0.34  0.35\n8/3   0.27  0.34  0.35\n\n\nSo this is interesting. The first eigenvector of the decomposition of both \\(\\mathbf{P_{PP}}\\) and \\(\\mathbf{P_{GG}}\\) is just the same number for each person and group. Note that this is the eigenvector that is associated with the first eigenvalue which happens to be \\(\\lambda_1 = 1.0\\).\nSo it looks like the first eigenvector is a pretty useless quantity (a constant) so we can discard it, keeping all the other ones. Now the old second eigenvector is the first, the old third is the second, and so on:\n\n   eig.vec.p <- CA.p$vectors[, 2:ncol(CA.p$vectors)]\n   eig.vec.g <- CA.g$vectors[, 2:ncol(CA.g$vectors)]\n\nNote that the rest of the eigenvalues (discarding the 1.0 one) are arranged in descending order:\n\n   eig.vals <- CA.p$values[2:length(CA.p$values)]\n   round(eig.vals, 3)\n\n [1] 0.627 0.319 0.179 0.138 0.107 0.099 0.064 0.044 0.036 0.021 0.012 0.005\n[13] 0.000 0.000 0.000 0.000 0.000\n\n\nThe magnitude of the eigenvalue tells us how important is the related eigenvector in containing information about the original matrix. So it looks like here, the first two eigenvectors contain a good chunk of the info:\n\n   round(sum(eig.vals[1:2])/sum(eig.vals), 2)\n\n[1] 0.57\n\n\nBecause the magnitude of the CA eigenvectors don’t have a natural scale, it is common to normalize to have a variance of 1.0.\nWe can do this as follows:\n\n   N <- sum(A)\n   d.p <- diag(rowSums(A))\n   d.g <- diag(colSums(A))\n   eig.vec.g[, 2] <- eig.vec.g[, 2] * -1\n   for (i in 1:nrow(A)-1) {\n      norm <- as.numeric(t(eig.vec.p[, i]) %*% d.p %*% eig.vec.p[, i])\n      eig.vec.p[, i] <- eig.vec.p[, i] * sqrt(N/norm)\n   }\n   for (j in 1:ncol(A)-1) {\n      norm <- as.numeric(t(eig.vec.g[, j]) %*% d.g %*% eig.vec.g[, j])\n      eig.vec.g[, j] <- eig.vec.g[, j] * sqrt(N/norm)\n      }\n\nJust like with the eigenvector centrality there is a duality between the CA scores assigned to the person and the groups on each dimension, such that the scores for each person are a weighted sum of the sum of the scores assigned to each group on that dimension and vice versa (Faust 1997, 171). The main difference is that this time we sum scores across the \\(\\mathbf{P}\\) matrices rather than the original affiliation matrix.\nSo for any given person, let’s say \\(EVELYN\\), the CA score on the first dimension is given by the sum of their degree-weighted CA group scores on the same dimension:\n\n   sum(P.p[\"EVELYN\", ] * eig.vec.g[, 1])\n\n[1] -0.7994396\n\n\nWhich is the same as their (eigenvalue-weighted) normalized score:\n\n   sqrt(eig.vals[1]) * eig.vec.p[\"EVELYN\", 1]\n\n    EVELYN \n-0.7994396 \n\n\nThe same goes for groups. Each group score is the degree-weighted sum of the people who join it:\n\n   sum(P.g[\"6/27\", ] * eig.vec.p[, 1])\n\n[1] -1.051052\n\n\nWhich is the same as that group’s (eigenvalue-weighted) normalized score:\n\n   sqrt(eig.vals[1]) * eig.vec.g[\"6/27\", 1]\n\n     6/27 \n-1.051052"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Social Networks Sequence",
    "section": "",
    "text": "Website containing syllabi, reading schedules, and other instructional materials for the courses in the Social Network Analysis computational sequence (208A & 208B) at UCLA Sociology."
  },
  {
    "objectID": "inoutdegdist.html",
    "href": "inoutdegdist.html",
    "title": "Graph Degree Metrics in Directed Networks",
    "section": "",
    "text": "As we saw in the Basic Network Statistics Handout, the degree distribution and the degree correlation are two basic things we want to have a sense of when characterizing a network, and we showed examples for the undirected graph case.\nAs we also saw on the Centrality handout, the number of things you have to compute in terms of degrees “doubles” in the directed graph case; for instance, instead of a single degree set and sequence, now we have two: An out and an indegree set and sequence.\nThe same thing applies to the degree distribution and the degree correlation."
  },
  {
    "objectID": "inoutdegdist.html#in-and-out-degree-distributions",
    "href": "inoutdegdist.html#in-and-out-degree-distributions",
    "title": "Graph Degree Metrics in Directed Networks",
    "section": "In and Out Degree Distributions",
    "text": "In and Out Degree Distributions\nIn the case of the degree distribution, now we have two distributions: An outdegree distribution and an indegree distribution.\nLet’s see an example using the law_advice data.\n\n   library(networkdata)\n   library(igraph)\n   g <- law_advice\n   i.prop <- degree_distribution(g, mode = \"in\")\n   o.prop <- degree_distribution(g, mode = \"out\")\n\nSo the main complication is that now we have to specify a value for the “mode” argument; “in” for indegree and “out” for outdegree.\nThat also means that when plotting, we have to create two data frames and present two plots.\nFirst the data frames:\n\n   i.d <- degree(g, mode = \"in\")\n   o.d <- degree(g, mode = \"out\")\n   i.d.vals <- c(0:max(i.d))\n   o.d.vals <- c(0:max(o.d))\n   i.deg.dist <- data.frame(i.d.vals, i.prop)\n   o.deg.dist <- data.frame(o.d.vals, o.prop)\n   head(i.deg.dist)\n\n  i.d.vals     i.prop\n1        0 0.01408451\n2        1 0.02816901\n3        2 0.07042254\n4        3 0.02816901\n5        4 0.08450704\n6        5 0.02816901\n\n   head(o.deg.dist)\n\n  o.d.vals     o.prop\n1        0 0.01408451\n2        1 0.00000000\n3        2 0.01408451\n4        3 0.05633803\n5        4 0.04225352\n6        5 0.04225352\n\n\nNow, to plotting. To be effective, the resulting plot has to show the outdegree and indegree distribution side by side so as to allow the reader to compare. To do that, we first generate each plot separately:\n\n   library(ggplot2)\n   p <- ggplot(data = o.deg.dist, aes(x = o.d.vals, y = o.prop))\n   p <- p + geom_bar(stat = \"identity\", fill = \"red\", color = \"red\")\n   p <- p + theme_minimal()\n   p <- p + labs(x = \"\", y = \"Proportion\", \n                 title = \"Outdegree Distribution in Law Advice Network\") \n   p <- p + geom_vline(xintercept = mean(o.d), \n                       linetype = 2, linewidth = 0.75, color = \"blue\")\n   p1 <- p + scale_x_continuous(breaks = c(0, 5, 10, 15, 20, 25, 30, 35, 40)) + xlim(0, 40)\n\n   p <- ggplot(data = i.deg.dist, aes(x = i.d.vals, y = i.prop))\n   p <- p + geom_bar(stat = \"identity\", fill = \"red\", color = \"red\")\n   p <- p + theme_minimal()\n   p <- p + labs(x = \"\", y = \"Proportion\", \n                 title = \"Indegree Distribution in Law Advice Network\") \n   p <- p + geom_vline(xintercept = mean(i.d), \n                       linetype = 2, linewidth = 0.75, color = \"blue\")\n   p2 <- p + scale_x_continuous(breaks = c(0, 5, 10, 15, 20, 25, 30, 35, 40)) + xlim(0, 40)\n\nThen we use the magical package patchwork to combine the plots:\n\n   # install.packages(\"patchwork\")\n   library(patchwork)\n   p <- p1 / p2\n   p\n\n\n\n\nThe data clearly shows that while both distributions are skewed, the indegree distribution is more heterogeneous, with a larger proportion of nodes in the high end of receiving advice as compared to giving advice.\nNote also that since the mean degree is the same regardless of whether we use the out or indegree distribution, then the blue line pointing to the mean degree falls in the same spot on the x-axis for both plots."
  },
  {
    "objectID": "inoutdegdist.html#in-and-out-degree-correlations",
    "href": "inoutdegdist.html#in-and-out-degree-correlations",
    "title": "Graph Degree Metrics in Directed Networks",
    "section": "In and Out Degree Correlations",
    "text": "In and Out Degree Correlations\nThe same doubling (really quadrupling) happens to degree correlations in directed graphs. While in an undirected graph, there is a single degree correlation, in the directed case we have four quantities to compute: The out-out degree correlation, the in-in degree correlation, the out-in degree correlation, and the in-out degree correlation (see here, p. 38).\nTo proceed, we need to create an edge list data set with six columns: The node id of the “from” node, the node id of the “to” node, the indegree of the “from” node, the outdegree of the “from” node, the indegree of the “to” node, and the outdegree of the “to” node.\nWe can adapt the code we used for the undirected case for this purpose. First, we create an edge list data frame using the igraph function as_data_frame:\n\n   library(dplyr)\n   g.el <- igraph::as_data_frame(g) %>% \n      rename(fr = from)\n   head(g.el)\n\n  fr to\n1  1  2\n2  1 17\n3  1 20\n4  2  1\n5  2  6\n6  2 17\n\n\nNote that we have to specify that this is in an igraph function by typing igraph:: in front of the as_data_frame because there is an (older) dplyr function with the same name that was used for data wrangling.\nSecond, we create data frames containing the in and outdegrees of each node in the network:\n\n    deg.dat.fr <- data.frame(fr = 1:vcount(g), o.d, i.d)\n    deg.dat.to <- data.frame(to = 1:vcount(g), o.d, i.d)\n\nThird, we merge this info into the edge list data frame to get the in and outdegrees of the from and to nodes in the directed edge:\n\n   d.el <- g.el %>% \n      left_join(deg.dat.fr) %>% \n      rename(o.d.fr = o.d, i.d.fr = i.d) %>% \n      left_join(deg.dat.to, by = \"to\") %>% \n      rename(o.d.to = o.d, i.d.to = i.d) \n   head(d.el)\n\n  fr to o.d.fr i.d.fr o.d.to i.d.to\n1  1  2      3     22      7     23\n2  1 17      3     22     21     26\n3  1 20      3     22     11     22\n4  2  1      7     23      3     22\n5  2  6      7     23      0     21\n6  2 17      7     23     21     26\n\n\nNow we can compute the four different flavors of the degree correlation for directed graphs:\n\n   round(cor(d.el$o.d.fr, d.el$o.d.to), 4) #out-out correlation\n\n[1] -0.0054\n\n   round(cor(d.el$i.d.fr, d.el$i.d.to), 4) #in-in correlation\n\n[1] 0.187\n\n   round(cor(d.el$i.d.fr, d.el$o.d.to), 4) #in-out correlation\n\n[1] -0.0283\n\n   round(cor(d.el$o.d.fr, d.el$i.d.to), 4) #out-in correlation\n\n[1] -0.0843\n\n\nThese results tell us that there is not much degree assortativity going on in the law advice network, except for a slight tendency of people who receive advice from lots of others to give advice to people who also receive advice from a lot of other people (the “in-in” correlation)\nNote that by default, the assortativity_degree function in igraph only returns the out-in correlation for directed graphs:\n\n   round(assortativity_degree(g, directed = TRUE), 4)\n\n[1] -0.0843\n\n\nThat is, assortativity_degree checks if more active senders are more likely to send ties to people who are popular receivers of ties."
  },
  {
    "objectID": "louvain.html",
    "href": "louvain.html",
    "title": "An Agglomertive Approach to Community Detection",
    "section": "",
    "text": "library(networkdata)\n   library(igraph)\n\n\n    louvain <- function(x)  {\n      iter <- 0\n      n.moves <- 1\n      Q <- modularity_matrix(x, directed = FALSE)\n      v.names <- V(x)$name\n      while(n.moves != 0) {\n        n.moves <- 0\n        if (iter == 0) {\n          I <- diag(1, vcount(x), vcount(x)) #identity matrix\n          U <- I\n          l <- 1:vcount(x)\n          names(l) <- 1:vcount(x)\n          A <- as.matrix(as_adjacency_matrix(x))\n          vol <- sum(A)\n          }\n        node.list <- 1:vcount(x)\n        sampled.nodes <- 0\n        m <- 1\n        while (length(sampled.nodes) < 5*vcount(x)) {\n          i <- sample(node.list, 1, replace = TRUE)\n          sampled.nodes[m] <- i\n          #sampled.nodes <- sort(unique(sampled.nodes))\n          #print(sampled.nodes)\n          u.k <- U[, l[i]] #i's group\n          N <- unique(as.numeric(neighbors(x, i))) #i's neighbors\n          adj.clus <- l[N] #adjacent clusters\n          e.i <- I[, i]\n          Q.delta <- 0\n          k <- 1\n          for (j in N) { #populating Q.delta vector\n            u.l <- U[, adj.clus[k]] #j's group\n            Q.new <- (t(u.k - e.i) %*% Q %*% (u.k - e.i) + \n                      t(u.l + e.i) %*% Q %*% (u.l + e.i))\n            Q.old <- (t(u.k) %*% Q %*% u.k) + (t(u.l) %*% Q %*% u.l)\n            Q.delta[k] <- (Q.new - Q.old)/vol\n            k <- k + 1\n            } #end j for loop\n          max.Q <- max(Q.delta)\n          max.Q.pos <- which(Q.delta == max.Q)\n          if (max.Q > 0 & length(max.Q.pos) == 1) {\n            j <- N[max.Q.pos]\n            U[i, l[i]] <- 0\n            U[i, l[j]] <- 1\n            n.moves <- n.moves + 1\n            } #end if\n          if (max.Q > 0 & length(max.Q.pos) > 1) {\n            j <- N[sample(max.Q.pos, 1)]\n            U[i, l[i]] <- 0\n            U[i, l[j]] <- 1\n            n.moves <- n.moves + 1\n            } #end if\n          m <- m + 1\n          } # end main while loop\n        U <- U[, colSums(U)> 0] #eliminating zero columns\n        for (k in 1:ncol(U)) {\n          l[which(U[, k] == 1)] <- k #re-assigning node labels\n          }\n        #print(l)\n        print(n.moves)\n        A.new <- t(U) %*% A %*% U #coarsening adjacency matrix\n        x <- graph_from_adjacency_matrix(A.new, mode = \"undirected\")\n        iter <- iter + 1\n        } #end while\n      rownames(U) <- v.names\n      return(list(U, l))\n      } #end function\n  g <- movie_651\n  louvain(g)\n\n[1] 87\n[1] 3\n[1] 0\n\n\n[[1]]\n                    [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\nBEACH                  1    0    0    0    0    0    0    0\nBONES                  0    0    0    0    1    0    0    0\nCADET                  1    0    0    0    0    0    0    0\nCAROL                  0    1    1    0    0    0    0    0\nCHEKOV                 1    0    0    0    0    0    0    0\nCOMPUTER VOICE         0    0    0    0    1    0    0    0\nDAVID                  0    1    0    0    0    0    0    0\nHELMSMAN               0    0    0    1    0    0    0    0\nHURRY                  0    0    0    0    1    0    0    0\nINTERCOM VOICE         0    0    0    0    1    0    0    1\nJEDDA                  0    1    0    0    0    0    0    0\nJOACHIM                0    0    0    1    0    0    0    0\nKHAN                   0    0    0    1    0    0    0    0\nKIRK                   0    0    0    0    1    0    0    0\nKYLE                   0    0    0    0    0    1    0    0\nMADISON                0    1    1    0    0    0    0    0\nMAIN TITLE SEQUENCE    0    0    0    0    0    0    1    0\nPRESTON                0    0    0    0    1    0    0    0\nSAAVIK                 0    0    0    0    1    0    0    0\nSCOTTY                 0    0    0    0    1    0    0    0\nSPOCK                  0    0    0    0    1    0    0    0\nSTATIC                 0    1    0    0    1    0    0    0\nSULU                   0    0    0    0    1    0    0    0\nTERRELL                1    0    0    0    0    0    0    0\nUHURA                  0    0    0    0    1    0    0    0\n\n[[2]]\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 \n 1  5  1  3  1  5  2  4  5  8  2  4  4  5  6  3  7  5  5  5  5  5  5  1  5"
  },
  {
    "objectID": "schedule-208A-F24.html",
    "href": "schedule-208A-F24.html",
    "title": "SOCIOL 208A Reading Schedule (Fall 2024)",
    "section": "",
    "text": "Prell, C. & Schaefer, D. R. (2023). Introducing Social Network Analysis. In J. McLevey, J. Scott, P. J. Carrington (Eds.) The SAGE Handbook of Social Network Analysis. Sage Publications. link\nLight, R. & Moody, J. (2021). Network Basics: Points, Lines, and Positions. In R. Light, and J. Moody (Eds.) The Oxford Handbook of Social Networks Oxford University Press. link\nHarary, F. & Norman., R. Z. (1953). Graph Theory as a Mathematical Model in Social Science. Research Center for Group Dynamics, University of Michigan. link\n\n\n\n\n\nBasic Network Concepts and Definitions Cheat Sheet.\nBasic Introduction to R\nThe Basics of the R Programming Language Short Intergraph Tutorial\n\nPackage networkdata"
  },
  {
    "objectID": "schedule-208A-F24.html#week-2-october-9-centrality",
    "href": "schedule-208A-F24.html#week-2-october-9-centrality",
    "title": "SOCIOL 208A Reading Schedule (Fall 2024)",
    "section": "Week 2, October 9: Centrality",
    "text": "Week 2, October 9: Centrality\n\nReadings\n\nMartin G. Everett & Steve P. Borgatti (2023). “Centrality.” In J. McLevey, J. Scott, P. J. Carrington (Eds.) The SAGE Handbook of Social Network Analysis. Sage Publications. link\nFreeman, L. C. (1978). Centrality in Social Networks Conceptual Clarification. Social Networks, 1(3), 215-239. pdf\nKoschützki, D., Lehmann, K.A., Peeters, L., Richter, S., Tenfelde-Podehl, D., Zlotowski, O. (2005). Centrality Indices. In: Brandes, U., Erlebach, T. (eds) Network Analysis. Lecture Notes in Computer Science, vol 3418. Springer, Berlin, Heidelberg (secs. 3.2, 3.3, and 3.4). link\nNeal, Z. P. (2014). A network perspective on the processes of empowered organizations. American Journal of Community Psychology, 53, 407-418. https://doi.org/10.1007/s10464-013-9623-1\nAgneessens, F., Borgatti, S. P., & Everett, M. G. (2017). Geodesic based centrality: Unifying the local and the global. Social Networks, 49, 12-26. link\n\n\n\nExplainers\n\nLizardo, O. (n.d). Centralities based on Degree. link\nLizardo, O. (n.d). Centralities based on the Geodesic Distance. link\nLizardo, O. (n.d). Centralities based on Shortest Paths. link\n\n\n\nOther Material & Further Reading\n\nBorgatti, S. P., & Everett, M. G. (2006). A graph-theoretic perspective on centrality. Social Networks, 28(4), 466-484. link\nBrandes, U., Borgatti, S. P., & Freeman, L. C. (2016). Maintaining the duality of closeness and betweenness centrality. Social networks, 44, 153-159. link\nKoschützki, D., Lehmann, K.A., Tenfelde-Podehl, D., Zlotowski, O. (2005). Advanced Centrality Concepts. In: Brandes, U., Erlebach, T. (eds) Network Analysis. Lecture Notes in Computer Science, vol 3418. Springer, Berlin, Heidelberg. link\nComprehensive list of centrality measures with formulas and software"
  },
  {
    "objectID": "schedule-208A-F24.html#week-3-october-16-status-and-prestige",
    "href": "schedule-208A-F24.html#week-3-october-16-status-and-prestige",
    "title": "SOCIOL 208A Reading Schedule (Fall 2024)",
    "section": "Week 3, October 16: Status and Prestige",
    "text": "Week 3, October 16: Status and Prestige\n\nFranceschet, M. (2011). PageRank: standing on the shoulders of giants. Communications of the ACM, 54(6), 92-101. link\nMartin, J. L. & Murphy, J. P. (2021). Networks, Status, and Inequality. In R. Light, and J. Moody (Eds.) The Oxford Handbook of Social Networks Oxford University Press. link\nKoschützki, D., Lehmann, K.A., Peeters, L., Richter, S., Tenfelde-Podehl, D., Zlotowski, O. (2005). Centrality Indices. In: Brandes, U., Erlebach, T. (eds) Network Analysis. Lecture Notes in Computer Science, vol 3418. Springer, Berlin, Heidelberg (sec. 3.9). link"
  },
  {
    "objectID": "schedule-208A-F24.html#week-4-october-23",
    "href": "schedule-208A-F24.html#week-4-october-23",
    "title": "SOCIOL 208A Reading Schedule (Fall 2024)",
    "section": "Week 4, October 23:",
    "text": "Week 4, October 23:\n\nNo Class (Traveling)"
  },
  {
    "objectID": "schedule-208A-F24.html#week-5-october-30-similarity-roles-and-positions",
    "href": "schedule-208A-F24.html#week-5-october-30-similarity-roles-and-positions",
    "title": "SOCIOL 208A Reading Schedule (Fall 2024)",
    "section": "Week 5, October 30: Similarity, Roles, and Positions",
    "text": "Week 5, October 30: Similarity, Roles, and Positions\n\nReadings\n\nBurt, R. S. (1976). Positions in networks. Social Forces, 55(1), 93-122. link\nBreiger, R. L., Boorman, S. A., & Arabie, P. (1975). An algorithm for clustering relational data with applications to social network analysis and comparison with multidimensional scaling. Journal of Mathematical Psychology, 12(3), 328-383. link\nLü, L., Jin, C. H., & Zhou, T. (2009). Similarity index based on local paths for link prediction of complex networks. Physical Review E—Statistical, Nonlinear, and Soft Matter Physics, 80(4), 046122. link\nJeh, G., & Widom, J. (2002). Simrank: a measure of structural-context similarity. In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 538-543). link\nLeicht, E. A., Holme, P., & Newman, M. E. (2006). Vertex similarity in networks. Physical Review E—Statistical, Nonlinear, and Soft Matter Physics, 73(2), 026120. link\n\n\n\nFurther Reading\n\nFouss, F., Pirotte, A., Renders, J. M., & Saerens, M. (2007). Random-walk computation of similarities between nodes of a graph with application to collaborative recommendation. IEEE Transactions on knowledge and data engineering, 19(3), 355-369. link\nKovács, B. (2010). A generalized model of relational similarity. Social Networks, 32(3), 197-211. link\nLiben-Nowell, D., & Kleinberg, J. (2003). The link prediction problem for social networks. In Proceedings of the Twelfth Annual ACM International Conference on Information and Knowledge Management (CIKM’03) (pp. 556-559). link to longer paper\nLü, L., & Zhou, T. (2011). Link prediction in complex networks: A survey. Physica A: statistical mechanics and its applications, 390(6), 1150-1170. link\n\n\n\nCheat Sheets:\n\nCentrality, Status, and Node Similarity Cheat Sheet.\nChroł, B & Bojanowski, M. (2018). Proximity-based Methods for Link Prediction. https://cran.r-project.org/web/packages/linkprediction/vignettes/proxfun.html"
  },
  {
    "objectID": "schedule-208A-F24.html#week-6-november-6-subgroups-and-communities",
    "href": "schedule-208A-F24.html#week-6-november-6-subgroups-and-communities",
    "title": "SOCIOL 208A Reading Schedule (Fall 2024)",
    "section": "Week 6, November 6: Subgroups and Communities",
    "text": "Week 6, November 6: Subgroups and Communities\n\nReadings\n\nMoody, J., & Mucha, P. J. (2023). Structural Cohesion and Cohesive Groups. In J. McLevey, J. Scott, P. J. Carrington (Eds.) The SAGE Handbook of Social Network Analysis. Sage Publications. link\nShai, S., Stanley, N., Granell, C., Taylor, D. & Mucha, P. J. (2021). Case Studies in Network Community Detection. In R. Light, and J. Moody (Eds.) The Oxford Handbook of Social Networks Oxford University Press. link\nNewman, M. E. (2018). Community Structure. In Networks, 2nd Edition. Oxford, Online Edition, Oxford Academic. link\nFortunato, S. (2010). Community Detection in Graphs. Physics Reports, 486(3-5), 75-174. link\n\n\n\nFurther Reading\n\nNewman, M. E. (2006). Modularity and Community Structure in Networks. Proceedings of the National Academy of Sciences, 103(23), 8577-8582. link\nClauset, A., Newman, M. E., & Moore, C. (2004). Finding community structure in very large networks. Physical Review E—Statistical, Nonlinear, and Soft Matter Physics, 70(6), 066111. link\nNewman, M. E., & Girvan, M. (2003). Mixing patterns and community structure in networks. In Statistical mechanics of complex networks (pp. 66-87). Berlin, Heidelberg: Springer Berlin Heidelberg.\nNewman, M. E. (2003). Mixing Patterns in Networks. Physical review E 67(2), 026126. link\nGirvan, M., & Newman, M. E. (2002). Community Structure in Social and Biological Networks. Proceedings of the National academy of Sciences, 99(12), 7821-7826. link\nNewman, M. E., & Girvan, M. (2004). Finding and evaluating community structure in networks. Physical review E, 69(2), 026113. link\nLeicht, E. A., and Newman, M. E. (2008). Community Structure in Directed Networks. Physical Review Letters 100, 118703. link"
  },
  {
    "objectID": "schedule-208A-F24.html#week-7-november-13-analyzing-two-mode-networks",
    "href": "schedule-208A-F24.html#week-7-november-13-analyzing-two-mode-networks",
    "title": "SOCIOL 208A Reading Schedule (Fall 2024)",
    "section": "Week 7, November 13: Analyzing Two-Mode Networks",
    "text": "Week 7, November 13: Analyzing Two-Mode Networks\n\nReadings\n\nBreiger, R. L. (1974). The Duality of Persons and Groups. Social Forces, 53(2), 181–190. link\nBorgatti, S. P., & Everett, M. G. (1997). Network Analysis of 2-Mode Data. Social Networks, 19(3), 243-269. pdf\nEverett, M. G., & Borgatti, S. P. (2013). The Dual-Projection Approach for Two-Mode Networks. Social Networks, 35(2), 204-210. link\nNeal, Z. (2014). The backbone of bipartite projections: Inferring relationships from co-authorship, co-sponsorship, co-attendance and other co-behaviors. Social Networks, 39, 84-97. link"
  },
  {
    "objectID": "schedule-208A-F24.html#further-reading-2",
    "href": "schedule-208A-F24.html#further-reading-2",
    "title": "SOCIOL 208A Reading Schedule (Fall 2024)",
    "section": "Further Reading",
    "text": "Further Reading\n\nBorgatti, S., & Halgin, D. (2014). Analyzing affiliation networks. In The SAGE Handbook of Social Network Analysis, First Edition (pp. 417-433), SAGE Publications Ltd. link\nFaust, K. (1997). Centrality in affiliation networks. Social Networks, 19(2), 157-191. link\n\n\nOther Material\n\nMurphy, Phil, and Brendan Knapp. (2018). Bipartite/two-mode networks in igraph. RPubs https://rpubs.com/pjmurphy/317838\nDomagalski, R., Neal, Z. P., & Sagan, B. (2021). Backbone: An R package for extracting the backbone of bipartite projections. Plos one, 16(1), e0244363. link\nNeal, Z. P. (2022). backbone: An R package to extract network backbones. PloS one, 17(5), e0269137. link"
  },
  {
    "objectID": "schedule-208A-F24.html#week-8-november-20-ego-networks",
    "href": "schedule-208A-F24.html#week-8-november-20-ego-networks",
    "title": "SOCIOL 208A Reading Schedule (Fall 2024)",
    "section": "Week 8, November 20: Ego Networks",
    "text": "Week 8, November 20: Ego Networks\n\nReadings\n\nSmith, J. A. (2021). The Continued Relevance of Ego Network Data. In R. Light, and J. Moody (Eds.) The Oxford Handbook of Social Networks Oxford University Press. link"
  },
  {
    "objectID": "schedule-208A-F24.html#week-9-november-27-statistical-models-of-network-structure",
    "href": "schedule-208A-F24.html#week-9-november-27-statistical-models-of-network-structure",
    "title": "SOCIOL 208A Reading Schedule (Fall 2024)",
    "section": "Week 9, November 27: Statistical Models of Network Structure",
    "text": "Week 9, November 27: Statistical Models of Network Structure\n\nReadings\n\nLusher D., Wang, P., Brennecke, J., Brailly J., Faye, M., Gallagher, C. (2021). Advances in Exponential Random Graph Models. In R. Light, and J. Moody (Eds.) The Oxford Handbook of Social Networks Oxford University Press. link"
  },
  {
    "objectID": "schedule-208A-F24.html#week-10-december-4-dynamic-networks-and-relational-events",
    "href": "schedule-208A-F24.html#week-10-december-4-dynamic-networks-and-relational-events",
    "title": "SOCIOL 208A Reading Schedule (Fall 2024)",
    "section": "Week 10, December 4: Dynamic Networks and Relational Events",
    "text": "Week 10, December 4: Dynamic Networks and Relational Events\n\nTBA"
  },
  {
    "objectID": "schedule-208B-S22.html",
    "href": "schedule-208B-S22.html",
    "title": "SOCIOL 208B Reading Schedule (Spring 2022)",
    "section": "",
    "text": "Marsden, P. V., & Laumann, E. O. (1984). Mathematical Ideas In Social Structural Analysis. Journal of Mathematical Sociology, 10(3-4), 271-294. [pdf]\nWellman, B. (1988). Structural Analysis: From Method and Metaphor To Theory and Substance. In B. Wellman & S. D. Berkowitz (Eds.), Social Structures: A Network Approach. (Pp. 19–61). Cambridge University Press. [pdf]\nEmirbayer, M. (1997). Manifesto For A Relational Sociology. American Journal of Sociology, 103(2), 281–317. [pdf]\n\n\n\n\n\nErikson, E. (2013). Formalist and Relationalist Theory In Social Network Analysis. Sociological Theory, 31(3), 219–242. [pdf]\nEmirbayer, M., & Goodwin, J. (1994). Network Analysis, Culture, and The Problem of Agency. American Journal of Sociology, 99(6), 1411–1454. [pdf]\nMische, A. (2011). Relational Sociology, Culture, and Agency. In J. Scott & P. J. Carrington (Eds.), The Sage Handbook of Social Network Analysis (Pp. 80–97). Sage. [pdf]"
  },
  {
    "objectID": "schedule-208B-S22.html#week-2-the-greatest-hits",
    "href": "schedule-208B-S22.html#week-2-the-greatest-hits",
    "title": "SOCIOL 208B Reading Schedule (Spring 2022)",
    "section": "Week 2: The Greatest Hits",
    "text": "Week 2: The Greatest Hits\n\nTue., Apr. 5\n\nWhite, H. C., Boorman, S. A., & Breiger, R. L. (1976). Social Structure From Multiple Networks. I. Blockmodels of Roles and Positions. American Journal of Sociology, 81(4), 730–780. [pdf]\nBreiger, R. L. (1974). The Duality of Persons and Groups. Social Forces, 53(2), 181–190. [pdf]\nFreeman, L. C. (1978). Centrality in social networks conceptual clarification. Social networks, 1(3), 215-239. [pdf]\n\n\n\nThu., Apr. 7\n\nGranovetter, M. S. (1973). The Strength of Weak Ties. American Journal of Sociology, 78(3), 1360–1380. [pdf]\n\nRejection letter from American Sociological Review of the first (1969) version of the paper [pdf]\nGranovetter, M. S. (1969) ``Alienation Reconsidered: The Strength of Weak Ties.’’ Reprinted in Connections 5(2): 4-16. [pdf]\n\nCentola, D., & Macy, M. (2007). Complex Contagions and the Weakness of Long Ties. American Journal of Sociology, 113(3), 702-734. [pdf]\nMcPherson, M., Smith-Lovin, L., & Cook, J. M. (2001). Birds of a feather: Homophily in social networks. Annual Review of Sociology, 27(1), 415-444. [pdf]"
  },
  {
    "objectID": "schedule-208B-S22.html#week-3-brokerage-and-intermediation",
    "href": "schedule-208B-S22.html#week-3-brokerage-and-intermediation",
    "title": "SOCIOL 208B Reading Schedule (Spring 2022)",
    "section": "Week 3: Brokerage and Intermediation",
    "text": "Week 3: Brokerage and Intermediation\n\nTue., Apr. 12\n\nGould, R. V., & Fernandez, R. M. (1989). Structures of Mediation: A Formal Approach To Brokerage In Transaction Networks. Sociological Methodology, 89-126. [pdf]\nObstfeld, D., Borgatti, S.P. and Davis, J. (2014). Brokerage As a Process: Decoupling Third Party Action From Social Network Structure. Research In The Sociology of Organizations, 40), 135-159. [pdf]\nStovel, K., & Shaw, L. (2012). Brokerage. Annual Review of Sociology, 38, 139-158. [pdf]\n\n\n\nThu., Apr. 14\n\nBurt, R. S. (2004). Structural Holes and Good Ideas. American Journal of Sociology, 110(2), 349-399.[pdf]\nGoldberg, A., Srivastava, S. B., Manian, V. G., Monroe, W., & Potts, C. (2016). Fitting In Or Standing Out? The Tradeoffs of Structural and Cultural Embeddedness. American Sociological Review, 81(6), 1190-1222. [pdf]\nAral, S., & Van Alstyne, M. (2011). The diversity-bandwidth trade-off. American Journal of Sociology, 117(1), 90-171. [pdf]"
  },
  {
    "objectID": "schedule-208B-S22.html#week-4-networks-in-science",
    "href": "schedule-208B-S22.html#week-4-networks-in-science",
    "title": "SOCIOL 208B Reading Schedule (Spring 2022)",
    "section": "Week 4: Networks In Science",
    "text": "Week 4: Networks In Science\n\nTue., Apr. 19\n\nMoody, J. (2004). The Structure of A Social Scientific Collaboration Network. American Sociological Review, 69, 213-238. [pdf]\nShwed, U., & Bearman, P. S. (2010). The Temporal Structure of Scientific Consensus Formation. American Sociological Review, 75(6), 817-840. [pdf]\nMoody, J., & Light, R. (2006). A view from above: The evolving sociological landscape. The American Sociologist, 37(2), 67-86. [pdf]\n\n\n\nThu., Apr. 21\n\nFoster, J. G., Rzhetsky, A., & Evans, J. A. (2015). Tradition and Innovation In Scientists’ Research Strategies. American Sociological Review, 80(5), 875-908. [pdf]\nMcmahan, P., & Mcfarland, D. A. (2021). Creative Destruction: The Structural Consequences of Scientific Curation. American Sociological Review, 86(2), 341-376. [pdf]"
  },
  {
    "objectID": "schedule-208B-S22.html#week-5-collaboration-creativity-and-field-dynamics",
    "href": "schedule-208B-S22.html#week-5-collaboration-creativity-and-field-dynamics",
    "title": "SOCIOL 208B Reading Schedule (Spring 2022)",
    "section": "Week 5: Collaboration, Creativity, and Field Dynamics",
    "text": "Week 5: Collaboration, Creativity, and Field Dynamics\n\nTue., Apr. 26\n\nVedres, B., & Stark, D. (2010). Structural Folds: Generative Disruption In Overlapping Groups. American Journal of Sociology, 115(4), 1150-1190. [pdf]\nUzzi, B, & Spiro, J. (2005). Collaboration and Creativity: The Small World Problem. American Journal of Sociology, 111, 447-504. [pdf]\n\n\n\nThu., Apr. 28\n\nPowell, W. W., White, D. R., Koput, K. W., & Owen-Smith, J. (2005). Network Dynamics and Field Evolution: The Growth of Interorganizational Collaboration In The Life Sciences. American Journal of Sociology, 110(4), 1132-1205. [pdf]\nRossman, G., Esparza, N., & Bonacich, P. (2010). I’d Like To Thank The Academy, Team Spillovers, and Network Centrality. American Sociological Review, 75(1), 31-51. [pdf]"
  },
  {
    "objectID": "schedule-208B-S22.html#week-6-networks-and-culture-and-culture-in-networks",
    "href": "schedule-208B-S22.html#week-6-networks-and-culture-and-culture-in-networks",
    "title": "SOCIOL 208B Reading Schedule (Spring 2022)",
    "section": "Week 6: Networks and Culture and Culture in Networks",
    "text": "Week 6: Networks and Culture and Culture in Networks\n\nTue., May. 3\n\nBreiger, R. L. (2010). Dualities of culture and structure: Seeing through cultural holes. Pp. 37-47 in Relationale soziologie. VS Verlag für Sozialwissenschaften. [pdf]\nLizardo, 0. (2023). Culture and Networks. Pp. 188-201 in The SAGE Handbook of Social Network Analysis. SAGE Publications Ltd. [pdf]\nLewis, K., & Kaufman, J. (2018). The Conversion of Cultural Tastes Into Social Network Ties. American Journal of Sociology, 123(6), 1684-1742. [pdf]\n\n\n\nThu., May. 5\n\nFuhse, J. A. (2009). The Meaning Structure of Social Networks. Sociological Theory, 27(1), 51-73. [pdf]\nIkegami, E. (2000). A Sociological Theory of Publics: Identity and Culture As Emergent Properties In Networks. Social Research, 989-1029. [pdf]\nMützel, S., & Breiger, R. (2021). Duality Beyond Persons and Groups. In The Oxford Handbook of Social Networks. Oxford University Press. [pdf]"
  },
  {
    "objectID": "schedule-208B-S22.html#week-7-difussion-in-networks",
    "href": "schedule-208B-S22.html#week-7-difussion-in-networks",
    "title": "SOCIOL 208B Reading Schedule (Spring 2022)",
    "section": "Week 7: Difussion In Networks",
    "text": "Week 7: Difussion In Networks\n\nTue., May. 10\n\nDellaposta, D., Shi, Y., & Macy, M. (2015) Why Do Liberals Drink Lattes?. American Journal of Sociology, 120(5), 1473-1511. [pdf]\nCentola, D. (2015). The social origins of networks and diffusion. American Journal of Sociology, 120(5), 1295-1338. [pdf]\nBecker, S. O., Hsiao, Y., Pfaff, S., & Rubin, J. (2020). Multiplex Network Ties and the Spatial Diffusion of Radical Innovations: Martin Luther’s Leadership In The Early Reformation. American Sociological Review, 85(5), 857-894. [pdf]\n\n\n\nThu., May. 12\n\nGoldberg, A., & Stein, S. K. (2018). Beyond Social Contagion: Associative Diffusion and The Emergence of Cultural Variation. American Sociological Review, 83(5), 897-932. [pdf]\nBail, C. A., Brown, T. W., & Mann, M. (2017). Channeling hearts and minds: Advocacy organizations, cognitive-emotional currents, and public conversation. American Sociological Review, 82(6), 1188-1213. [pdf]"
  },
  {
    "objectID": "schedule-208B-S22.html#week-8-networks-in-history",
    "href": "schedule-208B-S22.html#week-8-networks-in-history",
    "title": "SOCIOL 208B Reading Schedule (Spring 2022)",
    "section": "Week 8: Networks In History",
    "text": "Week 8: Networks In History\n\nTue., May. 17\n\nPadgett, J. F., & Ansell, C. K. (1993). Robust Action and the Rise of the Medici, 1400-1434. American Journal of Sociology, 98(6), 1259–1319. [pdf]\nGould, R. V. (1991). Multiple Networks and Mobilization In The Paris Commune, 1871. American Sociological Review, 56(6), 716-729. [pdf]\nErikson, E., & Feltham, E. (2021). Historical Network Research. In The Oxford Handbook of Social Networks. Oxford University Press. [pdf]\n\n\n\nThu., May. 19\n\nErikson, E., & Bearman, P. (2006). Malfeasance and the Foundations For Global Trade: The Structure of English Trade In The East Indies, 1601–1833. American Journal of Sociology, 112(1), 195-230. [pdf]\nBearman, P., Faris, R., & Moody, J. (1999). Blocking The Future: New Solutions For Old Problems In Historical Social Science. Social Science History, 23(4), 501-533. [pdf]"
  },
  {
    "objectID": "schedule-208B-S22.html#week-9-networks-and-inequality",
    "href": "schedule-208B-S22.html#week-9-networks-and-inequality",
    "title": "SOCIOL 208B Reading Schedule (Spring 2022)",
    "section": "Week 9: Networks and Inequality",
    "text": "Week 9: Networks and Inequality\n\nTue., May. 24\n\nGould, R. V. (2002). The Origins of Status Hierarchies: A Formal Theory and Empirical Test. American Journal of Sociology, 107(5), 1143-1178. [pdf]\nGondal, N. (2015). Inequality preservation through uneven diffusion of Cultural materials across stratified groups. Social Forces, 93(3), 1109-1137. [pdf]\nThomas, R. J., & Mark, N. P. (2013). Population size, network density, and the emergence of inherited inequality. Social Forces, 92(2), 521-544. [pdf]\n\n\n\nThu., May. 26\n\nBurris, V. (2004). The Academic Caste System: Prestige Hierarchies In PhD Exchange Networks. American Sociological Review, 69, 239-264. [pdf]\nFowler, J. H., Grofman, B., & Masuoka, N. (2007). Social networks in political science: Hiring and placement of Ph.Ds, 1960–2002. PS: Political Science & Politics, 40(4), 729-739. [pdf]\nClauset, A., Arbesman, S., & Larremore, D. B. (2015). Systematic inequality and hierarchy in faculty hiring networks. Science Advances, 1(1), e1400005. [pdf]\nGondal, N. (2018). Duality of departmental specializations and PhD exchange: A Weberian analysis of status in interaction using multilevel exponential random graph models (mERGM). Social Networks, 55, 202-212. [pdf]"
  },
  {
    "objectID": "schedule-208B-S22.html#week-10-networks-and-the-micromacro-link",
    "href": "schedule-208B-S22.html#week-10-networks-and-the-micromacro-link",
    "title": "SOCIOL 208B Reading Schedule (Spring 2022)",
    "section": "Week 10: Networks and The Micro/Macro Link",
    "text": "Week 10: Networks and The Micro/Macro Link\n\nTue., May. 31\n\nMcfarland, D. A., Moody, J., Diehl, D., Smith, J. A., & Thomas, R. J. (2014). Network Ecology and Adolescent Social Structure. American Sociological Review, 79(6), 1088-1121. [pdf]\nBearman, P. S., Moody, J., & Stovel, K. (2004). Chains of Affection: The Structure of Adolescent Romantic and Sexual Networks. American Journal of Sociology, 110(1), 44-91. [pdf]\n\n\n\nThu., Jun. 2\n\nLevy, B. L., Phillips, N. E., & Sampson, R. J. (2020). Triple Disadvantage: Neighborhood Networks of Everyday Urban Mobility and Violence In US Cities. American Sociological Review, 85(6), 925-956. [pdf]\nPapachristos, A. V. (2009). Murder By Structure: Dominance Relations and the Social Structure of Gang Homicide. American Journal of Sociology, 115(1), 74-128. [pdf]"
  },
  {
    "objectID": "schedule-208B-S24.html",
    "href": "schedule-208B-S24.html",
    "title": "SOCIOL 208B Reading Schedule (Spring 2024)",
    "section": "",
    "text": "Wellman, B. (1988). Structural Analysis: From Method and Metaphor To Theory and Substance. In B. Wellman & S. D. Berkowitz (Eds.), Social Structures: A Network Approach. (Pp. 19–61). Cambridge University Press.\nFreeman, L. (2004). The Development of Social Network Analysis. Vancouver, BC: Empirical Press (Introduction, and Chap. 9)\n\n\n\n\n\nEmirbayer, M. (1997). Manifesto For A Relational Sociology. American Journal of Sociology, 103(2), 281–317.\nErikson, E. (2013). Formalist and Relationalist Theory In Social Network Analysis. Sociological Theory, 31(3), 219–242.\nMische, A. (2011). Relational Sociology, Culture, and Agency. In J. Scott & P. J. Carrington (Eds.), The Sage Handbook of Social Network Analysis (Pp. 80–97). Sage."
  },
  {
    "objectID": "schedule-208B-S24.html#week-2-the-greatest-hits",
    "href": "schedule-208B-S24.html#week-2-the-greatest-hits",
    "title": "SOCIOL 208B Reading Schedule (Spring 2024)",
    "section": "Week 2: The Greatest Hits",
    "text": "Week 2: The Greatest Hits\n\nTuesday, April 9\n\nWhite, H. C., Boorman, S. A., & Breiger, R. L. (1976). Social Structure From Multiple Networks. I. Blockmodels of Roles and Positions. American Journal of Sociology, 81(4), 730–780.\nBreiger, R. L. (1974). The Duality of Persons and Groups. Social Forces, 53(2), 181–190.\nFreeman, L. C. (1978). Centrality in social networks conceptual clarification. Social networks, 1(3), 215-239.\n\n\n\nThursday, April 11\n\nGranovetter, M. S. (1973). The Strength of Weak Ties. American Journal of Sociology, 78(3), 1360–1380.\n\nRejection letter from American Sociological Review of the first (1969) version of the paper\nGranovetter, M. S. (1969) “Alienation Reconsidered: The Strength of Weak Ties.” Reprinted in Connections 5(2): 4-16.\n\nWang, D., & Uzzi, B. (2022). Weak ties, failed tries, and success. Science, 377(6612), 1256-1258.\nMcPherson, M., Smith-Lovin, L., & Cook, J. M. (2001). Birds of a feather: Homophily in social networks. Annual Review of Sociology, 27(1), 415-444."
  },
  {
    "objectID": "schedule-208B-S24.html#week-3-brokerage",
    "href": "schedule-208B-S24.html#week-3-brokerage",
    "title": "SOCIOL 208B Reading Schedule (Spring 2024)",
    "section": "Week 3: Brokerage",
    "text": "Week 3: Brokerage\n\nTuesday, April 16 (Virtual Meeting)\n\nGould, R. V., & Fernandez, R. M. (1989). Structures of Mediation: A Formal Approach To Brokerage In Transaction Networks. Sociological Methodology, 89-126.\nObstfeld, D., Borgatti, S.P. and Davis, J. (2014). Brokerage As a Process: Decoupling Third Party Action From Social Network Structure. Research In The Sociology of Organizations, 40), 135-159.\nStovel, K., & Shaw, L. (2012). Brokerage. Annual Review of Sociology, 38, 139-158.\n\n\n\nThursday, April 18 (Virtual Meeting)\n\nBurt, R. S. (2004). Structural Holes and Good Ideas. American Journal of Sociology, 110(2), 349-399.\nGoldberg, A., Srivastava, S. B., Manian, V. G., Monroe, W., & Potts, C. (2016). Fitting In Or Standing Out? The Tradeoffs of Structural and Cultural Embeddedness. American Sociological Review, 81(6), 1190-1222.\nAral, S. (2016). The future of weak ties. American Journal of Sociology, 121(6), 1931-1939."
  },
  {
    "objectID": "schedule-208B-S24.html#week-4-science",
    "href": "schedule-208B-S24.html#week-4-science",
    "title": "SOCIOL 208B Reading Schedule (Spring 2024)",
    "section": "Week 4: Science",
    "text": "Week 4: Science\n\nTuesday, April 23\n\nMoody, J. (2004). The Structure of A Social Scientific Collaboration Network. American Sociological Review, 69, 213-238.\nClauset, A., Arbesman, S., & Larremore, D. B. (2015). Systematic inequality and hierarchy in faculty hiring networks. Science Advances, 1(1), e1400005.\n\n\n\nThursday, April 25\n\nGondal, N. (2018). Duality of departmental specializations and PhD exchange: A Weberian analysis of status in interaction using multilevel exponential random graph models (mERGM). Social Networks, 55, 202-212.\nMcmahan, P., & Mcfarland, D. A. (2021). Creative Destruction: The Structural Consequences of Scientific Curation. American Sociological Review, 86(2), 341-376.\nShwed, U., & Bearman, P. S. (2010). The Temporal Structure of Scientific Consensus Formation. American Sociological Review, 75(6), 817-840."
  },
  {
    "objectID": "schedule-208B-S24.html#week-5-collaboration-creativity",
    "href": "schedule-208B-S24.html#week-5-collaboration-creativity",
    "title": "SOCIOL 208B Reading Schedule (Spring 2024)",
    "section": "Week 5: Collaboration & Creativity",
    "text": "Week 5: Collaboration & Creativity\n\nTuesday, April 30\n\nUzzi, B, & Spiro, J. (2005). Collaboration and Creativity: The Small World Problem. American Journal of Sociology, 111, 447-504.\nDe Vaan, M., Stark, D., & Vedres, B. (2015). Game changer: The topology of creativity. American Journal of Sociology, 120(4), 1144-1194.\n\n\n\nThursday, May 2\n\nRossman, G., Esparza, N., & Bonacich, P. (2010). I’d Like To Thank The Academy, Team Spillovers, and Network Centrality. American Sociological Review, 75(1), 31-51.\nChoi, Y., Ingram, P., & Han, S. W. (2023). Cultural breadth and embeddedness: The individual adoption of organizational culture as a determinant of creativity. Administrative Science Quarterly, 68(2), 429-464.\nSilver, D., Childress, C., Lee, M., Slez, A., & Dias, F. (2022). Balancing categorical conventionality in music. American Journal of Sociology, 128(1), 224-286."
  },
  {
    "objectID": "schedule-208B-S24.html#week-6-difussion",
    "href": "schedule-208B-S24.html#week-6-difussion",
    "title": "SOCIOL 208B Reading Schedule (Spring 2024)",
    "section": "Week 6: Difussion",
    "text": "Week 6: Difussion\n\nTuesday, May 7\n\nDellaPosta, D., Shi, Y., & Macy, M. (2015). Why do liberals drink lattes?. American Journal of Sociology, 120(5), 1473-1511.\nDellaPosta, D. (2020). Pluralistic collapse: The “oil spill” model of mass opinion polarization. American Sociological Review, 85(3), 507-536.\nGoldberg, A., & Stein, S. K. (2018). Beyond Social Contagion: Associative Diffusion and The Emergence of Cultural Variation. American Sociological Review, 83(5), 897-932.\n\n\n\nThursday, May 9\n\nCentola, D., & Macy, M. (2007). Complex Contagions and the Weakness of Long Ties. American Journal of Sociology, 113(3), 702-734.\nGondal, N. (2023). Diffusion of innovations through social networks: Determinants and implications. Sociology Compass, 17(5), e13084.\nBail, C. A., Brown, T. W., & Wimmer, A. (2019). Prestige, proximity, and prejudice: how Google search terms diffuse across the world. American Journal of Sociology, 124(5), 1496-1548."
  },
  {
    "objectID": "schedule-208B-S24.html#week-7-culture",
    "href": "schedule-208B-S24.html#week-7-culture",
    "title": "SOCIOL 208B Reading Schedule (Spring 2024)",
    "section": "Week 7: Culture",
    "text": "Week 7: Culture\n\nTuesday, May 14\n\nLizardo, O. (2023). Culture and Networks. Pp. 188-201 in The SAGE Handbook of Social Network Analysis. SAGE Publications Ltd.\nLewis, K., & Kaufman, J. (2018). The Conversion of Cultural Tastes Into Social Network Ties. American Journal of Sociology, 123(6), 1684-1742.\nRawlings, C. M., & Childress, C. (2023). The Polarization of Popular Culture: Tracing the Size, Shape, and Depth of the “Oil Spill”. Social Forces, soad150.\n\n\n\nThursday, May 16 (No Meeting)"
  },
  {
    "objectID": "schedule-208B-S24.html#week-8-history",
    "href": "schedule-208B-S24.html#week-8-history",
    "title": "SOCIOL 208B Reading Schedule (Spring 2024)",
    "section": "Week 8: History",
    "text": "Week 8: History\n\nTuesday, May 21 (Virtual Meeting)\n\nPadgett, J. F., & Ansell, C. K. (1993). Robust Action and the Rise of the Medici, 1400-1434. American Journal of Sociology, 98(6), 1259–1319.\nGould, R. V. (1991). Multiple Networks and Mobilization In The Paris Commune, 1871. American Sociological Review, 56(6), 716-729.\nBearman, P., Moody, J., & Faris, R. (2002). Networks and History. Complexity, 8(1), 61-71.\n\n\n\nThursday, May 23\n\nErikson, E., & Feltham E., (2021) Historical Network Research, Pp. 432–442 in R Light, and J Moody (eds), The Oxford Handbook of Social Networks. Oxford University Press.\nErikson, E., & Bearman, P. (2006). Malfeasance and the Foundations For Global Trade: The Structure of English Trade In The East Indies, 1601–1833. American Journal of Sociology, 112(1), 195-230.\nBecker, S. O., Hsiao, Y., Pfaff, S., & Rubin, J. (2020). Multiplex Network Ties and the Spatial Diffusion of Radical Innovations: Martin Luther’s Leadership In The Early Reformation. American Sociological Review, 85(5), 857-894."
  },
  {
    "objectID": "schedule-208B-S24.html#week-9-inequality",
    "href": "schedule-208B-S24.html#week-9-inequality",
    "title": "SOCIOL 208B Reading Schedule (Spring 2024)",
    "section": "Week 9: Inequality",
    "text": "Week 9: Inequality\n\nTuesday, May 28\n\nDiMaggio, P., & Garip, F. (2012). Network effects and social inequality. Annual Review of Sociology, 38, 93-118.\nPedulla, D. S., & Pager, D. (2019). Race and networks in the job search process. American Sociological Review, 84(6), 983-1012.\n\n\n\nThursday, May 30\n\nErikson, E., & Occhiuto, N. (2017). Social networks and macrosocial change. Annual Review of Sociology, 43, 229-248.\nZhang, J., & Centola, D. (2019). Social networks and health: New developments in diffusion, online and offline. Annual Review of Sociology, 45, 91-109.\nHofstra, B., Corten, R., Van Tubergen, F., & Ellison, N. B. (2017). Sources of segregation in social networks: A novel approach using Facebook. American Sociological Review, 82(3), 625-656."
  },
  {
    "objectID": "schedule-208B-S24.html#week-10-linking-micro-and-macro",
    "href": "schedule-208B-S24.html#week-10-linking-micro-and-macro",
    "title": "SOCIOL 208B Reading Schedule (Spring 2024)",
    "section": "Week 10: Linking Micro and Macro",
    "text": "Week 10: Linking Micro and Macro\n\nTuesday, June 4\n\nMcfarland, D. A., Moody, J., Diehl, D., Smith, J. A., & Thomas, R. J. (2014). Network Ecology and Adolescent Social Structure. American Sociological Review, 79(6), 1088-1121.\nBearman, P. S., Moody, J., & Stovel, K. (2004). Chains of Affection: The Structure of Adolescent Romantic and Sexual Networks. American Journal of Sociology, 110(1), 44-91.\n\n\n\nThursday, June 6\n\nLevy, B. L., Phillips, N. E., & Sampson, R. J. (2020). Triple Disadvantage: Neighborhood Networks of Everyday Urban Mobility and Violence In US Cities. American Sociological Review, 85(6), 925-956.\nPapachristos, A. V., & Bastomski, S. (2018). Connected in crime: the enduring effect of neighborhood networks on the spatial patterning of violence. American Journal of Sociology, 124(2), 517-568."
  },
  {
    "objectID": "syllabus-208A.html",
    "href": "syllabus-208A.html",
    "title": "208A Syllabus",
    "section": "",
    "text": "THIS IS A LIVE DOCUMENT CHECK REGULARLY FOR CHANGES"
  },
  {
    "objectID": "syllabus-208A.html#class-description",
    "href": "syllabus-208A.html#class-description",
    "title": "208A Syllabus",
    "section": "Class Description",
    "text": "Class Description\nThis class is an introductory graduate-level seminar focused on techniques in Social Network Analysis (SNA). The seminar covers the most common data analytic tasks that people engage in when analyzing “network data.” What is network data? What counts as network data is itself a point of contention—as we will see, for some people all data is network data—but let us say for the sake of this class that network data is data in which the unit of analysis is the relation or the interaction between at least two actors or objects, and the data come typically arranged in this “dyadic” form. At the end of the course, you will be familiar with (and will have acquired some practice) the basic techniques used to analyze social network data."
  },
  {
    "objectID": "syllabus-208A.html#course-content",
    "href": "syllabus-208A.html#course-content",
    "title": "208A Syllabus",
    "section": "Course Content",
    "text": "Course Content\n\nBasic SNA\nSo, what are the things that people usually do when they have network data? Well, they typically want to figure out basic statistics about the interaction system formed by the set of dyads in the data, where a dyad is any two pairs of actors (whether they are connected or not). This task requires computing basic network quantities like the number of nodes and the number of links between entities as well as more advanced statistics based on representing the network as a graph (like the average path length, number of components, etc., all notions we will cover in the first week of class).\n\n\nCentrality and Prestige\nThen come the various things that almost everyone is interested in computing when using network data to answer social science questions. Primarily, this includes measures and indices of a node’s position in the network (e.g., differentiating between more or less central or more or less prestigious nodes), which we will cover in weeks 2 and 3.\n\n\nClasses and Communities\nAfter taking a break in Week 4, we move to the common case of people wanting to see if the nodes in the network fall into definable clusters or classes, where the criterion for being in the same cluster is based on how they connect to other nodes. Here, we want to find clusters of nodes that are similar to one another by some graph theoretic criterion and partition the graph into clusters based on that criterion.\nWeek 6 is dedicated to the next thing we may want to do, and that is to see if we can uncover clumps of densely connected nodes in the network indicating some natural partition into subgroups or communities, defined as nodes that interact more among themselves than they do among those outside the group, leading to the myriad of group and community detection techniques designed to partition a graph into clusters based on the underlying connectivity structure.\n\n\nTwo-Mode and Ego Networks\nThe next two weeks are dedicated to the analysis of some pretty common “non-standard” types of network data (e.g., data that doesn’t use the dyadic relation between objects of the same type as the analytic unit). The first is ego networks, where we first sample a set of units (egos), and then within each ego, we sample a subset of their contacts (e.g., by asking the people who are their most important friends or figuring out the most frequent interaction partners). These data come closest to the traditional data in social science (a rectangular matrix of cases by variables), so various standard techniques—like regression—apply (with some twists).\nThe second type of non-standard network data comes in a two-mode network form, in which some sets of objects are linked to objects of a different set, but there is no data on the links between objects of the same set. Standard cases by variables data in surveys are two-mode data (people connect to variables), as is any web or archival data collecting memberships or interactions between persons and objects (like attendance at events or people buying books on Amazon). We will see that due to a neat mathematical trick, we can transform two-mode into standard dyadic network data and thus deploy the whole panoply of techniques we learned in weeks 1-6 (which means that we can do SNA on all types of data, not just network data, and therefore all data is network data).\n\n\nProbabilistic Models of Networks\nThe bulk of SNA assumes that the ties exist as recorded in the data. Recently (e.g., over the last two decades or so) developed approaches to social network analysis make the ties the dependent variable and thus see the observed network data as a realization of some stochastic process governing the probability that two objects will be linked and thus one that can be modeled statistically. We analyze the theory and methods behind this approach to thinking of network structure from the bottom up and also cover some models designed to treat networks as composed of “relational events” and thus model how events that link entities in networks evolve."
  },
  {
    "objectID": "syllabus-208A.html#requirements",
    "href": "syllabus-208A.html#requirements",
    "title": "208A Syllabus",
    "section": "Requirements",
    "text": "Requirements\nThere are three main requirements in the class. Participation (mainly attendance and contributions made during our seminar meetings), a short weekly data exercise, and a longer data analysis paper due at the end of the quarter.\n\nClass Attendance and Class Discussion (25% of grade)\nAttendance is required not optional. If you need to miss a class meeting please let me know beforehand. It is part of your professional socialization to commit to attending class meetings and to inform me when that’s not possible (if only as a point of courtesy). The informal part of participation will be gauged by your contribution to our class discussion in the form of questions, comments, suggestions, wonderings, problems.\n\n\nWeekly Data Analysis Exercises (25% of grade)\nThese will be short weekly assignments in which I will ask you to take a (small) social network data set of your choice and compute some of the basic statistics or implement some of the techniques that we covered the week before. They will be due on Sunday at the end of each week. What you submit will take the form of a file containing the code and results from your analysis (typically an R Markdown file). These will not be graded, but will just be counted as submitted or not submitted.\n\n\nFinal Data Analysis Paper (50% of grade)\nThe basic goal here is for you to end up with something that could be useful to you at the end of the day. Hopefully a basic data exercise that can be the basis of a longer substantive paper or as a standalone research note.\nThis will be a 2500 to 5000 word (single-spaced, Times New Roman Font, 12pt, 1in margins) write-up of a data analysis, including some type of network data and/or some kind of network analytic technique (to be discussed and cleared by me). The data source can be obtained from a public repository of network data, or it could be network data that you already have access to or collected yourself.\nIn the paper, you will describe the data, provide key network metrics, describe the data-analytic approach that you will use, and provide a summary of the key empirical patterns that you found, along with a brief conclusion. The paper should be written in the style of a “research note” focused on key empirical findings (not long theory windup or literature review).\nYou will submit an extended abstract of your final project, outlining your main research idea (e.g., data source and type of analysis) due on Sunday of week 6. This will be a one-page, single-spaced document with 12pt Times New Roman Font and 1in margins."
  },
  {
    "objectID": "syllabus-208B.html",
    "href": "syllabus-208B.html",
    "title": "208B Syllabus",
    "section": "",
    "text": "THIS IS A LIVE DOCUMENT CHECK REGULARLY FOR CHANGES"
  },
  {
    "objectID": "syllabus-208B.html#participation-50-of-grade",
    "href": "syllabus-208B.html#participation-50-of-grade",
    "title": "208B Syllabus",
    "section": "Participation (50% of grade)",
    "text": "Participation (50% of grade)\n\nWeekly Analytic Memo\nThe formal side of participation will come in the form of you submitting a short memo (500 to 1000 words) where you try to put two or more of the readings for that week in conversation with one another. The first memo will be due starting on week 2.\nMemo specifications:\n\nYou should pick at least one reading from the Tuesday meeting and at least one reading from the Thursday meeting as the focus of your memo.\nIn your memo you should feel free to raise questions or issues the readings brought up for you as well as any questions, problems, or weaknesses you identify in the argument or the analyses.\nYou may also feel free to connect the readings to other work you are familiar with, pointing to key points of commonality and difference. The main point of the memo is for me to see evidence of you thinking thought the material, as well as providing fodder for discussion during our meeting.\nThe memo will be due by 5p the day before our first class meeting of the week (that’s Monday) so that I have a chance to read it and comment on it. You will submit it via the assignments tool on Canvas.\n\n\n\nClass Attendance\nAttendance is required not optional. If you need to miss a class meeting please let me know before hand. It is part of your professional socialization to commit to attending class meetings and to inform me when that’s not possible (if only as a point of courtesy).\n\n\nClass Discussion\nThe informal part of participation will be gauged by your contribution to our class discussion. You can use the thoughts developed in your analytic memo as a take-off point for framing your contribution."
  },
  {
    "objectID": "syllabus-208B.html#paper-50-of-grade",
    "href": "syllabus-208B.html#paper-50-of-grade",
    "title": "208B Syllabus",
    "section": "Paper (50% of grade)",
    "text": "Paper (50% of grade)\nThe basic goal here is for you to end up with something that could be useful to you at the end of the day. As such, I’ll give you a set of options here, but if you none of these work, we can talk about something that can be customized for your needs and goals. So by the end of the course you will submit one of the following:\n\nDraft of a research paper.- This will be a 3500 to 9000 word (double-spaced, Times New Roman Font, 12pt, 1in margins) draft of a research paper. This paper will contain some kind of data analysis, involving networks broadly defined. It will include an introduction reviewing literature and setting up a research problem or question. It will then move on to a methods section describing your data and analytic approach, and will close with a discussion section summarizing key findings, outlining implications for substantive research and theory, and describing potential future work and extensions. The goal here is to come up with a draft of a paper that could one day be submitted to the various social science and sociology-oriented journal focused on substantive research, whether “generalist” (e.g., American Sociological Review) or “specialist” (e.g., Social Networks).\nDraft of a Conceptual Paper.- This will be a 5000 to 10000 word page (double-spaced, Times New Roman Font, 12pt, 1in margins) draft of a research paper. The paper will focus on a set of concepts, theoretical ideas, or overall perspectives for approaching the study of social life that are based, inspired, extend, or incorporate network ideas, network thinking, or network concepts and techniques (broadly defined). The goal here is to come up with a draft of a paper that could one day be submitted to the various social science and sociology-oriented journal focused on “theory.”\nExtended Literature Review Draft.- This will be a 10-15 page (double-spaced, Times New Roman Font, 12pt, 1in Margins) draft of a literature review of work done from a social network perspective on a topic of your interest. The paper will cover what has been done in the field so far, what the strengths and limitations of previous is, and will note gaps or opportunities for future work addressing those limitations or extending the literature to new substantive domains, perhaps linking previous work to the some of the stuff we will be reading in class.\nDraft of a Research Proposal.- This will be a 10 page (single-spaced, Times New Roman Font, 12pt, 1in margins, including references) draft of a research proposal for a project incorporating either network thinking, theories, or techniques that you plan to start in the near future. The proposal will include a background section reviewing previous work, noting their strengths and limitations, and pointing to gaps in the literature. It will also include an “approach” section describing your research project, your main research questions, the data gathering procedures you will use, and the data-analytic techniques you plan to implement once your data is collected. It will close with an implication sections describing what the contributions of your project will be and why it is relevant and important.\nData Analysis Exercise.- This will be a 2500 to 5000 word (single-spaced, Times New Roman Font, 12pt, 1in margins) write-up of a data analysis, including some type of network data and/or some kind of network analytic technique (to be discussed and cleared by me). The data source can be obtained from a public repository of network data, or it could be network data that you already have access to or collected yourself. In the paper, you will describe the data, provide key network metrics, describe the data-analytic approach that you will use, and provide a summary of the key empirical patterns that you found, along with a brief conclusion. The paper should be written in the style of a “research note” focused on key empirical findings (not long theory windup or literature review).\n\nWhatever you decide, you will submit an extended abstract of your final project, due on Friday of week 6. This will be a one-page, single-spaced document with 12pt Times New Roman Font and 1in margins."
  },
  {
    "objectID": "handout7.html#structural-equivalence",
    "href": "handout7.html#structural-equivalence",
    "title": "Analyzing Two-Mode Networks",
    "section": "Structural Equivalence",
    "text": "Structural Equivalence\nAnd, of course, once we have a similarity we can cluster nodes based on approximate structural equivalence by transforming proximities to distances:\n\n   D <- as.dist(1- J.p)\n   hc.p <- hclust(D, method = \"ward.D2\")\n   plot(hc.p)\n\n\n\n\nAnd for events:\n\n   J.g <- vertex.sim(g)$J.g\n   rownames(J.g) <- colnames(A)\n   colnames(J.g) <- colnames(A)\n   D <- as.dist(1- J.g)\n   hc.g <- hclust(D, method = \"ward.D2\")\n   plot(hc.g)\n\n\n\n\nWe can then derive cluster memberships for people and groups from the hclust object:\n\n   library(dendextend)\n   clus.p <- sort(cutree(hc.p, 4)) #selecting four clusters for people\n   clus.p\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n        1         1         1         1         1         1         1         2 \n     RUTH     VERNE   DOROTHY     MYRNA KATHERINE    SYLVIA      NORA     HELEN \n        2         2         2         3         3         3         3         3 \n   OLIVIA     FLORA \n        4         4 \n\n   clus.g <- sort(cutree(hc.g, 3)) #selecting three clusters for groups\n   clus.g\n\n 6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10  2/23   4/7 11/21 \n    1     1     1     1     1     1     2     2     2     3     3     3     3 \n  8/3 \n    3 \n\n\nAnd finally we can block the original affiliation matrix, as recommended by Everett and Borgatti (2013, 210, table 5):\n\n   library(ggcorrplot)\n   p <- ggcorrplot(t(A[names(clus.p), names(clus.g)]), \n                   colors = c(\"white\", \"white\", \"red\")) \n   p <- p + theme(legend.position = \"none\", \n                  axis.text.y = element_text(size = 8),\n                  axis.text.x = element_text(size = 8, angle = 0),\n                  )\n   p <- p + scale_x_discrete(position = \"top\") \n   p <- p + geom_hline(yintercept = 7.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_hline(yintercept = 11.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_hline(yintercept = 16.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_vline(xintercept = 6.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_vline(xintercept = 9.5, linewidth = 2, color = \"blue\")\n   p\n\n\n\n\nWhich reveals a number of almost complete (one-blocks) and almost null (zero-blocks) in the social structure, with a reduced image matrix that looks like:\n\n   library(kableExtra)\n   IM <- matrix(0, 4, 3)\n   IM[1, ] <- c(0, 1, 0)\n   IM[2, ] <- c(0, 1, 1)\n   IM[3, ] <- c(0, 1, 0)\n   IM[4, ] <- c(1, 1, 0)\n   rownames(IM) <- c(\"P.Block1\", \"P.Block2\", \"P.Block3\", \"P.Block4\")\n   colnames(IM) <- c(\"E.Block1\", \"E.Block2\", \"E.Block3\")\n   kbl(IM, format = \"html\", , align = \"c\") %>% \n      column_spec(1, bold = TRUE) %>% \n      kable_styling(full_width = TRUE,\n                     bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\n \n  \n      \n    E.Block1 \n    E.Block2 \n    E.Block3 \n  \n \n\n  \n    P.Block1 \n    0 \n    1 \n    0 \n  \n  \n    P.Block2 \n    0 \n    1 \n    1 \n  \n  \n    P.Block3 \n    0 \n    1 \n    0 \n  \n  \n    P.Block4 \n    1 \n    1 \n    0"
  },
  {
    "objectID": "handout7.html#pagerank-status",
    "href": "handout7.html#pagerank-status",
    "title": "Two-Mode Networks",
    "section": "PageRank Status",
    "text": "PageRank Status\nWe can, of course, also play our status game with degree-normalized versions of the affiliation matrix (a.k.a. PageRank).\nFirst we need to create row stochastic versions of \\(\\mathbf{A}\\) and \\(\\mathbf{A}^T\\). Recall that a matrix is row stochastic if their rows sum to one.\nFor the people, we can do this by taking the original affiliation matrix, and pre-multiplying it by a diagonal square matrix \\(\\mathbf{D}_P^{-1}\\) of dimensions \\(M \\times M\\) containing the inverse of the degrees of each person in the affiliation network along the diagonals and zeros everywhere else, yielding the row-stochastic matrix \\(\\mathbf{P}_P\\) of dimensions \\(M \\times N\\):\n\\[\n\\mathbf{P}_P = \\mathbf{D}_P^{-1}\\mathbf{A}\n\\]\nAnd we can do the same with the groups, except that we pre-multiply the transpose of the original affiliation matrix by \\(\\mathbf{D}_G^{-1}\\) which is an \\(N \\times N\\) matrix containing the inverse of the size of each group along the diagonals and zero everywhere else, this yields the matrix \\(\\mathbf{P}_G\\) of dimensions \\(N \\times M\\):\n\\[\n\\mathbf{P}_G = \\mathbf{D}_G^{-1}\\mathbf{A}^T\n\\]\nIn R can compute \\(\\mathbf{P}_P\\) and \\(\\mathbf{P}_G\\) as follows:\n\n   D.p <- diag(1/rowSums(A))\n   P.p <- D.p %*% A\n   D.g <- diag(1/colSums(A))\n   P.g <- D.g %*% t(A)\n\nAnd we can check that both P.p (for people) and P.g (groups) are row stochastic:\n\n   rowSums(P.p)\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n   rowSums(P.g)\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n\nAnd that they are of the predicted dimensions:\n\n   dim(P.p)\n\n[1] 18 14\n\n   dim(P.g)\n\n[1] 14 18\n\n\nGreat! Now, we can obtain the degree-normalized projections for people by multiplying \\(\\mathbf{P}_P\\) times \\(\\mathbf{P}_G\\):\n\\[\n\\mathbf{P}_{PP} = \\mathbf{P}_P\\mathbf{P}_G\n\\]\nWhich produces the matrix \\(\\mathbf{P}_{PP}\\) a square \\(M \\times M\\) matrix containing the degree-normalized similarities between each pair of people.\nWe then do the same for groups:\n\\[\n\\mathbf{P}_{GG} = \\mathbf{P}_G\\mathbf{P}_P\n\\]\nWhich produces the matrix \\(\\mathbf{P}_{GG}\\) a square \\(N \\times N\\) matrix containing the degree-normalized similarities between each pair of groups.\nIn R we obtain these matrices as follows:\n\n   P.pp <- P.p %*% P.g\n   P.gg <- P.g %*% P.p\n   rownames(P.pp) <- colnames(P.pp)\n   rownames(P.gg) <- colnames(P.gg)\n\nWhich are still row stochastic–but now square–matrices:\n\n   rowSums(P.pp)\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n        1         1         1         1         1         1         1         1 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n        1         1         1         1         1         1         1         1 \n   OLIVIA     FLORA \n        1         1 \n\n   rowSums(P.gg)\n\n 6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10  2/23   4/7 11/21 \n    1     1     1     1     1     1     1     1     1     1     1     1     1 \n  8/3 \n    1 \n\n   dim(P.pp)\n\n[1] 18 18\n\n   dim(P.gg)\n\n[1] 14 14\n\n\nLet’s peek inside one of these matrices:\n\n   round(P.pp[1:10, 1:10], 2)\n\n          EVELYN LAURA THERESA BRENDA CHARLOTTE FRANCES ELEANOR PEARL RUTH\nEVELYN      0.19  0.14    0.14   0.13      0.07    0.06    0.04  0.03 0.03\nLAURA       0.16  0.18    0.13   0.13      0.06    0.07    0.06  0.03 0.04\nTHERESA     0.14  0.12    0.16   0.10      0.08    0.06    0.05  0.03 0.05\nBRENDA      0.15  0.13    0.12   0.17      0.09    0.07    0.06  0.03 0.04\nCHARLOTTE   0.14  0.10    0.16   0.16      0.16    0.07    0.06  0.00 0.06\nFRANCES     0.12  0.12    0.12   0.12      0.07    0.12    0.08  0.05 0.05\nELEANOR     0.08  0.11    0.11   0.11      0.06    0.08    0.11  0.05 0.07\nPEARL       0.09  0.07    0.09   0.07      0.00    0.07    0.07  0.09 0.05\nRUTH        0.07  0.07    0.09   0.07      0.06    0.05    0.07  0.04 0.09\nVERNE       0.04  0.04    0.06   0.04      0.03    0.02    0.04  0.04 0.06\n          VERNE\nEVELYN     0.02\nLAURA      0.02\nTHERESA    0.03\nBRENDA     0.02\nCHARLOTTE  0.03\nFRANCES    0.02\nELEANOR    0.04\nPEARL      0.05\nRUTH       0.06\nVERNE      0.11\n\n\nWhat are these numbers? Well, they can be interpreted as probabilities that a random walker starting at the row node and, following any sequence of \\(person-group-person'-group'\\) hops, will reach the column person. Thus, higher values indicate an affinity or proximity between the people (and the groups in the corresponding matrix).\nWe can now play the PageRank status game on the transpose of \\(\\mathbf{P}_{PP}\\) and \\(\\mathbf{P}_{GG}\\), just like we did in the one-mode case, to get the scores we want:\n\n   pr.p <- status1(t(P.pp))\n   pr.g <- status1(t(P.gg))\n\nWhich leads to the following PageRank centrality rankings for persons and groups:\n\n   p.dat <- data.frame(People = rownames(A), PageRank.Cent = round(pr.p, 3))\n   p.dat <- p.dat[order(p.dat$PageRank.Cent, decreasing = TRUE), ]\n   kbl(p.dat, format = \"html\", , align = c(\"l\", \"c\"), row.names = FALSE) %>% \n      column_spec(1, bold = TRUE) %>% \n      kable_styling(full_width = TRUE,\n                     bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\n \n  \n    People \n    PageRank.Cent \n  \n \n\n  \n    EVELYN \n    0.352 \n  \n  \n    THERESA \n    0.352 \n  \n  \n    NORA \n    0.352 \n  \n  \n    LAURA \n    0.308 \n  \n  \n    BRENDA \n    0.308 \n  \n  \n    SYLVIA \n    0.308 \n  \n  \n    KATHERINE \n    0.264 \n  \n  \n    HELEN \n    0.220 \n  \n  \n    CHARLOTTE \n    0.176 \n  \n  \n    FRANCES \n    0.176 \n  \n  \n    ELEANOR \n    0.176 \n  \n  \n    RUTH \n    0.176 \n  \n  \n    VERNE \n    0.176 \n  \n  \n    MYRNA \n    0.176 \n  \n  \n    PEARL \n    0.132 \n  \n  \n    DOROTHY \n    0.088 \n  \n  \n    OLIVIA \n    0.088 \n  \n  \n    FLORA \n    0.088 \n  \n\n\n\n\n\n\n   g.dat <- data.frame(Groups = colnames(A), PageRank.Cent = round(pr.g, 3))\n   g.dat <- g.dat[order(g.dat$PageRank.Cent, decreasing = TRUE), ]\n   kbl(g.dat, format = \"html\", align = c(\"l\", \"c\"), row.names = FALSE) %>% \n      column_spec(1, bold = TRUE) %>% \n      kable_styling(full_width = TRUE,\n                     bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\n \n  \n    Groups \n    PageRank.Cent \n  \n \n\n  \n    9/16 \n    0.517 \n  \n  \n    4/8 \n    0.444 \n  \n  \n    3/15 \n    0.369 \n  \n  \n    2/25 \n    0.295 \n  \n  \n    5/19 \n    0.295 \n  \n  \n    4/7 \n    0.222 \n  \n  \n    4/12 \n    0.221 \n  \n  \n    6/10 \n    0.185 \n  \n  \n    2/23 \n    0.148 \n  \n  \n    9/26 \n    0.147 \n  \n  \n    6/27 \n    0.111 \n  \n  \n    3/2 \n    0.111 \n  \n  \n    11/21 \n    0.111 \n  \n  \n    8/3 \n    0.111 \n  \n\n\n\n\n\nThe PageRank two-mode scores lead to a different ranking of the nodes, because now people are central if they belong to selective (not just large) groups and groups are central if their members are discerning (not just very active) people.\nJust like before we can use the PageRank scores to reveal a pattern in the affiliation matrix:\n\n   p <- ggcorrplot(t(A[order(pr.p), order(-pr.g)]), \n                   colors = c(\"white\", \"white\", \"red\")) \n   p <- p + theme(legend.position = \"none\", \n                  axis.text.y = element_text(size = 8),\n                  axis.text.x = element_text(size = 8, angle = 0),\n                  )\n   p <- p + scale_x_discrete(position = \"top\") \n   p <- p + geom_abline(intercept = 0, slope = 1.26, linewidth = 2, color = \"blue\")\n   p\n\n\n\n\nIn the PageRank status score setup, events are ordered (from left to right) by popularity (most popular events on the left) and actors are ordered by activity (from top to bottom; active actors on top), creating a purely triangular pattern. Core actors and core events are in the upper-right block."
  },
  {
    "objectID": "handout3.html#eigenvalues-eigenvectors-oh-my",
    "href": "handout3.html#eigenvalues-eigenvectors-oh-my",
    "title": "Status and Prestige",
    "section": "Eigenvalues, Eigenvectors, Oh My!",
    "text": "Eigenvalues, Eigenvectors, Oh My!\nNote that all of this obscure talk about eigenvalues and eigenvectors is just matrix math stuff. It has nothing to do with networks and social structure.\nIn contrast, because the big three centrality measures have a direct foundation in graph theory, and graph theory is an isomorphic model of social structures (points map to actor and lines map to relations) the “math” we do with graph theory is directly meaningful as a model of networks (the counts of the number of edges incident to a node is the count of other actors they someone is directly connected to).\nEigenvalues and eigenvectors are not a model of social structure in the way graph theory is (their first scientific application was in Chemistry and Physics). They are just a mechanical math fix to a circular equation problem.\nThis is why it’s a mistake to introduce network measures of status and prestige by jumping directly to the machinery of linear algebra (or worse talk about the idea of eigenvector centrality which means nothing to most people).\nA better approach is to see if we can motivate the use of measures like the ones above using the simple model of the distribution of status and prestige we started with earlier. We will see that we can, and that doing that leads us back to solutions that are the mathematical equivalent of all the eigenvector stuff."
  },
  {
    "objectID": "handout3.html#bonacich-prestige",
    "href": "handout3.html#bonacich-prestige",
    "title": "Status and Prestige",
    "section": "Bonacich Prestige",
    "text": "Bonacich Prestige\nIn a classic paper, Philip Bonacich (1972) noted the above connection between different ways people conceptualized status and prestige in networks and the leading eigenvector of the adjacency matrix. He then noted that we can extend similar ideas to the directed case.\nHere, people get status from receiving nominations from high status others (i.e., those who receive a lot of nominations), whose partners also get status from receiving a lot of nominations from high status others, and so forth.\nThis means that in a directed system of relations, status distribution operates primarily via the in-degree of each node, so that if \\(\\mathbf{A}\\) is the asymmetric adjacency matrix corresponding to the directed graph, then if we play our status game on the transpose of this matrix \\(\\mathbf{A}^T\\) we will get the scores we seek (Fouss, Saerens, and Shimbo 2016, 204).\nRecall that in transposing the matrix of a directed graph, we change it from being a from/to matrix (nodes in the rows send ties to nodes in the columns) to a to/from matrix: Nodes in the rows receive ties from nodes in the columns. So we want to play our status game in this matrix, because we want to rank nodes according to their receipt of ties from high-status others.\nLet’s see a real-life example, this time using the directed version of the Krackhardt friendship nomination network among the high-tech managers:\n\n   g <- ht_friends\n   A <- as.matrix(as_adjacency_matrix(g))\n   s <- status1(t(A))\n   s <- s/max(s)\n   round(s, 3)\n\n [1] 0.922 1.000 0.379 0.639 0.396 0.190 0.240 0.531 0.411 0.117 0.413 0.769\n[13] 0.082 0.428 0.368 0.449 0.592 0.440 0.425 0.225 0.583\n\n\nWhich are the same scores we would have gotten using the eigen_centrality function in igraph with the argument directed set to TRUE:\n\n   round(eigen_centrality(g, directed = TRUE)$vector, 3)\n\n [1] 0.922 1.000 0.379 0.639 0.396 0.190 0.240 0.531 0.411 0.117 0.413 0.769\n[13] 0.082 0.428 0.368 0.450 0.592 0.440 0.425 0.225 0.583\n\n\nAnd, like before, we can treat these scores as centrality measures and rank the nodes in the graph according to them.\nHere are the top ten nodes:\n\n\n\n\nTop Ten Eigenvector Scores for a Directed Graph.\n \n  \n    Nodes \n    Eigen.Cent \n    In.Deg.Cent \n  \n \n\n  \n    2 \n    1.000 \n    10 \n  \n  \n    1 \n    0.922 \n    8 \n  \n  \n    12 \n    0.769 \n    8 \n  \n  \n    4 \n    0.639 \n    5 \n  \n  \n    17 \n    0.592 \n    6 \n  \n  \n    21 \n    0.583 \n    5 \n  \n  \n    8 \n    0.531 \n    5 \n  \n  \n    16 \n    0.449 \n    4 \n  \n  \n    18 \n    0.440 \n    4 \n  \n  \n    14 \n    0.428 \n    5 \n  \n\n\n\n\n\nWhile the to in-degree centrality node (2) gets the top Eigenvector Centrality scores, we see mmanny nodes with equal in-degree centrality that get substantively different Eigenvector scores. So who you are connected matters in addition to how many connections you have."
  },
  {
    "objectID": "schedule-208A-F24.html#further-substantive-reading",
    "href": "schedule-208A-F24.html#further-substantive-reading",
    "title": "SOCIOL 208A Reading Schedule (Fall 2024)",
    "section": "Further (Substantive) Reading",
    "text": "Further (Substantive) Reading\n\nRossman, G., Esparza, N., & Bonacich, P. (2010). I’d Like To Thank The Academy, Team Spillovers, and Network Centrality. American Sociological Review, 75(1), 31-51. link\n\n\nFurther (Mathy) Reading\n\nVigna, S. (2016). Spectral ranking. Network Science, 4(4), 433-445. pdf\nBaltz, A., Kliemann, L. (2005). Spectral Analysis. In: Brandes, U., Erlebach, T. (eds) Network Analysis. Lecture Notes in Computer Science, vol 3418. Springer, Berlin, Heidelberg. link\nBonacich, P. (1972). Factoring and Weighting Approaches to Status Scores and Clique Identification. Journal of Mathematical Sociology, 2(1), 113-120. pdf\nKatz, L. (1953). A New Status Index Derived from Sociometric Analysis. Psychometrika, 18(1), 39-43. pdf"
  },
  {
    "objectID": "handout7.html#the-bi-adjacency-affiliation-matrix",
    "href": "handout7.html#the-bi-adjacency-affiliation-matrix",
    "title": "Analyzing Two-Mode Networks",
    "section": "The Bi-Adjacency (Affiliation) Matrix",
    "text": "The Bi-Adjacency (Affiliation) Matrix\nOnce you have your bipartite graph loaded up, you may want (if the graph is small enough) to check out the graph’s affiliation matrix \\(A\\).\nThis works just like before, except that now we use the as_biadjacency_matrix function:\n\n   A <- as.matrix(as_biadjacency_matrix(g))\n   A\n\n          6/27 3/2 4/12 9/26 2/25 5/19 3/15 9/16 4/8 6/10 2/23 4/7 11/21 8/3\nEVELYN       1   1    1    1    1    1    0    1   1    0    0   0     0   0\nLAURA        1   1    1    0    1    1    1    1   0    0    0   0     0   0\nTHERESA      0   1    1    1    1    1    1    1   1    0    0   0     0   0\nBRENDA       1   0    1    1    1    1    1    1   0    0    0   0     0   0\nCHARLOTTE    0   0    1    1    1    0    1    0   0    0    0   0     0   0\nFRANCES      0   0    1    0    1    1    0    1   0    0    0   0     0   0\nELEANOR      0   0    0    0    1    1    1    1   0    0    0   0     0   0\nPEARL        0   0    0    0    0    1    0    1   1    0    0   0     0   0\nRUTH         0   0    0    0    1    0    1    1   1    0    0   0     0   0\nVERNE        0   0    0    0    0    0    1    1   1    0    0   1     0   0\nMYRNA        0   0    0    0    0    0    0    1   1    1    0   1     0   0\nKATHERINE    0   0    0    0    0    0    0    1   1    1    0   1     1   1\nSYLVIA       0   0    0    0    0    0    1    1   1    1    0   1     1   1\nNORA         0   0    0    0    0    1    1    0   1    1    1   1     1   1\nHELEN        0   0    0    0    0    0    1    1   0    1    1   1     0   0\nDOROTHY      0   0    0    0    0    0    0    1   1    0    0   0     0   0\nOLIVIA       0   0    0    0    0    0    0    0   1    0    1   0     0   0\nFLORA        0   0    0    0    0    0    0    0   1    0    1   0     0   0\n\n\nIn this matrix we list one set of nodes in the rows and the other set is in the columns. Each cell \\(a_{ij} = 1\\) if row node \\(i\\) is affiliated with column node \\(j\\), otherwise \\(a_{ij} = 0\\)."
  },
  {
    "objectID": "handout7.html#generalized-vertex-similarity",
    "href": "handout7.html#generalized-vertex-similarity",
    "title": "Analyzing Two-Mode Networks",
    "section": "Generalized Vertex Similarity",
    "text": "Generalized Vertex Similarity\nRecall that vertex similarity works using the principle of structural equivalence: Two people are similar if the choose the same objects (groups), and two objects (groups) are similar if they are chosen by the same people.\nWe can, like we did in the one mode case, be after a more general version of similarity, which says that: Two people are similar if they choose similar objects, and two objects are similar if they are chosen by similar people.\nThis leads to the same problem setup that inspired the SimRank approach (Jeh and Widom 2002).\nA function to compute the SimRank similarity between nodes in a two mode network goes as follows:\n\n   TM.SimRank <- function(A, C = 0.8, iter = 10) {\n        nr <- nrow(A)\n        nc <- ncol(A)\n        dr <- rowSums(A)\n        dc <- colSums(A)\n        Sr <- diag(1, nr, nr) #baseline similarity: every node maximally similar to themselves\n        Sc <- diag(1, nc, nc) #baseline similarity: every node maximally similar to themselves\n        rn <- rownames(A)\n        cn <- colnames(A)\n        rownames(Sr) <- rn\n        colnames(Sr) <- rn\n        rownames(Sc) <- cn\n        colnames(Sc) <- cn\n        m <- 1\n        while(m < iter) {\n             Sr.pre <- Sr\n             Sc.pre <- Sc\n             for(i in 1:nr) {\n                  for(j in 1:nr) {\n                       if (i != j) {\n                            a <- names(which(A[i, ] == 1)) #objects chosen by i\n                            b <- names(which(A[j, ] == 1)) #objects chosen by j\n                            Scij <- 0\n                            for (k in a) {\n                                 for (l in b) {\n                                      Scij <- Scij + Sc[k, l] #i's similarity to j\n                                 }\n                            }\n                            Sr[i, j] <- C/(dr[i] * dr[j]) * Scij\n                       }\n                  }\n             }\n             for(i in 1:nc) {\n                  for(j in 1:nc) {\n                       if (i != j) {\n                            a <- names(which(A[, i] == 1)) #people who chose object i\n                            b <- names(which(A[, j] == 1)) #people who chose object j\n                            Srij <- 0\n                            for (k in a) {\n                                 for (l in b) {\n                                      Srij <- Srij + Sr[k, l] #i's similarity to j\n                                 }\n                            }\n                            Sc[i, j] <- C/(dc[i] * dc[j]) * Srij\n                       }\n                  }\n             }\n             m <- m + 1\n        }\n        return(list(Sr = Sr, Sc = Sc))\n   }\n\nThis function takes the bi-adjacency matrix \\(\\mathbf{A}\\) as input and returns two similarity matrices: One for the people (row objects) and the other one for the groups (column objects).\nHere’s how that would work in the Southern Women data. First we compute the SimRank scores:\n\n   sim.res <- TM.SimRank(A)\n\nThen we peek inside the people similarity matrix:\n\n   round(sim.res$Sr[1:10, 1:10], 3)\n\n          EVELYN LAURA THERESA BRENDA CHARLOTTE FRANCES ELEANOR PEARL  RUTH\nEVELYN     1.000 0.267   0.262  0.266     0.259   0.275   0.248 0.255 0.237\nLAURA      0.267 1.000   0.262  0.277     0.270   0.287   0.280 0.237 0.247\nTHERESA    0.262 0.262   1.000  0.262     0.273   0.270   0.264 0.254 0.256\nBRENDA     0.266 0.277   0.262  1.000     0.290   0.287   0.279 0.235 0.246\nCHARLOTTE  0.259 0.270   0.273  0.290     1.000   0.276   0.269 0.175 0.256\nFRANCES    0.275 0.287   0.270  0.287     0.276   1.000   0.305 0.280 0.256\nELEANOR    0.248 0.280   0.264  0.279     0.269   0.305   1.000 0.279 0.294\nPEARL      0.255 0.237   0.254  0.235     0.175   0.280   0.279 1.000 0.279\nRUTH       0.237 0.247   0.256  0.246     0.256   0.256   0.294 0.279 1.000\nVERNE      0.201 0.207   0.222  0.206     0.198   0.202   0.246 0.276 0.288\n          VERNE\nEVELYN    0.201\nLAURA     0.207\nTHERESA   0.222\nBRENDA    0.206\nCHARLOTTE 0.198\nFRANCES   0.202\nELEANOR   0.246\nPEARL     0.276\nRUTH      0.288\nVERNE     1.000\n\n\nAnd the group similarity matrix:\n\n   round(sim.res$Sc[1:10, 1:10], 3)\n\n      6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10\n6/27 1.000 0.343 0.314 0.312 0.287 0.277 0.224 0.226 0.178 0.137\n3/2  0.343 1.000 0.312 0.311 0.285 0.276 0.224 0.228 0.200 0.141\n4/12 0.314 0.312 1.000 0.314 0.288 0.265 0.226 0.220 0.179 0.138\n9/26 0.312 0.311 0.314 1.000 0.287 0.256 0.230 0.214 0.186 0.137\n2/25 0.287 0.285 0.288 0.287 1.000 0.260 0.235 0.226 0.187 0.146\n5/19 0.277 0.276 0.265 0.256 0.260 1.000 0.224 0.226 0.200 0.171\n3/15 0.224 0.224 0.226 0.230 0.235 0.224 1.000 0.221 0.204 0.209\n9/16 0.226 0.228 0.220 0.214 0.226 0.226 0.221 1.000 0.221 0.214\n4/8  0.178 0.200 0.179 0.186 0.187 0.200 0.204 0.221 1.000 0.234\n6/10 0.137 0.141 0.138 0.137 0.146 0.171 0.209 0.214 0.234 1.000\n\n\nLike before we can use these results to define two sets of distances:\n\n   D.p <- as.dist(1 - sim.res$Sr)\n   D.g <- as.dist(1 - sim.res$Sc)\n\nSubject to hierarchical clustering:\n\n   hc.p <- hclust(D.p, method = \"ward.D2\")\n   hc.g <- hclust(D.g, method = \"ward.D2\")\n\nAnd plot:\n\n   plot(hc.p)\n\n\n\n   plot(hc.g)\n\n\n\n\nGet cluster memberships for people and groups from the hclust object:\n\n   clus.p <- sort(cutree(hc.p, 4)) #selecting four clusters for people\n   clus.p\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n        1         1         1         1         1         1         1         2 \n     RUTH     VERNE   DOROTHY     MYRNA KATHERINE    SYLVIA      NORA     HELEN \n        2         2         2         3         3         3         3         3 \n   OLIVIA     FLORA \n        4         4 \n\n   clus.g <- sort(cutree(hc.g, 3)) #selecting three clusters for groups\n   clus.g\n\n 6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  2/23  6/10   4/7 11/21 \n    1     1     1     1     1     1     2     2     2     2     3     3     3 \n  8/3 \n    3 \n\n\nAnd block the bi-adjacency matrix:\n\n   p <- ggcorrplot(t(A[names(clus.p), names(clus.g)]), \n                   colors = c(\"white\", \"white\", \"red\")) \n   p <- p + theme(legend.position = \"none\", \n                  axis.text.y = element_text(size = 8),\n                  axis.text.x = element_text(size = 8, angle = 0),\n                  )\n   p <- p + scale_x_discrete(position = \"top\") \n   p <- p + geom_hline(yintercept = 7.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_hline(yintercept = 11.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_hline(yintercept = 16.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_vline(xintercept = 8.5, linewidth = 2, color = \"blue\")\n   p\n\n\n\n\nWhich produces a familiar block partition of persons and events."
  },
  {
    "objectID": "handout7.html#the-bipartite-adjacency-matrix",
    "href": "handout7.html#the-bipartite-adjacency-matrix",
    "title": "Analyzing Two-Mode Networks",
    "section": "The Bipartite Adjacency Matrix",
    "text": "The Bipartite Adjacency Matrix\nNote that if we were to use the regular as_adjacency_matrix function on a bipartite graph, we get a curious version of the adjacency matrix:\n\n   B <- as.matrix(as_adjacency_matrix(g))\n   B\n\n          EVELYN LAURA THERESA BRENDA CHARLOTTE FRANCES ELEANOR PEARL RUTH\nEVELYN         0     0       0      0         0       0       0     0    0\nLAURA          0     0       0      0         0       0       0     0    0\nTHERESA        0     0       0      0         0       0       0     0    0\nBRENDA         0     0       0      0         0       0       0     0    0\nCHARLOTTE      0     0       0      0         0       0       0     0    0\nFRANCES        0     0       0      0         0       0       0     0    0\nELEANOR        0     0       0      0         0       0       0     0    0\nPEARL          0     0       0      0         0       0       0     0    0\nRUTH           0     0       0      0         0       0       0     0    0\nVERNE          0     0       0      0         0       0       0     0    0\nMYRNA          0     0       0      0         0       0       0     0    0\nKATHERINE      0     0       0      0         0       0       0     0    0\nSYLVIA         0     0       0      0         0       0       0     0    0\nNORA           0     0       0      0         0       0       0     0    0\nHELEN          0     0       0      0         0       0       0     0    0\nDOROTHY        0     0       0      0         0       0       0     0    0\nOLIVIA         0     0       0      0         0       0       0     0    0\nFLORA          0     0       0      0         0       0       0     0    0\n6/27           1     1       0      1         0       0       0     0    0\n3/2            1     1       1      0         0       0       0     0    0\n4/12           1     1       1      1         1       1       0     0    0\n9/26           1     0       1      1         1       0       0     0    0\n2/25           1     1       1      1         1       1       1     0    1\n5/19           1     1       1      1         0       1       1     1    0\n3/15           0     1       1      1         1       0       1     0    1\n9/16           1     1       1      1         0       1       1     1    1\n4/8            1     0       1      0         0       0       0     1    1\n6/10           0     0       0      0         0       0       0     0    0\n2/23           0     0       0      0         0       0       0     0    0\n4/7            0     0       0      0         0       0       0     0    0\n11/21          0     0       0      0         0       0       0     0    0\n8/3            0     0       0      0         0       0       0     0    0\n          VERNE MYRNA KATHERINE SYLVIA NORA HELEN DOROTHY OLIVIA FLORA 6/27 3/2\nEVELYN        0     0         0      0    0     0       0      0     0    1   1\nLAURA         0     0         0      0    0     0       0      0     0    1   1\nTHERESA       0     0         0      0    0     0       0      0     0    0   1\nBRENDA        0     0         0      0    0     0       0      0     0    1   0\nCHARLOTTE     0     0         0      0    0     0       0      0     0    0   0\nFRANCES       0     0         0      0    0     0       0      0     0    0   0\nELEANOR       0     0         0      0    0     0       0      0     0    0   0\nPEARL         0     0         0      0    0     0       0      0     0    0   0\nRUTH          0     0         0      0    0     0       0      0     0    0   0\nVERNE         0     0         0      0    0     0       0      0     0    0   0\nMYRNA         0     0         0      0    0     0       0      0     0    0   0\nKATHERINE     0     0         0      0    0     0       0      0     0    0   0\nSYLVIA        0     0         0      0    0     0       0      0     0    0   0\nNORA          0     0         0      0    0     0       0      0     0    0   0\nHELEN         0     0         0      0    0     0       0      0     0    0   0\nDOROTHY       0     0         0      0    0     0       0      0     0    0   0\nOLIVIA        0     0         0      0    0     0       0      0     0    0   0\nFLORA         0     0         0      0    0     0       0      0     0    0   0\n6/27          0     0         0      0    0     0       0      0     0    0   0\n3/2           0     0         0      0    0     0       0      0     0    0   0\n4/12          0     0         0      0    0     0       0      0     0    0   0\n9/26          0     0         0      0    0     0       0      0     0    0   0\n2/25          0     0         0      0    0     0       0      0     0    0   0\n5/19          0     0         0      0    1     0       0      0     0    0   0\n3/15          1     0         0      1    1     1       0      0     0    0   0\n9/16          1     1         1      1    0     1       1      0     0    0   0\n4/8           1     1         1      1    1     0       1      1     1    0   0\n6/10          0     1         1      1    1     1       0      0     0    0   0\n2/23          0     0         0      0    1     1       0      1     1    0   0\n4/7           1     1         1      1    1     1       0      0     0    0   0\n11/21         0     0         1      1    1     0       0      0     0    0   0\n8/3           0     0         1      1    1     0       0      0     0    0   0\n          4/12 9/26 2/25 5/19 3/15 9/16 4/8 6/10 2/23 4/7 11/21 8/3\nEVELYN       1    1    1    1    0    1   1    0    0   0     0   0\nLAURA        1    0    1    1    1    1   0    0    0   0     0   0\nTHERESA      1    1    1    1    1    1   1    0    0   0     0   0\nBRENDA       1    1    1    1    1    1   0    0    0   0     0   0\nCHARLOTTE    1    1    1    0    1    0   0    0    0   0     0   0\nFRANCES      1    0    1    1    0    1   0    0    0   0     0   0\nELEANOR      0    0    1    1    1    1   0    0    0   0     0   0\nPEARL        0    0    0    1    0    1   1    0    0   0     0   0\nRUTH         0    0    1    0    1    1   1    0    0   0     0   0\nVERNE        0    0    0    0    1    1   1    0    0   1     0   0\nMYRNA        0    0    0    0    0    1   1    1    0   1     0   0\nKATHERINE    0    0    0    0    0    1   1    1    0   1     1   1\nSYLVIA       0    0    0    0    1    1   1    1    0   1     1   1\nNORA         0    0    0    1    1    0   1    1    1   1     1   1\nHELEN        0    0    0    0    1    1   0    1    1   1     0   0\nDOROTHY      0    0    0    0    0    1   1    0    0   0     0   0\nOLIVIA       0    0    0    0    0    0   1    0    1   0     0   0\nFLORA        0    0    0    0    0    0   1    0    1   0     0   0\n6/27         0    0    0    0    0    0   0    0    0   0     0   0\n3/2          0    0    0    0    0    0   0    0    0   0     0   0\n4/12         0    0    0    0    0    0   0    0    0   0     0   0\n9/26         0    0    0    0    0    0   0    0    0   0     0   0\n2/25         0    0    0    0    0    0   0    0    0   0     0   0\n5/19         0    0    0    0    0    0   0    0    0   0     0   0\n3/15         0    0    0    0    0    0   0    0    0   0     0   0\n9/16         0    0    0    0    0    0   0    0    0   0     0   0\n4/8          0    0    0    0    0    0   0    0    0   0     0   0\n6/10         0    0    0    0    0    0   0    0    0   0     0   0\n2/23         0    0    0    0    0    0   0    0    0   0     0   0\n4/7          0    0    0    0    0    0   0    0    0   0     0   0\n11/21        0    0    0    0    0    0   0    0    0   0     0   0\n8/3          0    0    0    0    0    0   0    0    0   0     0   0\n\n\nThis bipartite adjacency matrix \\(\\mathbf{B}\\) is of dimensions \\((M + N) \\times (M + N)\\), which is \\((18 + 14) \\times (18 + 14) = 32 \\times 32\\) in the Southern Women data; it has the following block structure (Fouss, Saerens, and Shimbo 2016, 12):\n\\[\n\\mathbf{B} = \\left[\n\\begin{matrix}\n\\mathbf{O}_{M \\times M} & \\mathbf{A}_{M \\times N} \\\\\n\\mathbf{A}^T_{N \\times M} & \\mathbf{O}_{N \\times N}\n\\end{matrix}\n\\right]\n\\]\nWhere \\(\\mathbf{O}\\) is just the all zeros matrix of the relevant dimensions, and \\(\\mathbf{A}\\) is the bi-adjacency (affiliation) matrix as defined earlier. Thus, the bipartite adjacency matrix necessarily has two big diagonal “zero blocks” in it (upper-left and lower-right) corresponding to where the links between nodes in the same set would be (but necessarily aren’t because this is a two-mode network). The non-zero blocks are just the affiliation matrix (upper-right) and its transpose(lower-left)."
  },
  {
    "objectID": "schedule-208A-F24.html#week-6-november-6-cohesive-subgroups-and-communities",
    "href": "schedule-208A-F24.html#week-6-november-6-cohesive-subgroups-and-communities",
    "title": "SOCIOL 208A Reading Schedule (Fall 2024)",
    "section": "Week 6, November 6: Cohesive Subgroups and Communities",
    "text": "Week 6, November 6: Cohesive Subgroups and Communities\n\nReadings\n\nMoody, J., & Mucha, P. J. (2023). Structural Cohesion and Cohesive Groups. In J. McLevey, J. Scott, P. J. Carrington (Eds.) The SAGE Handbook of Social Network Analysis. Sage Publications. link\nShai, S., Stanley, N., Granell, C., Taylor, D. & Mucha, P. J. (2021). Case Studies in Network Community Detection. In R. Light, and J. Moody (Eds.) The Oxford Handbook of Social Networks Oxford University Press. link\nNewman, M. E. (2018). Community Structure. In Networks, 2nd Edition. Oxford, Online Edition, Oxford Academic. link\n\n\n\nFurther Reading\n\nClauset, A., Newman, M. E., & Moore, C. (2004). Finding community structure in very large networks. Physical Review E—Statistical, Nonlinear, and Soft Matter Physics, 70(6), 066111. link\nFortunato, S. (2010). Community Detection in Graphs. Physics Reports, 486(3-5), 75-174. link\nGirvan, M., & Newman, M. E. (2002). Community Structure in Social and Biological Networks. Proceedings of the National academy of Sciences, 99(12), 7821-7826. link\nLeicht, E. A., and Newman, M. E. (2008). Community Structure in Directed Networks. Physical Review Letters 100, 118703. link\nNewman, M. E. (2006). Modularity and Community Structure in Networks. Proceedings of the National Academy of Sciences, 103(23), 8577-8582. link\nNewman, M. E., & Girvan, M. (2003). Mixing patterns and community structure in networks. In Statistical mechanics of complex networks (pp. 66-87). Berlin, Heidelberg: Springer Berlin Heidelberg.\nNewman, M. E. (2003). Mixing Patterns in Networks. Physical review E 67(2), 026126. link\nNewman, M. E., & Girvan, M. (2004). Finding and evaluating community structure in networks. Physical review E, 69(2), 026113. link"
  },
  {
    "objectID": "ahn.html#detecting-overlapping-communities",
    "href": "ahn.html#detecting-overlapping-communities",
    "title": "Assigning Nodes to Multiple Communities",
    "section": "Detecting Overlapping Communities",
    "text": "Detecting Overlapping Communities\nMethodologically, overlapping community detection methods are not as well-developed as classical community detection methods. Here, we review one simple an intuitive approach that combines the idea of clustering nodes by computing a quantity on the links but instead of computing a rank order (like Newman and Girvan’s edge betweenness), we compute pairwise similarities between links like we did in handout 5. We then cluster the links using standard hierarchical clustering methods, which because nodes are incident to many links results in a multigroup clustering of the nodes for free. This approach is called link clustering (Ahn, Bagrow, and Lehmann 2010).\nLet’s see how it works.\nFirst we load data from an undirected graph:\n\n   library(igraph)\n   library(networkdata)\n   g <- movie_559 #Pulp Fiction\n\nAnd we plot:\n\n\n\n\n\n\nOriginal Graph.\n\n\n\n\n\n\n\nNodes Clusterered Into Communities According to the Modularity.\n\n\n\n\n\n\nThe Pulp Fiction Movie Network.\n\n\n\nThe key idea behind link clustering is that similar links should be assigned to the same clusters. How do we compute the similarity between links?"
  },
  {
    "objectID": "ahn.html#measuring-edge-similarity",
    "href": "ahn.html#measuring-edge-similarity",
    "title": "Assigning Nodes to Multiple Communities",
    "section": "Measuring Edge Similarity",
    "text": "Measuring Edge Similarity\nAccording to Ahn, Bagrow, and Lehmann (2010) two links \\(e_{ik}\\) and \\(e_{jk}\\) are similar if they share a node \\(v_k\\) and the other two nodes incident to each link (\\(v_i, v_k\\)) are themselves similar. To measure the similarity between these two nodes, we can use any of the off-the-shelf vertex similarity measures that we have seen in action, like Jaccard, cosine, or Dice.\nThe first step is thus to build a link by link similarity matrix based on this idea. The following function loops through each pair of links in the graph and computes the similarity between two links featuring a common node \\(v_k\\) based on the Jaccard vertex similarity of the two other nodes:\n\n    edge.sim <- function(x) {\n      el <- as_edgelist(x)\n      A <- as.matrix(as_adjacency_matrix(x))\n      S <- A %*% A #shared neighbors\n      d <- degree(x)\n      E <- nrow(el)\n      E.sim <- matrix(0, E, E)\n      for (e1 in 1:E) {\n        for (e2 in 1:E) {\n          if (e1 < e2 & sum(as.numeric(intersect(el[e1,], el[e2,])!=\"0\"))==1) {\n              v <- setdiff(union(el[e1,], el[e2,]), intersect(el[e1,], el[e2,]))\n              E.sim[e1, e2] <- S[v[1], v[2]]/(S[v[1], v[2]] + d[v[1]] + d[v[2]])\n              E.sim[e2, e1] <- S[v[1], v[2]]/(S[v[1], v[2]] + d[v[1]] + d[v[2]])\n            }\n          }\n        }\n    return(round(E.sim, 3))\n    }\n\nThe function takes the graph as input and returns an inter-link similarity matrix of dimensions \\(E \\times E\\) where \\(E\\) is the number of edges in the graph:\n\n    E.sim <- edge.sim(g)\n    E.sim[1:10, 1:10]\n\n       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10]\n [1,] 0.000 0.238 0.238 0.167 0.000 0.000 0.250 0.000 0.000 0.000\n [2,] 0.238 0.000 0.294 0.139 0.000 0.000 0.179 0.000 0.000 0.000\n [3,] 0.238 0.294 0.000 0.139 0.000 0.000 0.179 0.000 0.000 0.000\n [4,] 0.167 0.139 0.139 0.000 0.000 0.100 0.192 0.000 0.000 0.000\n [5,] 0.000 0.000 0.000 0.000 0.000 0.217 0.000 0.000 0.000 0.000\n [6,] 0.000 0.000 0.000 0.100 0.217 0.000 0.000 0.000 0.000 0.000\n [7,] 0.250 0.179 0.179 0.192 0.000 0.000 0.000 0.143 0.111 0.167\n [8,] 0.000 0.000 0.000 0.000 0.000 0.000 0.143 0.000 0.143 0.111\n [9,] 0.000 0.000 0.000 0.000 0.000 0.000 0.111 0.143 0.000 0.200\n[10,] 0.000 0.000 0.000 0.000 0.000 0.000 0.167 0.111 0.200 0.000\n\n\nWe can then transform the link similarities into distances, and cluster them:\n\n    D <- 1 - E.sim\n    D <- as.dist(D)\n    hc.res <- hclust(D, method = \"ward.D2\")\n\nThe resulting dendrogram looks like this:\n\n    par(cex = 0.5)\n    plot(hc.res)\n\n\n\n\nThe leaves of the dendrogram (bottom-most objects) represent each link in the graph (\\(E = 102\\) in this case), and the clusters are “link communities” (Ahn, Bagrow, and Lehmann 2010)."
  },
  {
    "objectID": "ahn.html#clustering-nodes",
    "href": "ahn.html#clustering-nodes",
    "title": "Assigning Nodes to Multiple Communities",
    "section": "Clustering Nodes",
    "text": "Clustering Nodes\nAs we noted, because it is the links that got clustered, the nodes incident to each link can go to more than one cluster (because nodes with degree \\(k>1\\) will be incident to multiple links).\nThe following function uses the dendrogram information to return a list of node assignments to multiple communities, controlled by the parameter k:\n\n    create.clus <- function(x, k) {\n      library(dendextend)\n      link.clus <- cutree(x, k = k)\n      link.dat <- data.frame(as_edgelist(g), link.clus)\n      clus.list <- list()\n      for (i in 1:max(link.clus)) {\n        sub.dat <- link.dat[link.dat$link.clus == i, ]\n        clus.list[[i]] <- unique(c(sub.dat[, 1], sub.dat[, 2]))\n        }\n    return(clus.list)\n    }\n\nLet’s see the list with twelve overlapping communities:\n\n    C <- create.clus(hc.res, k = 12)\n    C\n\n[[1]]\n[1] \"BRETT\"      \"GAWKER #2\"  \"JULES\"      \"MARSELLUS\"  \"MARVIN\"    \n[6] \"PEDESTRIAN\" \"ROGER\"      \"VINCENT\"   \n\n[[2]]\n [1] \"BRETT\"           \"FABIENNE\"        \"MARVIN\"          \"SPORTSCASTER #1\"\n [5] \"JIMMIE\"          \"RAQUEL\"          \"ROGER\"           \"JULES\"          \n [9] \"SPORTSCASTER #2\" \"THE WOLF\"        \"WINSTON\"        \n\n[[3]]\n [1] \"BRETT\"       \"FOURTH MAN\"  \"RAQUEL\"      \"THE WOLF\"    \"HONEY BUNNY\"\n [6] \"JIMMIE\"      \"JULES\"       \"MANAGER\"     \"MARVIN\"      \"PATRON\"     \n[11] \"PUMPKIN\"     \"ROGER\"       \"VINCENT\"     \"WINSTON\"    \n\n[[4]]\n [1] \"BUDDY\"        \"JODY\"         \"LANCE\"        \"BUTCH\"        \"CAPT KOONS\"  \n [6] \"ED SULLIVAN\"  \"ENGLISH DAVE\" \"MARSELLUS\"    \"MIA\"          \"MOTHER\"      \n[11] \"WOMAN\"        \"VINCENT\"     \n\n[[5]]\n [1] \"BUDDY\"        \"LANCE\"        \"PREACHER\"     \"CAPT KOONS\"   \"ED SULLIVAN\" \n [6] \"ENGLISH DAVE\" \"JODY\"         \"MOTHER\"       \"VINCENT\"      \"WOMAN\"       \n\n[[6]]\n [1] \"BRETT\"           \"BUTCH\"           \"ENGLISH DAVE\"    \"CAPT KOONS\"     \n [5] \"ESMARELDA\"       \"GAWKER #2\"       \"JULES\"           \"MARSELLUS\"      \n [9] \"PEDESTRIAN\"      \"SPORTSCASTER #1\" \"FABIENNE\"        \"MARVIN\"         \n[13] \"MAYNARD\"         \"MOTHER\"          \"ROGER\"           \"VINCENT\"        \n[17] \"WOMAN\"          \n\n[[7]]\n [1] \"FOURTH MAN\"  \"JIMMIE\"      \"BRETT\"       \"HONEY BUNNY\" \"JULES\"      \n [6] \"MANAGER\"     \"MARVIN\"      \"PATRON\"      \"PUMPKIN\"     \"RAQUEL\"     \n[11] \"ROGER\"       \"WINSTON\"     \"THE WOLF\"   \n\n[[8]]\n[1] \"HONEY BUNNY\" \"MANAGER\"     \"PATRON\"      \"PUMPKIN\"    \n\n[[9]]\n[1] \"JODY\"     \"LANCE\"    \"PREACHER\"\n\n[[10]]\n[1] \"MAYNARD\"  \"THE GIMP\" \"ZED\"     \n\n[[11]]\n[1] \"CAPT KOONS\" \"MOTHER\"     \"WOMAN\"     \n\n[[12]]\n[1] \"HONEY BUNNY\" \"PUMPKIN\"     \"WAITRESS\"    \"YOUNG MAN\"   \"YOUNG WOMAN\"\n\n\nBecause there are nodes that belong to multiple communities, the resulting actor by community ties form a two-mode network.\nWe can see re-construct this network from the list of community memberships for each node using this function:\n\n    create.two <- function(x) {\n      v <- unique(unlist(x))\n      B <- as.numeric(v %in% x[[1]])\n      for(j in 2:length(x)) {\n        B <- cbind(B, as.numeric(v %in% x[[j]]))\n      }\n      rownames(B) <- v\n      colnames(B) <- paste(\"c\", 1:12, sep = \"\")\n      return(B)\n      }\n\nHere’s the two mode matrix of characters by communities:\n\n    B <- create.two(C)\n    B\n\n                c1 c2 c3 c4 c5 c6 c7 c8 c9 c10 c11 c12\nBRETT            1  1  1  0  0  1  1  0  0   0   0   0\nGAWKER #2        1  0  0  0  0  1  0  0  0   0   0   0\nJULES            1  1  1  0  0  1  1  0  0   0   0   0\nMARSELLUS        1  0  0  1  0  1  0  0  0   0   0   0\nMARVIN           1  1  1  0  0  1  1  0  0   0   0   0\nPEDESTRIAN       1  0  0  0  0  1  0  0  0   0   0   0\nROGER            1  1  1  0  0  1  1  0  0   0   0   0\nVINCENT          1  0  1  1  1  1  0  0  0   0   0   0\nFABIENNE         0  1  0  0  0  1  0  0  0   0   0   0\nSPORTSCASTER #1  0  1  0  0  0  1  0  0  0   0   0   0\nJIMMIE           0  1  1  0  0  0  1  0  0   0   0   0\nRAQUEL           0  1  1  0  0  0  1  0  0   0   0   0\nSPORTSCASTER #2  0  1  0  0  0  0  0  0  0   0   0   0\nTHE WOLF         0  1  1  0  0  0  1  0  0   0   0   0\nWINSTON          0  1  1  0  0  0  1  0  0   0   0   0\nFOURTH MAN       0  0  1  0  0  0  1  0  0   0   0   0\nHONEY BUNNY      0  0  1  0  0  0  1  1  0   0   0   1\nMANAGER          0  0  1  0  0  0  1  1  0   0   0   0\nPATRON           0  0  1  0  0  0  1  1  0   0   0   0\nPUMPKIN          0  0  1  0  0  0  1  1  0   0   0   1\nBUDDY            0  0  0  1  1  0  0  0  0   0   0   0\nJODY             0  0  0  1  1  0  0  0  1   0   0   0\nLANCE            0  0  0  1  1  0  0  0  1   0   0   0\nBUTCH            0  0  0  1  0  1  0  0  0   0   0   0\nCAPT KOONS       0  0  0  1  1  1  0  0  0   0   1   0\nED SULLIVAN      0  0  0  1  1  0  0  0  0   0   0   0\nENGLISH DAVE     0  0  0  1  1  1  0  0  0   0   0   0\nMIA              0  0  0  1  0  0  0  0  0   0   0   0\nMOTHER           0  0  0  1  1  1  0  0  0   0   1   0\nWOMAN            0  0  0  1  1  1  0  0  0   0   1   0\nPREACHER         0  0  0  0  1  0  0  0  1   0   0   0\nESMARELDA        0  0  0  0  0  1  0  0  0   0   0   0\nMAYNARD          0  0  0  0  0  1  0  0  0   1   0   0\nTHE GIMP         0  0  0  0  0  0  0  0  0   1   0   0\nZED              0  0  0  0  0  0  0  0  0   1   0   0\nWAITRESS         0  0  0  0  0  0  0  0  0   0   0   1\nYOUNG MAN        0  0  0  0  0  0  0  0  0   0   0   1\nYOUNG WOMAN      0  0  0  0  0  0  0  0  0   0   0   1\n\n\nAnd the matrix of inter-community ties based on shared characters:\n\n    M <- t(B) %*% B\n    M\n\n    c1 c2 c3 c4 c5 c6 c7 c8 c9 c10 c11 c12\nc1   8  4  5  2  1  8  4  0  0   0   0   0\nc2   4 11  8  0  0  6  8  0  0   0   0   0\nc3   5  8 14  1  1  5 13  4  0   0   0   2\nc4   2  0  1 12  9  7  0  0  2   0   3   0\nc5   1  0  1  9 10  5  0  0  3   0   3   0\nc6   8  6  5  7  5 17  4  0  0   1   3   0\nc7   4  8 13  0  0  4 13  4  0   0   0   2\nc8   0  0  4  0  0  0  4  4  0   0   0   2\nc9   0  0  0  2  3  0  0  0  3   0   0   0\nc10  0  0  0  0  0  1  0  0  0   3   0   0\nc11  0  0  0  3  3  3  0  0  0   0   3   0\nc12  0  0  2  0  0  0  2  2  0   0   0   5\n\n\nAnd we can visualize the nodes connected to multiple communities as follows:\n\n    library(RColorBrewer)\n    set.seed(45)\n    g <- graph_from_biadjacency_matrix(B)\n    V(g)$type <- bipartite_mapping(g)$type\n    V(g)$shape <- ifelse(V(g)$type, \"square\", \"circle\")\n    V(g)$color <- c(rep(\"orange\", 38), \n                    c(brewer.pal(8, \"Paired\"), brewer.pal(4, \"Dark2\"))) \n    E(g)$color <- \"lightgray\"\n    plot(g, \n    vertex.size=5, vertex.frame.color=\"lightgray\", \n    vertex.label = V(g)$name,\n    vertex.label.dist=1, vertex.label.cex = 1)"
  },
  {
    "objectID": "handout7.html#visualizing-two-mode-networks-using-ca",
    "href": "handout7.html#visualizing-two-mode-networks-using-ca",
    "title": "Two-Mode Networks",
    "section": "Visualizing Two-Mode Networks Using CA",
    "text": "Visualizing Two-Mode Networks Using CA\nAnd, finally, we can use the first two normalized eigenvectors to plot the persons and groups in a common space:\n\n   plot.dat <- data.frame(rbind(eig.vec.p[, 1:2], eig.vec.g[, 1:2])) %>% \n      cbind(type = as.factor(c(rep(1, 18), rep(2, 14))))\n   library(ggplot2)\n   # install.packages(\"ggrepel\")\n   library(ggrepel)\n   p <- ggplot(data = plot.dat, aes(X1, y = X2, color = type))\n   p <- p + geom_hline(aes(yintercept = 0), color = \"gray\")\n   p <- p + geom_vline(aes(xintercept = 0), color = \"gray\")\n   p <- p + geom_text_repel(aes(label = rownames(plot.dat)), \n                            max.overlaps = 20, size = 2.75)\n   p <- p + theme_minimal()\n   p <- p + theme(legend.position = \"none\",\n                  axis.title = element_text(size = 14),\n                  axis.text = element_text(size = 12))\n   p <- p + scale_color_manual(values = c(\"red\", \"blue\"))\n   p <- p + labs(x = \"First Dimension\", y = \"Second Dimension\")\n   p\n\n\n\n\nIn this space, people with the most similar patterns of memberships to the most similar groups are placed close to one another. In the same way, groups with the most similar members are placed closed to one another.\nAlso like before, we can use the scores obtained from the CA analysis to re-arrange the rows and columns of the original matrix to reveal blocks of maximally similar persons and events:\n\n   p <- ggcorrplot(t(A[order(eig.vec.p[,1]), order(eig.vec.g[,1])]), \n                   colors = c(\"white\", \"white\", \"red\")) \n   p <- p + theme(legend.position = \"none\", \n                  axis.text.y = element_text(size = 8),\n                  axis.text.x = element_text(size = 8, angle = 0),\n                  )\n   p <- p + scale_x_discrete(position = \"top\") \n   p <- p + geom_hline(yintercept = 6.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_hline(yintercept = 10.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_vline(xintercept = 9.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_vline(xintercept = 6.5, linewidth = 2, color = \"blue\")\n   p\n\n\n\n\nHere CA seems to have detected two separate clusters of actors who preferentially attend two distinct clusters of events!\nThe three events in the middle \\(\\{3/15, 9/16, 4/8\\}\\) don’t seem to differentiate between participants in each cluster (everyone attends)–they thus appear near the origin in the CA diagram, indicating a weak association with either dimension.\nHowever, the events to the left (with clusters of participants in the lower-left) and to the right of the x-axis (with clusters of participants in the upper-right) are attended preferentially by distinct groups of participants; they thus appear at the extreme left and right positions of the first dimension of the CA diagram.\nIn the same way, the four people in the middle \\(\\{Ruth, Dorothy, Pearl, Verne\\}\\) only attend the undifferentiated, popular events, so that means that they are not strongly associated with either cluster of actors (and thus appear near the origin in the CA diagram). The top and bottom participants, by contrast, appear to the extreme right and left in the CA diagram, indicating a strong association with the underlying dimensions.\nNote the similarity between this blocking and that obtained from the structural equivalence analysis."
  },
  {
    "objectID": "ca.html",
    "href": "ca.html",
    "title": "Correspondence Analysis (CA) of Two-Mode Networks",
    "section": "",
    "text": "Correspondence Analysis (CA) a relatively simple way to analyze and visualize two-mode data. However, there are a few additional computational details to discuss."
  },
  {
    "objectID": "ca.html#visualizing-two-mode-networks-using-ca",
    "href": "ca.html#visualizing-two-mode-networks-using-ca",
    "title": "Correspondence Analysis (CA) of Two-Mode Networks",
    "section": "Visualizing Two-Mode Networks Using CA",
    "text": "Visualizing Two-Mode Networks Using CA\nAnd, finally, we can use the first two (eigenvalue variance) normalized CA scores to plot the persons and groups in a common two-dimensional space:\n\n   val.g[, 2] <- val.g[, 2]*-1 #flippling sign of group scores on second dimension for plotting purposes\n   plot.dat <- data.frame(rbind(uni.p, val.g)) %>% \n      cbind(type = as.factor(c(rep(1, 18), rep(2, 14))))\n   library(ggplot2)\n   # install.packages(\"ggrepel\")\n   library(ggrepel)\n   p <- ggplot(data = plot.dat, aes(x = X1, y = X2, color = type))\n   p <- p + geom_hline(aes(yintercept = 0), color = \"gray\")\n   p <- p + geom_vline(aes(xintercept = 0), color = \"gray\")\n   p <- p + geom_text_repel(aes(label = rownames(plot.dat)), \n                            max.overlaps = 20, size = 2.75)\n   p <- p + theme_minimal()\n   p <- p + theme(legend.position = \"none\",\n                  axis.title = element_text(size = 14),\n                  axis.text = element_text(size = 12))\n   p <- p + scale_color_manual(values = c(\"red\", \"blue\"))\n   p <- p + labs(x = \"First Dimension\", y = \"Second Dimension\") \n   p <- p + xlim(-2, 2) + ylim(-2, 4)\n   p\n\n\n\n\nIn this space, people with the most similar patterns of memberships to the most similar groups are placed close to one another. In the same way, groups with the most similar members are placed closed to one another.\nAlso like before, we can use the scores obtained from the CA analysis to re-arrange the rows and columns of the original matrix to reveal blocks of maximally similar persons and events:\n\n   library(ggcorrplot)\n   p <- ggcorrplot(t(A[order(val.p[,1]), order(val.g[,1])]), \n                   colors = c(\"white\", \"white\", \"red\")) \n   p <- p + theme(legend.position = \"none\", \n                  axis.text.y = element_text(size = 8),\n                  axis.text.x = element_text(size = 8, angle = 0),\n                  )\n   p <- p + scale_x_discrete(position = \"top\") \n   p <- p + geom_hline(yintercept = 6.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_hline(yintercept = 11.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_hline(yintercept = 16.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_vline(xintercept = 9.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_vline(xintercept = 6.5, linewidth = 2, color = \"blue\")\n   p\n\n\n\n\nHere CA seems to have detected two separate clusters of actors who preferentially attend two distinct clusters of events!\nThe three events in the middle \\(\\{3/15, 9/16, 4/8\\}\\) don’t seem to differentiate between participants in each cluster (everyone attends)–they thus appear near the origin in the CA diagram, indicating a weak association with either dimension.\nHowever, the events to the left (with clusters of participants in the lower-left) and to the right of the x-axis (with clusters of participants in the upper-right) are attended preferentially by distinct groups of participants; they thus appear at the extreme left and right positions of the first dimension of the CA diagram.\nIn the same way, the four people in the middle \\(\\{Ruth, Dorothy, Pearl, Verne\\}\\) only attend the undifferentiated, popular events, so that means that they are not strongly associated with either cluster of actors (and thus appear near the origin in the CA diagram). The top and bottom participants, by contrast, appear to the extreme right and left in the CA diagram, indicating a strong association with the underlying dimensions.\nNote the similarity between this blocking and that obtained from the structural equivalence analysis in the previous handout."
  },
  {
    "objectID": "ca.html#the-eigendecomposition-of-a-square-matrix",
    "href": "ca.html#the-eigendecomposition-of-a-square-matrix",
    "title": "Correspondence Analysis (CA) of Two-Mode Networks",
    "section": "The Eigendecomposition of a Square Matrix",
    "text": "The Eigendecomposition of a Square Matrix\nFirst, let us review the idea of an eigendecomposition of a square matrix. Let’s say we have the following matrix \\(\\mathbf{B}\\) of dimensions \\(3 \\times 3\\):\n\n   set.seed(567)\n   B <- matrix(round(runif(9), 2), nrow = 3, ncol = 3)\n   B\n\n     [,1] [,2] [,3]\n[1,] 0.74 0.49 0.07\n[2,] 0.88 0.26 0.51\n[3,] 0.63 0.24 0.59\n\n\nMost matrices like this can be decomposed into two other matrices \\(\\mathbf{U}\\) and \\(\\mathbf{\\lambda}\\), such that the following matrix multiplication equation is true:\n\\[\n\\mathbf{B} = \\mathbf{U}\\mathbf{\\lambda}\\mathbf{U}^{-1}\n\\]\nBoth \\(\\mathbf{U}\\) and \\(\\mathbf{\\lambda}\\) are of the same dimensions as the original, with \\(\\mathbf{U}\\) having numbers in each cell and \\(\\mathbf{\\lambda}\\) being a matrix with values along the diagonals and zeros everywhere else.\nThe column values of \\(\\mathbf{U}\\) are called the eigenvectors of \\(\\mathbf{B}\\) and the diagonal values of \\(\\mathbf{\\lambda}\\) are called the eigenvalues of \\(\\mathbf{B}\\).\nIn R you can find the values that yield the eigendecomposition of any square matrix (if one exists) using the function eigen.\nSo in our case this would be:\n\n   eig.res <- eigen(B)\n   eig.res\n\neigen() decomposition\n$values\n[1]  1.4256541  0.3195604 -0.1552145\n\n$vectors\n           [,1]       [,2]       [,3]\n[1,] -0.5148954 -0.4669390  0.4838633\n[2,] -0.6388257  0.2808667 -0.8653808\n[3,] -0.5716507  0.8384997 -0.1303551\n\n\nThe function eigen returns a list with two components, one called values are the diagonal values of \\(\\mathbf{\\lambda}\\), and the other one called vectors is the eigenvector matrix \\(\\mathbf{U}\\).\nWe can check that these two elements can help us reconstruct the original matrix as follows:\n\n   lambda <- diag(eig.res$values)\n   U <- eig.res$vectors\n   B.rec <- U %*% lambda %*% solve(U)\n   B.rec\n\n     [,1] [,2] [,3]\n[1,] 0.74 0.49 0.07\n[2,] 0.88 0.26 0.51\n[3,] 0.63 0.24 0.59\n\n\nWhich are indeed the original values of \\(\\mathbf{B}\\)!\nNow imagine that the matrix \\(\\mathbf{B}\\) is symmetrical:\n\n   B[upper.tri(B)] <- B[lower.tri(B)]\n   B\n\n     [,1] [,2] [,3]\n[1,] 0.74 0.88 0.63\n[2,] 0.88 0.26 0.24\n[3,] 0.63 0.24 0.59\n\n\nAnd let’s do the eigendecomposition of this matrix:\n\n   eig.res <- eigen(B)\n   eig.res\n\neigen() decomposition\n$values\n[1]  1.7709909  0.2757779 -0.4567688\n\n$vectors\n           [,1]       [,2]       [,3]\n[1,] -0.7200185 -0.2389854  0.6515054\n[2,] -0.4963684 -0.4787344 -0.7241766\n[3,] -0.4849657  0.8448073 -0.2260728\n\n\nThe interesting thing here is that now the reconstruction equation boils down to:\n\\[\n\\mathbf{B} = \\mathbf{U}\\mathbf{\\lambda}\\mathbf{U}^T\n\\]\nNote that now we just need to post-multiply \\(\\mathbf{U}\\mathbf{\\lambda}\\) by the transpose of \\(\\mathbf{U}\\) rather than the inverse, which is a much simpler matrix operation.\nWe can check that this is true as follows:\n\n   lambda <- diag(eig.res$values)\n   U <- eig.res$vectors\n   B.rec <- U %*% lambda %*% t(U)\n   B.rec\n\n     [,1] [,2] [,3]\n[1,] 0.74 0.88 0.63\n[2,] 0.88 0.26 0.24\n[3,] 0.63 0.24 0.59\n\n\nWhich are indeed the original values of the symmetric version of \\(\\mathbf{B}\\)!\nNow, the idea is that we can perform the asymmetric or symmetric eigendecomposition with any matrix, including a network adjacency matrix or a proximity matrix derived from it.\nIn fact, we have already done a partial version of the matrix eigendecomposition many times before, because the reflective status game is a way to compute the first column (leading eigenvector) of the \\(\\mathbf{U}\\) matrix for any proximity or adjacency matrix you feed into it.\nThe more important thing is that, once you have the eigendecomposition of a matrix, and the full set of eigenvectors stored in \\(\\mathbf{U}\\), the first few columns of \\(\\mathbf{U}\\), gives us the best low dimensional approximation of the original matrix.\nFor instance, in the above case, the one-dimensional (also called “rank one”) approximation of the original matrix is given by:\n\\[\n\\mathbf{B}_{1-dim} = u_1\\lambda_1u_1^T\n\\]\nWhere \\(u\\) is just the first column (eigenvector) of \\(\\mathbf{U}\\), and \\(\\lambda_1\\) is just the first eigenvalue.\nIn R we can do this approximation as follows:\n\n   u.1 <- as.matrix(U[, 1]) #column vector\n   B.1dim <- u.1 %*% lambda[1, 1] %*% t(u.1)\n   round(B.1dim, 2)\n\n     [,1] [,2] [,3]\n[1,] 0.92 0.63 0.62\n[2,] 0.63 0.44 0.43\n[3,] 0.62 0.43 0.42\n\n\nWhich are not quite the same as the original values of \\(\\mathbf{B}\\), but they are not wildly far off either.\nIf we wanted to be more accurate, however, we would use a two-dimensional approximation (rank two) and thus use more of the information:\n\\[\n\\mathbf{B}_{2-dim} = u_1\\lambda_1u_1^T + u_2\\lambda_2u_2^T\n\\]\nIn R:\n\n   u.2 <- as.matrix(U[, 2])\n   B.2dim <- u.1 %*% lambda[1, 1] %*% t(u.1) + u.2 %*% lambda[2, 2] %*% t(u.2)\n   round(B.2dim, 2)\n\n     [,1] [,2] [,3]\n[1,] 0.93 0.66 0.56\n[2,] 0.66 0.50 0.31\n[3,] 0.56 0.31 0.61\n\n\nWhich are not the same as the original, but are now a bit closer!\nOf course, if we wanted to reconstruct the original matrix, all we have to do is add the product of the eigenvector, eigenvalue, and transpose of the eigenvector across all three dimensions of the matrix:\n\n   u.3 <- as.matrix(U[, 3])\n   B.3dim <- \n      u.1 %*% lambda[1, 1] %*% t(u.1) + \n      u.2 %*% lambda[2, 2] %*% t(u.2) + \n      u.3 %*% lambda[3, 3] %*% t(u.3)\n   round(B.3dim, 2)\n\n     [,1] [,2] [,3]\n[1,] 0.74 0.88 0.63\n[2,] 0.88 0.26 0.24\n[3,] 0.63 0.24 0.59\n\n\nWhich reconstructs the original values.\nSo in general, if you have a symmetric square matrix \\(\\mathbf{B}\\) of dimensions \\(k \\times k\\), and you obtain an eigenvalue decomposition of \\(\\mathbf{B}\\) with eigenvectors stored in the columns of \\(\\mathbf{U}\\) and eigenvalues in \\(\\lambda\\), then the rank-\\(p\\) approximation of the original matrix is given by:\n\\[\n\\mathbf{B}_{rank-p} = \\sum_{m = 1}^p u_{m}\\lambda_mu_m^T\n\\]\nWhen \\(p = k\\), the equation above gives you the original matrix back. When \\(p<k\\) you get the best guess as to what the original was, given \\(p\\) dimensions."
  },
  {
    "objectID": "ca.html#the-ca-matrix",
    "href": "ca.html#the-ca-matrix",
    "title": "Correspondence Analysis (CA) of Two-Mode Networks",
    "section": "The CA Matrix",
    "text": "The CA Matrix\nCA boils down to the eigendecomposition of a suitable matrix, derived from the original affiliation (bi-adjacency) matrix of a two-mode network.\nSo which matrix should be use for CA? Let’s find out:\nFirst we need to create row stochastic versions of the affiliation matrix and its transpose \\(\\mathbf{A}\\) and \\(\\mathbf{A}^T\\). Recall that a matrix is row stochastic if their rows sum to one.\nFor the people, we can do this by taking the original affiliation matrix, and pre-multiplying it by a diagonal square matrix \\(\\mathbf{D}_P^{-1}\\) of dimensions \\(M \\times M\\) containing the inverse of the degrees of each person in the affiliation network along the diagonals and zeros everywhere else, yielding the row-stochastic matrix \\(\\mathbf{P}_P\\) of dimensions \\(M \\times N\\):\n\\[\n\\mathbf{P}_P = \\mathbf{D}_P^{-1}\\mathbf{A}\n\\]\nAnd we can do the same with the groups, except that we pre-multiply the transpose of the original affiliation matrix by \\(\\mathbf{D}_G^{-1}\\) which is an \\(N \\times N\\) matrix containing the inverse of the size of each group along the diagonals and zero everywhere else, this yields the matrix \\(\\mathbf{P}_G\\) of dimensions \\(N \\times M\\):\n\\[\n\\mathbf{P}_G = \\mathbf{D}_G^{-1}\\mathbf{A}^T\n\\]\nIn R can compute \\(\\mathbf{P}_P\\) and \\(\\mathbf{P}_G\\) as follows:\n\n   library(igraph)\n   library(networkdata)\n   g <- southern_women\n   A <- as.matrix(as_biadjacency_matrix(g))\n   D.p <- diag(1/rowSums(A))\n   P.p <- D.p %*% A\n   rownames(P.p) <- rownames(A)\n   D.g <- diag(1/colSums(A))\n   P.g <- D.g %*% t(A)\n   rownames(P.g) <- colnames(A)\n\nAnd we can check that both P.p (for people) and P.g (groups) are row stochastic:\n\n   rowSums(P.p)\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n        1         1         1         1         1         1         1         1 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n        1         1         1         1         1         1         1         1 \n   OLIVIA     FLORA \n        1         1 \n\n   rowSums(P.g)\n\n 6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10  2/23   4/7 11/21 \n    1     1     1     1     1     1     1     1     1     1     1     1     1 \n  8/3 \n    1 \n\n\nAnd that they are of the predicted dimensions:\n\n   dim(P.p)\n\n[1] 18 14\n\n   dim(P.g)\n\n[1] 14 18\n\n\nGreat! Now, we can obtain the degree-normalized projections for people by multiplying \\(\\mathbf{P}_P\\) times \\(\\mathbf{P}_G\\):\n\\[\n\\mathbf{P}_{PP} = \\mathbf{P}_P\\mathbf{P}_G\n\\]\nWhich produces the matrix \\(\\mathbf{P}_{PP}\\) a square \\(M \\times M\\) matrix containing the degree-normalized similarities between each pair of people.\nWe then do the same for groups:\n\\[\n\\mathbf{P}_{GG} = \\mathbf{P}_G\\mathbf{P}_P\n\\]\nWhich produces the matrix \\(\\mathbf{P}_{GG}\\) a square \\(N \\times N\\) matrix containing the degree-normalized similarities between each pair of groups.\nIn R we obtain these matrices as follows:\n\n   P.pp <- P.p %*% P.g\n   P.gg <- P.g %*% P.p\n\nWhich are still row stochastic–but now square–matrices:\n\n   rowSums(P.pp)\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n        1         1         1         1         1         1         1         1 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n        1         1         1         1         1         1         1         1 \n   OLIVIA     FLORA \n        1         1 \n\n   rowSums(P.gg)\n\n 6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10  2/23   4/7 11/21 \n    1     1     1     1     1     1     1     1     1     1     1     1     1 \n  8/3 \n    1 \n\n   dim(P.pp)\n\n[1] 18 18\n\n   dim(P.gg)\n\n[1] 14 14\n\n\nLet’s peek inside one of these matrices:\n\n   round(P.pp[1:10, 1:10], 2)\n\n          EVELYN LAURA THERESA BRENDA CHARLOTTE FRANCES ELEANOR PEARL RUTH\nEVELYN      0.19  0.14    0.14   0.13      0.07    0.06    0.04  0.03 0.03\nLAURA       0.16  0.18    0.13   0.13      0.06    0.07    0.06  0.03 0.04\nTHERESA     0.14  0.12    0.16   0.10      0.08    0.06    0.05  0.03 0.05\nBRENDA      0.15  0.13    0.12   0.17      0.09    0.07    0.06  0.03 0.04\nCHARLOTTE   0.14  0.10    0.16   0.16      0.16    0.07    0.06  0.00 0.06\nFRANCES     0.12  0.12    0.12   0.12      0.07    0.12    0.08  0.05 0.05\nELEANOR     0.08  0.11    0.11   0.11      0.06    0.08    0.11  0.05 0.07\nPEARL       0.09  0.07    0.09   0.07      0.00    0.07    0.07  0.09 0.05\nRUTH        0.07  0.07    0.09   0.07      0.06    0.05    0.07  0.04 0.09\nVERNE       0.04  0.04    0.06   0.04      0.03    0.02    0.04  0.04 0.06\n          VERNE\nEVELYN     0.02\nLAURA      0.02\nTHERESA    0.03\nBRENDA     0.02\nCHARLOTTE  0.03\nFRANCES    0.02\nELEANOR    0.04\nPEARL      0.05\nRUTH       0.06\nVERNE      0.11\n\n\nWhat are these numbers? Well, they can be interpreted as probabilities that a random walker starting at the row node and, following any sequence of \\(person-group-person'-group'\\) hops, will reach the column person. Thus, higher values indicate an affinity or proximity between the people (and the groups in the corresponding matrix)."
  },
  {
    "objectID": "ca.html#the-duality-of-ca-scores-between-persons-and-groups",
    "href": "ca.html#the-duality-of-ca-scores-between-persons-and-groups",
    "title": "Correspondence Analysis (CA) of Two-Mode Networks",
    "section": "The Duality of CA Scores Between Persons and Groups",
    "text": "The Duality of CA Scores Between Persons and Groups\nJust like as we saw with the eigenvector centrality in the last handout, there is a duality between the CA scores assigned to the person and the groups on each dimension, such that the scores for each person are a weighted sum of the scores assigned to each group on that dimension and vice versa (Faust 1997, 171).\nThe main difference is that this time we sum scores across the \\(\\mathbf{P_P}\\) and \\(\\mathbf{P_G}\\) matrices rather than the original affiliation matrix and its transpose, resulting in degree-weighted sums of scores for both persons and groups.\nSo for any given person, on any given dimension, let’s say \\(EVELYN\\), her CA score is given by the sum of the (unit variance normalized) CA scores of the groups she belongs to weighted by her degree (done by multiplying each CA score by the relevant cell in Evelyn’s row of the \\(\\mathbf{P}_{PG}\\) matrix):\n\n   sum(P.pg[\"EVELYN\", ] * uni.g[, 1]) \n\n[1] -0.7994396\n\n\nWhich is the same as the (eigenvalue variance) normalized score we obtained via CA for \\(EVELYN\\):\n\n   val.p[\"EVELYN\", 1]\n\n    EVELYN \n-0.7994396 \n\n\nA similar story applies to groups. Each group score is the group-size-weighted sum of the (unit variance normalized) CA scores of the people who join it:\n\n   sum(P.gp[\"6/27\", ] * uni.p[, 1])\n\n[1] -1.051052\n\n\nWhich is the same as the (eigenvalue variance normalized) score we obtained via CA:\n\n    val.g[\"6/27\", 1]  \n\n     6/27 \n-1.051052 \n\n\nNeat! Duality at work."
  },
  {
    "objectID": "ca.html#obtaining-the-matrices-we-need",
    "href": "ca.html#obtaining-the-matrices-we-need",
    "title": "Correspondence Analysis (CA) of Two-Mode Networks",
    "section": "Obtaining the Matrices We Need",
    "text": "Obtaining the Matrices We Need\nCA boils down to the eigendecomposition of a suitable matrix, derived from the original affiliation (bi-adjacency) matrix of a two-mode network.\nSo which matrix should be use for CA? Let’s find out:\nFirst we need to create row stochastic versions of the affiliation matrix and its transpose \\(\\mathbf{A}\\) and \\(\\mathbf{A}^T\\). Recall that a matrix is row stochastic if their rows sum to one.\nFor the people, we can do this by taking the original affiliation matrix, and pre-multiplying it by a diagonal square matrix \\(\\mathbf{D}_P^{-1}\\) of dimensions \\(M \\times M\\) containing the inverse of the degrees of each person in the affiliation network along the diagonals and zeros everywhere else, yielding the row-stochastic matrix \\(\\mathbf{P}_P\\) of dimensions \\(M \\times N\\):\n\\[\n\\mathbf{P}_P = \\mathbf{D}_P^{-1}\\mathbf{A}\n\\]\nAnd we can do the same with the groups, except that we pre-multiply the transpose of the original affiliation matrix by \\(\\mathbf{D}_G^{-1}\\) which is an \\(N \\times N\\) matrix containing the inverse of the size of each group along the diagonals and zero everywhere else, this yields the matrix \\(\\mathbf{P}_G\\) of dimensions \\(N \\times M\\):\n\\[\n\\mathbf{P}_G = \\mathbf{D}_G^{-1}\\mathbf{A}^T\n\\]\nIn R can compute \\(\\mathbf{P}_P\\) and \\(\\mathbf{P}_G\\), using the classic Southern Women two-mode data, as follows:\n\n   library(igraph)\n   library(networkdata)\n   g <- southern_women #southern women data\n   A <- as.matrix(as_biadjacency_matrix(g)) #bi-adjacency matrix\n   D.p <- diag(1/rowSums(A)) #inverse of degree matrix of persons\n   P.p <- D.p %*% A\n   rownames(P.p) <- rownames(A) \n   D.g <- diag(1/colSums(A)) #inverse of degree matrix of groups\n   P.g <- D.g %*% t(A)\n   rownames(P.g) <- colnames(A)\n\nAnd we can check that both \\(\\mathbf{P}_P\\) and \\(\\mathbf{P}_G\\) are indeed row stochastic:\n\n   rowSums(P.p)\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n        1         1         1         1         1         1         1         1 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n        1         1         1         1         1         1         1         1 \n   OLIVIA     FLORA \n        1         1 \n\n   rowSums(P.g)\n\n 6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10  2/23   4/7 11/21 \n    1     1     1     1     1     1     1     1     1     1     1     1     1 \n  8/3 \n    1 \n\n\nAnd that they are of the predicted dimensions:\n\n   dim(P.p)\n\n[1] 18 14\n\n   dim(P.g)\n\n[1] 14 18\n\n\nGreat! Now, we can obtain the degree-normalized projections for people by multiplying \\(\\mathbf{P}_P\\) times \\(\\mathbf{P}_G\\):\n\\[\n\\mathbf{P}_{PP} = \\mathbf{P}_P\\mathbf{P}_G\n\\]\nWhich produces the matrix \\(\\mathbf{P}_{PP}\\) a square \\(M \\times M\\) matrix containing the degree-normalized similarities between each pair of people.\nWe then do the same for groups:\n\\[\n\\mathbf{P}_{GG} = \\mathbf{P}_G\\mathbf{P}_P\n\\]\nWhich produces the matrix \\(\\mathbf{P}_{GG}\\) a square \\(N \\times N\\) matrix containing the degree-normalized similarities between each pair of groups.\nIn R we obtain these matrices as follows:\n\n   P.pp <- P.p %*% P.g\n   P.gg <- P.g %*% P.p\n\nWhich are still row stochastic–but now square–matrices:\n\n   rowSums(P.pp)\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n        1         1         1         1         1         1         1         1 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n        1         1         1         1         1         1         1         1 \n   OLIVIA     FLORA \n        1         1 \n\n   rowSums(P.gg)\n\n 6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10  2/23   4/7 11/21 \n    1     1     1     1     1     1     1     1     1     1     1     1     1 \n  8/3 \n    1 \n\n   dim(P.pp)\n\n[1] 18 18\n\n   dim(P.gg)\n\n[1] 14 14\n\n\nLet’s peek inside one of these matrices:\n\n   round(head(P.pp), 3)\n\n          EVELYN LAURA THERESA BRENDA CHARLOTTE FRANCES ELEANOR PEARL  RUTH\nEVELYN     0.186 0.144   0.144  0.134     0.068   0.061   0.040 0.035 0.035\nLAURA      0.165 0.179   0.132  0.132     0.056   0.070   0.060 0.028 0.042\nTHERESA    0.144 0.115   0.157  0.105     0.080   0.061   0.053 0.035 0.047\nBRENDA     0.153 0.132   0.120  0.167     0.092   0.070   0.060 0.028 0.042\nCHARLOTTE  0.135 0.098   0.160  0.160     0.160   0.073   0.056 0.000 0.056\nFRANCES    0.122 0.122   0.122  0.122     0.073   0.122   0.080 0.049 0.049\n          VERNE MYRNA KATHERINE SYLVIA  NORA HELEN DOROTHY OLIVIA FLORA\nEVELYN    0.019 0.019     0.019  0.019 0.026 0.009   0.019   0.01  0.01\nLAURA     0.024 0.010     0.010  0.024 0.032 0.024   0.010   0.00  0.00\nTHERESA   0.032 0.019     0.019  0.032 0.039 0.021   0.019   0.01  0.01\nBRENDA    0.024 0.010     0.010  0.024 0.032 0.024   0.010   0.00  0.00\nCHARLOTTE 0.025 0.000     0.000  0.025 0.025 0.025   0.000   0.00  0.00\nFRANCES   0.018 0.018     0.018  0.018 0.031 0.018   0.018   0.00  0.00\n\n\nWhat are these numbers? Well, they can be interpreted as probabilities that a random walker starting at the row node and, following any sequence of \\(person-group-person'-group'\\) hops, will reach the column person. Thus, higher values indicate an affinity or proximity between the people (and the groups in the corresponding matrix)."
  },
  {
    "objectID": "ca.html#performing-ca",
    "href": "ca.html#performing-ca",
    "title": "Correspondence Analysis (CA) of Two-Mode Networks",
    "section": "Performing CA",
    "text": "Performing CA\nWe went through all these steps because CA is equivalent to the eigendecomposition of the last two square matrices we obtained, namely, \\(\\mathbf{P_{PP}}\\) and \\(\\mathbf{P_{GG}}\\):\n\n   CA.p <- eigen(P.pp)\n   CA.g <- eigen(P.gg)\n\nLet’s see what we have here:\n\n   round(CA.p$values, 2)\n\n [1] 1.00 0.63 0.32 0.18 0.14 0.11 0.10 0.06 0.04 0.04 0.02 0.01 0.01 0.00 0.00\n[16] 0.00 0.00 0.00\n\n   round(CA.g$values, 2)\n\n [1] 1.00 0.63 0.32 0.18 0.14 0.11 0.10 0.06 0.04 0.04 0.02 0.01 0.01 0.00\n\n\nSo the two matrices have identical eigenvalues, and the first one is 1.0.\nLet’s check out the first three eigenvectors:\n\n   rownames(CA.p$vectors) <- rownames(A)\n   rownames(CA.g$vectors) <- colnames(A)\n   round(CA.p$vectors[, 1:3], 2)\n\n           [,1]  [,2]  [,3]\nEVELYN    -0.24 -0.24  0.03\nLAURA     -0.24 -0.25 -0.01\nTHERESA   -0.24 -0.20  0.02\nBRENDA    -0.24 -0.26 -0.01\nCHARLOTTE -0.24 -0.29 -0.01\nFRANCES   -0.24 -0.24 -0.02\nELEANOR   -0.24 -0.15 -0.03\nPEARL     -0.24 -0.01  0.06\nRUTH      -0.24 -0.05  0.03\nVERNE     -0.24  0.13 -0.04\nMYRNA     -0.24  0.25 -0.10\nKATHERINE -0.24  0.31 -0.22\nSYLVIA    -0.24  0.26 -0.20\nNORA      -0.24  0.26 -0.03\nHELEN     -0.24  0.24  0.07\nDOROTHY   -0.24  0.09  0.09\nOLIVIA    -0.24  0.33  0.66\nFLORA     -0.24  0.33  0.66\n\n   round(CA.g$vectors[, 1:3], 2)\n\n      [,1]  [,2]  [,3]\n6/27  0.27 -0.30 -0.01\n3/2   0.27 -0.28 -0.04\n4/12  0.27 -0.30  0.00\n9/26  0.27 -0.30 -0.02\n2/25  0.27 -0.25  0.00\n5/19  0.27 -0.16  0.00\n3/15  0.27 -0.04  0.05\n9/16  0.27 -0.01  0.05\n4/8   0.27  0.15 -0.19\n6/10  0.27  0.32  0.22\n2/23  0.27  0.35 -0.79\n4/7   0.27  0.29  0.20\n11/21 0.27  0.34  0.35\n8/3   0.27  0.34  0.35\n\n\nSo this is interesting. The first eigenvector of the decomposition of both \\(\\mathbf{P_{PP}}\\) and \\(\\mathbf{P_{GG}}\\) is just the same number for each person and group. Note that this is the eigenvector that is associated with the first eigenvalue which happens to be \\(\\lambda_1 = 1.0\\).\nSo it looks like the first eigenvector is a pretty useless quantity (a constant) so we can discard it, keeping all the other ones. Now the old second eigenvector is the first, the old third is the second, and so on:\n\n   eig.vec.p <- CA.p$vectors[, 2:ncol(CA.p$vectors)]\n   eig.vec.g <- CA.g$vectors[, 2:ncol(CA.g$vectors)]\n\nNote that the rest of the eigenvalues (discarding the 1.0 one) are arranged in descending order:\n\n   eig.vals <- CA.p$values[2:length(CA.p$values)]\n   round(eig.vals, 3)\n\n [1] 0.627 0.319 0.179 0.138 0.107 0.099 0.064 0.044 0.036 0.021 0.012 0.005\n[13] 0.000 0.000 0.000 0.000 0.000\n\n\nThe magnitude of the eigenvalue tells us how important is the related eigenvector in containing information about the original matrix. It looks like here, the first two eigenvectors contain a good chunk of the info.\nWe can check how much exactly by computing the ratio between the sum of the first two eigenvalues over the sum of all the eigenvalues:\n\n   round(sum(eig.vals[1:2])/sum(eig.vals), 2)\n\n[1] 0.57\n\n\nWhich tells us that the first two eigenvectors account for about 57% of the action (or more precisely we could reconstruct the original matrix with 57% accuracy using just these two eigenvectors and associated eigenvalues).\nBecause the magnitude of the CA eigenvectors don’t have a natural scale, it is common to normalize them to have a variance of 1.0 (Fouss, Saerens, and Shimbo 2016, 399), and the multiplying them by the square root of the eigenvalue corresponding to that dimension, so that the new variance is scaled to the importance of that dimension.\nWe can perform the normalization of the raw CA scores using the following function, which performs CA on the affiliation matrix:\n\n   norm.CA.vec <- function(x, s = 0.000001) {\n      D.p <- diag(rowSums(x)) #degree matrix for persons\n      D.g <- diag(colSums(x)) #degree matrix for groups\n      i.Dp <- solve(D.p) #inverse of degree matrix for persons\n      i.Dg <- solve(D.g) #inverse of degree matrix for groups\n      CA.p <- eigen(i.Dp %*% x %*% i.Dg %*% t(x)) #person CA\n      CA.g <- eigen(i.Dg %*% t(x) %*% i.Dp %*% x) #group CA\n      ev <- CA.p$values[2:length(CA.p$values)]\n      ev <- ev[which(ev > s)]\n      m <- length(ev)\n      CA.p <- CA.p$vectors[, 2:ncol(CA.p$vectors)]\n      CA.g <- CA.g$vectors[, 2:ncol(CA.g$vectors)]\n      rownames(CA.p) <- rownames(A)\n      rownames(CA.g) <- colnames(A)\n      Z.u.p <- matrix(0, nrow(x), m)\n      Z.u.g <- matrix(0, ncol(x), m)\n      Z.v.p <- matrix(0, nrow(x), m)\n      Z.v.g <- matrix(0, ncol(x), m)\n      rownames(Z.u.p) <- rownames(x)\n      rownames(Z.u.g) <- colnames(x)\n      rownames(Z.v.p) <- rownames(x)\n      rownames(Z.v.g) <- colnames(x)\n      for (i in 1:m) {\n         ev.p <- as.matrix(CA.p[, i])\n         ev.g <- as.matrix(CA.g[, i])\n         norm.p <- as.numeric(t(ev.p) %*% D.p %*% ev.p) #person norm\n         Z.u.p[, i] <- ev.p * sqrt(sum(A)/norm.p) #normalizing to unit variance\n         Z.v.p[, i] <- Z.u.p[, i] * sqrt(ev[i]) #normalizing to square root of eigenvalue\n         norm.g <- as.numeric(t(ev.g) %*% D.g %*% ev.g) #group norm\n         Z.u.g[, i] <- ev.g * sqrt(sum(A)/norm.g) #normalizing to unit variance\n         Z.v.g[, i] <- Z.u.g[, i] * sqrt(ev[i]) #normalizing to square root of eigenvalue\n      }\n   return(list(Z.u.p = Z.u.p, Z.u.g = Z.u.g,\n               Z.v.p = Z.v.p, Z.v.g = Z.v.g))\n   }\n\nThis function takes the bi-adjacency matrix as input and returns two new set of normalized CA scores for both persons and groups as output. The normalized CA scores are stored in four separate matrices: \\(\\mathbf{Z_P^U}, \\mathbf{Z_G^U}, \\mathbf{Z_P^V}, \\mathbf{Z_G^V}\\).\nOne person-group set of scores is normalized to unit variance (Z.u.p and Z.u.g) and the other person-group set of scores is normalized to the scale of the eigenvalue corresponding to each CA dimension for both persons and groups (Z.v.p and Z.v.g).\nLet’s see the normalization function at work, extracting the first two dimensions for persons and groups (the first two columns of each \\(\\mathbf{Z}\\) matrix):\n\n   CA.res <- norm.CA.vec(A)\n   uni.p <- CA.res$Z.u.p[, 1:2]\n   uni.g <- CA.res$Z.u.g[, 1:2]\n   val.p <- CA.res$Z.v.p[, 1:2]\n   val.g <- CA.res$Z.v.g[, 1:2]\n   round(uni.p, 2)\n\n           [,1]  [,2]\nEVELYN    -1.01  0.20\nLAURA     -1.06 -0.07\nTHERESA   -0.83  0.14\nBRENDA    -1.08 -0.09\nCHARLOTTE -1.23 -0.07\nFRANCES   -1.01 -0.10\nELEANOR   -0.65 -0.21\nPEARL     -0.05  0.37\nRUTH      -0.21  0.17\nVERNE      0.55 -0.23\nMYRNA      1.04 -0.58\nKATHERINE  1.32 -1.33\nSYLVIA     1.10 -1.20\nNORA       1.10 -0.19\nHELEN      1.02  0.44\nDOROTHY    0.38  0.55\nOLIVIA     1.38  3.98\nFLORA      1.38  3.98\n\n   round(uni.g, 2)\n\n       [,1]  [,2]\n6/27  -1.33 -0.02\n3/2   -1.22 -0.16\n4/12  -1.31  0.00\n9/26  -1.31 -0.08\n2/25  -1.12  0.00\n5/19  -0.72 -0.01\n3/15  -0.16  0.23\n9/16  -0.04  0.24\n4/8    0.65 -0.87\n6/10   1.41  1.01\n2/23   1.54 -3.64\n4/7    1.29  0.91\n11/21  1.48  1.61\n8/3    1.48  1.61\n\n   round(val.p, 2)\n\n           [,1]  [,2]\nEVELYN    -0.80  0.11\nLAURA     -0.84 -0.04\nTHERESA   -0.65  0.08\nBRENDA    -0.86 -0.05\nCHARLOTTE -0.97 -0.04\nFRANCES   -0.80 -0.06\nELEANOR   -0.51 -0.12\nPEARL     -0.04  0.21\nRUTH      -0.17  0.10\nVERNE      0.43 -0.13\nMYRNA      0.83 -0.33\nKATHERINE  1.05 -0.75\nSYLVIA     0.87 -0.68\nNORA       0.87 -0.11\nHELEN      0.81  0.25\nDOROTHY    0.30  0.31\nOLIVIA     1.10  2.25\nFLORA      1.10  2.25\n\n   round(val.g, 2)\n\n       [,1]  [,2]\n6/27  -1.05 -0.01\n3/2   -0.97 -0.09\n4/12  -1.04  0.00\n9/26  -1.04 -0.05\n2/25  -0.88  0.00\n5/19  -0.57 -0.01\n3/15  -0.13  0.13\n9/16  -0.03  0.14\n4/8    0.51 -0.49\n6/10   1.12  0.57\n2/23   1.22 -2.05\n4/7    1.02  0.52\n11/21  1.17  0.91\n8/3    1.17  0.91\n\n\nGreat! Now we have two sets (unit variance versus eigenvalue variance) of normalized CA scores for persons and groups on the first two dimensions."
  },
  {
    "objectID": "handout7.html#bipartiteness-as-anti-community",
    "href": "handout7.html#bipartiteness-as-anti-community",
    "title": "Analyzing Two-Mode Networks",
    "section": "Bipartiteness as “Anti-Community”",
    "text": "Bipartiteness as “Anti-Community”\nRecall from the previous handout, that community structure is defined by clusters of nodes that have more connections among themselves than they do with outsiders. If you think about it, a bipartite graph has the opposite of this going on. Nodes of the same type have zero connections among themselves, and they have all their connections with nodes of the other group!\nSo that means that bipartite structure is the mirror image of community structure (in the two group case). This also means that if we were to compute the modularity of a bipartite graph, using the node type as the grouping variable we should get the theoretical minimum of this measure (which you may recall is \\(Q = -\\frac{1}{2}\\)).\nLet’s try it out, by computing the modularity from the bipartite adjacency matrix of the Southern Women data, using node type as the grouping variable:\n\n   V(g)$comm <- as.numeric(bipartite_mapping(g)$type) + 1\n   modularity(g, V(g)$comm)\n\n[1] -0.5\n\n\nAnd indeed, we recover the theoretical minimum value of the modularity (Brandes et al. 2007, 173)! This also means that this method can be used to test whether a graph is bipartite, or whether any network approximates bipartiteness (Newman 2006, 13). Values that are close to \\(-0.5\\) would indicate that the network in question has bipartite structure."
  },
  {
    "objectID": "ca.html#obtaining-the-matrices-we-need-for-ca",
    "href": "ca.html#obtaining-the-matrices-we-need-for-ca",
    "title": "Correspondence Analysis (CA) of Two-Mode Networks",
    "section": "Obtaining the Matrices We Need for CA",
    "text": "Obtaining the Matrices We Need for CA\nAs you might have already guessed, two-mode CA boils down to the eigendecomposition of a suitable matrix, derived from the original affiliation (bi-adjacency) matrix of a two-mode network.\nThe goal is to come up with a low-rank (usually two-dimensional) approximation of the original affiliation network using the eigenvectors and eigenvalues obtained from the decomposition, as we did above with our toy example.\nSo which matrix should be use for CA?\nLet’s find out:\nFirst we need to create row stochastic versions of the affiliation matrix and its transpose \\(\\mathbf{A}\\) and \\(\\mathbf{A}^T\\). Recall that a matrix is row stochastic if their rows sum to one.\nFor the people, we can do this by taking the original affiliation matrix, and pre-multiplying it by a diagonal square matrix \\(\\mathbf{D}_P^{-1}\\) of dimensions \\(M \\times M\\) containing the inverse of the degrees of each person in the affiliation network along the diagonals and zeros everywhere else, yielding the row-stochastic matrix \\(\\mathbf{P}_{PG}\\) of dimensions \\(M \\times N\\):\n\\[\n\\mathbf{P}_{PG} = \\mathbf{D}_P^{-1}\\mathbf{A}\n\\]\nAnd we can do the same with the groups, except that we pre-multiply the transpose of the original affiliation matrix by \\(\\mathbf{D}_G^{-1}\\) which is an \\(N \\times N\\) matrix containing the inverse of the size of each group along the diagonals and zero everywhere else, this yields the matrix \\(\\mathbf{P}_{GP}\\) of dimensions \\(N \\times M\\):\n\\[\n\\mathbf{P}_{GP} = \\mathbf{D}_G^{-1}\\mathbf{A}^T\n\\]\nIn R can compute \\(\\mathbf{P}_{PG}\\) and \\(\\mathbf{P}_{GP}\\), using the classic Southern Women two-mode data, as follows:\n\n   library(igraph)\n   library(networkdata)\n   g <- southern_women #southern women data\n   A <- as.matrix(as_biadjacency_matrix(g)) #bi-adjacency matrix\n   D.p <- diag(1/rowSums(A)) #inverse of degree matrix of persons\n   P.pg <- D.p %*% A\n   rownames(P.pg) <- rownames(A) \n   D.g <- diag(1/colSums(A)) #inverse of degree matrix of groups\n   P.gp <- D.g %*% t(A)\n   rownames(P.gp) <- colnames(A)\n\nAnd we can check that both \\(\\mathbf{P}_{PG}\\) and \\(\\mathbf{P}_{GP}\\) are indeed row stochastic:\n\n   rowSums(P.pg)\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n        1         1         1         1         1         1         1         1 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n        1         1         1         1         1         1         1         1 \n   OLIVIA     FLORA \n        1         1 \n\n   rowSums(P.gp)\n\n 6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10  2/23   4/7 11/21 \n    1     1     1     1     1     1     1     1     1     1     1     1     1 \n  8/3 \n    1 \n\n\nAnd that they are of the predicted dimensions:\n\n   dim(P.pg)\n\n[1] 18 14\n\n   dim(P.pg)\n\n[1] 18 14\n\n\nGreat! Now, we can obtain the degree-normalized projections for people by multiplying \\(\\mathbf{P}_{PG}\\) times \\(\\mathbf{P}_{GP}\\):\n\\[\n\\mathbf{P}_{PP} = \\mathbf{P}_{PG}\\mathbf{P}_{GP}\n\\]\nWhich produces the matrix \\(\\mathbf{P}_{PP}\\) a square \\(M \\times M\\) matrix containing the degree-normalized similarities between each pair of people.\nWe then do the same for groups:\n\\[\n\\mathbf{P}_{GG} = \\mathbf{P}_{GP}\\mathbf{P}_{PG}\n\\]\nWhich produces the matrix \\(\\mathbf{P}_{GG}\\) a square \\(N \\times N\\) matrix containing the degree-normalized similarities between each pair of groups.\nIn R we obtain these matrices as follows:\n\n   P.pp <- P.pg %*% P.gp\n   P.gg <- P.gp %*% P.pg\n\nWhich are still row stochastic–but now square–matrices:\n\n   rowSums(P.pp)\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n        1         1         1         1         1         1         1         1 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n        1         1         1         1         1         1         1         1 \n   OLIVIA     FLORA \n        1         1 \n\n   rowSums(P.gg)\n\n 6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10  2/23   4/7 11/21 \n    1     1     1     1     1     1     1     1     1     1     1     1     1 \n  8/3 \n    1 \n\n   dim(P.pp)\n\n[1] 18 18\n\n   dim(P.gg)\n\n[1] 14 14\n\n\nLet’s peek inside one of these matrices:\n\n   round(head(P.pp), 3)\n\n          EVELYN LAURA THERESA BRENDA CHARLOTTE FRANCES ELEANOR PEARL  RUTH\nEVELYN     0.186 0.144   0.144  0.134     0.068   0.061   0.040 0.035 0.035\nLAURA      0.165 0.179   0.132  0.132     0.056   0.070   0.060 0.028 0.042\nTHERESA    0.144 0.115   0.157  0.105     0.080   0.061   0.053 0.035 0.047\nBRENDA     0.153 0.132   0.120  0.167     0.092   0.070   0.060 0.028 0.042\nCHARLOTTE  0.135 0.098   0.160  0.160     0.160   0.073   0.056 0.000 0.056\nFRANCES    0.122 0.122   0.122  0.122     0.073   0.122   0.080 0.049 0.049\n          VERNE MYRNA KATHERINE SYLVIA  NORA HELEN DOROTHY OLIVIA FLORA\nEVELYN    0.019 0.019     0.019  0.019 0.026 0.009   0.019   0.01  0.01\nLAURA     0.024 0.010     0.010  0.024 0.032 0.024   0.010   0.00  0.00\nTHERESA   0.032 0.019     0.019  0.032 0.039 0.021   0.019   0.01  0.01\nBRENDA    0.024 0.010     0.010  0.024 0.032 0.024   0.010   0.00  0.00\nCHARLOTTE 0.025 0.000     0.000  0.025 0.025 0.025   0.000   0.00  0.00\nFRANCES   0.018 0.018     0.018  0.018 0.031 0.018   0.018   0.00  0.00\n\n\nWhat are these numbers? Well, they can be interpreted as probabilities that a random walker starting at the row node and, following any sequence of \\(person-group-person'-group'\\) hops, will reach the column person. Thus, higher values indicate an affinity or proximity between the people (and the groups in the corresponding matrix)."
  },
  {
    "objectID": "modularity-kmeans.html",
    "href": "modularity-kmeans.html",
    "title": "Community Detection Using Spectral Clustering",
    "section": "",
    "text": "In the handout on communities, we saw how to use the leading (first) eigenvector of the modularity matrix to split a network in two (and then split those partitions in two and so on).\nIn the handout on CA, we saw that the eigendecomposition of a square matrix actually results in \\(p\\) eigenvectors and eigenvalues (not just the leading or first).\nWe also learned from the eigendecomposition lesson that selecting some number \\(k < p\\) of eigenvectors and eigenvalues helps us reconstruct the best approximation of the original matrix given that number of dimensions.\nPutting these three lessons together suggests a simple way to detect multiple communities in a network using the eigenvalues of a suitable matrix, along with standard clustering algorithms such as k-means clustering. This approach does not have to iterate between binary divisions based on a single (leading) eigenvector, but can instead use multiple eigenvectors at once to partition the data into any desired number of clusters.\nThis general approach to community detection is sometimes referred to as spectral clustering.1"
  },
  {
    "objectID": "modularity-kmeans.html#using-the-eigenvectors-of-the-laplacian-ratio-cut-clustering",
    "href": "modularity-kmeans.html#using-the-eigenvectors-of-the-laplacian-ratio-cut-clustering",
    "title": "Community Detection Using Spectral Clustering",
    "section": "Using the Eigenvectors of the Laplacian: Ratio-Cut Clustering",
    "text": "Using the Eigenvectors of the Laplacian: Ratio-Cut Clustering\nThe issue then becomes, which matrix should we use the eigenvalues and eigenvectors of? As suggested by Von Luxburg (2007), an obvious candidate is what is called the graph Laplacian (\\(\\mathbf{L}\\)), which is given by:\n\\[\n\\mathbf{L} = \\mathbf{D} - \\mathbf{A}\n\\]\nWhere \\(\\mathbf{D}\\) is the graph’s degree matrix a matrix with the degree vector of the graph along the diagonals and zeros everywhere else.\nLike the modularity matrix, the \\(\\mathbf{L}\\) matrix is doubly-centered (rows and columns sum to zero), which means that one of the eigenvalues of \\(\\mathbf{L}\\) is guaranteed to be zero. The other interesting thing is that the second smallest eigenvalue of \\(\\mathbf{L}\\) (sometimes called the Fiedler vector) provides the best two-community partition of the graph (just like the leading eigenvector of the modularity matrix does).\nIf we want to check for the existence of multiple groups, therefore, what we need to do is to create a new \\(n \\times k\\) matrix \\(\\mathbf{U}\\) composed of the \\(k\\) eigenvectors of \\(\\mathbf{L}\\) associated with the \\(k\\) smallest eigenvalues.\nWe then row-normalize each of these values (e.g., using the Eucledian norm) and use the \\(\\mathbf{U}\\) vector as input to a kmeans algorithm with known number of clusters (Fouss, Saerens, and Shimbo 2016, 320).\nLet’s see how that would work. Let’s load the law_friends data from the networkdata package containing information on the friendship nominations of 68 lawyers in a firm. We have already analyzed these data using other community detection algorithms in a previous handout.\n\n   library(networkdata)\n   library(igraph)\n   g <- law_friends\n   g <- as_undirected(g, mode = \"collapse\")\n   g <- subgraph(g, degree(g)>=2) #removing low degree nodes\n\nWe can then write a function called ratio.cut that accomplishes what we described earlier. It looks like this:\n\n   ratio.cut <- function(x, k = 6) {\n         A <- as.matrix(as_adjacency_matrix(x))\n         D <- diag(degree(x))\n         n <- vcount(x)\n         L <- D - A\n         eig.L <- eigen(L)\n         b <- n - k\n         e <- n - 1\n         U <- eig.L$vectors[, b:e]\n         U <- U[, k:1]\n         for (i in 1:nrow(U)) {\n            U[i, ] <- U[i, ]/norm(as.matrix(U[i, ]), type = \"E\")\n            }\n      return(U)\n   }\n\nThis function takes a graph as input and produces the \\(\\mathbf{U}\\) matrix with \\(n\\) rows and \\(k\\) columns, with \\(k=6\\) by default. It first computes the adjacency matrix (line 2), then the degree matrix (line 3), then the Laplacian matrix (line 5). It then computes the eigendecomposition of the Laplacian (line 6), row normalizes the values taken by the eigenvectors corresponding to the \\(k\\) smallest eigenvalues in lines 10-13 (reverse-ordered from smallest to biggest in line 9), and returns the resulting matrix \\(\\mathbf{U}\\) in line 14.\nLet’s see the function at work:\n\n   U <- ratio.cut(g)\n   round(U, 2)[1:10, ]\n\n       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]\n [1,]  0.08  0.34  0.66  0.37 -0.54  0.13\n [2,]  0.07  0.29  0.60  0.37 -0.53  0.37\n [3,]  0.47  0.13  0.08 -0.87  0.03  0.04\n [4,]  0.34  0.38  0.71  0.16 -0.45  0.10\n [5,]  0.57  0.26  0.06 -0.78  0.00  0.01\n [6,]  0.58 -0.76 -0.21  0.20 -0.05  0.03\n [7,]  0.37  0.20 -0.01 -0.90  0.08  0.03\n [8,] -0.02  0.33  0.65  0.41 -0.55 -0.01\n [9,]  0.14  0.37  0.67  0.36 -0.52  0.04\n[10,]  0.05  0.36  0.66  0.42 -0.50  0.06\n\n\nWhich shows the corresponding values of \\(\\mathbf{U}\\) for each node across the six eigenvectors of the Laplacian we selected.\nNow, the basic idea is to treat each of the normalized scores along the six eigenvectors as if they were “variables” or “features” in a standard kmeans clustering problem. The algorithm will then group nodes based on how similar their scores are on each six-dimensional vector. Similar nodes will correspond to communities (kmeans clusters) in our data.\nKmeans requires knowing how many groups we want in advanced (as opposed to hierarchical clustering). Since we don’t know in advance which is the best community partition, we compute a bunch of partitions and check the modularity of each.\nThe following function computes cluster assignments of nodes up to ten partitions (starting from the minimum two):\n\n   k.cuts <- function(x, max = 9) {\n      clus <- list()\n      for (i in 1:max) {\n         set.seed(456) #setting seed because kmeans uses random starting nodes for cluster centroids\n         k <- i + 1\n         clus[[i]] <- kmeans(x, k)$cluster\n         }\n      return(clus)\n   }\n\nThis function takes \\(\\mathbf{U}\\) matrix as input and returns a list of cluster assignments for each nodes with nine elements, each corresponding to partitions \\(k = \\{2, 3, \\ldots 10\\}\\).\nLet’s see the clustering function at work:\n\n   clus <- k.cuts(U)\n\nGreat! Now that we have our partitions, we need to check the modularity corresponding to each one of them.\nWe can do this with the following quick function:\n\n   mod.check <- function(x, c) {\n      k <- length(c)\n      m <- rep(0, k)\n      for (i in 1:k) {\n         m[i] <- modularity(x, c[[i]])\n         }\n      names(m) <- 2:(k+1)\n      return(m)\n      }\n\nWhich takes an igraph graph object and the list of cluster assignments as input and produces a vector of the same length as the list of cluster assignments with the modularity corresponding to that assignment.\nLet’s see this function at work in our running example:\n\n   mod.res <- mod.check(g, clus)\n   round(mod.res, 3)\n\n    2     3     4     5     6     7     8     9    10 \n0.305 0.356 0.328 0.319 0.316 0.300 0.302 0.293 0.286 \n\n\nWhich suggests that the three-cluster partition of the network (shown in Figure 1 (b)) does pretty well in separating densely connected subgraphs (\\(Q = 0.36\\)).\nNote that this partition looks a lot like the one we settled on using Newman’s divisive leading eigenvector approach based on the modularity matrix: One core dense group of lawyers surrounded by a looser couple of communities.\n\n\n\n\n\n\n\n(a) Original Network\n\n\n\n\n\n\n\n(b) Maximum Modularity Solution)\n\n\n\n\n\n\n\n\n\n(c) Four Community Solution\n\n\n\n\n\n\n\n(d) Five Community Solution\n\n\n\n\n\n\n\n\n\n(e) Six Community Solution\n\n\n\n\n\n\n\n(f) Seven Community Solution\n\n\n\n\nFigure 1: Clustering of Nodes in the Law Firm Friendship Network Using the Ratio Cut\n\n\nNote also that the four, five, six and even seven-community partitions are not too shabby either. We can see those in Figure 1 (c)-Figure 1 (f) as they reveal further insights into the group structure of the network beyond the main three-community division.\nNote that further subdivisions of the network split the more loosely structured community in the upper-right, while the more densely linked community in the lower-left remains largely undisturbed."
  },
  {
    "objectID": "modularity-kmeans.html#using-the-eigenvectors-of-the-normalized-laplacian-normalized-cut-clustering",
    "href": "modularity-kmeans.html#using-the-eigenvectors-of-the-normalized-laplacian-normalized-cut-clustering",
    "title": "Community Detection Using Spectral Clustering",
    "section": "Using the Eigenvectors of the Normalized Laplacian: Normalized-Cut Clustering",
    "text": "Using the Eigenvectors of the Normalized Laplacian: Normalized-Cut Clustering\n\n   norm.ratio.cut <- function(x, k = 6) {\n         A <- as.matrix(as_adjacency_matrix(x))\n         n <- vcount(x)\n         D <- diag(1/sqrt(degree(x)))\n         I <- diag(1, n, n)\n         L <- I - (D %*% A %*% D)\n         eig.L <- eigen(L)\n         b <- n - k\n         e <- n - 1\n         U <- eig.L$vectors[, b:e]\n         U <- U[, k:1]\n         for (i in 1:nrow(U)) {\n            U[i, ] <- U[i, ]/norm(as.matrix(U[i, ]), type = \"E\")\n            }\n      return(U)\n   }\n\n\n   U <- norm.ratio.cut(g)\n   clus <- k.cuts(U)\n   mod.res <- mod.check(g, clus)\n   round(mod.res, 3)\n\n    2     3     4     5     6     7     8     9    10 \n0.250 0.316 0.320 0.352 0.350 0.331 0.293 0.284 0.246 \n\n\n\n\n\n\n\n\n\n(a) Five Community Solution\n\n\n\n\n\n\n\n(b) Six Community Solution\n\n\n\n\nFigure 2: Clustering of Nodes in the Law Firm Friendship Network Using the Ratio Cut"
  },
  {
    "objectID": "modularity-kmeans.html#clustering-using-the-eigenvectors-of-the-normalized-laplacian",
    "href": "modularity-kmeans.html#clustering-using-the-eigenvectors-of-the-normalized-laplacian",
    "title": "Community Detection Using Spectral Clustering",
    "section": "Clustering Using the Eigenvectors of the Normalized Laplacian",
    "text": "Clustering Using the Eigenvectors of the Normalized Laplacian\nWe saw how to cluster a network in a way that results in a good community partition using the Laplacian of the adjacency matrix \\(\\mathbf{L}\\). Another approach is to use a (degree) normalized version of the same matrix \\(\\mathbf{\\hat{L}}\\), defined as follows:\n\\[\n\\mathbf{\\hat{L}} = \\mathbf{I} - \\mathbf{D}^{-\\frac{1}{2}}\\mathbf{A}\\mathbf{D}^{-\\frac{1}{2}}\n\\]\nWhere everything else is as before and \\(\\mathbf{I}\\) is the identity matrix (an \\(n \\times n\\) matrix with ones along the diagonals and zero everywhere else), and \\(\\mathbf{D}^{-\\frac{1}{2}}\\) is a matrix containing the inverse of the square root of degrees of each node (\\(1/\\sqrt{k_i}\\)) in the diagonals and zeros everywhere else.\nWe can just adapt our previous ratio.cut function code to perform this new job:\n\n   norm.ratio.cut <- function(x, k = 6) {\n         A <- as.matrix(as_adjacency_matrix(x))\n         n <- vcount(x)\n         I <- diag(1, n, n)\n         D <- diag(1/sqrt(degree(x)))\n         L <- I - (D %*% A %*% D)\n         eig.L <- eigen(L)\n         b <- n - k\n         e <- n - 1\n         U <- eig.L$vectors[, b:e]\n         U <- U[, k:1]\n         for (i in 1:nrow(U)) {\n            U[i, ] <- U[i, ]/norm(as.matrix(U[i, ]), type = \"E\")\n            }\n      return(U)\n   }\n\nWhere we just have to modify the way we define the \\(\\mathbf{D}\\) and \\(\\mathbf{L}\\) matrices in lines 5 and 6 respectively, after creating the \\(\\mathbf{I}\\) matrix in line 4.\nNow let’s see if the normalized cut can help us finds some communities:\n\n   U <- norm.ratio.cut(g)\n   clus <- k.cuts(U)\n   mod.res <- mod.check(g, clus)\n   round(mod.res, 3)\n\n    2     3     4     5     6     7     8     9    10 \n0.250 0.316 0.320 0.352 0.350 0.331 0.293 0.284 0.246 \n\n\n\n\n\n\n\n\n\n(a) Five Community Solution\n\n\n\n\n\n\n\n(b) Six Community Solution\n\n\n\n\nFigure 2: Clustering of Nodes in the Law Firm Friendship Network Using the Normalized Ratio Cut\n\n\nAs we can see, the normalized ratio cut approach performs almost as well as the ratio cut approach in terms of the maximum modularity it finds (\\(Q = 0.35\\)), but suggests a finer grained partition, with the maximum at either five or six communities.\nThe resulting node clusters are shown in Figure 2 (a) and Figure 2 (b)."
  },
  {
    "objectID": "modularity-kmeans.html#clustering-using-the-eigenvectors-of-the-modularity-matrix",
    "href": "modularity-kmeans.html#clustering-using-the-eigenvectors-of-the-modularity-matrix",
    "title": "Community Detection Using Spectral Clustering",
    "section": "Clustering Using the Eigenvectors of the Modularity Matrix",
    "text": "Clustering Using the Eigenvectors of the Modularity Matrix\nAs noted by Fender et al. (2017, 1796), we can extend the spectral clustering approach based on the Laplacian and the normalized Laplacian to the modularity matrix \\(\\mathbf{B}\\). That is, we cluster the graph by embedding the nodes in a set of dimensions defined by the eigendecomposition of \\(\\mathbf{B}\\).\nThe main difference is that rather than using the eigenvectors corresponding to the smallest eigenvalues (as we did with \\(\\mathbf{L}\\)) we proceed in more typical fashion (as done with PCA and CA) and choose the eigenvectors corresponding to the largest ones.\nThis approach, once again, only requires small modifications to the one we used for the Laplacian:\n\n   mod.mat.cut <- function(x, k = 2) {\n         A <- as.matrix(as_adjacency_matrix(x))\n         n <- vcount(x)\n         d <- as.matrix(degree(x))\n         B <- A - (d %*% t(d))/sum(A)\n         eig.B <- eigen(B)\n         U <- eig.B$vectors[, 1:k]\n         for (i in 1:nrow(U)) {\n            U[i, ] <- U[i, ]/norm(as.matrix(U[i, ]), type = \"E\")\n            }\n      return(U)\n   }\n\nThe key difference here is that we compute the modularity matrix \\(\\mathbf{B}\\) rather than \\(\\mathbf{L}\\) from the adjacency matrix \\(\\mathbf{A}\\) in line 5; we then plug \\(\\mathbf{B}\\) into the eigen function and proceed with normalizing in the same way as before.\nAnother difference is that rather than using a large number of eigenvalues (e.g., \\(k = 6\\)), as we did when we were picking from the smallest ones, we now go for parsimony and pick a small rank (two-dimensional) representation of the original modularity matrix (\\(k = 2\\)).\nLet’s see how this works:\n\n   U <- mod.mat.cut(g)\n   clus <- k.cuts(U)\n   mod.res <- mod.check(g, clus)\n   round(mod.res, 3)\n\n    2     3     4     5     6     7     8     9    10 \n0.264 0.366 0.331 0.317 0.306 0.304 0.224 0.187 0.166 \n\n\nWe can see that the three-cluster solution does really well modularity-wise (\\(Q = 0.37\\)), however, the four cluster solution also seems promising. The resulting communities are shown in Figure 3 (a) and Figure 3 (b).\n\n\n\n\n\n\n\n(a) Three Community Solution\n\n\n\n\n\n\n\n(b) Four Community Solution\n\n\n\n\nFigure 3: Clustering of Nodes in the Law Firm Friendship Network Using the Spectral Decomposition of the Modularity Matrix.\n\n\nAs we can see, spectral clustering via the Laplacian, normalized Laplacian or the modularity matrix produces a high-quality partition, highlighting the core groups in the data. It is a simple and easy to implement option to add to your arsenal."
  },
  {
    "objectID": "modularity-kmeans.html#clustering-using-the-eigenvectors-of-the-laplacian",
    "href": "modularity-kmeans.html#clustering-using-the-eigenvectors-of-the-laplacian",
    "title": "Community Detection Using Spectral Clustering",
    "section": "Clustering Using the Eigenvectors of the Laplacian",
    "text": "Clustering Using the Eigenvectors of the Laplacian\nThe issue then becomes, which matrix should we use the eigenvalues and eigenvectors of? As suggested by Von Luxburg (2007), an obvious candidate is what is called the graph Laplacian (\\(\\mathbf{L}\\)), which is given by:\n\\[\n\\mathbf{L} = \\mathbf{D} - \\mathbf{A}\n\\]\nWhere \\(\\mathbf{D}\\) is the graph’s degree matrix a matrix with the degree vector of the graph along the diagonals and zeros everywhere else.\nLike the modularity matrix, the \\(\\mathbf{L}\\) matrix is doubly-centered (rows and columns sum to zero), which means, via some math other smarter people have worked out, that one of the eigenvalues of \\(\\mathbf{L}\\) is guaranteed to be zero.\nThe other interesting thing is that the second smallest eigenvalue of \\(\\mathbf{L}\\) (sometimes called the Fiedler vector) provides the best two-community partition of the graph (just like the leading eigenvector of the modularity matrix does).\nIf we want to check for the existence of multiple groups, therefore, what we need to do is to create a new \\(n \\times k\\) matrix \\(\\mathbf{U}\\) composed of the \\(k\\) eigenvectors of \\(\\mathbf{L}\\) associated with the \\(k\\) smallest eigenvalues.\nWe then normalize the node-specific row-vector of values of the\\(\\mathbf{U}\\) matrix (e.g., using the Euclidean norm), and the use normalized \\(\\mathbf{U}\\) matrix as input to a k-means algorithm with a known number of clusters (Fouss, Saerens, and Shimbo 2016, 320).\nLet’s see how that would work. Let’s load the law_friends data from the networkdata package containing information on the friendship nominations of 68 lawyers in a firm. We have already analyzed these data using other community detection algorithms in a previous handout. The original data are directed, so we constrain them to be undirected and remove nodes with degree less than two:\n\n   library(networkdata)\n   library(igraph)\n   g <- law_friends\n   g <- as_undirected(g, mode = \"collapse\")\n   g <- subgraph(g, degree(g)>=2) #removing low degree nodes\n\nWe can then write a function called ratio.cut that accomplishes the eigendecomposition of the graph Laplacian matrix described earlier. It looks like this:\n\n   ratio.cut <- function(x, k = 6) {\n         A <- as.matrix(as_adjacency_matrix(x))\n         D <- diag(degree(x))\n         n <- vcount(x)\n         L <- D - A\n         eig.L <- eigen(L)\n         b <- n - k\n         e <- n - 1\n         U <- eig.L$vectors[, b:e]\n         U <- U[, k:1] #re-ordering columns from smallest to biggest\n         for (i in 1:nrow(U)) {\n            U[i, ] <- U[i, ]/norm(as.matrix(U[i, ]), type = \"E\")\n            }\n      return(U)\n   }\n\nThis function takes a graph as input and produces the \\(\\mathbf{U}\\) matrix with \\(n\\) rows and \\(k\\) columns, with \\(k=6\\) by default. It first computes the adjacency matrix (line 2), then the degree matrix (line 3), then the Laplacian matrix (line 5). It then computes the eigendecomposition of the Laplacian (line 6), row normalizes the values taken by the eigenvectors corresponding to the \\(k\\) smallest eigenvalues in lines 10-13 (reverse-ordered from smallest to biggest in line 9), and returns the resulting matrix \\(\\mathbf{U}\\) in line 14.\nLet’s see the function at work:\n\n   U <- ratio.cut(g)\n   round(U, 2)[1:10, ]\n\n       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]\n [1,]  0.08  0.34  0.66  0.37 -0.54  0.13\n [2,]  0.07  0.29  0.60  0.37 -0.53  0.37\n [3,]  0.47  0.13  0.08 -0.87  0.03  0.04\n [4,]  0.34  0.38  0.71  0.16 -0.45  0.10\n [5,]  0.57  0.26  0.06 -0.78  0.00  0.01\n [6,]  0.58 -0.76 -0.21  0.20 -0.05  0.03\n [7,]  0.37  0.20 -0.01 -0.90  0.08  0.03\n [8,] -0.02  0.33  0.65  0.41 -0.55 -0.01\n [9,]  0.14  0.37  0.67  0.36 -0.52  0.04\n[10,]  0.05  0.36  0.66  0.42 -0.50  0.06\n\n\nWhich shows the corresponding values of \\(\\mathbf{U}\\) for each node across the six eigenvectors of the Laplacian we selected.\nNow, the basic idea is to treat each of the normalized scores along the six eigenvectors as if they were “variables” or “features” in a standard k-means clustering problem. The algorithm will then group nodes based on how similar their scores are on each six-dimensional vector. Similar nodes will correspond to communities (k-means clusters) in our data.\nK-means clustering requires knowing how many groups we want in advance (differently from hierarchical clustering). Since we don’t know which is the best community partition beforehand, we instead compute a bunch of partitions and check the modularity of each.\nThe following function computes cluster assignments of the nodes in the graph up to ten partitions (starting from the minimum two):\n\n   k.cuts <- function(x, max = 9) {\n      clus <- list()\n      for (i in 1:max) {\n         set.seed(456) #setting seed because kmeans uses random starting nodes for cluster centroids\n         k <- i + 1\n         clus[[i]] <- kmeans(x, k)$cluster\n         }\n      return(clus)\n   }\n\nThis function takes the \\(\\mathbf{U}\\) matrix as input and returns a nine-element list (with vectors of length \\(n\\) as its elements) of cluster assignments for each node in the graph, corresponding to partitions \\(k = \\{2, 3, \\ldots 10\\}\\) respectively.\nLet’s see the clustering function at work:\n\n   clus <- k.cuts(U)\n   clus\n\n[[1]]\n [1] 2 2 1 2 1 1 1 2 2 2 2 2 2 1 2 2 2 1 1 2 2 2 2 2 2 2 2 1 2 2 1 1 1 2 1 2 2 2\n[39] 2 1 1 2 1 1 2 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n[[2]]\n [1] 3 3 1 3 1 1 1 3 3 3 3 3 3 1 3 3 3 1 1 3 3 3 3 3 3 3 3 1 3 3 1 1 1 3 1 3 3 3\n[39] 3 2 2 3 2 1 3 2 1 1 2 3 2 2 2 2 1 1 1 1 2 1 2 2 2 2 2 2 2 2\n\n[[3]]\n [1] 3 3 4 3 4 4 4 3 3 3 3 3 3 4 3 3 3 4 1 3 3 3 3 3 3 3 3 4 3 3 4 4 4 3 1 3 3 3\n[39] 3 2 2 3 2 1 3 2 1 1 2 3 2 2 2 1 1 1 1 1 2 4 2 2 2 2 2 2 2 2\n\n[[4]]\n [1] 3 3 5 3 5 5 5 3 3 3 3 3 3 5 3 3 3 5 1 3 3 3 3 3 3 3 3 5 3 4 4 4 4 3 1 3 3 3\n[39] 3 2 2 3 2 1 3 2 1 1 2 3 2 2 2 1 1 1 1 1 2 4 2 2 2 2 2 2 2 2\n\n[[5]]\n [1] 3 3 5 3 5 5 5 3 3 3 3 3 3 5 3 3 3 5 1 3 3 3 3 3 4 3 3 5 3 4 6 6 6 3 1 4 3 3\n[39] 3 2 2 3 2 1 3 2 1 1 2 3 2 2 2 1 1 1 1 1 2 6 2 2 2 2 2 2 2 2\n\n[[6]]\n [1] 3 3 5 3 5 5 5 3 3 3 3 3 3 5 3 3 3 5 1 3 3 3 3 3 4 3 3 5 3 4 6 6 6 3 1 4 3 3\n[39] 3 2 2 3 2 1 3 2 1 1 2 3 2 2 2 1 1 1 1 1 2 6 7 2 7 7 7 7 7 7\n\n[[7]]\n [1] 8 8 5 8 5 5 5 8 8 8 8 8 8 5 8 8 8 5 1 8 8 8 8 8 4 8 8 5 8 4 6 6 6 3 1 3 3 3\n[39] 3 2 2 3 2 1 3 2 1 1 2 8 2 2 2 1 1 1 1 1 2 6 7 2 7 7 7 7 7 7\n\n[[8]]\n [1] 8 8 5 8 5 5 5 8 8 8 8 8 8 5 8 8 8 5 1 8 8 8 8 8 4 8 8 5 8 4 6 6 6 3 1 3 3 3\n[39] 3 2 2 3 7 1 3 2 1 1 2 8 2 2 2 7 1 1 1 1 7 6 7 2 9 9 9 9 9 9\n\n[[9]]\n [1]  3  3  5  3  5  5  5  3  3  3  3  3  3  5  8  3  3  5  1  3  3  3  3  3  4\n[26]  3  3  5  3  4  6  6  6 10  1 10 10 10 10  2  2 10  7  1 10  2  1  1  2  8\n[51]  2  2  2  7  1  1  1  1  7  6  7  2  9  9  9  9  9  9\n\n\nGreat! Now that we have our partitions, we need to check the modularity corresponding to each one of them.\nWe can do this with the following quick function:\n\n   mod.check <- function(x, c) {\n      k <- length(c)\n      m <- rep(0, k)\n      for (i in 1:k) {\n         m[i] <- modularity(x, c[[i]])\n         }\n      names(m) <- 2:(k+1)\n      return(m)\n      }\n\nWhich takes an igraph graph object and the list of cluster assignments as input and produces a vector of the same length as the list of cluster assignments with the modularity corresponding to that assignment.\nLet’s see this function at work in our running example:\n\n   mod.res <- mod.check(g, clus)\n   round(mod.res, 3)\n\n    2     3     4     5     6     7     8     9    10 \n0.305 0.356 0.328 0.319 0.316 0.300 0.302 0.293 0.286 \n\n\nWhich suggests that the three-cluster partition of the network (shown in Figure 1 (b)) does pretty well in separating densely connected subgraphs (\\(Q = 0.36\\)).\nNote that this partition looks a lot like the one we settled on using Newman’s divisive leading eigenvector approach based on the modularity matrix: One core dense group of lawyers surrounded by a looser couple of communities.\n\n\n\n\n\n\n\n(a) Original Network\n\n\n\n\n\n\n\n(b) Maximum Modularity Solution)\n\n\n\n\n\n\n\n\n\n(c) Four Community Solution\n\n\n\n\n\n\n\n(d) Five Community Solution\n\n\n\n\n\n\n\n\n\n(e) Six Community Solution\n\n\n\n\n\n\n\n(f) Seven Community Solution\n\n\n\n\nFigure 1: Clustering of Nodes in the Law Firm Friendship Network Using the Ratio Cut\n\n\nNote also that the four, five, six and even seven-community partitions are not too shabby either. We can see those in Figure 1 (c)-Figure 1 (f) as they reveal further insights into the group structure of the network beyond the main three-community division.\nNote that further subdivisions of the network split the more loosely structured community in the upper-right, while the more densely linked community in the lower-left remains largely undisturbed."
  },
  {
    "objectID": "handout3.html#a-degree-dependent-model-of-hubs-and-authorities-aka-salsa",
    "href": "handout3.html#a-degree-dependent-model-of-hubs-and-authorities-aka-salsa",
    "title": "Status and Prestige",
    "section": "A Degree-Dependent Model of Hubs and Authorities (AKA SALSA)",
    "text": "A Degree-Dependent Model of Hubs and Authorities (AKA SALSA)\nYou may have noticed that the Hubs and Authorities approach is the equivalent of the eigenvector centrality for directed networks. Therefore, it is subject to the same critique that the PageRank centralities levies at the eigenvector centrality, because it assumes that each authority receives the same number of authority points from every hub that points to them irrespective of those hubs’ outdegree. In the same way, it assumes that every hub receives the same number of hub points from the authorities they point to irrespective of those authorities’ indegree.\nBut we can imagine a world in which we want to penalize the number of hub and authority points that everyone gets by each node’s in and outdegree. That is, authorities that are pointed to by selective hubs should get more credit than authorities that are pointed to by non-selective hubs. In the same way, hubs that point to authorities that are pointed to by more selective hubs, should be get more hub points for their trouble.\nSo the idea is to combine the HITS reflective centrality idea with PageRank’s degree-weighting scheme. We can do this by using a variation of the status game we used for HITS, developed by Lempel and Moran (2001).3 Instead of using the original adjacency matrix, we use a couple of degree weighted variations:\n\\[\n\\mathbf{P}^a = \\mathbf{\\underset{\\rightarrow}{D}}^{-1}\\mathbf{A}\n\\]\n\\[\n\\mathbf{P}^h = \\mathbf{\\underset{\\leftarrow}{D}}^{-1}\\mathbf{A}^T\n\\]\nWhere \\(\\mathbf{\\underset{\\rightarrow}{D}}^{-1}\\) is a diagonal matrix containing the inverse of the outdegrees of each node (\\(1/\\sum_ja_{ij}\\)) along the diagonals and zeros everywhere else (the inverse of the diagonals of \\(\\mathbf{M}\\) above) and \\(\\mathbf{\\underset{\\leftarrow}{D}}^{-1}\\) is another diagonal matrix containing the inverse of the indegrees of each node (\\(1/\\sum_ia_{ij}\\)) along the diagonals and zeros everywhere else (the inverse of the diagonals of \\(\\mathbf{N}\\) above).\nOur new version of the HITS status game is then played using the elements of these matrices:\n\\[\n   x^a_i = \\sum_j p^a_{ij}x^h_j\n\\]\n\\[  \n   x^h_i = \\sum_i p^h_{ij}x^a_i\n\\]\nWhich says that the authority score of a node is the in-degree weighted hub score of the nodes that point to it (such that lower-in-degree node hub scores contribute more), and the hub score of a mode is the out-degree authority score of the nodes that it points to (such that lower-out-degree node authority scores contribute more). That means that “pure hubs” (lower in-degree) contribute more to the authority score and “pure authorities” (lower outdegree) also contribute more.\nHere’s a function that does all that we have said:\n\n   status3 <- function(A, e = 0.000001) { \n      n <- nrow(A)\n      h <- rep(1, n)  #initializing hub scores\n      a <- rep(1, n)  #initializing authority scores\n      D.o <- diag(1/rowSums(A)) #outdegree matrix\n      D.i <- diag(1/colSums(A)) #indegree  matrix\n      P.h <- D.i %*% t(A)\n      P.a <- D.o %*% A\n      diff <- 1\n      k <- 1\n      while (diff > e) {\n         o.h <- h\n         h <- t(P.h) %*% a \n         h <- h/norm(h, type = \"E\")\n         a <- t(P.a) %*% h \n         a <- a/norm(a, type = \"E\")\n         diff <- abs(sum(abs(o.h) - abs(h)))\n         k <- k + 1\n         }\n      return(list(a = as.vector(a), h = as.vector(h), k = k))\n      }\n\nThis function takes the adjacency matrix as input and returns the SALSA versions of the HITS scores as output.\nLet’s see it in action:\n\n   salsa <- status3(A)\n   round(salsa$a/max(salsa$a), 2)\n\n [1] 0.72 1.00 0.28 0.44 0.28 0.56 0.72 0.56 0.22 0.50 0.61 0.39 0.22 0.56 0.22\n[16] 0.44 0.50 0.83 0.22 0.44 0.83\n\n   round(salsa$h/max(salsa$h), 2)\n\n [1] 0.30 0.15 0.75 0.60 0.75 0.05 0.40 0.40 0.65 0.70 0.15 0.10 0.30 0.20 1.00\n[16] 0.20 0.25 0.85 0.55 0.60 0.55\n\n\nNeat! Also, like with HITS, it is clear that these scores are the leading eigenvectors of two matrices \\(\\mathbf{M^*}\\) and \\(\\mathbf{N^*}\\), themselves given by the reciprocal products of the row and column normalized versions of the adjacency matrix:\n\\[\n\\mathbf{M}^* = (\\mathbf{\\underset{\\rightarrow}{D}}^{-1}\\mathbf{A})^T(\\mathbf{\\underset{\\leftarrow}{D}}^{-1}\\mathbf{A}^T)^T\n\\]\n\\[\n\\mathbf{N}^* = (\\mathbf{\\underset{\\leftarrow}{D}}^{-1}\\mathbf{A}^T)^T(\\mathbf{\\underset{\\rightarrow}{D}}^{-1}\\mathbf{A})^T\n\\]\nWhich, given the above, is the same as:\n\\[\n\\mathbf{M}^* = (\\mathbf{P}^a)^T(\\mathbf{P}^h)^T\n\\]\n\\[\n\\mathbf{N}^* = (\\mathbf{P}^h)^T(\\mathbf{P}^a)^T\n\\]\nWhich we can check as follows:\n\n   D.o <- diag(1/rowSums(A)) #outdegree matrix\n   D.i <- diag(1/colSums(A)) #indegree  matrix\n   P.h <- D.i %*% t(A)\n   P.a <- D.o %*% A\n   M <- t(P.a) %*% t(P.h)\n   N <- t(P.h) %*% t(P.a) \n   a <- Re(eigen(M)$vectors[,1]) *-1\n   h <- Re(eigen(N)$vectors[,1]) *-1\n   round(a/max(a), 2)\n\n [1] 0.72 1.00 0.28 0.44 0.28 0.56 0.72 0.56 0.22 0.50 0.61 0.39 0.22 0.56 0.22\n[16] 0.44 0.50 0.83 0.22 0.44 0.83\n\n   round(h/max(h), 2)\n\n [1] 0.30 0.15 0.75 0.60 0.75 0.05 0.40 0.40 0.65 0.70 0.15 0.10 0.30 0.20 1.00\n[16] 0.20 0.25 0.85 0.55 0.60 0.55\n\n\nWhich are the same numbers we obtained via our reflective status game!"
  },
  {
    "objectID": "spectral.html",
    "href": "spectral.html",
    "title": "Community Detection Using Spectral Clustering",
    "section": "",
    "text": "In the handout on communities, we saw how to use the leading (first) eigenvector of the modularity matrix to split a network in two (and then split those partitions in two and so on).\nIn the handout on CA, we saw that the eigendecomposition of a square matrix actually results in \\(p\\) eigenvectors and eigenvalues (not just the leading or first).\nWe also learned from the eigendecomposition lesson that selecting some number \\(k < p\\) of eigenvectors and eigenvalues helps us reconstruct the best approximation of the original matrix given that number of dimensions.\nPutting these three lessons together suggests a simple way to detect multiple communities in a network using the eigenvalues of a suitable matrix, along with standard clustering algorithms such as k-means clustering. This approach does not have to iterate between binary divisions based on a single (leading) eigenvector, but can instead use multiple eigenvectors at once to partition the data into any desired number of clusters.\nThis general approach to community detection is sometimes referred to as spectral clustering.1"
  },
  {
    "objectID": "spectral.html#clustering-using-the-eigenvectors-of-the-laplacian",
    "href": "spectral.html#clustering-using-the-eigenvectors-of-the-laplacian",
    "title": "Community Detection Using Spectral Clustering",
    "section": "Clustering Using the Eigenvectors of the Laplacian",
    "text": "Clustering Using the Eigenvectors of the Laplacian\nThe issue then becomes, which matrix should we use the eigenvalues and eigenvectors of? As suggested by Von Luxburg (2007), an obvious candidate is what is called the graph Laplacian (\\(\\mathbf{L}\\)), which is given by:\n\\[\n\\mathbf{L} = \\mathbf{D} - \\mathbf{A}\n\\]\nWhere \\(\\mathbf{D}\\) is the graph’s degree matrix a matrix with the degree vector of the graph along the diagonals and zeros everywhere else.\nLike the modularity matrix, the \\(\\mathbf{L}\\) matrix is doubly-centered (rows and columns sum to zero), which means, via some math other smarter people have worked out, that one of the eigenvalues of \\(\\mathbf{L}\\) is guaranteed to be zero.\nThe other interesting thing is that the second smallest eigenvalue of \\(\\mathbf{L}\\) (sometimes called the Fiedler vector) provides the best two-community partition of the graph (just like the leading eigenvector of the modularity matrix does).\nIf we want to check for the existence of multiple groups, therefore, what we need to do is to create a new \\(n \\times k\\) matrix \\(\\mathbf{U}\\) composed of the \\(k\\) eigenvectors of \\(\\mathbf{L}\\) associated with the \\(k\\) smallest eigenvalues.\nWe then normalize the node-specific row-vector of values of the\\(\\mathbf{U}\\) matrix (e.g., using the Euclidean norm), and the use normalized \\(\\mathbf{U}\\) matrix as input to a k-means algorithm with a known number of clusters (Fouss, Saerens, and Shimbo 2016, 320).\nLet’s see how that would work. Let’s load the law_friends data from the networkdata package containing information on the friendship nominations of 68 lawyers in a firm. We have already analyzed these data using other community detection algorithms in a previous handout. The original data are directed, so we constrain them to be undirected and remove nodes with degree less than two:\n\n   library(networkdata)\n   library(igraph)\n   g <- law_friends\n   g <- as_undirected(g, mode = \"collapse\")\n   g <- subgraph(g, degree(g)>=2) #removing low degree nodes\n\nWe can then write a function called ratio.cut that accomplishes the eigendecomposition of the graph Laplacian matrix described earlier. It looks like this:\n\n   ratio.cut <- function(x, k = 6) {\n         A <- as.matrix(as_adjacency_matrix(x))\n         D <- diag(degree(x))\n         n <- vcount(x)\n         L <- D - A\n         eig.L <- eigen(L)\n         b <- n - k\n         e <- n - 1\n         U <- matrix(eig.L$vectors[, b:e], nrow = n, ncol = k)\n         if (k > 1) {\n            U <- U[, k:1] #re-ordering columns from smallest to biggest\n            }\n         for (i in 1:nrow(U)) {\n            U[i, ] <- U[i, ]/norm(as.matrix(U[i, ]), type = \"E\")\n            }\n      return(U)\n   }\n\nThis function takes a graph as input and produces the \\(\\mathbf{U}\\) matrix with \\(n\\) rows and \\(k\\) columns, with \\(k=6\\) by default. It first computes the adjacency matrix (line 2), then the degree matrix (line 3), then the Laplacian matrix (line 5). It then computes the eigendecomposition of the Laplacian (line 6), row normalizes the values taken by the eigenvectors corresponding to the \\(k\\) smallest eigenvalues in lines 10-13 (reverse-ordered from smallest to biggest in line 9), and returns the resulting matrix \\(\\mathbf{U}\\) in line 14.\nLet’s see the function at work:\n\n   U <- ratio.cut(g)\n   round(U, 2)[1:10, ]\n\n       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]\n [1,]  0.08  0.34  0.66  0.37 -0.54  0.13\n [2,]  0.07  0.29  0.60  0.37 -0.53  0.37\n [3,]  0.47  0.13  0.08 -0.87  0.03  0.04\n [4,]  0.34  0.38  0.71  0.16 -0.45  0.10\n [5,]  0.57  0.26  0.06 -0.78  0.00  0.01\n [6,]  0.58 -0.76 -0.21  0.20 -0.05  0.03\n [7,]  0.37  0.20 -0.01 -0.90  0.08  0.03\n [8,] -0.02  0.33  0.65  0.41 -0.55 -0.01\n [9,]  0.14  0.37  0.67  0.36 -0.52  0.04\n[10,]  0.05  0.36  0.66  0.42 -0.50  0.06\n\n\nWhich shows the corresponding values of \\(\\mathbf{U}\\) for each node across the six eigenvectors of the Laplacian we selected.\nNow, the basic idea is to treat each of the normalized scores along the six eigenvectors as if they were “variables” or “features” in a standard k-means clustering problem. The algorithm will then group nodes based on how similar their scores are on each six-dimensional vector. Similar nodes will correspond to communities (k-means clusters) in our data.\nK-means clustering requires knowing how many groups we want in advance (differently from hierarchical clustering). Since we don’t know which is the best community partition beforehand, we instead compute a bunch of partitions and check the modularity of each.\nThe following function computes cluster assignments of the nodes in the graph up to ten partitions (starting from the minimum two):\n\n   k.cuts <- function(x, max = 9) {\n      clus <- list()\n      for (i in 1:max) {\n         set.seed(456) #setting seed because kmeans uses random starting nodes for cluster centroids\n         k <- i + 1\n         clus[[i]] <- kmeans(x, k)$cluster\n         }\n      return(clus)\n   }\n\nThis function takes the \\(\\mathbf{U}\\) matrix as input and returns a nine-element list (with vectors of length \\(n\\) as its elements) of cluster assignments for each node in the graph, corresponding to partitions \\(k = \\{2, 3, \\ldots 10\\}\\) respectively.\nLet’s see the clustering function at work:\n\n   clus <- k.cuts(U)\n   clus\n\n[[1]]\n [1] 2 2 1 2 1 1 1 2 2 2 2 2 2 1 2 2 2 1 1 2 2 2 2 2 2 2 2 1 2 2 1 1 1 2 1 2 2 2\n[39] 2 1 1 2 1 1 2 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n[[2]]\n [1] 3 3 1 3 1 1 1 3 3 3 3 3 3 1 3 3 3 1 1 3 3 3 3 3 3 3 3 1 3 3 1 1 1 3 1 3 3 3\n[39] 3 2 2 3 2 1 3 2 1 1 2 3 2 2 2 2 1 1 1 1 2 1 2 2 2 2 2 2 2 2\n\n[[3]]\n [1] 3 3 4 3 4 4 4 3 3 3 3 3 3 4 3 3 3 4 1 3 3 3 3 3 3 3 3 4 3 3 4 4 4 3 1 3 3 3\n[39] 3 2 2 3 2 1 3 2 1 1 2 3 2 2 2 1 1 1 1 1 2 4 2 2 2 2 2 2 2 2\n\n[[4]]\n [1] 3 3 5 3 5 5 5 3 3 3 3 3 3 5 3 3 3 5 1 3 3 3 3 3 3 3 3 5 3 4 4 4 4 3 1 3 3 3\n[39] 3 2 2 3 2 1 3 2 1 1 2 3 2 2 2 1 1 1 1 1 2 4 2 2 2 2 2 2 2 2\n\n[[5]]\n [1] 3 3 5 3 5 5 5 3 3 3 3 3 3 5 3 3 3 5 1 3 3 3 3 3 4 3 3 5 3 4 6 6 6 3 1 4 3 3\n[39] 3 2 2 3 2 1 3 2 1 1 2 3 2 2 2 1 1 1 1 1 2 6 2 2 2 2 2 2 2 2\n\n[[6]]\n [1] 3 3 5 3 5 5 5 3 3 3 3 3 3 5 3 3 3 5 1 3 3 3 3 3 4 3 3 5 3 4 6 6 6 3 1 4 3 3\n[39] 3 2 2 3 2 1 3 2 1 1 2 3 2 2 2 1 1 1 1 1 2 6 7 2 7 7 7 7 7 7\n\n[[7]]\n [1] 8 8 5 8 5 5 5 8 8 8 8 8 8 5 8 8 8 5 1 8 8 8 8 8 4 8 8 5 8 4 6 6 6 3 1 3 3 3\n[39] 3 2 2 3 2 1 3 2 1 1 2 8 2 2 2 1 1 1 1 1 2 6 7 2 7 7 7 7 7 7\n\n[[8]]\n [1] 8 8 5 8 5 5 5 8 8 8 8 8 8 5 8 8 8 5 1 8 8 8 8 8 4 8 8 5 8 4 6 6 6 3 1 3 3 3\n[39] 3 2 2 3 7 1 3 2 1 1 2 8 2 2 2 7 1 1 1 1 7 6 7 2 9 9 9 9 9 9\n\n[[9]]\n [1]  3  3  5  3  5  5  5  3  3  3  3  3  3  5  8  3  3  5  1  3  3  3  3  3  4\n[26]  3  3  5  3  4  6  6  6 10  1 10 10 10 10  2  2 10  7  1 10  2  1  1  2  8\n[51]  2  2  2  7  1  1  1  1  7  6  7  2  9  9  9  9  9  9\n\n\nGreat! Now that we have our partitions, we need to check the modularity corresponding to each one of them.\nWe can do this with the following quick function:\n\n   mod.check <- function(x, c) {\n      k <- length(c)\n      m <- rep(0, k)\n      for (i in 1:k) {\n         m[i] <- modularity(x, c[[i]])\n         }\n      names(m) <- 2:(k+1)\n      return(m)\n      }\n\nWhich takes an igraph graph object and the list of cluster assignments as input and produces a vector of the same length as the list of cluster assignments with the modularity corresponding to that assignment.\nLet’s see this function at work in our running example:\n\n   mod.res <- mod.check(g, clus)\n   round(mod.res, 3)\n\n    2     3     4     5     6     7     8     9    10 \n0.305 0.356 0.328 0.319 0.316 0.300 0.302 0.293 0.286 \n\n\nWhich suggests that the three-cluster partition of the network (shown in Figure 1 (b)) does pretty well in separating densely connected subgraphs (\\(Q = 0.36\\)).\nNote that this partition looks a lot like the one we settled on using Newman’s divisive leading eigenvector approach based on the modularity matrix: One core dense group of lawyers surrounded by a looser couple of communities.\n\n\n\n\n\n\n\n(a) Original Network\n\n\n\n\n\n\n\n(b) Maximum Modularity Solution)\n\n\n\n\n\n\n\n\n\n(c) Four Community Solution\n\n\n\n\n\n\n\n(d) Five Community Solution\n\n\n\n\n\n\n\n\n\n(e) Six Community Solution\n\n\n\n\n\n\n\n(f) Seven Community Solution\n\n\n\n\nFigure 1: Clustering of Nodes in the Law Firm Friendship Network Using the Ratio Cut\n\n\nNote also that the four, five, six and even seven-community partitions are not too shabby either. We can see those in Figure 1 (c)-Figure 1 (f) as they reveal further insights into the group structure of the network beyond the main three-community division.\nNote that further subdivisions of the network split the more loosely structured community in the upper-right, while the more densely linked community in the lower-left remains largely undisturbed."
  },
  {
    "objectID": "spectral.html#clustering-using-the-eigenvectors-of-the-normalized-laplacian",
    "href": "spectral.html#clustering-using-the-eigenvectors-of-the-normalized-laplacian",
    "title": "Community Detection Using Spectral Clustering",
    "section": "Clustering Using the Eigenvectors of the Normalized Laplacian",
    "text": "Clustering Using the Eigenvectors of the Normalized Laplacian\nWe saw how to cluster a network in a way that results in a good community partition using the Laplacian of the adjacency matrix \\(\\mathbf{L}\\). Another approach is to use a (degree) normalized version of the same matrix \\(\\mathbf{\\hat{L}}\\), defined as follows:\n\\[\n\\mathbf{\\hat{L}} = \\mathbf{I} - \\mathbf{D}^{-\\frac{1}{2}}\\mathbf{A}\\mathbf{D}^{-\\frac{1}{2}}\n\\]\nWhere everything else is as before and \\(\\mathbf{I}\\) is the identity matrix (an \\(n \\times n\\) matrix with ones along the diagonals and zero everywhere else), and \\(\\mathbf{D}^{-\\frac{1}{2}}\\) is a matrix containing the inverse of the square root of degrees of each node (\\(1/\\sqrt{k_i}\\)) in the diagonals and zeros everywhere else.\nWe can just adapt our previous ratio.cut function code to perform this new job:\n\n   norm.ratio.cut <- function(x, k = 6) {\n         A <- as.matrix(as_adjacency_matrix(x))\n         n <- vcount(x)\n         I <- diag(1, n, n)\n         D <- diag(1/sqrt(degree(x)))\n         L <- I - (D %*% A %*% D)\n         eig.L <- eigen(L)\n         b <- n - k\n         e <- n - 1\n         U <- matrix(eig.L$vectors[, b:e], nrow = n, ncol = k)\n         if (k > 1) {\n            U <- U[, k:1] #re-ordering columns from smallest to biggest\n            }         \n         for (i in 1:nrow(U)) {\n            U[i, ] <- U[i, ]/norm(as.matrix(U[i, ]), type = \"E\")\n            }\n      return(U)\n   }\n\nWhere we just have to modify the way we define the \\(\\mathbf{D}\\) and \\(\\mathbf{L}\\) matrices in lines 5 and 6 respectively, after creating the \\(\\mathbf{I}\\) matrix in line 4.\nNow let’s see if the normalized cut can help us finds some communities:\n\n   U <- norm.ratio.cut(g)\n   clus <- k.cuts(U)\n   mod.res <- mod.check(g, clus)\n   round(mod.res, 3)\n\n    2     3     4     5     6     7     8     9    10 \n0.250 0.316 0.320 0.352 0.350 0.331 0.293 0.284 0.246 \n\n\n\n\n\n\n\n\n\n(a) Five Community Solution\n\n\n\n\n\n\n\n(b) Six Community Solution\n\n\n\n\nFigure 2: Clustering of Nodes in the Law Firm Friendship Network Using the Normalized Ratio Cut\n\n\nAs we can see, the normalized ratio cut approach performs almost as well as the ratio cut approach in terms of the maximum modularity it finds (\\(Q = 0.35\\)), but suggests a finer grained partition, with the maximum at either five or six communities.\nThe resulting node clusters are shown in Figure 2 (a) and Figure 2 (b)."
  },
  {
    "objectID": "spectral.html#clustering-using-the-eigenvectors-of-the-modularity-matrix",
    "href": "spectral.html#clustering-using-the-eigenvectors-of-the-modularity-matrix",
    "title": "Community Detection Using Spectral Clustering",
    "section": "Clustering Using the Eigenvectors of the Modularity Matrix",
    "text": "Clustering Using the Eigenvectors of the Modularity Matrix\nAs noted by Fender et al. (2017, 1796), we can extend the spectral clustering approach based on the Laplacian and the normalized Laplacian to the modularity matrix \\(\\mathbf{B}\\). That is, we cluster the graph by embedding the nodes in a set of dimensions defined by the eigendecomposition of \\(\\mathbf{B}\\).\nThe main difference is that rather than using the eigenvectors corresponding to the smallest eigenvalues (as we did with \\(\\mathbf{L}\\)) we proceed in more typical fashion (as done with PCA and CA) and choose the eigenvectors corresponding to the largest ones.\nThis approach, once again, only requires small modifications to the one we used for the Laplacian:\n\n   mod.mat.cut <- function(x, k = 2) {\n         A <- as.matrix(as_adjacency_matrix(x))\n         n <- vcount(x)\n         d <- as.matrix(degree(x))\n         B <- A - (d %*% t(d))/sum(A)\n         eig.B <- eigen(B)\n         U <- eig.B$vectors[, 1:k]\n         for (i in 1:nrow(U)) {\n            U[i, ] <- U[i, ]/norm(as.matrix(U[i, ]), type = \"E\")\n            }\n      return(U)\n   }\n\nThe key difference here is that we compute the modularity matrix \\(\\mathbf{B}\\) rather than \\(\\mathbf{L}\\) from the adjacency matrix \\(\\mathbf{A}\\) in line 5; we then plug \\(\\mathbf{B}\\) into the eigen function and proceed with normalizing in the same way as before.\nAnother difference is that rather than using a large number of eigenvalues (e.g., \\(k = 6\\)), as we did when we were picking from the smallest ones, we now go for parsimony and pick a small rank (two-dimensional) representation of the original modularity matrix (\\(k = 2\\)).\nLet’s see how this works:\n\n   U <- mod.mat.cut(g)\n   clus <- k.cuts(U)\n   mod.res <- mod.check(g, clus)\n   round(mod.res, 3)\n\n    2     3     4     5     6     7     8     9    10 \n0.264 0.366 0.331 0.317 0.306 0.304 0.224 0.187 0.166 \n\n\nWe can see that the three-cluster solution does really well modularity-wise (\\(Q = 0.37\\)), however, the four cluster solution also seems promising. The resulting communities are shown in Figure 3 (a) and Figure 3 (b).\n\n\n\n\n\n\n\n(a) Three Community Solution\n\n\n\n\n\n\n\n(b) Four Community Solution\n\n\n\n\nFigure 3: Clustering of Nodes in the Law Firm Friendship Network Using the Spectral Decomposition of the Modularity Matrix."
  },
  {
    "objectID": "handout3.html#a-final-ranking-of-prestige-scores",
    "href": "handout3.html#a-final-ranking-of-prestige-scores",
    "title": "Status and Prestige",
    "section": "A Final Ranking of Prestige Scores",
    "text": "A Final Ranking of Prestige Scores\nLike before, we can treat the Eigenvector, PageRank, Hub, and Authority Scores as “centralities” defined over nodes in the graph. In that case we might be interested in how different nodes in Krackhardt’s High Tech Managers network stack up according to the different status criteria:\n\n\n\n\nTop Prestige Scores Ordered by Indegree.\n \n  \n    Node.ID \n    Eig.Cent \n    PageRank \n    Hub.Score \n    Auth.Score \n    Indegree \n  \n \n\n  \n    2 \n    1.000 \n    0.573 \n    0.176 \n    1.000 \n    18 \n  \n  \n    18 \n    0.780 \n    0.440 \n    0.800 \n    0.871 \n    15 \n  \n  \n    21 \n    0.920 \n    1.000 \n    0.600 \n    0.776 \n    15 \n  \n  \n    1 \n    0.594 \n    0.302 \n    0.370 \n    0.782 \n    13 \n  \n  \n    7 \n    0.767 \n    0.635 \n    0.492 \n    0.684 \n    13 \n  \n  \n    11 \n    0.556 \n    0.237 \n    0.206 \n    0.769 \n    11 \n  \n  \n    6 \n    0.620 \n    0.437 \n    0.065 \n    0.644 \n    10 \n  \n  \n    8 \n    0.555 \n    0.256 \n    0.490 \n    0.711 \n    10 \n  \n  \n    14 \n    0.512 \n    0.269 \n    0.279 \n    0.677 \n    10 \n  \n  \n    10 \n    0.410 \n    0.180 \n    0.672 \n    0.615 \n    9 \n  \n  \n    17 \n    0.482 \n    0.258 \n    0.313 \n    0.645 \n    9 \n  \n  \n    4 \n    0.517 \n    0.281 \n    0.709 \n    0.496 \n    8 \n  \n  \n    16 \n    0.407 \n    0.169 \n    0.274 \n    0.570 \n    8 \n  \n  \n    20 \n    0.432 \n    0.200 \n    0.687 \n    0.589 \n    8 \n  \n  \n    12 \n    0.405 \n    0.242 \n    0.122 \n    0.498 \n    7 \n  \n  \n    3 \n    0.306 \n    0.165 \n    0.841 \n    0.356 \n    5 \n  \n  \n    5 \n    0.219 \n    0.100 \n    0.835 \n    0.330 \n    5 \n  \n  \n    9 \n    0.182 \n    0.091 \n    0.773 \n    0.290 \n    4 \n  \n  \n    13 \n    0.197 \n    0.086 \n    0.331 \n    0.323 \n    4 \n  \n  \n    15 \n    0.220 \n    0.097 \n    1.000 \n    0.267 \n    4 \n  \n  \n    19 \n    0.197 \n    0.086 \n    0.581 \n    0.323 \n    4 \n  \n\n\n\n\n\n\n\nTop Prestige Scores Ordered by Outdegree\n \n  \n    Node.ID \n    Eig.Cent \n    PageRank \n    Hub.Score \n    Auth.Score \n    Outdegree \n  \n \n\n  \n    15 \n    0.220 \n    0.097 \n    1.000 \n    0.267 \n    20 \n  \n  \n    18 \n    0.780 \n    0.440 \n    0.800 \n    0.871 \n    17 \n  \n  \n    3 \n    0.306 \n    0.165 \n    0.841 \n    0.356 \n    15 \n  \n  \n    5 \n    0.219 \n    0.100 \n    0.835 \n    0.330 \n    15 \n  \n  \n    10 \n    0.410 \n    0.180 \n    0.672 \n    0.615 \n    14 \n  \n  \n    9 \n    0.182 \n    0.091 \n    0.773 \n    0.290 \n    13 \n  \n  \n    4 \n    0.517 \n    0.281 \n    0.709 \n    0.496 \n    12 \n  \n  \n    20 \n    0.432 \n    0.200 \n    0.687 \n    0.589 \n    12 \n  \n  \n    19 \n    0.197 \n    0.086 \n    0.581 \n    0.323 \n    11 \n  \n  \n    21 \n    0.920 \n    1.000 \n    0.600 \n    0.776 \n    11 \n  \n  \n    7 \n    0.767 \n    0.635 \n    0.492 \n    0.684 \n    8 \n  \n  \n    8 \n    0.555 \n    0.256 \n    0.490 \n    0.711 \n    8 \n  \n  \n    1 \n    0.594 \n    0.302 \n    0.370 \n    0.782 \n    6 \n  \n  \n    13 \n    0.197 \n    0.086 \n    0.331 \n    0.323 \n    6 \n  \n  \n    17 \n    0.482 \n    0.258 \n    0.313 \n    0.645 \n    5 \n  \n  \n    14 \n    0.512 \n    0.269 \n    0.279 \n    0.677 \n    4 \n  \n  \n    16 \n    0.407 \n    0.169 \n    0.274 \n    0.570 \n    4 \n  \n  \n    2 \n    1.000 \n    0.573 \n    0.176 \n    1.000 \n    3 \n  \n  \n    11 \n    0.556 \n    0.237 \n    0.206 \n    0.769 \n    3 \n  \n  \n    12 \n    0.405 \n    0.242 \n    0.122 \n    0.498 \n    2 \n  \n  \n    6 \n    0.620 \n    0.437 \n    0.065 \n    0.644 \n    1"
  },
  {
    "objectID": "ca.html#community-detection-in-bipartite-networks",
    "href": "ca.html#community-detection-in-bipartite-networks",
    "title": "Correspondence Analysis (CA) of Two-Mode Networks",
    "section": "Community Detection in Bipartite Networks",
    "text": "Community Detection in Bipartite Networks\nYou may have noticed that the CA analysis of two-mode networks looks a lot like the identification of communities in one-mode networks. The main difference is that in a two-mode network, good communities are composed of clusters of persons and groups well-separated from other clusters of persons and groups.\nAs Barber (2007) noted, we can extend Newman’s modularity approach to ascertain whether a given partition identifies a good “community” in the bipartite case. For that, we need a bi-adjacency analog of the modularity matrix \\(\\mathbf{B}\\). This is given by:\n\\[\n\\mathbf{B} = \\mathbf{A}_{(ij)} - \\frac{k^p_ik^g_j}{|E|}\n\\]\nWhere \\(k^p_i\\) is the number of memberships of the \\(i^{th}\\) person, \\(k^g_i\\) is the number of members of the \\(j^{th}\\) group, and \\(|E|\\) is the number of edges in the bipartite graph.\nSo in our case, this would be:\n\n   d.p <- rowSums(A)\n   d.g <- colSums(A)\n   B <- A - matrix(d.p * d.g, nrow = nrow(A), ncol = ncol(A), byrow = TRUE)/sum(A)\n   round(B, 2)\n\n           6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10  2/23\nEVELYN     0.73  0.76  0.46  0.69  0.64  0.64 -0.45  0.53  0.46 -0.22 -0.18\nLAURA      0.83  0.93  0.87 -0.09  0.73  0.76  0.46  0.69 -0.36 -0.36 -0.45\nTHERESA   -0.18  0.60  0.76  0.73  0.83  0.93  0.87  0.91  0.73 -0.24 -0.54\nBRENDA     0.55 -0.47  0.46  0.78  0.82  0.60  0.76  0.73 -0.17 -0.07 -0.13\nCHARLOTTE -0.54 -0.31  0.64  0.64  0.55 -0.47  0.46 -0.22 -0.18 -0.40 -0.24\nFRANCES   -0.13 -0.09  0.73 -0.24  0.46  0.69 -0.36  0.64 -0.45 -0.47 -0.54\nELEANOR   -0.24 -0.27 -0.17 -0.07  0.87  0.91  0.73  0.76 -0.54 -0.31 -0.36\nPEARL     -0.54 -0.22 -0.18 -0.40 -0.24  0.73 -0.17  0.93  0.87 -0.09 -0.27\nRUTH      -0.36 -0.36 -0.45 -0.47  0.46 -0.22  0.82  0.60  0.76 -0.27 -0.17\nVERNE     -0.27 -0.24 -0.54 -0.31 -0.36 -0.36  0.55  0.53  0.46 -0.22 -0.18\nMYRNA     -0.17 -0.07 -0.13 -0.09 -0.27 -0.24 -0.54  0.69  0.64  0.64 -0.45\nKATHERINE -0.18 -0.40 -0.24 -0.27 -0.17 -0.07 -0.13  0.91  0.73  0.76 -0.54\nSYLVIA    -0.45 -0.47 -0.54 -0.22 -0.18 -0.40  0.76  0.73  0.83  0.93 -0.13\nNORA      -0.54 -0.31 -0.36 -0.36 -0.45  0.53  0.46 -0.22  0.82  0.60  0.76\nHELEN     -0.13 -0.09 -0.27 -0.24 -0.54 -0.31  0.64  0.64 -0.45  0.53  0.46\nDOROTHY   -0.24 -0.27 -0.17 -0.07 -0.13 -0.09 -0.27  0.76  0.46 -0.31 -0.36\nOLIVIA    -0.54 -0.22 -0.18 -0.40 -0.24 -0.27 -0.17 -0.07  0.87 -0.09  0.73\nFLORA     -0.36 -0.36 -0.45 -0.47 -0.54 -0.22 -0.18 -0.40  0.76 -0.27  0.83\n            4/7 11/21   8/3\nEVELYN    -0.40 -0.24 -0.27\nLAURA     -0.47 -0.54 -0.22\nTHERESA   -0.31 -0.36 -0.36\nBRENDA    -0.09 -0.27 -0.24\nCHARLOTTE -0.27 -0.17 -0.07\nFRANCES   -0.22 -0.18 -0.40\nELEANOR   -0.36 -0.45 -0.47\nPEARL     -0.24 -0.54 -0.31\nRUTH      -0.07 -0.13 -0.09\nVERNE      0.60 -0.24 -0.27\nMYRNA      0.53 -0.54 -0.22\nKATHERINE  0.69  0.64  0.64\nSYLVIA     0.91  0.73  0.76\nNORA       0.73  0.83  0.93\nHELEN      0.78 -0.18 -0.40\nDOROTHY   -0.36 -0.45 -0.47\nOLIVIA    -0.24 -0.54 -0.31\nFLORA     -0.07 -0.13 -0.09\n\n\nNeat! Like before the numbers in this matrix represent the expected probability of observing a tie in a world in which people keep their number of memberships and groups keep their observed sizes, but otherwise, people and groups connect at random.\nWe can also create a bipartite matrix version of the bi-adjacency modularity, as follows:\n\n   n <- vcount(g)\n   Np <- nrow(A)\n   names <- c(rownames(A), colnames(A))\n   B2 <- matrix(0, n, n) #all zeros matrix of dimensions (p + g) X (p + g)\n   B2[1:Np, (Np + 1):n] <- B #putting B in the top right block\n   B2[(Np + 1):n, 1:Np] <- t(B) #putting B transpose in the lower-left block\n   rownames(B2) <- names\n   colnames(B2) <- names\n   round(B2, 2)\n\n          EVELYN LAURA THERESA BRENDA CHARLOTTE FRANCES ELEANOR PEARL  RUTH\nEVELYN      0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\nLAURA       0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\nTHERESA     0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\nBRENDA      0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\nCHARLOTTE   0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\nFRANCES     0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\nELEANOR     0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\nPEARL       0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\nRUTH        0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\nVERNE       0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\nMYRNA       0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\nKATHERINE   0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\nSYLVIA      0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\nNORA        0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\nHELEN       0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\nDOROTHY     0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\nOLIVIA      0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\nFLORA       0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\n6/27        0.73  0.83   -0.18   0.55     -0.54   -0.13   -0.24 -0.54 -0.36\n3/2         0.76  0.93    0.60  -0.47     -0.31   -0.09   -0.27 -0.22 -0.36\n4/12        0.46  0.87    0.76   0.46      0.64    0.73   -0.17 -0.18 -0.45\n9/26        0.69 -0.09    0.73   0.78      0.64   -0.24   -0.07 -0.40 -0.47\n2/25        0.64  0.73    0.83   0.82      0.55    0.46    0.87 -0.24  0.46\n5/19        0.64  0.76    0.93   0.60     -0.47    0.69    0.91  0.73 -0.22\n3/15       -0.45  0.46    0.87   0.76      0.46   -0.36    0.73 -0.17  0.82\n9/16        0.53  0.69    0.91   0.73     -0.22    0.64    0.76  0.93  0.60\n4/8         0.46 -0.36    0.73  -0.17     -0.18   -0.45   -0.54  0.87  0.76\n6/10       -0.22 -0.36   -0.24  -0.07     -0.40   -0.47   -0.31 -0.09 -0.27\n2/23       -0.18 -0.45   -0.54  -0.13     -0.24   -0.54   -0.36 -0.27 -0.17\n4/7        -0.40 -0.47   -0.31  -0.09     -0.27   -0.22   -0.36 -0.24 -0.07\n11/21      -0.24 -0.54   -0.36  -0.27     -0.17   -0.18   -0.45 -0.54 -0.13\n8/3        -0.27 -0.22   -0.36  -0.24     -0.07   -0.40   -0.47 -0.31 -0.09\n          VERNE MYRNA KATHERINE SYLVIA  NORA HELEN DOROTHY OLIVIA FLORA  6/27\nEVELYN     0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00  0.73\nLAURA      0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00  0.83\nTHERESA    0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00 -0.18\nBRENDA     0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00  0.55\nCHARLOTTE  0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00 -0.54\nFRANCES    0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00 -0.13\nELEANOR    0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00 -0.24\nPEARL      0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00 -0.54\nRUTH       0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00 -0.36\nVERNE      0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00 -0.27\nMYRNA      0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00 -0.17\nKATHERINE  0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00 -0.18\nSYLVIA     0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00 -0.45\nNORA       0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00 -0.54\nHELEN      0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00 -0.13\nDOROTHY    0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00 -0.24\nOLIVIA     0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00 -0.54\nFLORA      0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00 -0.36\n6/27      -0.27 -0.17     -0.18  -0.45 -0.54 -0.13   -0.24  -0.54 -0.36  0.00\n3/2       -0.24 -0.07     -0.40  -0.47 -0.31 -0.09   -0.27  -0.22 -0.36  0.00\n4/12      -0.54 -0.13     -0.24  -0.54 -0.36 -0.27   -0.17  -0.18 -0.45  0.00\n9/26      -0.31 -0.09     -0.27  -0.22 -0.36 -0.24   -0.07  -0.40 -0.47  0.00\n2/25      -0.36 -0.27     -0.17  -0.18 -0.45 -0.54   -0.13  -0.24 -0.54  0.00\n5/19      -0.36 -0.24     -0.07  -0.40  0.53 -0.31   -0.09  -0.27 -0.22  0.00\n3/15       0.55 -0.54     -0.13   0.76  0.46  0.64   -0.27  -0.17 -0.18  0.00\n9/16       0.53  0.69      0.91   0.73 -0.22  0.64    0.76  -0.07 -0.40  0.00\n4/8        0.46  0.64      0.73   0.83  0.82 -0.45    0.46   0.87  0.76  0.00\n6/10      -0.22  0.64      0.76   0.93  0.60  0.53   -0.31  -0.09 -0.27  0.00\n2/23      -0.18 -0.45     -0.54  -0.13  0.76  0.46   -0.36   0.73  0.83  0.00\n4/7        0.60  0.53      0.69   0.91  0.73  0.78   -0.36  -0.24 -0.07  0.00\n11/21     -0.24 -0.54      0.64   0.73  0.83 -0.18   -0.45  -0.54 -0.13  0.00\n8/3       -0.27 -0.22      0.64   0.76  0.93 -0.40   -0.47  -0.31 -0.09  0.00\n            3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10  2/23   4/7\nEVELYN     0.76  0.46  0.69  0.64  0.64 -0.45  0.53  0.46 -0.22 -0.18 -0.40\nLAURA      0.93  0.87 -0.09  0.73  0.76  0.46  0.69 -0.36 -0.36 -0.45 -0.47\nTHERESA    0.60  0.76  0.73  0.83  0.93  0.87  0.91  0.73 -0.24 -0.54 -0.31\nBRENDA    -0.47  0.46  0.78  0.82  0.60  0.76  0.73 -0.17 -0.07 -0.13 -0.09\nCHARLOTTE -0.31  0.64  0.64  0.55 -0.47  0.46 -0.22 -0.18 -0.40 -0.24 -0.27\nFRANCES   -0.09  0.73 -0.24  0.46  0.69 -0.36  0.64 -0.45 -0.47 -0.54 -0.22\nELEANOR   -0.27 -0.17 -0.07  0.87  0.91  0.73  0.76 -0.54 -0.31 -0.36 -0.36\nPEARL     -0.22 -0.18 -0.40 -0.24  0.73 -0.17  0.93  0.87 -0.09 -0.27 -0.24\nRUTH      -0.36 -0.45 -0.47  0.46 -0.22  0.82  0.60  0.76 -0.27 -0.17 -0.07\nVERNE     -0.24 -0.54 -0.31 -0.36 -0.36  0.55  0.53  0.46 -0.22 -0.18  0.60\nMYRNA     -0.07 -0.13 -0.09 -0.27 -0.24 -0.54  0.69  0.64  0.64 -0.45  0.53\nKATHERINE -0.40 -0.24 -0.27 -0.17 -0.07 -0.13  0.91  0.73  0.76 -0.54  0.69\nSYLVIA    -0.47 -0.54 -0.22 -0.18 -0.40  0.76  0.73  0.83  0.93 -0.13  0.91\nNORA      -0.31 -0.36 -0.36 -0.45  0.53  0.46 -0.22  0.82  0.60  0.76  0.73\nHELEN     -0.09 -0.27 -0.24 -0.54 -0.31  0.64  0.64 -0.45  0.53  0.46  0.78\nDOROTHY   -0.27 -0.17 -0.07 -0.13 -0.09 -0.27  0.76  0.46 -0.31 -0.36 -0.36\nOLIVIA    -0.22 -0.18 -0.40 -0.24 -0.27 -0.17 -0.07  0.87 -0.09  0.73 -0.24\nFLORA     -0.36 -0.45 -0.47 -0.54 -0.22 -0.18 -0.40  0.76 -0.27  0.83 -0.07\n6/27       0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n3/2        0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n4/12       0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n9/26       0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n2/25       0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n5/19       0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n3/15       0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n9/16       0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n4/8        0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n6/10       0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n2/23       0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n4/7        0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n11/21      0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n8/3        0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n          11/21   8/3\nEVELYN    -0.24 -0.27\nLAURA     -0.54 -0.22\nTHERESA   -0.36 -0.36\nBRENDA    -0.27 -0.24\nCHARLOTTE -0.17 -0.07\nFRANCES   -0.18 -0.40\nELEANOR   -0.45 -0.47\nPEARL     -0.54 -0.31\nRUTH      -0.13 -0.09\nVERNE     -0.24 -0.27\nMYRNA     -0.54 -0.22\nKATHERINE  0.64  0.64\nSYLVIA     0.73  0.76\nNORA       0.83  0.93\nHELEN     -0.18 -0.40\nDOROTHY   -0.45 -0.47\nOLIVIA    -0.54 -0.31\nFLORA     -0.13 -0.09\n6/27       0.00  0.00\n3/2        0.00  0.00\n4/12       0.00  0.00\n9/26       0.00  0.00\n2/25       0.00  0.00\n5/19       0.00  0.00\n3/15       0.00  0.00\n9/16       0.00  0.00\n4/8        0.00  0.00\n6/10       0.00  0.00\n2/23       0.00  0.00\n4/7        0.00  0.00\n11/21      0.00  0.00\n8/3        0.00  0.00\n\n\nWhich is a bipartite version of modularity matrix (\\(\\mathbf{\\hat{B}}\\)) with the same block structure as the bipartite adjacency matrix:\n\\[\n\\mathbf{\\hat{B}} = \\left[\n\\begin{matrix}\n\\mathbf{O}_{M \\times M} & \\mathbf{B}_{M \\times N} \\\\\n\\mathbf{B}^T_{N \\times M} & \\mathbf{O}_{N \\times N}\n\\end{matrix}\n\\right]\n\\]\nNote that in \\(\\mathbf{\\hat{B}}\\) the modularity (expected number of edges) is set to zero for nodes of the same set (people and people; groups and groups), and to non-zero values for nodes of different sets (persons and groups).\nNow, we can use the same approach we used in the unipartite case to check the modularity of some hypothetical partition of the nodes in the graph.\nTake for instance, the CA scores in the first dimension that we obtained earlier. They do seem to divide the persons and groups into distinct communities. So let’s transform them into membership vectors (using dummy coding):\n\n   u1 <- rep(0, n)\n   u2 <- rep(0, n)\n   d <- c(eig.vec.p[, 1], eig.vec.g[, 1]) #original CA scores\n   u1[which(d > 0)] <- 1\n   u2[which(d <= 0)] <- 1\n   U <- cbind(u1, u2)\n   rownames(U) <- rownames(B2)\n   U\n\n          u1 u2\nEVELYN     0  1\nLAURA      0  1\nTHERESA    0  1\nBRENDA     0  1\nCHARLOTTE  0  1\nFRANCES    0  1\nELEANOR    0  1\nPEARL      0  1\nRUTH       0  1\nVERNE      1  0\nMYRNA      1  0\nKATHERINE  1  0\nSYLVIA     1  0\nNORA       1  0\nHELEN      1  0\nDOROTHY    1  0\nOLIVIA     1  0\nFLORA      1  0\n6/27       0  1\n3/2        0  1\n4/12       0  1\n9/26       0  1\n2/25       0  1\n5/19       0  1\n3/15       0  1\n9/16       0  1\n4/8        1  0\n6/10       1  0\n2/23       1  0\n4/7        1  0\n11/21      1  0\n8/3        1  0\n\n\nRecall that we can check the modularity of a partition coded in a dummy matrix like U using the formula:\n\\[\n\\frac{tr(U^TBU)}{\\sum_i\\sum_ja_{ij}}\n\\]\nWhere \\(tr\\) is the trace matrix operation (sum of the diagonals).\nLet’s check it out:\n\n   A2 <- as.matrix(as_adjacency_matrix(g))\n   round(sum(diag(t(U) %*% B2 %*% U))/sum(A2), 3)\n\n[1] 0.413\n\n\nWhich looks pretty good!\nHere’s a plot of the bipartite graph with nodes colored by the CA induced community bipartition:\n\n   V(g)$type <- bipartite_mapping(g)$type\n   V(g)$shape <- ifelse(V(g)$type, \"square\", \"circle\")\n   V(g)$color <- (U[, 1] - U[, 2]) + 2\n   set.seed(123)\n   plot(g, \n     vertex.size=7, vertex.frame.color=\"lightgray\", \n     vertex.label.dist = 1.5, edge.curved=0.2, \n     vertex.label.cex = 1.35)\n\n\n\n\nSouthern Women’s bipartite graph by the best two-community partition according to CA."
  },
  {
    "objectID": "spectral.html#clustering-two-mode-networks",
    "href": "spectral.html#clustering-two-mode-networks",
    "title": "Community Detection Using Spectral Clustering",
    "section": "Clustering Two-Mode Networks",
    "text": "Clustering Two-Mode Networks\nWe can use a variant of the spectral clustering approach to find multiple communities in two-mode networks (Wu, Gu, and Yang 2022). This approach combines Correspondence Analysis (CA)—which we covered in the previous handout—and k-means clustering on multiple dimensions.\nSo let’s bring back our old friend, the Southern Women data:\n\n   g <- southern_women #southern women data\n   A <- as.matrix(as_biadjacency_matrix(g)) #bi-adjacency matrix\n\nLet’s also compute the bipartite modularity matrix:\n\n   B <- A - matrix(rowSums(A) * colSums(A), \n                   nrow = nrow(A), ncol = ncol(A), byrow = TRUE)/sum(A)\n   n <- vcount(g)\n   Np <- nrow(A)\n   names <- c(rownames(A), colnames(A))\n   B2 <- matrix(0, n, n) #all zeros matrix of dimensions (p + g) X (p + g)\n   B2[1:Np, (Np + 1):n] <- B #putting B in the top right block\n   B2[(Np + 1):n, 1:Np] <- t(B) #putting B transpose in the lower-left block\n   rownames(B2) <- names\n   colnames(B2) <- names\n\nAnd now let’s find the CA scores. This time will use the canned function CA from the the package FactoMineR\n\n   #install.packages(\"FactoMineR\")\n   library(FactoMineR)\n   CA.res <- CA(A, graph = FALSE, ncp = 10)\n\nWhich computes CA directly on the bi-adjacency matrix (the argument ncp asks to keep the first ten dimensions).\nWe can now extract the CA scores for persons and groups fromm the resulting object:\n\n   eig.vec.p <- CA.res$row$coord\n   eig.vec.g <- CA.res$col$coord\n   head(eig.vec.p)\n\n               Dim 1       Dim 2       Dim 3        Dim 4       Dim 5\nEVELYN    -0.7994396 -0.11278306  0.12856965  0.491615655  0.36993238\nLAURA     -0.8426887  0.03973055  0.11862978  0.286643489  0.10696165\nTHERESA   -0.6538505 -0.08107422  0.03285721  0.066653892  0.11835519\nBRENDA    -0.8552592  0.05084420  0.26039689 -0.082978126  0.05028130\nCHARLOTTE -0.9735517  0.03683948  0.66023345 -0.774105247  0.08510784\nFRANCES   -0.7973597  0.05794469 -0.20558235 -0.001077288 -0.59307835\n                 Dim 6        Dim 7       Dim 8         Dim 9      Dim 10\nEVELYN    -0.007414199 -0.007447136 -0.02256155 -0.0160061655  0.08340311\nLAURA      0.519058804  0.358983310  0.06243870  0.1954326939 -0.10193085\nTHERESA   -0.238792769  0.037114413  0.44547214 -0.1638696977  0.04645662\nBRENDA     0.060849392 -0.093559773 -0.53495958 -0.1262516982  0.02818219\nCHARLOTTE -0.721861078 -0.222554541  0.02152725 -0.0001022383 -0.04432666\nFRANCES    0.178454033 -0.648007177  0.12713961  0.3578859472 -0.29201035\n\n   head(eig.vec.g)\n\n          Dim 1        Dim 2       Dim 3        Dim 4       Dim 5      Dim 6\n6/27 -1.0510521 -0.013102803  0.40045025  0.624502007  0.53693139  0.6062955\n3/2  -0.9662871 -0.090934069  0.22094083  0.758901614  0.60626508  0.2889617\n4/12 -1.0357695 -0.002506996  0.39252639 -0.005949514  0.07005287 -0.1110437\n9/26 -1.0359803 -0.046981456  0.64023822 -0.201296127  0.47641399 -0.7205873\n2/25 -0.8840034  0.002527085  0.00616034 -0.304708001 -0.31330029 -0.1008760\n5/19 -0.5723848 -0.007363883 -0.15907563  0.336653678 -0.55713599  0.3565649\n            Dim 7       Dim 8       Dim 9       Dim 10\n6/27  0.341243814 -0.78585220  0.09303009  0.022211664\n3/2   0.514095893  0.77040262  0.02721689  0.064255000\n4/12 -0.380608237  0.07861703  0.21614264 -0.322353047\n9/26 -0.284177968 -0.10776494 -0.40181489  0.196215569\n2/25  0.009376108  0.04679332  0.31794275  0.121060750\n5/19 -0.215101435  0.05292157 -0.34808611  0.004513752\n\n\nGreat! You can verify that these are the same scores we obtained in the last handout via a more elaborate route.\nNow, we can just create our U matrix by stacking the person and group scores using the first three dimensions:\n\n   U <- rbind(eig.vec.p, eig.vec.g)[, 1:3]\n   head(U)\n\n               Dim 1       Dim 2       Dim 3\nEVELYN    -0.7994396 -0.11278306  0.12856965\nLAURA     -0.8426887  0.03973055  0.11862978\nTHERESA   -0.6538505 -0.08107422  0.03285721\nBRENDA    -0.8552592  0.05084420  0.26039689\nCHARLOTTE -0.9735517  0.03683948  0.66023345\nFRANCES   -0.7973597  0.05794469 -0.20558235\n\n   tail(U)\n\n          Dim 1      Dim 2       Dim 3\n4/8   0.5140165 -0.4896890 -0.47959026\n6/10  1.1173628  0.5729577  0.23464730\n2/23  1.2219952 -2.0539686  0.69618973\n4/7   1.0223902  0.5159415 -0.04625319\n11/21 1.1742556  0.9078702  0.66121084\n8/3   1.1742556  0.9078702  0.66121084\n\n\nNice! Now we can just feed U to our k.cuts function to place persons and groups into cluster assignments beginning with two and ending with ten:\n\n   clus <- k.cuts(U)\n\nOf course, we can’t use the mod.check function because that uses the standard method for checking the modularity in one-mode networks.\nSo we need to come up with a custom method for the bipartite case.\nFirst, we need a function that takes a cluster assignment vector containing numbers for each cluster \\(k = \\{1, 2, 3, C\\}\\) and turns it into a dummy coded cluster assignment matrix:\n\n   make.dummies <- function(x) {\n      vals <- unique(x)\n      U <- matrix(0, length(x), length(vals))\n      for (k in vals) {\n         U[, k] <- as.numeric(x == k)\n      }\n   return(U)\n   }\n\nLet’s test it out:\n\n   make.dummies(clus[[3]])\n\n      [,1] [,2] [,3] [,4]\n [1,]    0    1    0    0\n [2,]    0    1    0    0\n [3,]    0    1    0    0\n [4,]    0    1    0    0\n [5,]    0    1    0    0\n [6,]    0    1    0    0\n [7,]    0    1    0    0\n [8,]    0    0    0    1\n [9,]    0    0    0    1\n[10,]    0    0    0    1\n[11,]    0    0    0    1\n[12,]    1    0    0    0\n[13,]    1    0    0    0\n[14,]    1    0    0    0\n[15,]    1    0    0    0\n[16,]    0    0    0    1\n[17,]    0    0    1    0\n[18,]    0    0    1    0\n[19,]    0    1    0    0\n[20,]    0    1    0    0\n[21,]    0    1    0    0\n[22,]    0    1    0    0\n[23,]    0    1    0    0\n[24,]    0    1    0    0\n[25,]    0    1    0    0\n[26,]    0    0    0    1\n[27,]    0    0    0    1\n[28,]    1    0    0    0\n[29,]    0    0    1    0\n[30,]    1    0    0    0\n[31,]    1    0    0    0\n[32,]    1    0    0    0\n\n\nGreat! Looks like it works.\nFinally, we need to write a custom function for bipartite modularity checking across different assignments:\n\n   mod.check2 <- function(x, c, w) {\n      k <- length(c)\n      m <- rep(0, k)\n      for (i in 1:k) {\n         u <- make.dummies(c[[i]])\n         m[i] <- sum(diag(t(u) %*% x %*% u))/sum(w)\n         }\n   names(m) <- 2:(k+1)\n   return(m)\n   }\n\nThe function mod.check2 needs three inputs: The bipartite modularity matrix, a list with different assignments of the nodes in the bipartite graph to different clusters, and the bipartite adjacency matrix. It returns a vector m with the modularities of each of the partitions in c.\nAnd, now, for the big reveal:\n\n   round(mod.check2(B2, clus, as.matrix(as_adjacency_matrix(g))), 3)\n\n    2     3     4     5     6     7     8     9    10 \n0.413 0.420 0.426 0.321 0.296 0.266 0.213 0.207 0.173 \n\n\nLooks like the spectral clustering results favor a four-community partition although the more parsimonious three and binary community partitions also look pretty good.\nHere’s a plot of the three and four community solutions (since we already saw the binary partition in the CA handout:\n\n\n\n\n\n\n\n(a) Three Community Solution\n\n\n\n\n\n\n\n(b) Four Community Solution\n\n\n\n\nFigure 4: Spectral Clustering of Nodes in the Southern Women Data.\n\n\nAs we have seen, spectral clustering via the Laplacian, normalized Laplacian or the modularity matrix (and CA in the bipartite case) produces a high-quality partition, highlighting the core groups in the data. It is a simple and easy to implement option to add to your arsenal."
  },
  {
    "objectID": "spectral.html#clustering-two-mode-networks-using-ca",
    "href": "spectral.html#clustering-two-mode-networks-using-ca",
    "title": "Community Detection Using Spectral Clustering",
    "section": "Clustering Two-Mode Networks Using CA",
    "text": "Clustering Two-Mode Networks Using CA\nWe can use a variant of the spectral clustering approach to find multiple communities in two-mode networks (Wu, Gu, and Yang 2022). This approach combines Correspondence Analysis (CA)—which we covered in the previous handout—and k-means clustering on multiple dimensions.\nSo let’s bring back our old friend, the Southern Women data:\n\n   g <- southern_women #southern women data\n   A <- as.matrix(as_biadjacency_matrix(g)) #bi-adjacency matrix\n\nLet’s also compute the bipartite modularity matrix:\n\n   B <- A - matrix(rowSums(A) * colSums(A), \n                   nrow = nrow(A), ncol = ncol(A), byrow = TRUE)/sum(A)\n   n <- vcount(g)\n   Np <- nrow(A)\n   names <- c(rownames(A), colnames(A))\n   B2 <- matrix(0, n, n) #all zeros matrix of dimensions (p + g) X (p + g)\n   B2[1:Np, (Np + 1):n] <- B #putting B in the top right block\n   B2[(Np + 1):n, 1:Np] <- t(B) #putting B transpose in the lower-left block\n   rownames(B2) <- names\n   colnames(B2) <- names\n\nAnd now let’s find the CA scores. This time will use the canned function CA from the the package FactoMineR\n\n   #install.packages(\"FactoMineR\")\n   library(FactoMineR)\n   CA.res <- CA(A, graph = FALSE, ncp = 10)\n\nWhich computes CA directly on the bi-adjacency matrix (the argument ncp asks to keep the first ten dimensions).\nWe can now extract the CA scores for persons and groups fromm the resulting object:\n\n   eig.vec.p <- CA.res$row$coord\n   eig.vec.g <- CA.res$col$coord\n   head(eig.vec.p)\n\n               Dim 1       Dim 2       Dim 3        Dim 4       Dim 5\nEVELYN    -0.7994396 -0.11278306  0.12856965  0.491615655  0.36993238\nLAURA     -0.8426887  0.03973055  0.11862978  0.286643489  0.10696165\nTHERESA   -0.6538505 -0.08107422  0.03285721  0.066653892  0.11835519\nBRENDA    -0.8552592  0.05084420  0.26039689 -0.082978126  0.05028130\nCHARLOTTE -0.9735517  0.03683948  0.66023345 -0.774105247  0.08510784\nFRANCES   -0.7973597  0.05794469 -0.20558235 -0.001077288 -0.59307835\n                 Dim 6        Dim 7       Dim 8         Dim 9      Dim 10\nEVELYN    -0.007414199 -0.007447136 -0.02256155 -0.0160061655  0.08340311\nLAURA      0.519058804  0.358983310  0.06243870  0.1954326939 -0.10193085\nTHERESA   -0.238792769  0.037114413  0.44547214 -0.1638696977  0.04645662\nBRENDA     0.060849392 -0.093559773 -0.53495958 -0.1262516982  0.02818219\nCHARLOTTE -0.721861078 -0.222554541  0.02152725 -0.0001022383 -0.04432666\nFRANCES    0.178454033 -0.648007177  0.12713961  0.3578859472 -0.29201035\n\n   head(eig.vec.g)\n\n          Dim 1        Dim 2       Dim 3        Dim 4       Dim 5      Dim 6\n6/27 -1.0510521 -0.013102803  0.40045025  0.624502007  0.53693139  0.6062955\n3/2  -0.9662871 -0.090934069  0.22094083  0.758901614  0.60626508  0.2889617\n4/12 -1.0357695 -0.002506996  0.39252639 -0.005949514  0.07005287 -0.1110437\n9/26 -1.0359803 -0.046981456  0.64023822 -0.201296127  0.47641399 -0.7205873\n2/25 -0.8840034  0.002527085  0.00616034 -0.304708001 -0.31330029 -0.1008760\n5/19 -0.5723848 -0.007363883 -0.15907563  0.336653678 -0.55713599  0.3565649\n            Dim 7       Dim 8       Dim 9       Dim 10\n6/27  0.341243814 -0.78585220  0.09303009  0.022211664\n3/2   0.514095893  0.77040262  0.02721689  0.064255000\n4/12 -0.380608237  0.07861703  0.21614264 -0.322353047\n9/26 -0.284177968 -0.10776494 -0.40181489  0.196215569\n2/25  0.009376108  0.04679332  0.31794275  0.121060750\n5/19 -0.215101435  0.05292157 -0.34808611  0.004513752\n\n\nGreat! You can verify that these are the same scores we obtained in the last handout via a more elaborate route.\nNow, we can just create our U matrix by stacking the person and group scores using the first three dimensions:\n\n   U <- rbind(eig.vec.p, eig.vec.g)[, 1:3]\n   head(U)\n\n               Dim 1       Dim 2       Dim 3\nEVELYN    -0.7994396 -0.11278306  0.12856965\nLAURA     -0.8426887  0.03973055  0.11862978\nTHERESA   -0.6538505 -0.08107422  0.03285721\nBRENDA    -0.8552592  0.05084420  0.26039689\nCHARLOTTE -0.9735517  0.03683948  0.66023345\nFRANCES   -0.7973597  0.05794469 -0.20558235\n\n   tail(U)\n\n          Dim 1      Dim 2       Dim 3\n4/8   0.5140165 -0.4896890 -0.47959026\n6/10  1.1173628  0.5729577  0.23464730\n2/23  1.2219952 -2.0539686  0.69618973\n4/7   1.0223902  0.5159415 -0.04625319\n11/21 1.1742556  0.9078702  0.66121084\n8/3   1.1742556  0.9078702  0.66121084\n\n\nNice! Now we can just feed U to our k.cuts function to place persons and groups into cluster assignments beginning with two and ending with ten:\n\n   clus <- k.cuts(U)\n\nOf course, we can’t use the mod.check function because that uses the standard method for checking the modularity in one-mode networks.\nSo we need to come up with a custom method for the bipartite case.\nFirst, we need a function that takes a cluster assignment vector containing numbers for each cluster \\(k = \\{1, 2, 3, C\\}\\) and turns it into a dummy coded cluster assignment matrix:\n\n   make.dummies <- function(x) {\n      vals <- unique(x)\n      U <- matrix(0, length(x), length(vals))\n      for (k in vals) {\n         U[, k] <- as.numeric(x == k)\n      }\n   return(U)\n   }\n\nLet’s test it out:\n\n   make.dummies(clus[[3]])\n\n      [,1] [,2] [,3] [,4]\n [1,]    0    1    0    0\n [2,]    0    1    0    0\n [3,]    0    1    0    0\n [4,]    0    1    0    0\n [5,]    0    1    0    0\n [6,]    0    1    0    0\n [7,]    0    1    0    0\n [8,]    0    0    0    1\n [9,]    0    0    0    1\n[10,]    0    0    0    1\n[11,]    0    0    0    1\n[12,]    1    0    0    0\n[13,]    1    0    0    0\n[14,]    1    0    0    0\n[15,]    1    0    0    0\n[16,]    0    0    0    1\n[17,]    0    0    1    0\n[18,]    0    0    1    0\n[19,]    0    1    0    0\n[20,]    0    1    0    0\n[21,]    0    1    0    0\n[22,]    0    1    0    0\n[23,]    0    1    0    0\n[24,]    0    1    0    0\n[25,]    0    1    0    0\n[26,]    0    0    0    1\n[27,]    0    0    0    1\n[28,]    1    0    0    0\n[29,]    0    0    1    0\n[30,]    1    0    0    0\n[31,]    1    0    0    0\n[32,]    1    0    0    0\n\n\nGreat! Looks like it works.\nFinally, we need to write a custom function for bipartite modularity checking across different assignments:\n\n   mod.check2 <- function(x, c, w) {\n      k <- length(c)\n      m <- rep(0, k)\n      for (i in 1:k) {\n         u <- make.dummies(c[[i]])\n         m[i] <- sum(diag(t(u) %*% x %*% u))/sum(w)\n         }\n   names(m) <- 2:(k+1)\n   return(m)\n   }\n\nThe function mod.check2 needs three inputs: The bipartite modularity matrix, a list with different assignments of the nodes in the bipartite graph to different clusters, and the bipartite adjacency matrix. It returns a vector m with the modularities of each of the partitions in c.\nAnd, now, for the big reveal:\n\n   round(mod.check2(B2, clus, as.matrix(as_adjacency_matrix(g))), 3)\n\n    2     3     4     5     6     7     8     9    10 \n0.413 0.420 0.426 0.321 0.296 0.266 0.213 0.207 0.173 \n\n\nLooks like the spectral clustering results favor a four-community partition although the more parsimonious three and binary community partitions also look pretty good.\nHere’s a plot of the three and four community solutions (since we already saw the binary partition in the CA handout:\n\n\n\n\n\n\n\n(a) Three Community Solution\n\n\n\n\n\n\n\n(b) Four Community Solution\n\n\n\n\nFigure 4: Spectral Clustering of Nodes in the Southern Women Data.\n\n\nAs we have seen, spectral clustering via the Laplacian, normalized Laplacian or the modularity matrix (and CA in the bipartite case) produces a high-quality partition, highlighting the core groups in the data. It is a simple and easy to implement option to add to your arsenal."
  }
]