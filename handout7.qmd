---
title: "Two Mode Networks"
execute: 
  eval: true
  echo: true
  output: true
  warning: false
  message: false
format: 
   html:
      code-line-numbers: true
---

## Two Mode Networks

This handout deals with the network analysis of two-mode networks. Note that in the literature there is some terminological slippage. Two-mode networks are a type of social network. By definition two mode networks can be represented using rectangular adjacency matrices (sometimes called **affiliation matrices** in sociology). 

In this case, two mode networks fall under the general category of "two mode data." Any data set that has information on two types of objects (e.g., people and variables) is two-mode data so two-mode networks are just a special case of two-mode data.

In this sense, a useful distinction, due to Borgatti & Everett, is useful. This is that between the "modes" and the "ways" of a data matrix. So most data matrices are two-ways, in that they have at least two dimensions (e.g., the row and column dimensions). 

But some data matrices (like the usual adjacency matrix in regular network data) only collect information on a single type of entity, so they are "one mode, two ways." But sometimes we have network data on two sets of objects, in which case, we use a data matrix that has "two modes" (sets of nodes) *and* "two ways" (rows and columns).

So what makes a network a "two mode network"? Well, a two mode network is different from a regular network, because it has two sets of nodes not just one. So instead of $V$ now we have $V_1$ and $V_2$. Moreover, the edges in a two-mode network only go from nodes in one set to nodes in the other set; there are no within-node-set edges. 

## Bipartite Graphs

This restriction makes the graph that represents a two mode network a special kind of graph called a **bipartite graph**. A graph is bipartite if the set of nodes in the graph can be divided into two groups, such that relations go from nodes in one set to nodes in the other set. For instance, a dating network with 100% heterosexual people in it will yield a bipartite graph based on the dating relation, with men in one set and women on the other node set. 

So whether or not a graph is bipartite is something you can check for. 

Let's see how that works. Let us load the most famous two-mode network data set (kind of the Drosophila of two-mode network analysis; one of the most repeatedly analyzed social structures in history) a network composed of eighteen women from the social elite of a tiny town in the south in the 1930s who attended fourteen social events [@breiger74]:

```{r}
   library(igraph)
   library(networkdata)
   g <- southern_women
```

Now we already know this is a bipartite graph. However, let's say you are new and you've never heard of these data. You can check whether the graph you loaded up is bipartite or not by using the `igraph` function `is_bipartite`:

```{r}
   is_bipartite(g)
```

Which returns `TRUE` as an answer. Had we loaded up any old non-bipartite graph, the answer would have been:

```{r}
   g.whatever <- movie_45
   is_bipartite(g.whatever)
```

Which makes sense because that's just a regular old graph. 

Note that if we check the bipartite graph object, it looks like any other `igraph` object:

```{r}
   g
```

But we can tell that the graph is a two mode network because we have links starting with people with old lady names from the 1930s (which are also the names of a bunch of kids in middle school in 2024) and ending with events that have dates in them. So the (undirected) edge is $person-event$. 

The graph is undirected because the "membership" or "attendance" relation between a person and an organization/event doesn't have a natural directionality. 

Another way of checking the "bipartiteness" of a graph in `igraph` is by using the `bipartite_mapping` function. 

Let's see what it does:

```{r}
   bipartite_mapping(g)
```

This function takes the candidate bipartite graph as input and returns to objects: `res` is just a check to see if the graph is actually bipartite (`TRUE` in this case), `type` is a logical vector of dimensions $N + M$ (where $N$ is the number of nodes in one set and $M$ is the number of nodes in the other set) dividing the nodes into two groups. Here people get `FALSE` and events get `TRUE`. 

We can add this as a node attribute to our graph so that way we know which node is in which set:

```{r}
   V(g)$type <- bipartite_mapping(g)$type
```

## The Affiliation Matrix

Once you have your bipartite graph loaded up, you may want (if the graph is small enough) to check out the graph's affiliation matrix $A$. 

This works just like before, except that now we use the `as_biadjacency_matrix` function:

```{r}
   A <- as.matrix(as_biadjacency_matrix(g))
   A
```

In this matrix we list one set of nodes in the rows and the other set is in the columns. Each cell $a_{ij} = 1$ if row node $i$ is affiliated with column node $j$, otherwise $a_{ij} = 0$.

## Basic Two Mode Network Statistics

From the affiliation (bi-adjacency) matrix we can calculate some basic network statistics. We have two number of nodes to calculate, but only one number of edges figure.

The number of nodes on the people side $N$ is just the number of rows of $A$:

```{r}
   nrow(A)
```

And the number of events/groups $M$ is just the number of columns:

```{r}
   ncol(A)
```

Finally, the number of edges $E$ is just the sum of all the entries of $A$:

```{r}
   sum(A)
```

As we saw in the case of one-mode networks, one of the most basic network statistics that can be derived from the above quantities is the density. In a two-mode network, this is given by:

$$
d = \frac{E}{N \times M}
$$

Where $E$ is the number of edges in the network. In our case we can compute the density as follows:

```{r}
   d <- sum(A)/(nrow(A) * ncol(A))
   d
```

## Degree
In a two-mode network, there are two degree sets, each corresponding to one set of nodes. For the people, in this case, their degree (centrality) is just the number of events they attend, and for the groups, it's just the number of people that attend each event. 

We can get each from the affiliation matrix. The degree of the people are just the row sums:

```{r}
   rowSums(A)
```

And the degree of the events are just the column sums:

```{r}
   colSums(A)
```

As @borgatti_everett97 note, if we want *normalized* degree centrality measures, we need to divide by either $M$ (for people) or $N$ (for events). That is, for people we use the number of events as the norm (as this is the theoretical maximum) and for events the number of people.

So for people, normalized degree is:

```{r}
   round(rowSums(A)/ncol(A), 3)
```

And for events:

```{r}
   round(colSums(A)/nrow(A), 3)
```

## Geodesic Distances

Geodesic distances work a bit different in two mode networks because of the only between-node-sets edges restriction. 

For instance, the minimum geodesic distance $g_{ii'}$ between two people is two (a person cannot be adjacent to another person), but it is one between a person and a group (if the person is a member of the group). 

In the same way, a group $g$ cannot be at geodesic distance less than three from a person $p*$ who is not a member, because the shortest path is $g-p-g^*-p^*$. 

That is, there has to be some other group $g^*$ shared between a member $p$ of the focal group $g$ and another person $p^*$ for the shortest path between $g$ and the non-member $p^*$ to exist, and that involves three links at minimum: $g-p$, $p-g^*$, and $g^*-p^*$. This means that the links in paths in two-mode networks always alternate between persons and group nodes.

Beyond that geodesic distances work the same way. In `igraph` when we use the `distances` function on a bipartite graph, we get:

```{r}
   distances(g)
```

Which is a square matrix of dimensions $(N + M) \times (N + M)$; that's $(18 + 14) \times (18 + 14) = 32 \times 32$ in our case. 

We can check in `R`:

```{r}
   dim(distances(g))
```

As we can see in the distance matrix, distances between nodes in the same set are even $g_{ii'|jj'} = \{2, 4, \ldots\}$ but distances in nodes in different sets are odd $g_{ij|ji} = \{1, 3, \ldots\}$. Beyond this hiccup, distances can be interpreted in the same way as one-mode networks.

## Closeness Centrality in Two Mode Networks

This means that (unnormalized) closeness centrality works the same way as it does in regular networks:

```{r}
   round(closeness(g), 3)
```

Which is just the inverse of the sums of the distances matrix for people and groups counting their geodesic distances to nodes of both sets. 

However, as @borgatti_everett97 note, if we want *normalized* closeness centralities, we can't use the off-the-shelf normalization for one-mode networks in `igraph` ($n-1$) as it will give us non-sense results because now we have two sets of nodes.

Instead, we need to normalize the closeness score for each node set by its theoretical minimum for each node set. 

For people, this is:

$$
M + 2N -2
$$

And for groups/events this same quantity is:

$$
N + 2M - 2
$$

So in our case, we create a normalization vector with these quantities of length $M + N$:

```{r}
   M <- nrow(A)
   N <- ncol(A)
   n.p <- M + ((2 * N) - 2)
   n.e <- N + ((2 * M) - 2)
   norm.vec <- c(rep(n.p, M), rep(n.e, N))
```

And normalized closeness is:

```{r}
   round(norm.vec/rowSums(distances(g)), 3)
```

Which are the same numbers in @borgatti_everett97 (table 1, column 6).

## Betweenness Centrality in Two Mode Networks

As @borgatti_everett97 also note, the normalizations for betweenness centrality in the two mode case are a bit more involved. This is because they depend on which node set is larger than the other. 

For the larger node set, which in our case is the people, the normalization is:

$$
2(M-1)(N-1)
$$

For the smaller node set, which in our case is the groups/events, the normalization is:

$$
\frac{1}{2}(N)(N-1)+\frac{1}{2}(M-1)(M-2)+(M-1)(N-1)
$$

Remember that you have to switch this around if you are analyzing a network with more groups than people. 

Creating the relevant vectors:

```{r}
   n.p <- 2*(M-1)*(N-1)
   n.e <- (1/2)*(N*(N-1))+(1/2)*(M-1)*(M-2)+(M-1)*(N-1)
   norm.vec <- c(rep(n.p, M), rep(n.e, N))
```

And normalized betweenness is:

```{r}
   round(betweenness(g)/norm.vec, 4)*100
```

Which are (with some slight differences and rounding errors) the same numbers in @borgatti_everett97 (table 2, column 3).

## The Duality of Persons and Groups

Remember that in the one-mode case, multiplying the adjacency matrix times its transpose yields the **common neighbors matrix** $\mathbf{M}$:

$$
\mathbf{M} = \mathbf{A}\mathbf{A}^T
$$

As famously noted by @breiger74, doing the same for the affiliation matrix of a two-mode network also returns the common-neighbors matrix, but because objects in one mode can only connect to objects in another mode, this also reveals the **duality of persons and groups**: The connections between people are made up of the groups they share, and the connections between groups are revealed by the groups they share.

Thus, computing the common neighbors matrix for both persons and groups (also called the **projection** of the two-mode network into each of its modes) produces a one-mode similarity matrix between people and groups, where the similarities are defined by the number of objects in the other mode that they share. 

So for the people the relevant projection is:

$$
\mathbf{P} = \mathbf{A}\mathbf{A}^T
$$

And for the groups:

$$
\mathbf{G} = \mathbf{A}^T\mathbf{A}
$$

Which in our case yields:

```{r}
   P <- A %*% t(A)
   P
   G <- t(A) %*% A
   G
```

The off-diagonal entries of these square person by person (group by group) matrices is the number of groups (people) shared by each person (group) and the diagonals are the number of memberships of each person (the size of each group/event).

## Normalized Vertex Similarity Metrics

Note that the one-mode projections are unnormalized similarity matrices just like in the case of regular networks. That means that if we have the degrees of nodes in each mode, we can transform this matrix into any of the **normalized vertex similarity** metrics we discussed before, including Jaccard, Cosine, Dice, LHN, and so on. 

Thus repackaging our vertex similarity function for the two mode case, we have:

```{r}
   vertex.sim <- function(x) {
      A <- as.matrix(as_biadjacency_matrix(x))
      M <- nrow(A) #number of persons
      N <- ncol(A) #number of groups
      p.d <- rowSums(A) #person degrees
      g.d <- colSums(A) #group degrees
      P <- A %*% t(A) #person projection
      G <- t(A) %*% A #group projection
      J.p <- diag(1, M, M)
      J.g <- diag(1, N, N)
      C.p <- diag(1, M, M)
      C.g <- diag(1, N, N)
      D.p <- diag(1, M, M)
      D.g <- diag(1, N, N)
      L.p <- diag(1, M, M)
      L.g <- diag(1, N, N)
      for (i in 1:M) {
         for (j in 1:M) {
            if (i < j) {
               J.p[i,j] <- P[i,j]/(P[i,j] + p.d[i] + p.d[j])
               J.p[j,i] <- P[i,j]/(P[i,j] + p.d[i] + p.d[j])
               C.p[i,j] <- P[i,j]/(sqrt(p.d[i] * p.d[j]))
               C.p[j,i] <- P[i,j]/(sqrt(p.d[i] * p.d[j]))
               D.p[i,j] <- (2*P[i,j])/(2*P[i,j] + p.d[i] + p.d[j])
               D.p[j,i] <- (2*P[i,j])/(2*P[i,j] + p.d[i] + p.d[j])
               L.p[i,j] <- P[i,j]/(p.d[i] * p.d[j])
               L.p[j,i] <- P[i,j]/(p.d[i] * p.d[j])
               }
            }
         }
      for (i in 1:N) {
         for (j in 1:N) {
            if (i < j) {
               J.g[i,j] <- G[i,j]/(G[i,j] + g.d[i] + g.d[j])
               J.g[j,i] <- G[i,j]/(G[i,j] + g.d[i] + g.d[j])
               C.g[i,j] <- G[i,j]/(sqrt(g.d[i] * g.d[j]))
               C.g[j,i] <- G[i,j]/(sqrt(g.d[i] * g.d[j]))
               D.g[i,j] <- (2*G[i,j])/(2*G[i,j] + g.d[i] + g.d[j])
               D.g[j,i] <- (2*G[i,j])/(2*G[i,j] + g.d[i] + g.d[j])
               L.g[i,j] <- G[i,j]/(g.d[i] * g.d[j])
               L.g[j,i] <- G[i,j]/(g.d[i] * g.d[j])
               }
            }
         }
      return(list(J.p = J.p, C.p = C.p, D.p = D.p, L.p = L.p,
                  J.g = J.g, C.g = C.g, D.g = D.g, L.g = L.g))
      }
```

Using this function to compute the Cosine similarity between people yields:

```{r}
   C.p <- vertex.sim(g)$C.p
   rownames(C.p) <- rownames(A)
   colnames(C.p) <- rownames(A)
   round(C.p, 2)
```

And, of course, once we have a similarity we can cluster nodes based on approximate structural equivalence by transforming proximities to distances:

```{r}
   D <- as.dist(1- C.p)
   hc.res <- hclust(D, method = "ward.D2")
   plot(hc.res)
```

And for events:

```{r}
   C.g <- vertex.sim(g)$C.g
   rownames(C.g) <- colnames(A)
   colnames(C.g) <- colnames(A)
   D <- as.dist(1- C.g)
   hc.res <- hclust(D, method = "ward.D2")
   plot(hc.res)
```

## Status and Prestige

Measures of status and prestige are particularly applicable to two mode networks. The reason is that the *reflective* principle behind these measures interacts nicely with the *duality* principle. 

For instance, when it comes to **eigenvector**-style measures, the neat idea that *people are central if they belong to central groups and groups and central if their members are central people* (with people centrality defined by membership in central groups) can be effectively captured by these metrics [@bonacich91].

Thus if $x$ are the status scores for people, and $y$ are the status scores for groups, then the $x$ scores should be given by the sum of the $y$ scores of the groups each person belongs, and the $y$ scores should be given by the sum of the $x$ scores of their members. 

In mathese:

$$
x = \mathbf{A}^Ty
$$

$$
y = \mathbf{A}x
$$

Once again, producing another instance of a cat chasing its own tail (we need to know the values of $y$ to figure out the values of $x$ and we need to know the values of $x$ to figure out the values of $y$). 

How do we proceed? Well, let's bring back our trusty status distribution game:

```{r}
   status1 <- function(A) {
      n <- nrow(A) #number of actors
      x <- rep(1, n) #initial status vector set to all ones
      w <- 1 
      k <- 0 #initializing counter
      while (w > 0.0001) {
          o.x <- x #old status scores
          x <- A %*% x #new scores a function of old scores and adjacency matrix
          x <- x/norm(x, type = "E") #normalizing new status scores
          w <- abs(sum(abs(x) - abs(o.x))) #diff. between new and old scores
          k <- k + 1 #incrementing while counter
      }
   return(as.vector(x))
   }
```

Then the main question is over what matrix will the status game be played for *both* people and groups?

As @bonacich91 noted, the projection matrices of @breiger74 are natural candidates for this task. Let's try it out.

For people this would be:

```{r}
   p.s <- status1(P)
   round(p.s, 3)
```

And for groups:

```{r}
   g.s <- status1(G)
   round(g.s, 3)
```

Lo and behold, these are the status scores we seek. It turns out they can be computed by figuring out the leading eigenvector (what our status game does for any matrix) of the Breiger projection matrices [@bonacich91]:

$$
\lambda x = (\mathbf{A}\mathbf{A}^T)x
$$

$$
\lambda y = (\mathbf{A}^T\mathbf{A})y
$$

Neat! The scores are also readily interpretable: The most central people belong to the most central (largest membership) groups and the most central groups are the ones with the most central (highest activity) members. 

In the Southern Women data the dual centralities are:

```{r}
   library(kableExtra)
   p.dat <- data.frame(People = rownames(A), Eig.Cent = round(p.s, 3))
   p.dat <- p.dat[order(p.dat$Eig.Cent, decreasing = TRUE), ]
   kbl(p.dat, format = "html", , align = c("l", "c"), row.names = FALSE) %>% 
      column_spec(1, bold = TRUE) %>% 
      kable_styling(full_width = TRUE,
                     bootstrap_options = c("hover", "condensed", "responsive"))
```

```{r}
   g.dat <- data.frame(Groups = colnames(A), Eig.Cent = round(g.s, 3))
   g.dat <- g.dat[order(g.dat$Eig.Cent, decreasing = TRUE), ]
   kbl(g.dat, format = "html", align = c("l", "c"), row.names = FALSE) %>% 
      column_spec(1, bold = TRUE) %>% 
      kable_styling(full_width = TRUE,
                     bootstrap_options = c("hover", "condensed", "responsive"))
```

We can of course play our status game with degree-normalized versions of the affiliation matrix (a.k.a. PageRank).

First we need to create **row stochastic** versions of $\mathbf{A}$ and $\mathbf{A}^T$. Recall that a matrix is row stochastic if their rows sum to one. 

We can do this as follows:

```{r}
   D.p <- diag(1/rowSums(A))
   P.p <- D.p %*% A
   D.g <- diag(1/colSums(A))
   P.g <- D.g %*% t(A)
```

Where `D.p` is a **diagonal matrix** containing the inverse of the people degrees in the diagonal and zeroes everywhere elese and `D.g` contains the inverse of the group degrees in the diagonal and zero everywhere else. 

And we can check that both `P.p` (for people) and `P.g` (groups) are row stochastic:

```{r}
   rowSums(P.p)
   rowSums(P.g)
```

Great! Now, we compute the degree-normalized projections:

```{r}
   P.pp <- P.p %*% P.g
   P.gg <- P.g %*% P.p
   rownames(P.pp) <- colnames(P.pp)
   rownames(P.gg) <- colnames(P.gg)
```

Which are still row stochastic--but now square--matrices:

```{r}
   rowSums(P.pp)
   rowSums(P.gg)
```

We now can play the PageRank status game on the transpose of each to get the scores we want:

```{r}
   pr.p <- status1(t(P.pp))
   pr.g <- status1(t(P.gg))
```

Which leads to the following PageRank centrality rankings for persons and groups:

```{r}
   p.dat <- data.frame(People = rownames(A), PageRank.Cent = round(pr.p, 3))
   p.dat <- p.dat[order(p.dat$PageRank.Cent, decreasing = TRUE), ]
   kbl(p.dat, format = "html", , align = c("l", "c"), row.names = FALSE) %>% 
      column_spec(1, bold = TRUE) %>% 
      kable_styling(full_width = TRUE,
                     bootstrap_options = c("hover", "condensed", "responsive"))
```

```{r}
   g.dat <- data.frame(Groups = colnames(A), PageRank.Cent = round(pr.g, 3))
   g.dat <- g.dat[order(g.dat$PageRank.Cent, decreasing = TRUE), ]
   kbl(g.dat, format = "html", align = c("l", "c"), row.names = FALSE) %>% 
      column_spec(1, bold = TRUE) %>% 
      kable_styling(full_width = TRUE,
                     bootstrap_options = c("hover", "condensed", "responsive"))
```

The PageRank two-mode scores lead to a different ranking of the nodes, because now people are central if they belong to *selective* (not just large) groups and groups are central if their members are *discerning* (not just very active) people. 


