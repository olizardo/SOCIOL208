{
  "hash": "53fdd590a2940eabf94a422c3850453f",
  "result": {
    "markdown": "---\ntitle: \"Two-Mode Networks\"\nexecute: \n  eval: true\n  echo: true\n  output: true\n  warning: false\n  message: false\nformat: \n   html:\n      code-line-numbers: true\n---\n\n\n## Two-Mode Networks\n\nThis handout deals with the network analysis of two-mode networks. Note that in the literature there is some terminological slippage. Two-mode networks are a type of social network. By definition two-mode networks can be represented using rectangular adjacency matrices (sometimes called **affiliation matrices** in sociology). \n\nIn this case, two-mode networks fall under the general category of \"two-mode data.\" Any data set that has information on two types of objects (e.g., people and variables) is two-mode data so two-mode networks are just a special case of two-mode data.\n\nIn this sense, a useful distinction, due to Borgatti & Everett, is useful. This is that between the \"modes\" and the \"ways\" of a data matrix. So most data matrices are two-ways, in that they have at least two dimensions (e.g., the row and column dimensions). \n\nBut some data matrices (like the usual adjacency matrix in regular network data) only collect information on a single type of entity, so they are \"one mode, two ways.\" But sometimes we have network data on two sets of objects, in which case, we use a data matrix that has \"two-modes\" (sets of nodes) *and* \"two ways\" (rows and columns).\n\nSo what makes a network a \"two-mode network\"? Well, a two-mode network is different from a regular network, because it has two sets of nodes not just one. So instead of $V$ now we have $V_1$ and $V_2$. Moreover, the edges in a two-mode network only go from nodes in one set to nodes in the other set; there are no within-node-set edges. \n\n## Bipartite Graphs\n\nThis restriction makes the graph that represents a two-mode network a special kind of graph called a **bipartite graph**. A graph is bipartite if the set of nodes in the graph can be divided into two groups, such that relations go from nodes in one set to nodes in the other set. \n\nNote that bipartite graphs can be be used to represent both two-mode and regular one mode networks, as long as the above condition holds. For instance, a dating network with 100% heterosexual people in it will yield a bipartite graph based on the dating relation, with men in one set and women on the other node set, even though it's a one-mode network. \n\nSo whether or not a graph is bipartite is something you can check for. \n\nLet's see how that works. Let us load the most famous two-mode network data set (kind of the Drosophila of two-mode network analysis; one of the most repeatedly analyzed social structures in history: For a classic sampling of such analyses see [here](https://www.csc2.ncsu.edu/faculty/mpsingh/local/Social/f16/wrap/readings/Freeman-social-groups.pdf)) a network composed of eighteen women from the social elite of a tiny town in the south in the 1930s who attended fourteen social events [@breiger74]:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   library(igraph)\n   library(networkdata)\n   g <- southern_women\n```\n:::\n\n\nNow we already know this is a bipartite graph. However, let's say you are new and you've never heard of these data. You can check whether the graph you loaded up is bipartite or not by using the `igraph` function `is_bipartite`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   is_bipartite(g)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n:::\n\n\nWhich returns `TRUE` as an answer. Had we loaded up any old non-bipartite graph, the answer would have been:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   g.whatever <- movie_45\n   is_bipartite(g.whatever)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] FALSE\n```\n:::\n:::\n\n\nWhich makes sense because that's just a regular old graph. \n\nNote that if we check the bipartite graph object, it looks like any other `igraph` object:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   g\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIGRAPH 1074643 UN-B 32 89 -- \n+ attr: type (v/l), name (v/c)\n+ edges from 1074643 (vertex names):\n [1] EVELYN   --6/27 EVELYN   --3/2  EVELYN   --4/12 EVELYN   --9/26\n [5] EVELYN   --2/25 EVELYN   --5/19 EVELYN   --9/16 EVELYN   --4/8 \n [9] LAURA    --6/27 LAURA    --3/2  LAURA    --4/12 LAURA    --2/25\n[13] LAURA    --5/19 LAURA    --3/15 LAURA    --9/16 THERESA  --3/2 \n[17] THERESA  --4/12 THERESA  --9/26 THERESA  --2/25 THERESA  --5/19\n[21] THERESA  --3/15 THERESA  --9/16 THERESA  --4/8  BRENDA   --6/27\n[25] BRENDA   --4/12 BRENDA   --9/26 BRENDA   --2/25 BRENDA   --5/19\n[29] BRENDA   --3/15 BRENDA   --9/16 CHARLOTTE--4/12 CHARLOTTE--9/26\n+ ... omitted several edges\n```\n:::\n:::\n\n\nBut we can tell that the graph is a two-mode network because we have links starting with people with old lady names from the 1930s (which are also the names of a bunch of kids in middle school in 2024) and ending with events that have dates in them. So the (undirected) edge is $person-event$. \n\nThe graph is undirected because the \"membership\" or \"attendance\" relation between a person and an organization/event doesn't have a natural directionality. \n\nAnother way of checking the \"bipartiteness\" of a graph in `igraph` is by using the `bipartite_mapping` function. \n\nLet's see what it does:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   bipartite_mapping(g)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$res\n[1] TRUE\n\n$type\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n    FALSE     FALSE     FALSE     FALSE     FALSE     FALSE     FALSE     FALSE \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n    FALSE     FALSE     FALSE     FALSE     FALSE     FALSE     FALSE     FALSE \n   OLIVIA     FLORA      6/27       3/2      4/12      9/26      2/25      5/19 \n    FALSE     FALSE      TRUE      TRUE      TRUE      TRUE      TRUE      TRUE \n     3/15      9/16       4/8      6/10      2/23       4/7     11/21       8/3 \n     TRUE      TRUE      TRUE      TRUE      TRUE      TRUE      TRUE      TRUE \n```\n:::\n:::\n\n\nThis function takes the candidate bipartite graph as input and returns to objects: `res` is just a check to see if the graph is actually bipartite (`TRUE` in this case), `type` is a logical vector of dimensions $M + N$ (where $M$ is the number of nodes in the person set and $N$ is the number of nodes in the event set) dividing the nodes into two groups. Here people get `FALSE` and events get `TRUE`, but this designations are arbitrary (a kind of dummy coding; `FALSE` = 0 and `TRUE` = 1). \n\nWe can add this as a node attribute to our graph so that way we know which node is in which set:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   V(g)$type <- bipartite_mapping(g)$type\n```\n:::\n\n\n## The Bi-Adjacency (Affiliation) Matrix\n\nOnce you have your bipartite graph loaded up, you may want (if the graph is small enough) to check out the graph's affiliation matrix $A$. \n\nThis works just like before, except that now we use the `as_biadjacency_matrix` function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   A <- as.matrix(as_biadjacency_matrix(g))\n   A\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          6/27 3/2 4/12 9/26 2/25 5/19 3/15 9/16 4/8 6/10 2/23 4/7 11/21 8/3\nEVELYN       1   1    1    1    1    1    0    1   1    0    0   0     0   0\nLAURA        1   1    1    0    1    1    1    1   0    0    0   0     0   0\nTHERESA      0   1    1    1    1    1    1    1   1    0    0   0     0   0\nBRENDA       1   0    1    1    1    1    1    1   0    0    0   0     0   0\nCHARLOTTE    0   0    1    1    1    0    1    0   0    0    0   0     0   0\nFRANCES      0   0    1    0    1    1    0    1   0    0    0   0     0   0\nELEANOR      0   0    0    0    1    1    1    1   0    0    0   0     0   0\nPEARL        0   0    0    0    0    1    0    1   1    0    0   0     0   0\nRUTH         0   0    0    0    1    0    1    1   1    0    0   0     0   0\nVERNE        0   0    0    0    0    0    1    1   1    0    0   1     0   0\nMYRNA        0   0    0    0    0    0    0    1   1    1    0   1     0   0\nKATHERINE    0   0    0    0    0    0    0    1   1    1    0   1     1   1\nSYLVIA       0   0    0    0    0    0    1    1   1    1    0   1     1   1\nNORA         0   0    0    0    0    1    1    0   1    1    1   1     1   1\nHELEN        0   0    0    0    0    0    1    1   0    1    1   1     0   0\nDOROTHY      0   0    0    0    0    0    0    1   1    0    0   0     0   0\nOLIVIA       0   0    0    0    0    0    0    0   1    0    1   0     0   0\nFLORA        0   0    0    0    0    0    0    0   1    0    1   0     0   0\n```\n:::\n:::\n\n\nIn this matrix we list one set of nodes in the rows and the other set is in the columns. Each cell $a_{ij} = 1$ if row node $i$ is affiliated with column node $j$, otherwise $a_{ij} = 0$.\n\nNote that if we were to use the regular `as_adjacency_matrix` function on a bipartite graph, we get a curious version of the adjacency matrix:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   B <- as.matrix(as_adjacency_matrix(g))\n   B\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          EVELYN LAURA THERESA BRENDA CHARLOTTE FRANCES ELEANOR PEARL RUTH\nEVELYN         0     0       0      0         0       0       0     0    0\nLAURA          0     0       0      0         0       0       0     0    0\nTHERESA        0     0       0      0         0       0       0     0    0\nBRENDA         0     0       0      0         0       0       0     0    0\nCHARLOTTE      0     0       0      0         0       0       0     0    0\nFRANCES        0     0       0      0         0       0       0     0    0\nELEANOR        0     0       0      0         0       0       0     0    0\nPEARL          0     0       0      0         0       0       0     0    0\nRUTH           0     0       0      0         0       0       0     0    0\nVERNE          0     0       0      0         0       0       0     0    0\nMYRNA          0     0       0      0         0       0       0     0    0\nKATHERINE      0     0       0      0         0       0       0     0    0\nSYLVIA         0     0       0      0         0       0       0     0    0\nNORA           0     0       0      0         0       0       0     0    0\nHELEN          0     0       0      0         0       0       0     0    0\nDOROTHY        0     0       0      0         0       0       0     0    0\nOLIVIA         0     0       0      0         0       0       0     0    0\nFLORA          0     0       0      0         0       0       0     0    0\n6/27           1     1       0      1         0       0       0     0    0\n3/2            1     1       1      0         0       0       0     0    0\n4/12           1     1       1      1         1       1       0     0    0\n9/26           1     0       1      1         1       0       0     0    0\n2/25           1     1       1      1         1       1       1     0    1\n5/19           1     1       1      1         0       1       1     1    0\n3/15           0     1       1      1         1       0       1     0    1\n9/16           1     1       1      1         0       1       1     1    1\n4/8            1     0       1      0         0       0       0     1    1\n6/10           0     0       0      0         0       0       0     0    0\n2/23           0     0       0      0         0       0       0     0    0\n4/7            0     0       0      0         0       0       0     0    0\n11/21          0     0       0      0         0       0       0     0    0\n8/3            0     0       0      0         0       0       0     0    0\n          VERNE MYRNA KATHERINE SYLVIA NORA HELEN DOROTHY OLIVIA FLORA 6/27 3/2\nEVELYN        0     0         0      0    0     0       0      0     0    1   1\nLAURA         0     0         0      0    0     0       0      0     0    1   1\nTHERESA       0     0         0      0    0     0       0      0     0    0   1\nBRENDA        0     0         0      0    0     0       0      0     0    1   0\nCHARLOTTE     0     0         0      0    0     0       0      0     0    0   0\nFRANCES       0     0         0      0    0     0       0      0     0    0   0\nELEANOR       0     0         0      0    0     0       0      0     0    0   0\nPEARL         0     0         0      0    0     0       0      0     0    0   0\nRUTH          0     0         0      0    0     0       0      0     0    0   0\nVERNE         0     0         0      0    0     0       0      0     0    0   0\nMYRNA         0     0         0      0    0     0       0      0     0    0   0\nKATHERINE     0     0         0      0    0     0       0      0     0    0   0\nSYLVIA        0     0         0      0    0     0       0      0     0    0   0\nNORA          0     0         0      0    0     0       0      0     0    0   0\nHELEN         0     0         0      0    0     0       0      0     0    0   0\nDOROTHY       0     0         0      0    0     0       0      0     0    0   0\nOLIVIA        0     0         0      0    0     0       0      0     0    0   0\nFLORA         0     0         0      0    0     0       0      0     0    0   0\n6/27          0     0         0      0    0     0       0      0     0    0   0\n3/2           0     0         0      0    0     0       0      0     0    0   0\n4/12          0     0         0      0    0     0       0      0     0    0   0\n9/26          0     0         0      0    0     0       0      0     0    0   0\n2/25          0     0         0      0    0     0       0      0     0    0   0\n5/19          0     0         0      0    1     0       0      0     0    0   0\n3/15          1     0         0      1    1     1       0      0     0    0   0\n9/16          1     1         1      1    0     1       1      0     0    0   0\n4/8           1     1         1      1    1     0       1      1     1    0   0\n6/10          0     1         1      1    1     1       0      0     0    0   0\n2/23          0     0         0      0    1     1       0      1     1    0   0\n4/7           1     1         1      1    1     1       0      0     0    0   0\n11/21         0     0         1      1    1     0       0      0     0    0   0\n8/3           0     0         1      1    1     0       0      0     0    0   0\n          4/12 9/26 2/25 5/19 3/15 9/16 4/8 6/10 2/23 4/7 11/21 8/3\nEVELYN       1    1    1    1    0    1   1    0    0   0     0   0\nLAURA        1    0    1    1    1    1   0    0    0   0     0   0\nTHERESA      1    1    1    1    1    1   1    0    0   0     0   0\nBRENDA       1    1    1    1    1    1   0    0    0   0     0   0\nCHARLOTTE    1    1    1    0    1    0   0    0    0   0     0   0\nFRANCES      1    0    1    1    0    1   0    0    0   0     0   0\nELEANOR      0    0    1    1    1    1   0    0    0   0     0   0\nPEARL        0    0    0    1    0    1   1    0    0   0     0   0\nRUTH         0    0    1    0    1    1   1    0    0   0     0   0\nVERNE        0    0    0    0    1    1   1    0    0   1     0   0\nMYRNA        0    0    0    0    0    1   1    1    0   1     0   0\nKATHERINE    0    0    0    0    0    1   1    1    0   1     1   1\nSYLVIA       0    0    0    0    1    1   1    1    0   1     1   1\nNORA         0    0    0    1    1    0   1    1    1   1     1   1\nHELEN        0    0    0    0    1    1   0    1    1   1     0   0\nDOROTHY      0    0    0    0    0    1   1    0    0   0     0   0\nOLIVIA       0    0    0    0    0    0   1    0    1   0     0   0\nFLORA        0    0    0    0    0    0   1    0    1   0     0   0\n6/27         0    0    0    0    0    0   0    0    0   0     0   0\n3/2          0    0    0    0    0    0   0    0    0   0     0   0\n4/12         0    0    0    0    0    0   0    0    0   0     0   0\n9/26         0    0    0    0    0    0   0    0    0   0     0   0\n2/25         0    0    0    0    0    0   0    0    0   0     0   0\n5/19         0    0    0    0    0    0   0    0    0   0     0   0\n3/15         0    0    0    0    0    0   0    0    0   0     0   0\n9/16         0    0    0    0    0    0   0    0    0   0     0   0\n4/8          0    0    0    0    0    0   0    0    0   0     0   0\n6/10         0    0    0    0    0    0   0    0    0   0     0   0\n2/23         0    0    0    0    0    0   0    0    0   0     0   0\n4/7          0    0    0    0    0    0   0    0    0   0     0   0\n11/21        0    0    0    0    0    0   0    0    0   0     0   0\n8/3          0    0    0    0    0    0   0    0    0   0     0   0\n```\n:::\n:::\n\n\nThis matrix is of dimensions $(M + N) \\times (M + N)$, which is $(18 + 14) \\times (18 + 14) = 32 \\times 32$ in the Southern Women data. \n\nThe adjacency matrix of a bipartite graph necessarily has to big \"blocks\" of zeroes in it corresponding to where the links between nodes in the same set would be (but aren't because this is a two-mode network). \n\nSo the regular adjacency matrix of a bipartite graph $\\mathbf{B}$ has the following form [@fouss_etal16, p. 12]:\n\n$$\n\\mathbf{B} = \\left[\n\\begin{matrix}\n\\mathbf{O}_{M \\times M} & \\mathbf{A}_{M \\times N} \\\\\n\\mathbf{A}^T_{N \\times M} & \\mathbf{O}_{N \\times N}\n\\end{matrix}\n\\right]\n$$\n\nWhere $\\mathbf{O}$ is just the all zeros matrix of the relevant dimensions, and $\\mathbf{A}$ is the bi-adjacency (affiliation) matrix as defined earlier. \n\n## Basic two-mode Network Statistics\n\nWe can calculate some basic network statistics from the affiliation (bi-adjacency) matrix. We have two number of nodes to calculate, but only one quantity for the number of edges.\n\nThe number of nodes on the people side $N$ is just the number of rows of $A$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   nrow(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 18\n```\n:::\n:::\n\n\nAnd the number of events/groups $M$ is just the number of columns:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   ncol(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 14\n```\n:::\n:::\n\n\nFinally, the number of edges $E$ is just the sum of all the entries of $A$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   sum(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 89\n```\n:::\n:::\n\n\nNote that if you were to use the `igraph` function `vcount` on the original graph object, you get the wrong answer:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   vcount(g)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 32\n```\n:::\n:::\n\n\nThat's because `vcount` is working with the $32 \\times 32$ regular adjacency matrix, not the bi-adjacency matrix. Here, `vcount` is returning the *total* number of nodes in the graph summing across the two sets, which is $M + N$. \n\nIf you wanted to get the right answer for each set of edges from the regular `igraph` graph object, you could use the `type` node attribute we defined earlier along with the `subgraph` function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   vcount(subgraph(g, V(g)$type == FALSE))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 18\n```\n:::\n:::\n\n\nWhich gives us the number of women. For the events we do the same thing:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   vcount(subgraph(g, V(g)$type == TRUE))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 14\n```\n:::\n:::\n\n\nHowever, because there's only one set of edges, `ecount` still gives us the right answer:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   ecount(g)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 89\n```\n:::\n:::\n\n\nWhich is the same as:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   sum(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 89\n```\n:::\n:::\n\n\n### Degree Statistics\n\nBecause we have two sets of degrees, all the basic degree statistics in the network double up. So we have two mean degrees, two maximum degrees, and two minimum degree to take care of:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   mean.d.p <- mean(rowSums(A))\n   mean.d.g <- mean(colSums(A))\n   max.d.p <- max(rowSums(A))\n   max.d.g <- max(colSums(A))\n   min.d.p <- min(rowSums(A))\n   min.d.g <- min(colSums(A))\n```\n:::\n\n\nSo we have:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   round(mean.d.p, 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4.9\n```\n:::\n\n```{.r .cell-code}\n   round(mean.d.g, 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 6.4\n```\n:::\n\n```{.r .cell-code}\n   max.d.p\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 8\n```\n:::\n\n```{.r .cell-code}\n   max.d.g\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 14\n```\n:::\n\n```{.r .cell-code}\n   min.d.p\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2\n```\n:::\n\n```{.r .cell-code}\n   min.d.g\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3\n```\n:::\n:::\n\n\nHowever, note that because there's only one set of undirected edges, the total number of edges incident to each node in each of the two sets is always going to be the same.\n\nThat means that there's only one sum of degrees. So the sum of degrees for people:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   sum(rowSums(A))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 89\n```\n:::\n:::\n\n\nIs the same as the sum of degrees of events:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   sum(colSums(A))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 89\n```\n:::\n:::\n\n\nNote that in a bipartite graph, therefore, the sum of degrees of nodes in each node set is equal to the $|E|$, the number of edges in the graph!\n\n### Density\nAs we saw in the case of one-mode networks, one of the most basic network statistics that can be derived from the above quantities is the **density** (observed number of edges divided by maximum possible number of edges in the graph). \n\nIn a two-mode network, density is given by:\n\n$$\nd = \\frac{|E|}{N \\times M}\n$$\n\nWhere $|E|$ is the number of edges in the network. In our case we can compute the density as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   d <- sum(A)/(nrow(A) * ncol(A))\n   d\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.3531746\n```\n:::\n:::\n\n\n## Degree Centrality\nIn a two-mode network, there are two degree sets, each corresponding to one set of nodes. For the people, in this case, their degree (centrality) is just the number of events they attend, and for the groups, it's just the number of people that attend each event. \n\nAs we have already seen, we can get each from the affiliation matrix. The degree of the people are just the row sums:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   rowSums(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n        8         7         8         7         4         4         4         3 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n        4         4         4         6         7         8         5         2 \n   OLIVIA     FLORA \n        2         2 \n```\n:::\n:::\n\n\nAnd the degree of the events are just the column sums:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   colSums(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n 6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10  2/23   4/7 11/21 \n    3     3     6     4     8     8    10    14    12     5     4     6     3 \n  8/3 \n    3 \n```\n:::\n:::\n\n\nThe `igraph` function `degree` will also give us the right answer, but in the form of a single vector including both people and events:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   degree(g)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n        8         7         8         7         4         4         4         3 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n        4         4         4         6         7         8         5         2 \n   OLIVIA     FLORA      6/27       3/2      4/12      9/26      2/25      5/19 \n        2         2         3         3         6         4         8         8 \n     3/15      9/16       4/8      6/10      2/23       4/7     11/21       8/3 \n       10        14        12         5         4         6         3         3 \n```\n:::\n:::\n\n\nAs @borgatti_everett97 note, if we want *normalized* degree centrality measures, we need to divide by either $M$ (for people) or $N$ (for events). That is, for people we use the number of events as the norm (as this is the theoretical maximum) and for events the number of people.\n\nSo for people, normalized degree is:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   round(rowSums(A)/ncol(A), 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n    0.571     0.500     0.571     0.500     0.286     0.286     0.286     0.214 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n    0.286     0.286     0.286     0.429     0.500     0.571     0.357     0.143 \n   OLIVIA     FLORA \n    0.143     0.143 \n```\n:::\n:::\n\n\nAnd for events:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   round(colSums(A)/nrow(A), 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n 6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10  2/23   4/7 11/21 \n0.167 0.167 0.333 0.222 0.444 0.444 0.556 0.778 0.667 0.278 0.222 0.333 0.167 \n  8/3 \n0.167 \n```\n:::\n:::\n\n\nOr with `igraph`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   round(degree(g)/c(rep(14, 18), rep(18, 14)), 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n    0.571     0.500     0.571     0.500     0.286     0.286     0.286     0.214 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n    0.286     0.286     0.286     0.429     0.500     0.571     0.357     0.143 \n   OLIVIA     FLORA      6/27       3/2      4/12      9/26      2/25      5/19 \n    0.143     0.143     0.167     0.167     0.333     0.222     0.444     0.444 \n     3/15      9/16       4/8      6/10      2/23       4/7     11/21       8/3 \n    0.556     0.778     0.667     0.278     0.222     0.333     0.167     0.167 \n```\n:::\n:::\n\n\n\n## Geodesic Distances\n\nGeodesic distances work a bit different in two-mode networks because of the only between-node-sets edges restriction. \n\nFor instance, the minimum geodesic distance $g_{ii'}$ between two people is two (a person cannot be adjacent to another person), but it is one between a person and a group (if the person is a member of the group). \n\nIn the same way, a group $g$ cannot be at geodesic distance less than three from a person $p*$ who is not a member, because the shortest path is $g-p-g^*-p^*$. \n\nThat is, there has to be some other group $g^*$ shared between a member $p$ of the focal group $g$ and another person $p^*$ for the shortest path between $g$ and the non-member $p^*$ to exist, and that involves three links at minimum: $g-p$, $p-g^*$, and $g^*-p^*$. This means that the links in paths in two-mode networks always alternate between persons and group nodes.\n\nBeyond that geodesic distances work the same way. In `igraph` when we use the `distances` function on a bipartite graph, we get:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   distances(g)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          EVELYN LAURA THERESA BRENDA CHARLOTTE FRANCES ELEANOR PEARL RUTH\nEVELYN         0     2       2      2         2       2       2     2    2\nLAURA          2     0       2      2         2       2       2     2    2\nTHERESA        2     2       0      2         2       2       2     2    2\nBRENDA         2     2       2      0         2       2       2     2    2\nCHARLOTTE      2     2       2      2         0       2       2     4    2\nFRANCES        2     2       2      2         2       0       2     2    2\nELEANOR        2     2       2      2         2       2       0     2    2\nPEARL          2     2       2      2         4       2       2     0    2\nRUTH           2     2       2      2         2       2       2     2    0\nVERNE          2     2       2      2         2       2       2     2    2\nMYRNA          2     2       2      2         4       2       2     2    2\nKATHERINE      2     2       2      2         4       2       2     2    2\nSYLVIA         2     2       2      2         2       2       2     2    2\nNORA           2     2       2      2         2       2       2     2    2\nHELEN          2     2       2      2         2       2       2     2    2\nDOROTHY        2     2       2      2         4       2       2     2    2\nOLIVIA         2     4       2      4         4       4       4     2    2\nFLORA          2     4       2      4         4       4       4     2    2\n6/27           1     1       3      1         3       3       3     3    3\n3/2            1     1       1      3         3       3       3     3    3\n4/12           1     1       1      1         1       1       3     3    3\n9/26           1     3       1      1         1       3       3     3    3\n2/25           1     1       1      1         1       1       1     3    1\n5/19           1     1       1      1         3       1       1     1    3\n3/15           3     1       1      1         1       3       1     3    1\n9/16           1     1       1      1         3       1       1     1    1\n4/8            1     3       1      3         3       3       3     1    1\n6/10           3     3       3      3         3       3       3     3    3\n2/23           3     3       3      3         3       3       3     3    3\n4/7            3     3       3      3         3       3       3     3    3\n11/21          3     3       3      3         3       3       3     3    3\n8/3            3     3       3      3         3       3       3     3    3\n          VERNE MYRNA KATHERINE SYLVIA NORA HELEN DOROTHY OLIVIA FLORA 6/27 3/2\nEVELYN        2     2         2      2    2     2       2      2     2    1   1\nLAURA         2     2         2      2    2     2       2      4     4    1   1\nTHERESA       2     2         2      2    2     2       2      2     2    3   1\nBRENDA        2     2         2      2    2     2       2      4     4    1   3\nCHARLOTTE     2     4         4      2    2     2       4      4     4    3   3\nFRANCES       2     2         2      2    2     2       2      4     4    3   3\nELEANOR       2     2         2      2    2     2       2      4     4    3   3\nPEARL         2     2         2      2    2     2       2      2     2    3   3\nRUTH          2     2         2      2    2     2       2      2     2    3   3\nVERNE         0     2         2      2    2     2       2      2     2    3   3\nMYRNA         2     0         2      2    2     2       2      2     2    3   3\nKATHERINE     2     2         0      2    2     2       2      2     2    3   3\nSYLVIA        2     2         2      0    2     2       2      2     2    3   3\nNORA          2     2         2      2    0     2       2      2     2    3   3\nHELEN         2     2         2      2    2     0       2      2     2    3   3\nDOROTHY       2     2         2      2    2     2       0      2     2    3   3\nOLIVIA        2     2         2      2    2     2       2      0     2    3   3\nFLORA         2     2         2      2    2     2       2      2     0    3   3\n6/27          3     3         3      3    3     3       3      3     3    0   2\n3/2           3     3         3      3    3     3       3      3     3    2   0\n4/12          3     3         3      3    3     3       3      3     3    2   2\n9/26          3     3         3      3    3     3       3      3     3    2   2\n2/25          3     3         3      3    3     3       3      3     3    2   2\n5/19          3     3         3      3    1     3       3      3     3    2   2\n3/15          1     3         3      1    1     1       3      3     3    2   2\n9/16          1     1         1      1    3     1       1      3     3    2   2\n4/8           1     1         1      1    1     3       1      1     1    2   2\n6/10          3     1         1      1    1     1       3      3     3    4   4\n2/23          3     3         3      3    1     1       3      1     1    4   4\n4/7           1     1         1      1    1     1       3      3     3    4   4\n11/21         3     3         1      1    1     3       3      3     3    4   4\n8/3           3     3         1      1    1     3       3      3     3    4   4\n          4/12 9/26 2/25 5/19 3/15 9/16 4/8 6/10 2/23 4/7 11/21 8/3\nEVELYN       1    1    1    1    3    1   1    3    3   3     3   3\nLAURA        1    3    1    1    1    1   3    3    3   3     3   3\nTHERESA      1    1    1    1    1    1   1    3    3   3     3   3\nBRENDA       1    1    1    1    1    1   3    3    3   3     3   3\nCHARLOTTE    1    1    1    3    1    3   3    3    3   3     3   3\nFRANCES      1    3    1    1    3    1   3    3    3   3     3   3\nELEANOR      3    3    1    1    1    1   3    3    3   3     3   3\nPEARL        3    3    3    1    3    1   1    3    3   3     3   3\nRUTH         3    3    1    3    1    1   1    3    3   3     3   3\nVERNE        3    3    3    3    1    1   1    3    3   1     3   3\nMYRNA        3    3    3    3    3    1   1    1    3   1     3   3\nKATHERINE    3    3    3    3    3    1   1    1    3   1     1   1\nSYLVIA       3    3    3    3    1    1   1    1    3   1     1   1\nNORA         3    3    3    1    1    3   1    1    1   1     1   1\nHELEN        3    3    3    3    1    1   3    1    1   1     3   3\nDOROTHY      3    3    3    3    3    1   1    3    3   3     3   3\nOLIVIA       3    3    3    3    3    3   1    3    1   3     3   3\nFLORA        3    3    3    3    3    3   1    3    1   3     3   3\n6/27         2    2    2    2    2    2   2    4    4   4     4   4\n3/2          2    2    2    2    2    2   2    4    4   4     4   4\n4/12         0    2    2    2    2    2   2    4    4   4     4   4\n9/26         2    0    2    2    2    2   2    4    4   4     4   4\n2/25         2    2    0    2    2    2   2    4    4   4     4   4\n5/19         2    2    2    0    2    2   2    2    2   2     2   2\n3/15         2    2    2    2    0    2   2    2    2   2     2   2\n9/16         2    2    2    2    2    0   2    2    2   2     2   2\n4/8          2    2    2    2    2    2   0    2    2   2     2   2\n6/10         4    4    4    2    2    2   2    0    2   2     2   2\n2/23         4    4    4    2    2    2   2    2    0   2     2   2\n4/7          4    4    4    2    2    2   2    2    2   0     2   2\n11/21        4    4    4    2    2    2   2    2    2   2     0   2\n8/3          4    4    4    2    2    2   2    2    2   2     2   0\n```\n:::\n:::\n\n\nWhich is a square matrix of dimensions $(M + N) \\times (M + N)$; that's $(18 + 14) \\times (18 + 14) = 32 \\times 32$ in our case. \n\nWe can check in `R`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   dim(distances(g))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 32 32\n```\n:::\n:::\n\n\nAs we can see in the distance matrix, distances between nodes in the same set are even $g_{ii'|jj'} = \\{2, 4, \\ldots\\}$ but distances in nodes in different sets are odd $g_{ij|ji} = \\{1, 3, \\ldots\\}$. Beyond this hiccup, distances can be interpreted in the same way as one-mode networks.\n\n## Closeness Centrality in two-mode Networks\n\nThis means that (unnormalized) closeness centrality works the same way as it does in regular networks:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   round(closeness(g), 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n    0.017     0.015     0.017     0.015     0.013     0.014     0.014     0.014 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n    0.015     0.015     0.014     0.015     0.016     0.017     0.015     0.014 \n   OLIVIA     FLORA      6/27       3/2      4/12      9/26      2/25      5/19 \n    0.012     0.012     0.012     0.012     0.013     0.012     0.014     0.016 \n     3/15      9/16       4/8      6/10      2/23       4/7     11/21       8/3 \n    0.017     0.019     0.018     0.013     0.012     0.013     0.012     0.012 \n```\n:::\n:::\n\n\nWhich is just the inverse of the sums of the distances matrix for people and groups counting their geodesic distances to nodes of both sets. \n\nHowever, as @borgatti_everett97 note, if we want *normalized* closeness centralities, we can't use the off-the-shelf normalization for one-mode networks in `igraph` ($n-1$) as it will give us non-sense results because now we have two sets of nodes.\n\nInstead, we need to normalize the closeness score for each node set by its theoretical maximum for each node set. \n\nFor people, this is:\n\n$$\nN + 2(M - 1)\n$$\n\nAnd for groups/events this same quantity is:\n\n$$\nM + 2(N - 1)\n$$\n\nThe basic idea is that nodes can be at minimum geodesic distance $g = 1$ from nodes of the other set (for people, groups; for groups, people) and at minimum distance $g = 2$ from nodes of their own set, with their own presence eliminated by subtraction [@borgatti_everett97]. \n\nIn our case, we create a normalization vector with these quantities of length $M + N$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   M <- nrow(A)\n   N <- ncol(A)\n   n.p <- N + 2 * (M - 1)\n   n.e <- M + 2 * (N - 1)\n   norm.vec <- c(rep(n.p, M), rep(n.e, N))\n```\n:::\n\n\nAnd normalized closeness is:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   round(norm.vec/rowSums(distances(g)), 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n    0.800     0.727     0.800     0.727     0.600     0.667     0.667     0.667 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n    0.706     0.706     0.686     0.727     0.774     0.800     0.727     0.649 \n   OLIVIA     FLORA      6/27       3/2      4/12      9/26      2/25      5/19 \n    0.585     0.585     0.524     0.524     0.564     0.537     0.595     0.688 \n     3/15      9/16       4/8      6/10      2/23       4/7     11/21       8/3 \n    0.733     0.846     0.786     0.550     0.537     0.564     0.524     0.524 \n```\n:::\n:::\n\n\nWhich are the same numbers in @borgatti_everett97 [table 1, column 6].\n\n## Betweenness Centrality in two-mode Networks\n\nAs @borgatti_everett97 also note, the normalizations for betweenness centrality in the two-mode case are a bit more involved. This is because they depend on which node set is larger than the other. \n\nFor the larger node set, which in our case is the people, the normalization is:\n\n$$\n2(M-1)(N-1)\n$$\n\nFor the smaller node set, which in our case is the groups/events, the normalization is:\n\n$$\n\\frac{1}{2}(N)(N-1)+\\frac{1}{2}(M-1)(M-2)+(M-1)(N-1)\n$$\n\nRemember that you have to switch this around if you are analyzing a network with more groups than people. \n\nCreating the relevant vectors:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   n.p <- 2*(M-1)*(N-1)\n   n.e <- (1/2)*(N*(N-1))+(1/2)*(M-1)*(M-2)+(M-1)*(N-1)\n   norm.vec <- c(rep(n.p, M), rep(n.e, N))\n```\n:::\n\n\nAnd normalized betweenness is:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   round(betweenness(g)/norm.vec, 4)*100\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n     9.72      5.17      8.82      4.98      1.07      1.08      0.95      0.68 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n     1.69      1.58      1.65      4.77      7.22     11.42      4.27      0.20 \n   OLIVIA     FLORA      6/27       3/2      4/12      9/26      2/25      5/19 \n     0.51      0.51      0.22      0.21      1.84      0.78      3.80      6.56 \n     3/15      9/16       4/8      6/10      2/23       4/7     11/21       8/3 \n    13.07     24.60     22.75      1.15      1.98      1.83      0.23      0.23 \n```\n:::\n:::\n\n\nWhich are (with some slight differences and rounding errors) the same numbers in @borgatti_everett97 [table 2, column 3].\n\n## The Duality of Persons and Groups\n\nRemember that in the one-mode case, multiplying the adjacency matrix times its transpose yields the **common neighbors matrix** $\\mathbf{M}$:\n\n$$\n\\mathbf{M} = \\mathbf{A}\\mathbf{A}^T\n$$\n\nAs famously noted by @breiger74, doing the same for the affiliation matrix of a two-mode network also returns the common-neighbors matrix, but because objects in one mode can only connect to objects in another mode, this also reveals the **duality of persons and groups**: The connections between people are made up of the groups they share, and the connections between groups are revealed by the groups they share.\n\nThus, computing the common neighbors matrix for both persons and groups (also called the **projection** of the two-mode network into each of its modes) produces a one-mode similarity matrix between people and groups, where the similarities are defined by the number of objects in the other mode that they share. \n\nSo for the people the relevant projection is:\n\n$$\n\\mathbf{P} = \\mathbf{A}\\mathbf{A}^T\n$$\n\nAnd for the groups:\n\n$$\n\\mathbf{G} = \\mathbf{A}^T\\mathbf{A}\n$$\n\nWhich in our case yields:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   P <- A %*% t(A)\n   P\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          EVELYN LAURA THERESA BRENDA CHARLOTTE FRANCES ELEANOR PEARL RUTH\nEVELYN         8     6       7      6         3       4       3     3    3\nLAURA          6     7       6      6         3       4       4     2    3\nTHERESA        7     6       8      6         4       4       4     3    4\nBRENDA         6     6       6      7         4       4       4     2    3\nCHARLOTTE      3     3       4      4         4       2       2     0    2\nFRANCES        4     4       4      4         2       4       3     2    2\nELEANOR        3     4       4      4         2       3       4     2    3\nPEARL          3     2       3      2         0       2       2     3    2\nRUTH           3     3       4      3         2       2       3     2    4\nVERNE          2     2       3      2         1       1       2     2    3\nMYRNA          2     1       2      1         0       1       1     2    2\nKATHERINE      2     1       2      1         0       1       1     2    2\nSYLVIA         2     2       3      2         1       1       2     2    3\nNORA           2     2       3      2         1       1       2     2    2\nHELEN          1     2       2      2         1       1       2     1    2\nDOROTHY        2     1       2      1         0       1       1     2    2\nOLIVIA         1     0       1      0         0       0       0     1    1\nFLORA          1     0       1      0         0       0       0     1    1\n          VERNE MYRNA KATHERINE SYLVIA NORA HELEN DOROTHY OLIVIA FLORA\nEVELYN        2     2         2      2    2     1       2      1     1\nLAURA         2     1         1      2    2     2       1      0     0\nTHERESA       3     2         2      3    3     2       2      1     1\nBRENDA        2     1         1      2    2     2       1      0     0\nCHARLOTTE     1     0         0      1    1     1       0      0     0\nFRANCES       1     1         1      1    1     1       1      0     0\nELEANOR       2     1         1      2    2     2       1      0     0\nPEARL         2     2         2      2    2     1       2      1     1\nRUTH          3     2         2      3    2     2       2      1     1\nVERNE         4     3         3      4    3     3       2      1     1\nMYRNA         3     4         4      4    3     3       2      1     1\nKATHERINE     3     4         6      6    5     3       2      1     1\nSYLVIA        4     4         6      7    6     4       2      1     1\nNORA          3     3         5      6    8     4       1      2     2\nHELEN         3     3         3      4    4     5       1      1     1\nDOROTHY       2     2         2      2    1     1       2      1     1\nOLIVIA        1     1         1      1    2     1       1      2     2\nFLORA         1     1         1      1    2     1       1      2     2\n```\n:::\n\n```{.r .cell-code}\n   G <- t(A) %*% A\n   G\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      6/27 3/2 4/12 9/26 2/25 5/19 3/15 9/16 4/8 6/10 2/23 4/7 11/21 8/3\n6/27     3   2    3    2    3    3    2    3   1    0    0   0     0   0\n3/2      2   3    3    2    3    3    2    3   2    0    0   0     0   0\n4/12     3   3    6    4    6    5    4    5   2    0    0   0     0   0\n9/26     2   2    4    4    4    3    3    3   2    0    0   0     0   0\n2/25     3   3    6    4    8    6    6    7   3    0    0   0     0   0\n5/19     3   3    5    3    6    8    5    7   4    1    1   1     1   1\n3/15     2   2    4    3    6    5   10    8   5    3    2   4     2   2\n9/16     3   3    5    3    7    7    8   14   9    4    1   5     2   2\n4/8      1   2    2    2    3    4    5    9  12    4    3   5     3   3\n6/10     0   0    0    0    0    1    3    4   4    5    2   5     3   3\n2/23     0   0    0    0    0    1    2    1   3    2    4   2     1   1\n4/7      0   0    0    0    0    1    4    5   5    5    2   6     3   3\n11/21    0   0    0    0    0    1    2    2   3    3    1   3     3   3\n8/3      0   0    0    0    0    1    2    2   3    3    1   3     3   3\n```\n:::\n:::\n\n\nThe off-diagonal entries of these square person by person (group by group) matrices is the number of groups (people) shared by each person (group) and the diagonals are the number of memberships of each person (the size of each group/event).\n\n## Normalized Vertex Similarity Metrics\n\nNote that the one-mode projections are unnormalized similarity matrices just like in the case of regular networks. That means that if we have the degrees of nodes in each mode, we can transform this matrix into any of the **normalized vertex similarity** metrics we discussed before, including Jaccard, Cosine, Dice, LHN, and so on. \n\nThus repackaging our vertex similarity function for the two-mode case, we have:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   vertex.sim <- function(x) {\n      A <- as.matrix(as_biadjacency_matrix(x))\n      M <- nrow(A) #number of persons\n      N <- ncol(A) #number of groups\n      p.d <- rowSums(A) #person degrees\n      g.d <- colSums(A) #group degrees\n      P <- A %*% t(A) #person projection\n      G <- t(A) %*% A #group projection\n      J.p <- diag(1, M, M)\n      J.g <- diag(1, N, N)\n      C.p <- diag(1, M, M)\n      C.g <- diag(1, N, N)\n      D.p <- diag(1, M, M)\n      D.g <- diag(1, N, N)\n      L.p <- diag(1, M, M)\n      L.g <- diag(1, N, N)\n      for (i in 1:M) {\n         for (j in 1:M) {\n            if (i < j) {\n               J.p[i,j] <- P[i,j]/(P[i,j] + p.d[i] + p.d[j])\n               J.p[j,i] <- P[i,j]/(P[i,j] + p.d[i] + p.d[j])\n               C.p[i,j] <- P[i,j]/(sqrt(p.d[i] * p.d[j]))\n               C.p[j,i] <- P[i,j]/(sqrt(p.d[i] * p.d[j]))\n               D.p[i,j] <- (2*P[i,j])/(2*P[i,j] + p.d[i] + p.d[j])\n               D.p[j,i] <- (2*P[i,j])/(2*P[i,j] + p.d[i] + p.d[j])\n               L.p[i,j] <- P[i,j]/(p.d[i] * p.d[j])\n               L.p[j,i] <- P[i,j]/(p.d[i] * p.d[j])\n               }\n            }\n         }\n      for (i in 1:N) {\n         for (j in 1:N) {\n            if (i < j) {\n               J.g[i,j] <- G[i,j]/(G[i,j] + g.d[i] + g.d[j])\n               J.g[j,i] <- G[i,j]/(G[i,j] + g.d[i] + g.d[j])\n               C.g[i,j] <- G[i,j]/(sqrt(g.d[i] * g.d[j]))\n               C.g[j,i] <- G[i,j]/(sqrt(g.d[i] * g.d[j]))\n               D.g[i,j] <- (2*G[i,j])/(2*G[i,j] + g.d[i] + g.d[j])\n               D.g[j,i] <- (2*G[i,j])/(2*G[i,j] + g.d[i] + g.d[j])\n               L.g[i,j] <- G[i,j]/(g.d[i] * g.d[j])\n               L.g[j,i] <- G[i,j]/(g.d[i] * g.d[j])\n               }\n            }\n         }\n      return(list(J.p = J.p, C.p = C.p, D.p = D.p, L.p = L.p,\n                  J.g = J.g, C.g = C.g, D.g = D.g, L.g = L.g))\n      }\n```\n:::\n\n\nUsing this function to compute the Jaccard similarity between people yields:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   J.p <- vertex.sim(g)$J.p\n   rownames(J.p) <- rownames(A)\n   colnames(J.p) <- rownames(A)\n   round(J.p, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          EVELYN LAURA THERESA BRENDA CHARLOTTE FRANCES ELEANOR PEARL RUTH\nEVELYN      1.00  0.29    0.30   0.29      0.20    0.25    0.20  0.21 0.20\nLAURA       0.29  1.00    0.29   0.30      0.21    0.27    0.27  0.17 0.21\nTHERESA     0.30  0.29    1.00   0.29      0.25    0.25    0.25  0.21 0.25\nBRENDA      0.29  0.30    0.29   1.00      0.27    0.27    0.27  0.17 0.21\nCHARLOTTE   0.20  0.21    0.25   0.27      1.00    0.20    0.20  0.00 0.20\nFRANCES     0.25  0.27    0.25   0.27      0.20    1.00    0.27  0.22 0.20\nELEANOR     0.20  0.27    0.25   0.27      0.20    0.27    1.00  0.22 0.27\nPEARL       0.21  0.17    0.21   0.17      0.00    0.22    0.22  1.00 0.22\nRUTH        0.20  0.21    0.25   0.21      0.20    0.20    0.27  0.22 1.00\nVERNE       0.14  0.15    0.20   0.15      0.11    0.11    0.20  0.22 0.27\nMYRNA       0.14  0.08    0.14   0.08      0.00    0.11    0.11  0.22 0.20\nKATHERINE   0.12  0.07    0.12   0.07      0.00    0.09    0.09  0.18 0.17\nSYLVIA      0.12  0.12    0.17   0.12      0.08    0.08    0.15  0.17 0.21\nNORA        0.11  0.12    0.16   0.12      0.08    0.08    0.14  0.15 0.14\nHELEN       0.07  0.14    0.13   0.14      0.10    0.10    0.18  0.11 0.18\nDOROTHY     0.17  0.10    0.17   0.10      0.00    0.14    0.14  0.29 0.25\nOLIVIA      0.09  0.00    0.09   0.00      0.00    0.00    0.00  0.17 0.14\nFLORA       0.09  0.00    0.09   0.00      0.00    0.00    0.00  0.17 0.14\n          VERNE MYRNA KATHERINE SYLVIA NORA HELEN DOROTHY OLIVIA FLORA\nEVELYN     0.14  0.14      0.12   0.12 0.11  0.07    0.17   0.09  0.09\nLAURA      0.15  0.08      0.07   0.12 0.12  0.14    0.10   0.00  0.00\nTHERESA    0.20  0.14      0.12   0.17 0.16  0.13    0.17   0.09  0.09\nBRENDA     0.15  0.08      0.07   0.12 0.12  0.14    0.10   0.00  0.00\nCHARLOTTE  0.11  0.00      0.00   0.08 0.08  0.10    0.00   0.00  0.00\nFRANCES    0.11  0.11      0.09   0.08 0.08  0.10    0.14   0.00  0.00\nELEANOR    0.20  0.11      0.09   0.15 0.14  0.18    0.14   0.00  0.00\nPEARL      0.22  0.22      0.18   0.17 0.15  0.11    0.29   0.17  0.17\nRUTH       0.27  0.20      0.17   0.21 0.14  0.18    0.25   0.14  0.14\nVERNE      1.00  0.27      0.23   0.27 0.20  0.25    0.25   0.14  0.14\nMYRNA      0.27  1.00      0.29   0.27 0.20  0.25    0.25   0.14  0.14\nKATHERINE  0.23  0.29      1.00   0.32 0.26  0.21    0.20   0.11  0.11\nSYLVIA     0.27  0.27      0.32   1.00 0.29  0.25    0.18   0.10  0.10\nNORA       0.20  0.20      0.26   0.29 1.00  0.24    0.09   0.17  0.17\nHELEN      0.25  0.25      0.21   0.25 0.24  1.00    0.12   0.12  0.12\nDOROTHY    0.25  0.25      0.20   0.18 0.09  0.12    1.00   0.20  0.20\nOLIVIA     0.14  0.14      0.11   0.10 0.17  0.12    0.20   1.00  0.33\nFLORA      0.14  0.14      0.11   0.10 0.17  0.12    0.20   0.33  1.00\n```\n:::\n:::\n\n\n## Structural Equivalence\n\nAnd, of course, once we have a similarity we can cluster nodes based on approximate structural equivalence by transforming proximities to distances:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   D <- as.dist(1- J.p)\n   hc.p <- hclust(D, method = \"ward.D2\")\n   plot(hc.p)\n```\n\n::: {.cell-output-display}\n![](handout7_files/figure-html/unnamed-chunk-38-1.png){width=672}\n:::\n:::\n\n\nAnd for events:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   J.g <- vertex.sim(g)$J.g\n   rownames(J.g) <- colnames(A)\n   colnames(J.g) <- colnames(A)\n   D <- as.dist(1- J.g)\n   hc.g <- hclust(D, method = \"ward.D2\")\n   plot(hc.g)\n```\n\n::: {.cell-output-display}\n![](handout7_files/figure-html/unnamed-chunk-39-1.png){width=672}\n:::\n:::\n\n\nWe can then derive cluster memberships for people and groups from the `hclust` object:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   library(dendextend)\n   clus.p <- sort(cutree(hc.p, 4)) #selecting four clusters for people\n   clus.p\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n        1         1         1         1         1         1         1         2 \n     RUTH     VERNE   DOROTHY     MYRNA KATHERINE    SYLVIA      NORA     HELEN \n        2         2         2         3         3         3         3         3 \n   OLIVIA     FLORA \n        4         4 \n```\n:::\n\n```{.r .cell-code}\n   clus.g <- sort(cutree(hc.g, 3)) #selecting three clusters for groups\n   clus.g\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n 6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10  2/23   4/7 11/21 \n    1     1     1     1     1     1     2     2     2     3     3     3     3 \n  8/3 \n    3 \n```\n:::\n:::\n\n\nAnd finally we can block the original affiliation matrix, as recommended by @everett_borgatti13 [p. 210, table 5]:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   library(ggcorrplot)\n   p <- ggcorrplot(t(A[names(clus.p), names(clus.g)]), \n                   colors = c(\"white\", \"white\", \"red\")) \n   p <- p + theme(legend.position = \"none\", \n                  axis.text.y = element_text(size = 8),\n                  axis.text.x = element_text(size = 8, angle = 0),\n                  )\n   p <- p + scale_x_discrete(position = \"top\") \n   p <- p + geom_hline(yintercept = 7.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_hline(yintercept = 11.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_hline(yintercept = 16.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_vline(xintercept = 6.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_vline(xintercept = 9.5, linewidth = 2, color = \"blue\")\n   p\n```\n\n::: {.cell-output-display}\n![](handout7_files/figure-html/unnamed-chunk-41-1.png){width=672}\n:::\n:::\n\n\nWhich reveals a number of almost complete (one-blocks) and almost null (zero-blocks) in the social structure, with a reduced image matrix that looks like:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   library(kableExtra)\n   IM <- matrix(0, 4, 3)\n   IM[1, ] <- c(0, 1, 0)\n   IM[2, ] <- c(0, 1, 1)\n   IM[3, ] <- c(0, 1, 0)\n   IM[4, ] <- c(1, 1, 0)\n   rownames(IM) <- c(\"P.Block1\", \"P.Block2\", \"P.Block3\", \"P.Block4\")\n   colnames(IM) <- c(\"E.Block1\", \"E.Block2\", \"E.Block3\")\n   kbl(IM, format = \"html\", , align = \"c\") %>% \n      column_spec(1, bold = TRUE) %>% \n      kable_styling(full_width = TRUE,\n                     bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-hover table-condensed table-responsive\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:center;\"> E.Block1 </th>\n   <th style=\"text-align:center;\"> E.Block2 </th>\n   <th style=\"text-align:center;\"> E.Block3 </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> P.Block1 </td>\n   <td style=\"text-align:center;\"> 0 </td>\n   <td style=\"text-align:center;\"> 1 </td>\n   <td style=\"text-align:center;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> P.Block2 </td>\n   <td style=\"text-align:center;\"> 0 </td>\n   <td style=\"text-align:center;\"> 1 </td>\n   <td style=\"text-align:center;\"> 1 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> P.Block3 </td>\n   <td style=\"text-align:center;\"> 0 </td>\n   <td style=\"text-align:center;\"> 1 </td>\n   <td style=\"text-align:center;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> P.Block4 </td>\n   <td style=\"text-align:center;\"> 1 </td>\n   <td style=\"text-align:center;\"> 1 </td>\n   <td style=\"text-align:center;\"> 0 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n## Generalized Vertex Similarity\n\nRecall that vertex similarity works using the principle of *structural equivalence*: Two people are similar if the choose the same objects (groups), and two objects (groups) are similar if they are chosen by the same people. \n\nWe can, like we did in the one mode case, be after a more general version of similarity, which says that: Two people are similar if they choose *similar* objects, and two objects are similar if they are chosen by *similar* people.\n\nThis leads to the same problem setup that inspired the **SimRank** approach [@jeh_widom02]. \n\nA function to compute the SimRank similarity between nodes in a two mode network goes as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   TM.SimRank <- function(A, C = 0.8, iter = 10) {\n        nr <- nrow(A)\n        nc <- ncol(A)\n        dr <- rowSums(A)\n        dc <- colSums(A)\n        Sr <- diag(1, nr, nr) #baseline similarity: every node maximally similar to themselves\n        Sc <- diag(1, nc, nc) #baseline similarity: every node maximally similar to themselves\n        rn <- rownames(A)\n        cn <- colnames(A)\n        rownames(Sr) <- rn\n        colnames(Sr) <- rn\n        rownames(Sc) <- cn\n        colnames(Sc) <- cn\n        m <- 1\n        while(m < iter) {\n             Sr.pre <- Sr\n             Sc.pre <- Sc\n             for(i in 1:nr) {\n                  for(j in 1:nr) {\n                       if (i != j) {\n                            a <- names(which(A[i, ] == 1)) #objects chosen by i\n                            b <- names(which(A[j, ] == 1)) #objects chosen by j\n                            Scij <- 0\n                            for (k in a) {\n                                 for (l in b) {\n                                      Scij <- Scij + Sc[k, l] #i's similarity to j\n                                 }\n                            }\n                            Sr[i, j] <- C/(dr[i] * dr[j]) * Scij\n                       }\n                  }\n             }\n             for(i in 1:nc) {\n                  for(j in 1:nc) {\n                       if (i != j) {\n                            a <- names(which(A[, i] == 1)) #people who chose object i\n                            b <- names(which(A[, j] == 1)) #people who chose object j\n                            Srij <- 0\n                            for (k in a) {\n                                 for (l in b) {\n                                      Srij <- Srij + Sr[k, l] #i's similarity to j\n                                 }\n                            }\n                            Sc[i, j] <- C/(dc[i] * dc[j]) * Srij\n                       }\n                  }\n             }\n             m <- m + 1\n        }\n        return(list(Sr = Sr, Sc = Sc))\n   }\n```\n:::\n\n\nThis function takes the bi-adjacency matrix $\\mathbf{A}$ as input and returns two similarity matrices: One for the people (row objects) and the other one for the groups (column objects).\n\nHere's how that would work in the Southern Women data. First we compute the SimRank scores:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   sim.res <- TM.SimRank(A)\n```\n:::\n\n\nThen we peek inside the people similarity matrix:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   round(sim.res$Sr[1:10, 1:10], 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          EVELYN LAURA THERESA BRENDA CHARLOTTE FRANCES ELEANOR PEARL  RUTH\nEVELYN     1.000 0.267   0.262  0.266     0.259   0.275   0.248 0.255 0.237\nLAURA      0.267 1.000   0.262  0.277     0.270   0.287   0.280 0.237 0.247\nTHERESA    0.262 0.262   1.000  0.262     0.273   0.270   0.264 0.254 0.256\nBRENDA     0.266 0.277   0.262  1.000     0.290   0.287   0.279 0.235 0.246\nCHARLOTTE  0.259 0.270   0.273  0.290     1.000   0.276   0.269 0.175 0.256\nFRANCES    0.275 0.287   0.270  0.287     0.276   1.000   0.305 0.280 0.256\nELEANOR    0.248 0.280   0.264  0.279     0.269   0.305   1.000 0.279 0.294\nPEARL      0.255 0.237   0.254  0.235     0.175   0.280   0.279 1.000 0.279\nRUTH       0.237 0.247   0.256  0.246     0.256   0.256   0.294 0.279 1.000\nVERNE      0.201 0.207   0.222  0.206     0.198   0.202   0.246 0.276 0.288\n          VERNE\nEVELYN    0.201\nLAURA     0.207\nTHERESA   0.222\nBRENDA    0.206\nCHARLOTTE 0.198\nFRANCES   0.202\nELEANOR   0.246\nPEARL     0.276\nRUTH      0.288\nVERNE     1.000\n```\n:::\n:::\n\n\nAnd the group similarity matrix:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   round(sim.res$Sc[1:10, 1:10], 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10\n6/27 1.000 0.343 0.314 0.312 0.287 0.277 0.224 0.226 0.178 0.137\n3/2  0.343 1.000 0.312 0.311 0.285 0.276 0.224 0.228 0.200 0.141\n4/12 0.314 0.312 1.000 0.314 0.288 0.265 0.226 0.220 0.179 0.138\n9/26 0.312 0.311 0.314 1.000 0.287 0.256 0.230 0.214 0.186 0.137\n2/25 0.287 0.285 0.288 0.287 1.000 0.260 0.235 0.226 0.187 0.146\n5/19 0.277 0.276 0.265 0.256 0.260 1.000 0.224 0.226 0.200 0.171\n3/15 0.224 0.224 0.226 0.230 0.235 0.224 1.000 0.221 0.204 0.209\n9/16 0.226 0.228 0.220 0.214 0.226 0.226 0.221 1.000 0.221 0.214\n4/8  0.178 0.200 0.179 0.186 0.187 0.200 0.204 0.221 1.000 0.234\n6/10 0.137 0.141 0.138 0.137 0.146 0.171 0.209 0.214 0.234 1.000\n```\n:::\n:::\n\n\nLike before we can use these results to define two sets of distances:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   D.p <- as.dist(1 - sim.res$Sr)\n   D.g <- as.dist(1 - sim.res$Sc)\n```\n:::\n\n\nSubject to hierarchical clustering:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   hc.p <- hclust(D.p, method = \"ward.D2\")\n   hc.g <- hclust(D.g, method = \"ward.D2\")\n```\n:::\n\n\nAnd plot:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   plot(hc.p)\n```\n\n::: {.cell-output-display}\n![](handout7_files/figure-html/unnamed-chunk-49-1.png){width=672}\n:::\n\n```{.r .cell-code}\n   plot(hc.g)\n```\n\n::: {.cell-output-display}\n![](handout7_files/figure-html/unnamed-chunk-49-2.png){width=672}\n:::\n:::\n\n\nGet cluster memberships for people and groups from the `hclust` object:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   clus.p <- sort(cutree(hc.p, 4)) #selecting four clusters for people\n   clus.p\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n        1         1         1         1         1         1         1         2 \n     RUTH     VERNE   DOROTHY     MYRNA KATHERINE    SYLVIA      NORA     HELEN \n        2         2         2         3         3         3         3         3 \n   OLIVIA     FLORA \n        4         4 \n```\n:::\n\n```{.r .cell-code}\n   clus.g <- sort(cutree(hc.g, 3)) #selecting three clusters for groups\n   clus.g\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n 6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  2/23  6/10   4/7 11/21 \n    1     1     1     1     1     1     2     2     2     2     3     3     3 \n  8/3 \n    3 \n```\n:::\n:::\n\n\nAnd block the bi-adjacency matrix:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   p <- ggcorrplot(t(A[names(clus.p), names(clus.g)]), \n                   colors = c(\"white\", \"white\", \"red\")) \n   p <- p + theme(legend.position = \"none\", \n                  axis.text.y = element_text(size = 8),\n                  axis.text.x = element_text(size = 8, angle = 0),\n                  )\n   p <- p + scale_x_discrete(position = \"top\") \n   p <- p + geom_hline(yintercept = 7.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_hline(yintercept = 11.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_hline(yintercept = 16.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_vline(xintercept = 8.5, linewidth = 2, color = \"blue\")\n   p\n```\n\n::: {.cell-output-display}\n![](handout7_files/figure-html/unnamed-chunk-51-1.png){width=672}\n:::\n:::\n\n\nWhich produces a familiar block partition of persons and events. \n\n## Eigenvector Status\n\nMeasures of status and prestige are particularly applicable to two-mode networks. The reason is that the *reflective* principle behind these measures interacts nicely with the *duality* principle. \n\nFor instance, when it comes to **eigenvector**-style measures, the neat idea that *people are central if they belong to central groups and groups and central if their members are central people* (with people centrality defined by membership in central groups) can be effectively captured by these metrics [@bonacich91].\n\nThus if $x$ are the status scores for people, and $y$ are the status scores for groups, then the $x$ scores should be given by the sum of the $y$ scores of the groups each person belongs, and the $y$ scores should be given by the sum of the $x$ scores of their members. \n\nIn mathese:\n\n$$\nx = \\mathbf{A}^Ty\n$$\n\n$$\ny = \\mathbf{A}x\n$$\n\nOnce again, producing another instance of a cat chasing its own tail (we need to know the values of $y$ to figure out the values of $x$ and we need to know the values of $x$ to figure out the values of $y$). \n\nHow do we proceed? Well, let's bring back our trusty status distribution game:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   status1 <- function(A) {\n      n <- nrow(A) #number of actors\n      x <- rep(1, n) #initial status vector set to all ones\n      w <- 1 \n      k <- 0 #initializing counter\n      while (w > 0.0001) {\n          o.x <- x #old status scores\n          x <- A %*% x #new scores a function of old scores and adjacency matrix\n          x <- x/norm(x, type = \"E\") #normalizing new status scores\n          w <- abs(sum(abs(x) - abs(o.x))) #diff. between new and old scores\n          k <- k + 1 #incrementing while counter\n      }\n   return(as.vector(x))\n   }\n```\n:::\n\n\nThen the main question is over what matrix will the status game be played for *both* people and groups?\n\nAs @bonacich91 noted, the projection matrices of @breiger74 are natural candidates for this task. Let's try it out.\n\nFor people this would be:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   p.s <- status1(P)\n   names(p.s) <- rownames(P)\n   round(p.s, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n    0.335     0.309     0.371     0.313     0.168     0.209     0.228     0.180 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n    0.236     0.218     0.187     0.220     0.277     0.264     0.201     0.131 \n   OLIVIA     FLORA \n    0.070     0.070 \n```\n:::\n:::\n\n\nAnd for groups:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   g.s <- status1(G)\n   names(g.s) <- colnames(A)\n   round(g.s, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n 6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10  2/23   4/7 11/21 \n0.142 0.150 0.253 0.176 0.322 0.328 0.384 0.507 0.380 0.170 0.090 0.203 0.113 \n  8/3 \n0.113 \n```\n:::\n:::\n\n\nLo and behold, these are the status scores we seek. It turns out they can be computed by figuring out the leading eigenvector (what our status game does for any matrix) of the Breiger projection matrices [@bonacich91]:\n\n$$\n\\lambda x = (\\mathbf{A}\\mathbf{A}^T)x\n$$\n\n$$\n\\lambda y = (\\mathbf{A}^T\\mathbf{A})y\n$$\n\nNeat! The scores are also readily interpretable: The most central people belong to the most central (largest membership) groups and the most central groups are the ones with the most central (highest activity) members. \n\nIn the Southern Women data the dual centralities are:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   library(kableExtra)\n   p.dat <- data.frame(People = rownames(A), Eig.Cent = round(p.s, 3))\n   p.dat <- p.dat[order(p.dat$Eig.Cent, decreasing = TRUE), ]\n   kbl(p.dat, format = \"html\", , align = c(\"l\", \"c\"), row.names = FALSE) %>% \n      column_spec(1, bold = TRUE) %>% \n      kable_styling(full_width = TRUE,\n                     bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-hover table-condensed table-responsive\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> People </th>\n   <th style=\"text-align:center;\"> Eig.Cent </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> THERESA </td>\n   <td style=\"text-align:center;\"> 0.371 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> EVELYN </td>\n   <td style=\"text-align:center;\"> 0.335 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> BRENDA </td>\n   <td style=\"text-align:center;\"> 0.313 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> LAURA </td>\n   <td style=\"text-align:center;\"> 0.309 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> SYLVIA </td>\n   <td style=\"text-align:center;\"> 0.277 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> NORA </td>\n   <td style=\"text-align:center;\"> 0.264 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> RUTH </td>\n   <td style=\"text-align:center;\"> 0.236 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> ELEANOR </td>\n   <td style=\"text-align:center;\"> 0.228 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> KATHERINE </td>\n   <td style=\"text-align:center;\"> 0.220 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> VERNE </td>\n   <td style=\"text-align:center;\"> 0.218 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> FRANCES </td>\n   <td style=\"text-align:center;\"> 0.209 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> HELEN </td>\n   <td style=\"text-align:center;\"> 0.201 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> MYRNA </td>\n   <td style=\"text-align:center;\"> 0.187 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> PEARL </td>\n   <td style=\"text-align:center;\"> 0.180 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> CHARLOTTE </td>\n   <td style=\"text-align:center;\"> 0.168 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> DOROTHY </td>\n   <td style=\"text-align:center;\"> 0.131 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> OLIVIA </td>\n   <td style=\"text-align:center;\"> 0.070 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> FLORA </td>\n   <td style=\"text-align:center;\"> 0.070 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n   g.dat <- data.frame(Groups = colnames(A), Eig.Cent = round(g.s, 3))\n   g.dat <- g.dat[order(g.dat$Eig.Cent, decreasing = TRUE), ]\n   kbl(g.dat, format = \"html\", align = c(\"l\", \"c\"), row.names = FALSE) %>% \n      column_spec(1, bold = TRUE) %>% \n      kable_styling(full_width = TRUE,\n                     bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-hover table-condensed table-responsive\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Groups </th>\n   <th style=\"text-align:center;\"> Eig.Cent </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> 9/16 </td>\n   <td style=\"text-align:center;\"> 0.507 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> 3/15 </td>\n   <td style=\"text-align:center;\"> 0.384 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> 4/8 </td>\n   <td style=\"text-align:center;\"> 0.380 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> 5/19 </td>\n   <td style=\"text-align:center;\"> 0.328 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> 2/25 </td>\n   <td style=\"text-align:center;\"> 0.322 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> 4/12 </td>\n   <td style=\"text-align:center;\"> 0.253 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> 4/7 </td>\n   <td style=\"text-align:center;\"> 0.203 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> 9/26 </td>\n   <td style=\"text-align:center;\"> 0.176 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> 6/10 </td>\n   <td style=\"text-align:center;\"> 0.170 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> 3/2 </td>\n   <td style=\"text-align:center;\"> 0.150 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> 6/27 </td>\n   <td style=\"text-align:center;\"> 0.142 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> 11/21 </td>\n   <td style=\"text-align:center;\"> 0.113 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> 8/3 </td>\n   <td style=\"text-align:center;\"> 0.113 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> 2/23 </td>\n   <td style=\"text-align:center;\"> 0.090 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n## Core and Periphery\n\nAs noted by @borgatti_everett00, the Bonacich Eigenvector scores are also a model of a core-periphery partition in the two-mode network. This is already evident in the *definition* of the dual centralities: Popular actors (who participate in many events) make the events they participate in more central, and central events (that have many actors) become central when they attract the popular kids. \n\nSo the Eigenvector scores can be used to partition any two-mode network into a core (popular kids, popular events) and a periphery (less popular kids, less popular events). All we need to do to see the partition is to re-order the rows and columns of the affiliation matrix according to the value of the magnitude of the Eigenvector scores for people and events:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   p <- ggcorrplot(t(A[order(p.s), order(g.s)]), \n                   colors = c(\"white\", \"white\", \"red\")) \n   p <- p + theme(legend.position = \"none\", \n                  axis.text.y = element_text(size = 8),\n                  axis.text.x = element_text(size = 8, angle = 0),\n                  )\n   p <- p + scale_x_discrete(position = \"top\") \n   p <- p + geom_hline(yintercept = 10.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_vline(xintercept = 8.5, linewidth = 2, color = \"blue\")\n   p\n```\n\n::: {.cell-output-display}\n![](handout7_files/figure-html/unnamed-chunk-57-1.png){width=672}\n:::\n:::\n\n\nHere the upper-right block reveals the core actors and events in the network; namely, the events almost universally attended by the most active participants [@everett_borgatti13, table 2].\n\n\n## PageRank Status \nWe can, of course, also play our status game with degree-normalized versions of the affiliation matrix (a.k.a. PageRank).\n\nFirst we need to create **row stochastic** versions of $\\mathbf{A}$ and $\\mathbf{A}^T$. Recall that a matrix is row stochastic if their rows sum to one. \n\nFor the people, we can do this by taking the original affiliation matrix, and pre-multiplying it by a **diagonal square matrix** $\\mathbf{D}_P^{-1}$ of dimensions $M \\times M$ containing the *inverse* of the degrees of each person in the affiliation network along the diagonals and zeros everywhere else, yielding the row-stochastic matrix $\\mathbf{P}_P$ of dimensions $M \\times N$:\n\n$$\n\\mathbf{P}_P = \\mathbf{D}_P^{-1}\\mathbf{A}\n$$\n\nAnd we can do the same with the groups, except that we pre-multiply the *transpose* of the original affiliation matrix by $\\mathbf{D}_G^{-1}$ which is an $N \\times N$ matrix containing the inverse of the size of each group along the diagonals and zero everywhere else, this yields the matrix $\\mathbf{P}_G$ of dimensions $N \\times M$:\n\n$$\n\\mathbf{P}_G = \\mathbf{D}_G^{-1}\\mathbf{A}^T\n$$\n\nIn `R` can compute $\\mathbf{P}_P$ and $\\mathbf{P}_G$ as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   D.p <- diag(1/rowSums(A))\n   P.p <- D.p %*% A\n   D.g <- diag(1/colSums(A))\n   P.g <- D.g %*% t(A)\n```\n:::\n\n\nAnd we can check that both `P.p` (for people) and `P.g` (groups) are row stochastic:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   rowSums(P.p)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n```\n:::\n\n```{.r .cell-code}\n   rowSums(P.g)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n```\n:::\n:::\n\n\nAnd that they are of the predicted dimensions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   dim(P.p)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 18 14\n```\n:::\n\n```{.r .cell-code}\n   dim(P.g)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 14 18\n```\n:::\n:::\n\n\nGreat! Now, we can obtain the *degree-normalized projections* for people by multiplying $\\mathbf{P}_P$ times $\\mathbf{P}_G$:\n\n$$\n\\mathbf{P}_{PP} = \\mathbf{P}_P\\mathbf{P}_G\n$$\n\nWhich produces the matrix $\\mathbf{P}_{PP}$ a square $M \\times M$ matrix containing the *degree-normalized similarities* between each pair of people.\n\nWe then do the same for groups:\n\n$$\n\\mathbf{P}_{GG} = \\mathbf{P}_G\\mathbf{P}_P\n$$\n\nWhich produces the matrix $\\mathbf{P}_{GG}$ a square $N \\times N$ matrix containing the *degree-normalized similarities* between each pair of groups.\n\nIn `R` we obtain these matrices as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   P.pp <- P.p %*% P.g\n   P.gg <- P.g %*% P.p\n   rownames(P.pp) <- colnames(P.pp)\n   rownames(P.gg) <- colnames(P.gg)\n```\n:::\n\n\nWhich are still row stochastic--but now square--matrices:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   rowSums(P.pp)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n        1         1         1         1         1         1         1         1 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n        1         1         1         1         1         1         1         1 \n   OLIVIA     FLORA \n        1         1 \n```\n:::\n\n```{.r .cell-code}\n   rowSums(P.gg)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n 6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10  2/23   4/7 11/21 \n    1     1     1     1     1     1     1     1     1     1     1     1     1 \n  8/3 \n    1 \n```\n:::\n\n```{.r .cell-code}\n   dim(P.pp)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 18 18\n```\n:::\n\n```{.r .cell-code}\n   dim(P.gg)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 14 14\n```\n:::\n:::\n\n\nLet's peek inside one of these matrices:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   round(P.pp[1:10, 1:10], 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          EVELYN LAURA THERESA BRENDA CHARLOTTE FRANCES ELEANOR PEARL RUTH\nEVELYN      0.19  0.14    0.14   0.13      0.07    0.06    0.04  0.03 0.03\nLAURA       0.16  0.18    0.13   0.13      0.06    0.07    0.06  0.03 0.04\nTHERESA     0.14  0.12    0.16   0.10      0.08    0.06    0.05  0.03 0.05\nBRENDA      0.15  0.13    0.12   0.17      0.09    0.07    0.06  0.03 0.04\nCHARLOTTE   0.14  0.10    0.16   0.16      0.16    0.07    0.06  0.00 0.06\nFRANCES     0.12  0.12    0.12   0.12      0.07    0.12    0.08  0.05 0.05\nELEANOR     0.08  0.11    0.11   0.11      0.06    0.08    0.11  0.05 0.07\nPEARL       0.09  0.07    0.09   0.07      0.00    0.07    0.07  0.09 0.05\nRUTH        0.07  0.07    0.09   0.07      0.06    0.05    0.07  0.04 0.09\nVERNE       0.04  0.04    0.06   0.04      0.03    0.02    0.04  0.04 0.06\n          VERNE\nEVELYN     0.02\nLAURA      0.02\nTHERESA    0.03\nBRENDA     0.02\nCHARLOTTE  0.03\nFRANCES    0.02\nELEANOR    0.04\nPEARL      0.05\nRUTH       0.06\nVERNE      0.11\n```\n:::\n:::\n\n\nWhat are these numbers? Well, they can be interpreted as *probabilities* that a random walker starting at the row node and, following any sequence of $person-group-person'-group'$ hops, will reach the column person. Thus, higher values indicate an *affinity* or *proximity* between the people (and the groups in the corresponding matrix).\n\nWe can now play the PageRank status game on the transpose of $\\mathbf{P}_{PP}$ and $\\mathbf{P}_{GG}$, just like we did in the one-mode case, to get the scores we want:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   pr.p <- status1(t(P.pp))\n   pr.g <- status1(t(P.gg))\n```\n:::\n\n\nWhich leads to the following PageRank centrality rankings for persons and groups:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   p.dat <- data.frame(People = rownames(A), PageRank.Cent = round(pr.p, 3))\n   p.dat <- p.dat[order(p.dat$PageRank.Cent, decreasing = TRUE), ]\n   kbl(p.dat, format = \"html\", , align = c(\"l\", \"c\"), row.names = FALSE) %>% \n      column_spec(1, bold = TRUE) %>% \n      kable_styling(full_width = TRUE,\n                     bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-hover table-condensed table-responsive\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> People </th>\n   <th style=\"text-align:center;\"> PageRank.Cent </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> EVELYN </td>\n   <td style=\"text-align:center;\"> 0.352 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> THERESA </td>\n   <td style=\"text-align:center;\"> 0.352 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> NORA </td>\n   <td style=\"text-align:center;\"> 0.352 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> LAURA </td>\n   <td style=\"text-align:center;\"> 0.308 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> BRENDA </td>\n   <td style=\"text-align:center;\"> 0.308 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> SYLVIA </td>\n   <td style=\"text-align:center;\"> 0.308 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> KATHERINE </td>\n   <td style=\"text-align:center;\"> 0.264 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> HELEN </td>\n   <td style=\"text-align:center;\"> 0.220 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> CHARLOTTE </td>\n   <td style=\"text-align:center;\"> 0.176 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> FRANCES </td>\n   <td style=\"text-align:center;\"> 0.176 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> ELEANOR </td>\n   <td style=\"text-align:center;\"> 0.176 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> RUTH </td>\n   <td style=\"text-align:center;\"> 0.176 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> VERNE </td>\n   <td style=\"text-align:center;\"> 0.176 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> MYRNA </td>\n   <td style=\"text-align:center;\"> 0.176 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> PEARL </td>\n   <td style=\"text-align:center;\"> 0.132 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> DOROTHY </td>\n   <td style=\"text-align:center;\"> 0.088 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> OLIVIA </td>\n   <td style=\"text-align:center;\"> 0.088 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> FLORA </td>\n   <td style=\"text-align:center;\"> 0.088 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n   g.dat <- data.frame(Groups = colnames(A), PageRank.Cent = round(pr.g, 3))\n   g.dat <- g.dat[order(g.dat$PageRank.Cent, decreasing = TRUE), ]\n   kbl(g.dat, format = \"html\", align = c(\"l\", \"c\"), row.names = FALSE) %>% \n      column_spec(1, bold = TRUE) %>% \n      kable_styling(full_width = TRUE,\n                     bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-hover table-condensed table-responsive\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Groups </th>\n   <th style=\"text-align:center;\"> PageRank.Cent </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> 9/16 </td>\n   <td style=\"text-align:center;\"> 0.517 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> 4/8 </td>\n   <td style=\"text-align:center;\"> 0.444 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> 3/15 </td>\n   <td style=\"text-align:center;\"> 0.369 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> 2/25 </td>\n   <td style=\"text-align:center;\"> 0.295 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> 5/19 </td>\n   <td style=\"text-align:center;\"> 0.295 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> 4/7 </td>\n   <td style=\"text-align:center;\"> 0.222 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> 4/12 </td>\n   <td style=\"text-align:center;\"> 0.221 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> 6/10 </td>\n   <td style=\"text-align:center;\"> 0.185 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> 2/23 </td>\n   <td style=\"text-align:center;\"> 0.148 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> 9/26 </td>\n   <td style=\"text-align:center;\"> 0.147 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> 6/27 </td>\n   <td style=\"text-align:center;\"> 0.111 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> 3/2 </td>\n   <td style=\"text-align:center;\"> 0.111 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> 11/21 </td>\n   <td style=\"text-align:center;\"> 0.111 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;\"> 8/3 </td>\n   <td style=\"text-align:center;\"> 0.111 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nThe PageRank two-mode scores lead to a different ranking of the nodes, because now people are central if they belong to *selective* (not just large) groups and groups are central if their members are *discerning* (not just very active) people. \n\nJust like before we can use the PageRank scores to reveal a pattern in the affiliation matrix:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   p <- ggcorrplot(t(A[order(pr.p), order(-pr.g)]), \n                   colors = c(\"white\", \"white\", \"red\")) \n   p <- p + theme(legend.position = \"none\", \n                  axis.text.y = element_text(size = 8),\n                  axis.text.x = element_text(size = 8, angle = 0),\n                  )\n   p <- p + scale_x_discrete(position = \"top\") \n   p <- p + geom_abline(intercept = 0, slope = 1.26, linewidth = 2, color = \"blue\")\n   p\n```\n\n::: {.cell-output-display}\n![](handout7_files/figure-html/unnamed-chunk-67-1.png){width=672}\n:::\n:::\n\n\nIn the PageRank status score setup, events are ordered (from left to right) by popularity (most popular events on the left) and actors are ordered by activity (from top to bottom; active actors on top), creating a purely triangular pattern. Core actors and core events are in the upper-right block.\n\n## Correspondence Analysis\n\n**Correspondence Analysis** (CA) a relatively simple way to analyze and visualize two-mode data. In fact, we have already computed most of what we need to perform a CA of the two-mode network when figuring out the PageRank Status scores. However, there are a few additional computational details to discuss. \n\nFirst, let us review the idea of an **eigendecomposition** of a square matrix. Let's say we have the following matrix $\\mathbf{B}$ of dimensions $3 \\times 3$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   set.seed(567)\n   B <- matrix(round(runif(9), 2), nrow = 3, ncol = 3)\n   B\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,] 0.74 0.49 0.07\n[2,] 0.88 0.26 0.51\n[3,] 0.63 0.24 0.59\n```\n:::\n:::\n\n\nMost matrices like this can be decomposed into two other matrices $\\mathbf{U}$ and $\\mathbf{\\lambda}$, such that the following matrix multiplication equation is true:\n\n$$\n\\mathbf{B} = \\mathbf{U}\\mathbf{\\lambda}\\mathbf{U}^{-1}\n$$\n\nBoth $\\mathbf{U}$ and $\\mathbf{\\lambda}$ are of the same dimensions as the original, with $\\mathbf{U}$ having numbers in each cell and $\\mathbf{\\lambda}$ being a matrix with values along the diagonals and zeros everywhere else. \n\nThe column values of $\\mathbf{U}$ are called the **eigenvectors** of $\\mathbf{B}$ and the diagonal values of $\\mathbf{\\lambda}$ are called the **eigenvalues** of $\\mathbf{B}$.\n\nIn `R` you can find the values that yield the eigendecomposition of any square matrix (if one exists) using the function `eigen`. \n\nSo in our case this would be:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   eig.res <- eigen(B)\n   eig.res\n```\n\n::: {.cell-output .cell-output-stdout}\n```\neigen() decomposition\n$values\n[1]  1.4256541  0.3195604 -0.1552145\n\n$vectors\n           [,1]       [,2]       [,3]\n[1,] -0.5148954 -0.4669390  0.4838633\n[2,] -0.6388257  0.2808667 -0.8653808\n[3,] -0.5716507  0.8384997 -0.1303551\n```\n:::\n:::\n\n\nThe function `eigen` returns a list with two components, one called `values` are the diagonal values of $\\mathbf{\\lambda}$, and the other one called `vectors` is the eigenvector matrix $\\mathbf{U}$.\n\nWe can check that these two elements can help us reconstruct the original matrix as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   lambda <- diag(eig.res$values)\n   U <- eig.res$vectors\n   B.rec <- U %*% lambda %*% solve(U)\n   B.rec\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,] 0.74 0.49 0.07\n[2,] 0.88 0.26 0.51\n[3,] 0.63 0.24 0.59\n```\n:::\n:::\n\n\nWhich are indeed the original values of $\\mathbf{B}$!\n\nNow, the idea is that we can perform this eigendecomposition with any matrix, including a network adjacency matrix or a proximity matrix derived from it like the ones we used to calculate the PageRank Status scores earlier. \n\nIn fact, we have already done that in part many times before, because the status scores compute the first column (leading eigenvector) of the $\\mathbf{U}$ matrix for any proximity or adjacency matrix you feed into it.\n\nThe key point is that once you have the eigendecomposition of the matrix, and the full set of eigenvectors stored in $\\mathbf{U}$, you can always choose the first few columns of $\\mathbf{U}$, which gives us the best *low dimensional approximation* of the original matrix. \n\nCA is based on this principle, and it works with the eigendecomposition of the square matrices $\\mathbf{P_{PP}}$ and $\\mathbf{P_{GG}}$ as we defined them earlier:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   CA.p <- eigen(P.pp)\n   CA.g <- eigen(P.gg)\n```\n:::\n\n\nLet's see what we have here:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   round(CA.p$values, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 1.00 0.63 0.32 0.18 0.14 0.11 0.10 0.06 0.04 0.04 0.02 0.01 0.01 0.00 0.00\n[16] 0.00 0.00 0.00\n```\n:::\n\n```{.r .cell-code}\n   round(CA.g$values, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 1.00 0.63 0.32 0.18 0.14 0.11 0.10 0.06 0.04 0.04 0.02 0.01 0.01 0.00\n```\n:::\n:::\n\n\nSo the two matrices have identical eigenvalues, and the first one is 1.0. Let's check out the first three eigenvectors:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   rownames(CA.p$vectors) <- rownames(A)\n   rownames(CA.g$vectors) <- colnames(A)\n   round(CA.p$vectors[, 1:3], 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           [,1]  [,2]  [,3]\nEVELYN    -0.24 -0.24  0.03\nLAURA     -0.24 -0.25 -0.01\nTHERESA   -0.24 -0.20  0.02\nBRENDA    -0.24 -0.26 -0.01\nCHARLOTTE -0.24 -0.29 -0.01\nFRANCES   -0.24 -0.24 -0.02\nELEANOR   -0.24 -0.15 -0.03\nPEARL     -0.24 -0.01  0.06\nRUTH      -0.24 -0.05  0.03\nVERNE     -0.24  0.13 -0.04\nMYRNA     -0.24  0.25 -0.10\nKATHERINE -0.24  0.31 -0.22\nSYLVIA    -0.24  0.26 -0.20\nNORA      -0.24  0.26 -0.03\nHELEN     -0.24  0.24  0.07\nDOROTHY   -0.24  0.09  0.09\nOLIVIA    -0.24  0.33  0.66\nFLORA     -0.24  0.33  0.66\n```\n:::\n\n```{.r .cell-code}\n   round(CA.g$vectors[, 1:3], 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      [,1]  [,2]  [,3]\n6/27  0.27 -0.30 -0.01\n3/2   0.27 -0.28 -0.04\n4/12  0.27 -0.30  0.00\n9/26  0.27 -0.30 -0.02\n2/25  0.27 -0.25  0.00\n5/19  0.27 -0.16  0.00\n3/15  0.27 -0.04  0.05\n9/16  0.27 -0.01  0.05\n4/8   0.27  0.15 -0.19\n6/10  0.27  0.32  0.22\n2/23  0.27  0.35 -0.79\n4/7   0.27  0.29  0.20\n11/21 0.27  0.34  0.35\n8/3   0.27  0.34  0.35\n```\n:::\n:::\n\n\nSo this is interesting. The first eigenvector of the decomposition of both $\\mathbf{P_{PP}}$ and $\\mathbf{P_{GG}}$ is just the same number for each person and group. Note that this is the eigenvector that is associated with the first eigenvalue which happens to be $\\lambda_1 = 1.0$.\n\nSo it looks like the first eigenvector is a pretty useless quantity (a constant) so we can discard it, keeping all the other ones. Now the old second eigenvector is the first, the old third is the second, and so on:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   eig.vec.p <- CA.p$vectors[, 2:ncol(CA.p$vectors)]\n   eig.vec.g <- CA.g$vectors[, 2:ncol(CA.g$vectors)]\n```\n:::\n\n\nNote that the rest of the eigenvalues (discarding the 1.0 one) are arranged in descending order:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   round(CA.p$values[2:10], 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.63 0.32 0.18 0.14 0.11 0.10 0.06 0.04 0.04\n```\n:::\n:::\n\n\nThe magnitude of the eigenvalue tells us how important is the related eigenvector in containing information about the original matrix. So it looks like here, the first two eigenvectors contain a good chunk of the info:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   round(sum(CA.p$values[2:3])/sum(CA.p$values[2:length(CA.p$values)]), 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.57\n```\n:::\n:::\n\n\nBecause the magnitude of the eigenvectors don't have a natural scale, it is common to normalize to have a variance of 1.0. \n\nWe can do this as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   N <- sum(A)\n   d.p <- diag(rowSums(A))\n   d.g <- diag(colSums(A))\n   eig.vec.g[, 2] <- eig.vec.g[, 2] * -1\n   for (i in 1:nrow(A)-1) {\n      norm <- as.numeric(t(eig.vec.p[, i]) %*% d.p %*% eig.vec.p[, i])\n      eig.vec.p[, i] <- eig.vec.p[, i] * sqrt(N/norm)\n   }\n   for (j in 1:ncol(A)-1) {\n      norm <- as.numeric(t(eig.vec.g[, j]) %*% d.g %*% eig.vec.g[, j])\n      eig.vec.g[, j] <- eig.vec.g[, j] * sqrt(N/norm)\n      }\n```\n:::\n\n\nAnd we can use the first two normalized eigenvectors to plot the persons and groups in a common space:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   plot.dat <- data.frame(rbind(eig.vec.p[, 1:2], eig.vec.g[, 1:2])) %>% \n      cbind(type = as.factor(c(rep(1, 18), rep(2, 14))))\n   library(ggplot2)\n   # install.packages(\"ggrepel\")\n   library(ggrepel)\n   p <- ggplot(data = plot.dat, aes(X1, y = X2, color = type))\n   p <- p + geom_hline(aes(yintercept = 0), color = \"gray\")\n   p <- p + geom_vline(aes(xintercept = 0), color = \"gray\")\n   p <- p + geom_text_repel(aes(label = rownames(plot.dat)), \n                            max.overlaps = 20, size = 2.75)\n   p <- p + theme_minimal()\n   p <- p + theme(legend.position = \"none\",\n                  axis.title = element_text(size = 14),\n                  axis.text = element_text(size = 12))\n   p <- p + scale_color_manual(values = c(\"red\", \"blue\"))\n   p <- p + labs(x = \"First Dimension\", y = \"Second Dimension\")\n   p\n```\n\n::: {.cell-output-display}\n![](handout7_files/figure-html/unnamed-chunk-78-1.png){width=672}\n:::\n:::\n\n\nIn this space, people with the most similar patterns of memberships to the most similar groups are placed close to one another. In the same way, groups with the most similar members are placed closed to one another. \n\nAlso like before, we can use the scores obtained from the CA analysis to re-arrange the rows and columns of the original matrix to reveal blocks of maximally similar persons and events:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   p <- ggcorrplot(t(A[order(eig.vec.p[,1]), order(eig.vec.g[,1])]), \n                   colors = c(\"white\", \"white\", \"red\")) \n   p <- p + theme(legend.position = \"none\", \n                  axis.text.y = element_text(size = 8),\n                  axis.text.x = element_text(size = 8, angle = 0),\n                  )\n   p <- p + scale_x_discrete(position = \"top\") \n   p <- p + geom_hline(yintercept = 6.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_hline(yintercept = 10.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_vline(xintercept = 9.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_vline(xintercept = 6.5, linewidth = 2, color = \"blue\")\n   p\n```\n\n::: {.cell-output-display}\n![](handout7_files/figure-html/unnamed-chunk-79-1.png){width=672}\n:::\n:::\n\n\nHere CA seems to have detected two separate clusters of actors who preferentially attend two distinct clusters of events! \n\nThe three events in the middle $\\{3/15, 9/16, 4/8\\}$ don't seem to differentiate between participants in each cluster (everyone attends)--they thus appear near the origin in the CA diagram, indicating a weak association with either dimension. \n\nHowever, the events to the left (with clusters of participants in the lower-left) and to the right of the x-axis (with clusters of participants in the upper-right) are attended preferentially by distinct groups of participants; they thus appear at the extreme left and right positions of the first dimension of the CA diagram. \n\nIn the same way, the four people in the middle $\\{Ruth, Dorothy, Pearl, Verne\\}$ only attend the undifferentiated, popular events, so that means that they are not strongly associated with either cluster of actors (and thus appear near the origin in the CA diagram). The top and bottom participants, by contrast, appear to the extreme right and left in the CA diagram, indicating a strong association with the underlying dimensions.\n\nNote the similarity between this blocking and that obtained from the structural equivalence analysis.\n\n\n\n\n\n\n\n\n\n\n",
    "supporting": [
      "handout7_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\r\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}