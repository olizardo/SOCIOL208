[
  {
    "objectID": "cube.html",
    "href": "cube.html",
    "title": "The Cube",
    "section": "",
    "text": "Brandes, Borgatti, and Freeman (2016) discuss the centrality “cube,” an interesting and intuitive way to understand the way betweenness centrality works, as well as the dual connection between closeness and betweenness.\nLet us illustrate using a simple example. We begin by creating an Erdos-Renyi graph with eight nodes and connection probability \\(p = 0.5\\):\nThe resulting graph looks like:\nThe basic innovation behind the centrality cube is to store the intermediation information among every node triplet in the graph \\(s\\), \\(r\\), \\(b\\) (standing for “sender,” receiver,” and “broker”) in a three dimensional array rather than the usual two dimensional matrix.\nThe three dimensional array can be thought of as a “cube” by stacking multiple reachability matrices between every pair \\(s\\) and \\(r\\) along a three dimensional dimension \\(b\\). So each “b-slice” of the cube will contain the number of times node \\(b\\) stands in a shortest path between \\(s\\) and \\(r\\) divided by the total number of paths between \\(s\\) and \\(r\\) which as you recall computes the pairwise betweenness of \\(b\\) with respect to \\(s\\) and \\(r\\).\nLet’s see how that works."
  },
  {
    "objectID": "cube.html#building-the-cube",
    "href": "cube.html#building-the-cube",
    "title": "The Cube",
    "section": "Building the Cube",
    "text": "Building the Cube\nWe begin by writing a simple user-defined function to count the total number of shortest paths between each pair of nodes:\n\n   nsp <- function(x) {\n      n <- vcount(g)\n      S <- matrix(0, n, n)\n      for (i in 1:n) {\n            for (j in 1:n) {\n                  if (j %in% neighbors(x, i) == FALSE) {\n                     S[i, j] <- length(all_shortest_paths(x, i, j)$vpaths)\n               }\n            }\n         }\n      return(S)\n   }\n\nThe function is called nsp and takes a graph as input and returns and matrix called \\(\\mathbf{S}\\) with entries \\(s_{ij}\\) equal to the total number of shortest paths between \\(i\\) and \\(j\\). This is done by computing the length of the list returned by the all_shortest_paths function in igraph for each pair of non-adjacent nodes. This is done in two steps.\n\nFirst, we check whether \\(j\\) is a neighbor of \\(i\\) using the neighbors function in igraph. The neighbors function takes a graph and a node id as input and returns a vector of that node’s neighbors in the graph. We want the function to update the \\(S\\) matrix only when \\(i\\) and \\(j\\) are not adjacent (indirectly connected).\nSecond, we use the all_shortest_paths function to actually compute the number of shortest paths between \\(i\\) and \\(j\\). This function takes three inputs: (1) A graph object, (2) a sender node id, and (3) a receiver node id (which can be a vector of receiver nodes), and returns a list of the paths between the sender and receiver nodes in the form of vectors of node ids defining each path as elements of a list called “vpaths.”\n\nNow we are ready to write a user defined function to build the cube. Here’s a not-so-efficient (programming wise) but working example:\n\n   cube <- function(g) {\n      n <- vcount(g)\n      c <- array(rep(n^3, 0), c(n, n, n))\n      S <- nsp(g)\n      for (b in 1:n) {\n         for (s in 1:n) {\n            for (r in 1:n) {\n               if (s != r & r %in% neighbors(g, s) == FALSE) {\n                  p.sr <- all_shortest_paths(g, s, r)$vpaths\n                  b.sr <- lapply(p.sr, function(x) {x[-c(1, length(x))]}) \n                  c[s, r, b] <- sum(as.numeric(sapply(b.sr, function(x) {b %in% x})))\n                  c[s, r, b] <- c[s, r, b]/S[s, r]\n               }\n            }\n         }\n      }\n   c[is.na(c)] <- 0\n   return(c)\n   }\n\nIn line 1, we name the function cube. Line 3 initializes the array in R. It takes a string of zeros of length \\(n^3\\) where \\(n\\) is the number of nodes and crams them into an \\(n \\times n \\times n\\) array. In this case, since \\(n = 8\\), this means eight empty matrices of dimensions \\(8 \\times 8\\) stacked together to form our cube full of zeros. The \\(ijk^{th}\\) cell of the array corresponds to sender \\(i\\), receiver \\(j\\) and broker node \\(k\\). Line 4 computes the matrix \\(S\\) containing the number of shortest paths between every sender and receiver node in the graph.\nLines 5-15 populate the cube with the required information using an (inefficient) triple for loop. As noted some, useful igraph functions come into play here:\n\nIn line 8 the if conditional inside the triple loop uses the neighbors function in igraph and checks that node \\(r\\) is not a neighbor of \\(s\\) (if they are directly connected then node \\(b\\) cannot be a broker).\nAfter we check that \\(s\\) and \\(r\\) are not neighbors, we use the all_shortest_paths function in igraph to get all the shortest paths between \\(s\\) and \\(r\\) in line 9.\nLine 10 uses some lapply magic to drop the source and receiver nodes from the list of node ids vectors returned by the all_shortest_paths function.\nLine 11 uses additional sapply magic and the base R function %in% to check how many times each broker node \\(b\\) shows up in that list of shortest paths as an inner node between \\(s\\) and \\(r\\); we put that number in the \\(ijk^{th}\\) cell of the array, and loop through all triplets until we are done.\nLine 12 takes the number computed in line 11 and divides by the total number of shortest paths between \\(s\\) and \\(r\\) which is the betweenness ratio we are seeking."
  },
  {
    "objectID": "cube.html#exploring-the-cube",
    "href": "cube.html#exploring-the-cube",
    "title": "The Cube",
    "section": "Exploring the Cube",
    "text": "Exploring the Cube\nOnce we have our array, we can create all kinds of interesting sub-matrices containing the intermediation information in the graph by summing rows and columns of the array along different dimensions.\nFirst, let us see what’s in the cube. We can query specific two-dimensional sub-matrices using an extension of the usual format for querying matrices in R for three-dimensional arrays. For instance this:\n\n   srb <- cube(g)\n   round(srb[, , 2], 2)\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n[1,]  0.0    0 0.00    0 0.00  0.5    0 0.00\n[2,]  0.0    0 0.00    0 0.00  0.0    0 0.00\n[3,]  0.0    0 0.00    0 0.00  0.0    0 0.33\n[4,]  0.0    0 0.00    0 0.00  0.0    0 0.00\n[5,]  0.0    0 0.00    0 0.00  0.0    0 0.33\n[6,]  0.5    0 0.00    0 0.00  0.0    0 1.00\n[7,]  0.0    0 0.00    0 0.00  0.0    0 0.00\n[8,]  0.0    0 0.33    0 0.33  1.0    0 0.00\n\n\nCreates a three-dimensional matrix of pairwise betweenness probabilities and assigns it to the srb object in line 1, and looks at the \\(s_{\\bullet} \\times r_{\\bullet} \\times b_2\\) entry in line 2.\nEach entry in the matrix is the probability that node 2 stands on a shortest path between the row sender and the column receiver node. For instance, the 0.33 in the entry corresponding to row 5 and column 8 tells us that node 2 stands in one third of the shortest paths between nodes 5 and 8 (there are 3 distinct shortest paths between 5 and 8).\nBecause each sub-matrix in the cube is a matrix, we can do the usual matrix operations on them. For instance, let’s take the row sums of the \\(s\\) to \\(r\\) matrix corresponding to node 3 as the broker. This can be done like this:\n\n   d.3 <- rowSums(srb[ , , 3])\n   names(d.3) <- 1:vcount(g)\n   d.3\n\n  1   2   3   4   5   6   7   8 \n1.5 2.0 0.0 3.0 6.0 3.5 3.0 1.0 \n\n\nAs Brandes, Borgatti, and Freeman (2016), note this vector gives us the dependence of each node in the graph on node 3. Obviously node 3 doesn’t depend on itself so there is a zero on the third spot in the vector. As is clear from the plot, node 5 is the most dependent on 3 for intermediation with the rest of the nodes in the graph.\nWe can also pick a particular sender and receiver node and sum all their dyadic entries in the cube across the third (broker) dimension:\n\n   sum(srb[1, 6, ])\n\n[1] 2\n\n\nThis number is equivalent to the geodesic distance between the nodes minus one:\n\n   distances(g)[1, 6] - 1\n\n[1] 2"
  },
  {
    "objectID": "cube.html#betweenness-and-closeness-in-the-cube",
    "href": "cube.html#betweenness-and-closeness-in-the-cube",
    "title": "The Cube",
    "section": "Betweenness and Closeness in the Cube",
    "text": "Betweenness and Closeness in the Cube\nThe betweenness centrality of each node is encoded in the cube, because we already computed the main ratio that the measure depends on. For instance, let’s look at the matrix composed by taking the slice of cube that corresponds to node 3 as a broker:\n\n   srb[, , 3]\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n[1,]  0.0  0.0    0  0.0    1  0.5  0.0    0\n[2,]  0.0  0.0    0  0.5    1  0.0  0.5    0\n[3,]  0.0  0.0    0  0.0    0  0.0  0.0    0\n[4,]  0.0  0.5    0  0.0    1  1.0  0.5    0\n[5,]  1.0  1.0    0  1.0    0  1.0  1.0    1\n[6,]  0.5  0.0    0  1.0    1  0.0  1.0    0\n[7,]  0.0  0.5    0  0.5    1  1.0  0.0    0\n[8,]  0.0  0.0    0  0.0    1  0.0  0.0    0\n\n\nThe sum of the all the cells in this matrix (divided by two) correspond to node 3’s betweenness centrality:\n\n   sum(srb[, , 3])/2\n\n[1] 10\n\n   betweenness(g)[3]\n\n[1] 10\n\n\nSo to get each node’s betweenness we just can just sum up the entries in each of the cube’s sub-matrices:\n\n   b.cube <- round(colSums(srb, dims = 2)/2, 2)\n   b.igraph <- round(betweenness(g), 2)\n   names(b.cube) <- 1:vcount(g)\n   names(b.igraph) <- 1:vcount(g)\n   b.cube\n\n    1     2     3     4     5     6     7     8 \n 0.00  2.17 10.00  0.67  0.00  0.00  3.17  4.00 \n\n   b.igraph\n\n    1     2     3     4     5     6     7     8 \n 0.00  2.17 10.00  0.67  0.00  0.00  3.17  4.00 \n\n\nNote the neat trick of using the argument dims = 2 in the usual colSums command. This tells colSums that we are dealing with a three dimensional matrix, and that what we want is the sum of the columns across the cube’s third dimension (the brokers). Note also that we divide the cube betweenness by two because we are summing identical entries across the upper and lower triangle of the symmetric dyadic brokerage matrices inside the cube (not surprisingly, node 3 is the top betweenness centrality node).\nAs Brandes, Borgatti, and Freeman (2016) point out, using the cube info, we can build a matrix of dependencies between each pair of nodes. In this matrix, the rows correspond to a sender (or receiver) node, the columns to a broker node and the \\(sb^{th}\\) entry contains the sum of the proportion of paths containing the broker nodes that starts with the sender node and end with some other node in the graph.\nHere’s a function that uses the cube info to build the dependency matrix that Brandes, Borgatti, and Freeman (2016) talk about using the cube as input:\n\n   dep.ij <- function(c) {\n      n <- nrow(c)\n      dep.ij <- rowSums(c[, , 1])\n      for (i in 2:n) {\n         dep.ij <- cbind(dep.ij,  rowSums(c[, , i]))\n         \n         }\n      rownames(dep.ij) <- 1:n\n      colnames(dep.ij) <- 1:n\n      return(dep.ij)\n   }\n\nThis function just takes the various vectors formed by the row sums of the sender-receiver matrix across each value of the third dimension (which is just each node in the graph when playing the broker role). It then returns a regular old \\(n \\times n\\) containing the info.\nHere’s the result when applied to our little example:\n\n   library(kableExtra)\n   kbl(round(dep.ij(srb), 2), \n       format = \"html\", align = \"c\", row.names = TRUE,\n       caption = \"Dependence Matrix.\") %>% \n      column_spec(1, bold = TRUE) %>%\n   kable_styling(bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\nDependence Matrix.\n \n  \n      \n    1 \n    2 \n    3 \n    4 \n    5 \n    6 \n    7 \n    8 \n  \n \n\n  \n    1 \n    0 \n    0.50 \n    1.5 \n    0.00 \n    0 \n    0 \n    2.50 \n    2.5 \n  \n  \n    2 \n    0 \n    0.00 \n    2.0 \n    0.00 \n    0 \n    0 \n    0.00 \n    2.0 \n  \n  \n    3 \n    0 \n    0.33 \n    0.0 \n    0.33 \n    0 \n    0 \n    1.33 \n    0.0 \n  \n  \n    4 \n    0 \n    0.00 \n    3.0 \n    0.00 \n    0 \n    0 \n    0.00 \n    2.0 \n  \n  \n    5 \n    0 \n    0.33 \n    6.0 \n    0.33 \n    0 \n    0 \n    1.33 \n    0.0 \n  \n  \n    6 \n    0 \n    1.50 \n    3.5 \n    0.00 \n    0 \n    0 \n    0.50 \n    0.5 \n  \n  \n    7 \n    0 \n    0.00 \n    3.0 \n    0.00 \n    0 \n    0 \n    0.00 \n    1.0 \n  \n  \n    8 \n    0 \n    1.67 \n    1.0 \n    0.67 \n    0 \n    0 \n    0.67 \n    0.0 \n  \n\n\n\n\n\nNote that this is valued matrix that is also asymmetric. Take for instance, node 3. Every node in the graph depends on node 3 for access to other nodes, but node 3 does not depend on nodes 1, 5, 6, or 8.\nInterestingly, as Brandes, Borgatti, and Freeman (2016) also show, the betweenness centrality also can be calculated from the dependency matrix! All we need to do is compute the column sums, equivalent to in-degree in the directed dependence network:\n\n   round(colSums(dep.ij(srb))/2, 2)\n\n    1     2     3     4     5     6     7     8 \n 0.00  2.17 10.00  0.67  0.00  0.00  3.17  4.00 \n\n\nEven more interestingly, closeness centrality is also in the dependence matrix! It is given by the outdegree of each actor in the directed dependence network, corresponding to the row sums of the matrix (shifted by a constant given by \\(n-1\\)).\n\n   c.c <- rowSums(distances(g))\n   c.d <- rowSums(dep.ij(srb)) + (vcount(g) - 1)\n   names(c.c) <- 1:vcount(g)\n   names(c.d) <- 1:vcount(g)\n   round(1/c.c, 3)\n\n    1     2     3     4     5     6     7     8 \n0.071 0.091 0.111 0.083 0.067 0.077 0.091 0.091 \n\n   round(1/c.d, 3)\n\n    1     2     3     4     5     6     7     8 \n0.071 0.091 0.111 0.083 0.067 0.077 0.091 0.091 \n\n\nHere we see that node 3 is also the top in closeness, followed closely (pun intended) by nodes 2, 7, and 8. This makes sense because an actor with high closeness is one that has low dependence on key nodes to be able to reach others.\nOf course, closeness is also in the cube because of the mathematical relationship we saw earlier between the sum of entries between senders and receivers across brokers in the cube and the geodesic distance.\nFor instance, let’s get the matrix corresponding to node 3’s role as sender across all brokers and receivers:\n\n   round(srb[3, , ], 2)\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n[1,]    0 0.00    0 0.00    0    0 1.00    0\n[2,]    0 0.00    0 0.00    0    0 0.00    0\n[3,]    0 0.00    0 0.00    0    0 0.00    0\n[4,]    0 0.00    0 0.00    0    0 0.00    0\n[5,]    0 0.00    0 0.00    0    0 0.00    0\n[6,]    0 0.00    0 0.00    0    0 0.00    0\n[7,]    0 0.00    0 0.00    0    0 0.00    0\n[8,]    0 0.33    0 0.33    0    0 0.33    0\n\n\nThe entries of this matrix give us the probability that node 3 is the sender, whenever the row node is the is the broker (an inner node in the path) and the column node is the receiver.\nFor instance, the value 0.3 in row 8 and column 1 tells us that node 3 is the sender node in one third of the paths that end in node 1 and feature node 8 as a broker.\nInterestingly, the sums of the entries in this matrix are equivalent to the sum of the geodesic distances between node 3 and every other node in the graph shifted by a constant (\\(n- 1\\)):\n\n   sum(srb[3, , ]) + (vcount(g) - 1)\n\n[1] 9\n\n   sum(distances(g)[3, ])\n\n[1] 9\n\n\nSo the closeness centrality can be computed from the cube as follows:\n\n   c <- 0\n   n <- vcount(g)\n   for (i in 1:n) {\n      c[i] <- 1/(sum(srb[i, , ]) + (n - 1))\n   }\n   round(c, 3)\n\n[1] 0.071 0.091 0.111 0.083 0.067 0.077 0.091 0.091\n\n\nWhich is the same as:\n\n   round(closeness(g), 3)\n\n[1] 0.071 0.091 0.111 0.083 0.067 0.077 0.091 0.091"
  },
  {
    "objectID": "handout1.html",
    "href": "handout1.html",
    "title": "Basic Network Statistics",
    "section": "",
    "text": "Here we will analyze a small network and computer some basic statistics of interest. The first thing we need to do is get some data! For this purpose, we will use the package networkdata (available here). To install the package, use the following code:\n\n   #install.packages(\"remotes\") \n   remotes::install_github(\"schochastics/networkdata\")\n\nTo load the network datasets in the networkdata just type:\n\n   library(networkdata)\n\nThe package contains a bunch of human and animal social networks to browse through them, type:\n\n   data(package = \"networkdata\")\n\nWe will pick one of the movies for this analysis, namely, Pulp Fiction. This is movie_559. In the movie network two characters are linked by an edge if they appear in a scene together. The networkdata data sets come in igraph format, so we need to load that package (or install it using install.packages if you haven’t done that yet).\n\n   #install.packages(\"igraph\") \n   library(igraph)\n   g <- movie_559"
  },
  {
    "objectID": "handout1.html#number-of-nodes-and-edges",
    "href": "handout1.html#number-of-nodes-and-edges",
    "title": "Basic Network Statistics",
    "section": "Number of Nodes and Edges",
    "text": "Number of Nodes and Edges\nNow we are ready to compute some basic network statistics. As with any network, we want to know what the number of nodes and the number of edges (links) are. Since this is a relatively small network, we can begin by listing the actors.\n\n   V(g)\n\n+ 38/38 vertices, named, from 9e7cc7a:\n [1] BRETT           BUDDY           BUTCH           CAPT KOONS     \n [5] ED SULLIVAN     ENGLISH DAVE    ESMARELDA       FABIENNE       \n [9] FOURTH MAN      GAWKER #2       HONEY BUNNY     JIMMIE         \n[13] JODY            JULES           LANCE           MANAGER        \n[17] MARSELLUS       MARVIN          MAYNARD         MIA            \n[21] MOTHER          PATRON          PEDESTRIAN      PREACHER       \n[25] PUMPKIN         RAQUEL          ROGER           SPORTSCASTER #1\n[29] SPORTSCASTER #2 THE GIMP        THE WOLF        VINCENT        \n[33] WAITRESS        WINSTON         WOMAN           YOUNG MAN      \n[37] YOUNG WOMAN     ZED            \n\n\nThe function V takes the igraph network object as input and returns an igraph.vs object as output (short for “igraph vertex sequence”), listing the names (if given as a graph attribute) of each node. The first line also tells us that there are 38 nodes in this network.\nThe igraph.vs object operates much like an R character vector, so we can query its length to figure out the number of nodes:\n\n   length(V(g))\n\n[1] 38\n\n\nThe analogue function for edges in igraph is E which also takes the network object as input and returns an object of class igraph.es (“igraph edge sequence”) as output:\n\n   E(g)\n\n+ 102/102 edges from 9e7cc7a (vertex names):\n [1] BRETT      --MARSELLUS       BRETT      --MARVIN         \n [3] BRETT      --ROGER           BRETT      --VINCENT        \n [5] BUDDY      --MIA             BUDDY      --VINCENT        \n [7] BRETT      --BUTCH           BUTCH      --CAPT KOONS     \n [9] BUTCH      --ESMARELDA       BUTCH      --GAWKER #2      \n[11] BUTCH      --JULES           BUTCH      --MARSELLUS      \n[13] BUTCH      --PEDESTRIAN      BUTCH      --SPORTSCASTER #1\n[15] BUTCH      --ENGLISH DAVE    BRETT      --FABIENNE       \n[17] BUTCH      --FABIENNE        FABIENNE   --JULES          \n[19] FOURTH MAN --JULES           FOURTH MAN --VINCENT        \n+ ... omitted several edges\n\n\nThis tells us that there are 102 edges (connected dyads) in the network. Some of these include Brett and Marsellus and Fabienne and Jules, but not all can be listed for reasons of space.\nigraph also has two dedicated functions that return the number of nodes and edges in the graph in one fell swoop. They are called vcount and ecount and take the graph object as input:\n\n   vcount(g)\n\n[1] 38\n\n   ecount(g)\n\n[1] 102"
  },
  {
    "objectID": "handout1.html#graph-density",
    "href": "handout1.html#graph-density",
    "title": "Basic Network Statistics",
    "section": "Graph Density",
    "text": "Graph Density\nOnce we have the number of edges and nodes, we can calculate the most basic derived statistic in a network, which is the density. Since the movie network is an undirected graph, the density is given by:\n\\[\n   \\frac{2m}{n(n-1)}\n\\]\nWhere \\(m\\) is the number of edges and \\(n\\) is the number of nodes, or in our case:\n\n   (2 * 102) / (38 * (38 - 1))\n\n[1] 0.1450925\n\n\nOf course, igraph has a dedicated function called edge_density to compute the density too, which takes the igraph object as input:\n\n   edge_density(g)\n\n[1] 0.1450925"
  },
  {
    "objectID": "handout1.html#degree",
    "href": "handout1.html#degree",
    "title": "Basic Network Statistics",
    "section": "Degree",
    "text": "Degree\nThe next set of graph metrics are based on the degree of the graph. We can list the graph’s degree set using the igraph function degree:\n\n   degree(g)\n\n          BRETT           BUDDY           BUTCH      CAPT KOONS     ED SULLIVAN \n              7               2              17               5               2 \n   ENGLISH DAVE       ESMARELDA        FABIENNE      FOURTH MAN       GAWKER #2 \n              4               1               3               2               3 \n    HONEY BUNNY          JIMMIE            JODY           JULES           LANCE \n              8               3               4              16               4 \n        MANAGER       MARSELLUS          MARVIN         MAYNARD             MIA \n              5              10               6               3              11 \n         MOTHER          PATRON      PEDESTRIAN        PREACHER         PUMPKIN \n              5               5               3               3               8 \n         RAQUEL           ROGER SPORTSCASTER #1 SPORTSCASTER #2        THE GIMP \n              3               6               2               1               2 \n       THE WOLF         VINCENT        WAITRESS         WINSTON           WOMAN \n              3              25               4               3               5 \n      YOUNG MAN     YOUNG WOMAN             ZED \n              4               4               2 \n\n\nThe degree function takes the igraph network object as input and returns a plain old R named vector as output with the names being the names attribute of vertices in the network object.\nUsually we are interested in who are the “top nodes” in the network by degree (a kind of centrality). To figure that out, all we need to do is sort the degree set (to generate the graph’s degree sequence) and list the top entries:\n\n   d <- degree(g)\n   d.sort <- sort(d, decreasing = TRUE)\n   d.sort[1:8]\n\n    VINCENT       BUTCH       JULES         MIA   MARSELLUS HONEY BUNNY \n         25          17          16          11          10           8 \n    PUMPKIN       BRETT \n          8           7 \n\n\nLine 1 stores the degrees in an object “d”, line 2 creates a “sorted” version of the same object (from bigger to smaller) and line 3 shows the first eight entries of the sorted degree sequence.\nBecause the degree vector “d” is just a regular old vector we can use native R mathematical operations to figure out things like the sum, maximum, minimum, and average degree of the graph:\n\n   sum(d)\n\n[1] 204\n\n   max(d)\n\n[1] 25\n\n   min(d)\n\n[1] 1\n\n   mean(d)\n\n[1] 5.368421\n\n\nSo the sum of degrees is 204, the maximum degree is 25 (belonging to Vincent), the minimum is one, and the average is about 5.4.\nNote that these numbers recreate some well-known equalities in graph theory:\n\nThe sum of degrees is twice the number of edges (the first theorem of graph theory):\n\n\n   2 * ecount(g)\n\n[1] 204\n\n\n\nThe average degree is just the sum of degrees divided by the number of nodes:\n\n\n   sum(d)/vcount(g)\n\n[1] 5.368421\n\n\n\nThe density is just the average degree divided by the number of nodes minus one, as explained here:\n\n\n   mean(d)/(vcount(g) - 1)\n\n[1] 0.1450925\n\n\nSome people also consider the degree variance of the graph as a measure of inequality of connectivity in the system. It is equal to the average sum of square deviations of each node’s degree from the average:\n\\[\n  \\mathcal{v}(G) = \\frac{\\sum_i (k_i - \\bar{k})^2}{n}\n\\]\n\n   sum((d - mean(d))^2)/vcount(g)\n\n[1] 22.96953\n\n\nThis tells us that there is a lot of inequality in the distribution of degrees in the graph (a graph with all nodes equal degree would have variance zero)."
  },
  {
    "objectID": "handout1.html#the-degree-distribution",
    "href": "handout1.html#the-degree-distribution",
    "title": "Basic Network Statistics",
    "section": "The Degree Distribution",
    "text": "The Degree Distribution\nAnother way of looking at inequalities of degrees in a graph is to examine its degree distribution. This gives us the probability of observing a node with a given degree k in the graph.\n\n   deg.dist <- degree_distribution(g)\n   deg.dist <- round(deg.dist, 3)\n   deg.dist\n\n [1] 0.000 0.053 0.158 0.237 0.158 0.132 0.053 0.026 0.053 0.000 0.026 0.026\n[13] 0.000 0.000 0.000 0.000 0.026 0.026 0.000 0.000 0.000 0.000 0.000 0.000\n[25] 0.000 0.026\n\n\nThe igraph function degree_distribution just returns a numeric vector of the same length as the maximum degree of the graph plus one. In this case that’s a vector of length 25 + 1 = 26. The first entry gives us the proportion of nodes with degree zero (isolates), the second the proportion of nodes of degree one, and so on up to the graph’s maximum degree.\nSince there are no isolates in the network, we can ignore the first element of this vector, to get the proportion of nodes of each degree in the Pulp Fiction network. To that, we fist create a two-column data.frame with the degrees in the first column and the proportions in the second:\n\n   degree <- c(1:max(d))\n   prop <- deg.dist\n   prop <- prop[-1]\n   deg.dist <- data.frame(degree, prop)\n   deg.dist\n\n   degree  prop\n1       1 0.053\n2       2 0.158\n3       3 0.237\n4       4 0.158\n5       5 0.132\n6       6 0.053\n7       7 0.026\n8       8 0.053\n9       9 0.000\n10     10 0.026\n11     11 0.026\n12     12 0.000\n13     13 0.000\n14     14 0.000\n15     15 0.000\n16     16 0.026\n17     17 0.026\n18     18 0.000\n19     19 0.000\n20     20 0.000\n21     21 0.000\n22     22 0.000\n23     23 0.000\n24     24 0.000\n25     25 0.026\n\n\nOf course, a better way to display the degree distribution of a graph is via some kind of data visualization, particularly for large networks where a long table of numbers is just not feasible. To do that, we can call on our good friend ggplot:\n\n   # install.packages(ggplot2)\n   library(ggplot2)\n   p <- ggplot(data = deg.dist, aes(x = degree, y = prop))\n   p <- p + geom_bar(stat = \"identity\", fill = \"red\", color = \"red\")\n   p <- p + theme_minimal()\n   p <- p + labs(x = \"\", y = \"Proportion\", \n                 title = \"Degree Distribution in Pulp Fiction Network\") \n   p <- p + geom_vline(xintercept = mean(d), \n                       linetype = 2, linewidth = 0.5, color = \"blue\")\n   p <- p + scale_x_continuous(breaks = c(1, 5, 10, 15, 20, 25))\n   p\n\n\n\n\nThe plot clearly shows that the Pulp Fiction network degree distribution is skewed with a small number of characters having a large degree \\(k \\geq 15\\) while most other characters in the movie have a small degree \\(k \\leq 5\\) indicating inequality of connectivity in the system."
  },
  {
    "objectID": "handout1.html#the-degree-correlation",
    "href": "handout1.html#the-degree-correlation",
    "title": "Basic Network Statistics",
    "section": "The Degree Correlation",
    "text": "The Degree Correlation\nAnother overall network statistic we may want to know is the degree correlation (Newman 2002). How do we compute it? Imagine taking each edge in the network and creating two degree vectors, one based on the degree of the node in one end and the degre of the node in another. Then the degree assortativity coefficient is just the Pearson product moment correlation between these two vectors.\nLet’s see how this would work for the Pulp Fiction network. First we need to extract an edge list from the graph:\n\n   g.el <- as_edgelist(g) #transforming graph to edgelist\n   head(g.el)\n\n     [,1]    [,2]       \n[1,] \"BRETT\" \"MARSELLUS\"\n[2,] \"BRETT\" \"MARVIN\"   \n[3,] \"BRETT\" \"ROGER\"    \n[4,] \"BRETT\" \"VINCENT\"  \n[5,] \"BUDDY\" \"MIA\"      \n[6,] \"BUDDY\" \"VINCENT\"  \n\n\nWe can see that the as_edgelist function takes the igraph network object as input and returns an \\(E \\times 2\\) matrix, with \\(E = 102\\) being the number of rows. Each column of the matrix records the name of the node on each end of the edge. So the first row of the edge list with entries “BRETT” and “MARSELLUS” tells us that there is an edge linking Brett and Marsellus, and so forth for each row.\nTo compute the correlation between the degrees of each node, all we need to do is attach the corresponding degrees to each name for each of the columns of the edge list, which can be done via data wrangling magic from the dplyr package (part of the tidyverse):\n\n   # install.packages(dplyr)\n   library(dplyr)\n   deg.dat <- data.frame(name1 = names(d), name2 = names(d), d)\n   el.temp <- data.frame(name2 = g.el[, 2]) %>% \n      left_join(deg.dat, by = \"name2\") %>% \n      dplyr::select(c(\"name2\", \"d\")) %>% \n      rename(d2 = d) \n   d.el <- data.frame(name1 = g.el[, 1]) %>% \n      left_join(deg.dat, by = \"name1\") %>% \n      dplyr::select(c(\"name1\", \"d\")) %>% \n      rename(d1 = d) %>% \n      cbind(el.temp)\nhead(d.el)\n\n  name1 d1     name2 d2\n1 BRETT  7 MARSELLUS 10\n2 BRETT  7    MARVIN  6\n3 BRETT  7     ROGER  6\n4 BRETT  7   VINCENT 25\n5 BUDDY  2       MIA 11\n6 BUDDY  2   VINCENT 25\n\n\nLine 3 creates a two-column data frame called “deg.dat” with as many rows as there are nodes in the network. The first two columns contain the names of each node (identically listed with different names) and the third columns contains the corresponding node’s degree.\nLines 4-7 use dplyr functions to create a new object “el.temp” joining the degree information to each of the node names listed in the second position in the original edge list “g.el,” and rename the imported column of degrees “d2.”\nLines 8-12 do the same for the nodes listed in the first position in the original edge list, renames the imported columns of degrees “d1,” and the binds the columns of the “el.temp” object to the new object “d.el.” The resulting object has four columns: Two for the names of the nodes incident to each edge on the edge list (columns 1 and 3), and two other ones corresponding to the degrees of the corresponding nodes (columns 2 and 4).\nWe can see from the output of the first few rows of the “d.el” object that indeed “BRETT” is assigned a degree of 7 in each row of the edge list, “BUDDY” a degree of 2, “MARSELLUS” a degree of 10, “VINCENT” a degree of 25 and so forth.\nNow to compute the degree correlation in the network all we need to do is call the native R function cor on the two columns from “d.el” that containing the degree information. Note that because each degree appears twice at the end of each edge in an undirected graph (as both “sender” and “receiver”), we need to double each column by appending the other degree column at the end. So the first degree column is the vector:\n\n   d1 <- c(d.el$d1, d.el$d2)\n\nAnd the second degree column is the vector:\n\n   d2 <- c(d.el$d2, d.el$d1)\n\nAnd the graph’s degree correlation (Newman 2003) is just the Pearson correlation between these two degree vectors:\n\n   cor(d1, d2)\n\n[1] -0.2896427\n\n\nThe result \\(r_{deg} = -0.29\\) tells us that there is anti-correlation by degree in the Pulp Fiction network. That is high-degree characters tend to appear with low degree characters, or conversely, high-degree characters (like Marsellus and Jules) don’t appear together very often.\nOf course, igraph has a function called assortativity_degree that does all the work for us:\n\n   assortativity_degree(g)\n\n[1] -0.2896427"
  },
  {
    "objectID": "handout1.html#the-average-shortest-path-length",
    "href": "handout1.html#the-average-shortest-path-length",
    "title": "Basic Network Statistics",
    "section": "The Average Shortest Path Length",
    "text": "The Average Shortest Path Length\nThe final statistic people use to characterize networks is the average shortest path length. In a network, even non-adjacent nodes, could be indirectly connected to other nodes via a path of some length (\\(l > 1\\)) So it is useful to know what the average of this quantity is across all dyads in the network.\nTo do that, we first need to compute the length of the shortest path \\(l\\) for each pair of nodes in the network (also known as the geodesic distance). Adjacent nodes get an automatic score of \\(l = 1\\). In igraph this is done as follows:\n\n   S <- distances(g)\n   S[1:7, 1:7]\n\n             BRETT BUDDY BUTCH CAPT KOONS ED SULLIVAN ENGLISH DAVE ESMARELDA\nBRETT            0     2     1          2           2            2         3\nBUDDY            2     0     2          2           2            2         4\nBUTCH            1     2     0          1           2            1         2\nCAPT KOONS       2     2     1          0           2            2         3\nED SULLIVAN      2     2     2          2           0            2         4\nENGLISH DAVE     2     2     1          2           2            0         3\nESMARELDA        3     4     2          3           4            3         0\n\n\nThe igraph function distances takes the network object as input and returns the desired shortest path matrix. So for instance, Brett is directly connected to Butch (they appear in a scene together) but indirectly connected to Buddy via a path of length two (they both appear in scenes with common neighbors even if they don’t appear together).\nThe maximum distance between two nodes in the graph (the longest shortest path to put it confusingly) is called the graph diameter. We can find this out simply by using the native R function for the maximum on the shortest paths matrix:\n\n   max(S)\n\n[1] 8\n\n\nThis means that in the Pulp Fiction network the maximum degree of separation between two characters is a path of length 8.\nOf course, we cann also call the igraph function diammeter:\n\n   diameter(g)\n\n[1] 8\n\n\nOnce we have the geodesic distance matrix, it is easy to calculate the average path length of the graph:\n\n   rs.S <- rowSums(S)\n   rm.S <- rs.S/(vcount(g) - 1)\n   mean(rm.S)\n\n[1] 2.769559\n\n\n\nFirst (line 1) we sum all the rows (or columns) of the geodesic distance matrix. This vector (of the same length as the number of nodes) gives us the sum of the geodesic distance of each node to each of the nodes (we will use this to compute closeness centrality later).\nThen (line 2) we divide this vector by the number of nodes minus one (to exclude the focal node) to create a vector of the average distance of each node to each of the other nodes.\nFinally (line 3) we take the average across all nodes of this average distance vector to get the graph’s average shortest path length, which in this case equals L = 2.8.\n\nThis means that, on average, each character in Pulp Fiction is separated by little less than three contacts in the co-appearance network (a fairly small world).\nOf course this can also be done in just one step on igraph:\n\n   mean_distance(g)\n\n[1] 2.769559"
  },
  {
    "objectID": "handout1.html#putting-it-all-together",
    "href": "handout1.html#putting-it-all-together",
    "title": "Basic Network Statistics",
    "section": "Putting it all Together",
    "text": "Putting it all Together\nNow we can put together all the basic network statistics that we have computed into some sort of summary table, like the ones here. We first create a vector with the names of each statistic:\n\n   Stat <- c(\"Nodes\", \"Edges\", \"Min. Degree\", \"Max. Degree\", \"Avg. Degree\", \"Degree Corr.\", \"Diameter\", \"Avg. Shortest Path Length\")\n\nThen we create a vector with the values:\n\n   Value <- c(vcount(g), ecount(g), min(d), max(d), round(mean(d), 2), round(assortativity_degree(g), 2), max(S), round(mean_distance(g), 2))\n\nWe can then put these two vector together into a data frame:\n\n   net.stats <- data.frame(Stat, Value)\n\nWe can then use the package kableExtra (a nice table maker) to create a nice html table:\n\n   # intall.packages(kableExtra)\n   library(kableExtra)\n   kbl(net.stats, format = \"pipe\", align = c(\"l\", \"c\"),\n       caption = \"Key Statistics for Pulp Fiction Network.\") %>% \n   kable_styling(bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\nKey Statistics for Pulp Fiction Network.\n \n  \n    Stat \n    Value \n  \n \n\n  \n    Nodes \n    38.00 \n  \n  \n    Edges \n    102.00 \n  \n  \n    Min. Degree \n    1.00 \n  \n  \n    Max. Degree \n    25.00 \n  \n  \n    Avg. Degree \n    5.37 \n  \n  \n    Degree Corr. \n    -0.29 \n  \n  \n    Diameter \n    8.00 \n  \n  \n    Avg. Shortest Path Length \n    2.77"
  },
  {
    "objectID": "handout1.html#appendix-loading-network-data-from-a-file",
    "href": "handout1.html#appendix-loading-network-data-from-a-file",
    "title": "Basic Network Statistics",
    "section": "Appendix: Loading Network Data from a File",
    "text": "Appendix: Loading Network Data from a File\nWhen get network data from an archival source, and it will be in the form of a matrix or an edge list, typically in some kind of comma separated value (csv) format. Here will show how to input that into R to create an igraph network object from an outside file.\nFirst we will write the Pulp Fiction data into an edge list and save it to disk. We already did that earlier with the “g.el” object. So all we have to do is save it to your local folder as a csv file:\n\n   #install.packages(here)\n   library(here)\n   write.csv(d.el[c(\"name1\", \"name2\")], here(\"pulp.csv\"))\n\nThe write.csv function just saves an R object into a .csv file. Here the R object is “g.el” and we asked it to save just the columns which contain the name of each character. This represents the adjacency relations in the network as an edge list. We use the package here to keep track of our working directory. See here (pun intended) for details.\nNow suppose that’s the network we want to work with and it’s saved in our hard drive. To load it, we just type:\n\n   g.el <- read.csv(here(\"pulp.csv\"), \n                    col.names = c(\"name1\", \"name2\"))\n   head(g.el)\n\n  name1     name2\n1 BRETT MARSELLUS\n2 BRETT    MARVIN\n3 BRETT     ROGER\n4 BRETT   VINCENT\n5 BUDDY       MIA\n6 BUDDY   VINCENT\n\n\nWhich gives us the edge list we want now saved into an R object of class data.frame. So all we need is to convert that into an igraph object. To do that we use one of the many graph_from... functions in the igraph package. In this case, we want graph_from_edgelist because our network is stored as an edge list:\n\n   g.el <- as.matrix(g.el)\n   g <- graph_from_edgelist(g.el, directed = FALSE)\n   V(g)\n\n+ 38/38 vertices, named, from 0438684:\n [1] BRETT           MARSELLUS       MARVIN          ROGER          \n [5] VINCENT         BUDDY           MIA             BUTCH          \n [9] CAPT KOONS      ESMARELDA       GAWKER #2       JULES          \n[13] PEDESTRIAN      SPORTSCASTER #1 ENGLISH DAVE    FABIENNE       \n[17] FOURTH MAN      HONEY BUNNY     MANAGER         JIMMIE         \n[21] JODY            PATRON          PUMPKIN         RAQUEL         \n[25] WINSTON         LANCE           MAYNARD         THE GIMP       \n[29] ZED             ED SULLIVAN     MOTHER          WOMAN          \n[33] PREACHER        SPORTSCASTER #2 THE WOLF        WAITRESS       \n[37] YOUNG MAN       YOUNG WOMAN    \n\n   E(g)\n\n+ 102/102 edges from 0438684 (vertex names):\n [1] BRETT      --MARSELLUS       BRETT      --MARVIN         \n [3] BRETT      --ROGER           BRETT      --VINCENT        \n [5] BUDDY      --MIA             VINCENT    --BUDDY          \n [7] BRETT      --BUTCH           BUTCH      --CAPT KOONS     \n [9] BUTCH      --ESMARELDA       BUTCH      --GAWKER #2      \n[11] BUTCH      --JULES           MARSELLUS  --BUTCH          \n[13] BUTCH      --PEDESTRIAN      BUTCH      --SPORTSCASTER #1\n[15] BUTCH      --ENGLISH DAVE    BRETT      --FABIENNE       \n[17] BUTCH      --FABIENNE        JULES      --FABIENNE       \n[19] JULES      --FOURTH MAN      VINCENT    --FOURTH MAN     \n+ ... omitted several edges\n\n\nWhich gives us back the original igraph object we have been working with. Note that first we converted the data.frame object into a matrix object. We also specified that the graph is undirected by setting the option directed to false."
  },
  {
    "objectID": "handout2.html",
    "href": "handout2.html",
    "title": "Centrality",
    "section": "",
    "text": "In this handout we will go through the basic centrality metrics. Particularly, the “big three” according to Freeman (1979), namely, degree, closeness (in two flavors) and betweenness.\nWe first load our trusty Pulp Fiction data set from the networkdata package, which is an undirected graph of character scene co-appearances in the film:"
  },
  {
    "objectID": "handout2.html#degree-centrality",
    "href": "handout2.html#degree-centrality",
    "title": "Centrality",
    "section": "Degree Centrality",
    "text": "Degree Centrality\nDegree centrality is the simplest and most straightforward measure. In fact, we are already computed in handout 1 since it is the same as obtaining the graph’s degree sequence. So the igraph function degree would do it as we already saw.\nHere we follow a different approach using the row (or column) sums of the graph’s adjacency matrix:\n\n   A <- as_adjacency_matrix(g)\n   A <- as.matrix(A)\n   rowSums(A)\n\n          BRETT           BUDDY           BUTCH      CAPT KOONS     ED SULLIVAN \n              7               2              17               5               2 \n   ENGLISH DAVE       ESMARELDA        FABIENNE      FOURTH MAN       GAWKER #2 \n              4               1               3               2               3 \n    HONEY BUNNY          JIMMIE            JODY           JULES           LANCE \n              8               3               4              16               4 \n        MANAGER       MARSELLUS          MARVIN         MAYNARD             MIA \n              5              10               6               3              11 \n         MOTHER          PATRON      PEDESTRIAN        PREACHER         PUMPKIN \n              5               5               3               3               8 \n         RAQUEL           ROGER SPORTSCASTER #1 SPORTSCASTER #2        THE GIMP \n              3               6               2               1               2 \n       THE WOLF         VINCENT        WAITRESS         WINSTON           WOMAN \n              3              25               4               3               5 \n      YOUNG MAN     YOUNG WOMAN             ZED \n              4               4               2 \n\n\nThe igraph function as_adjancency_matrix doesn’t quite return a regular R matrix object, so we have to further coerce the resulting object into a numerical matrix containing zeroes and ones using the as.matrix function in line 2. Then we can apply the native rowSums function to obtain each node’s degree. Note that this is same output we got using the degree function before."
  },
  {
    "objectID": "handout2.html#indegree-and-outdegree",
    "href": "handout2.html#indegree-and-outdegree",
    "title": "Centrality",
    "section": "Indegree and Outdegree",
    "text": "Indegree and Outdegree\nThe movie network is based on the relationship of co-appearance in a scene which by nature lacks any natural directionality (it’s a symmetric relation) and can therefore be represented in an undirected graph. The concepts of in and outdegree, by contrast, are only applicable to directed relations. So to illustrate them, we need to switch to a different source of data.\nWe pick an advice network which is a classical directed kind of (asymmetric) relation. I can give advice to you, but that doesn’t necessarily mean you can give advice to me. The networkdata package contains one such data set collected in the late 80s early 1990s in a New England law firm (see the description here), called law_advice:\n\n   d.g <- law_advice\n   V(d.g)\n\n+ 71/71 vertices, from d1a9da7:\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50\n[51] 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n\n   vertex_attr(d.g)\n\n$status\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2\n[39] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n\n$gender\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 1 1 1 1 2 1 1 1 2\n[39] 2 1 1 1 2 2 1 2 1 2 1 1 2 1 1 1 1 1 2 1 2 2 2 1 1 2 1 1 2 1 2 1 2\n\n$office\n [1] 1 1 2 1 2 2 2 1 1 1 1 1 1 2 3 1 1 2 1 1 1 1 1 1 2 1 1 2 1 2 2 2 2 1 2 1 3 1\n[39] 1 1 1 1 1 3 1 2 3 1 1 2 2 1 1 1 1 1 1 2 2 1 1 1 2 1 1 1 1 1 1 1 1\n\n$seniority\n [1] 31 32 13 31 31 29 29 28 25 25 23 24 22  1 21 20 23 18 19 19 17  9 16 15 15\n[26] 15 13 11 10  7  8  8  8  8  8  5  5  7  6  6  5  4  5  5  3  3  3  1  4  3\n[51]  4  4 10  3  3  3  3  3  2  2  2  2  2  2  2  1  1  1  1  1  1\n\n$age\n [1] 64 62 67 59 59 55 63 53 53 53 50 52 57 56 48 46 50 45 46 49 43 49 45 44 43\n[26] 41 47 38 38 39 34 33 37 36 33 43 44 53 37 34 31 31 47 53 38 42 38 35 36 31\n[51] 29 29 38 29 34 38 33 33 30 31 34 32 29 45 28 43 35 26 38 31 26\n\n$practice\n [1] 1 2 1 2 1 1 2 1 2 2 1 2 1 2 2 2 2 1 2 1 1 1 1 1 2 1 1 2 2 1 1 1 1 2 2 1 2 1\n[39] 1 1 1 2 1 2 2 2 1 2 1 2 1 1 2 1 1 1 1 1 1 2 2 2 2 2 1 1 1 1 1 2 1\n\n$law_school\n [1] 1 1 1 3 2 1 3 3 1 3 1 2 2 1 3 1 1 2 1 1 2 3 2 2 2 3 1 2 3 3 2 3 3 2 3 3 3 2\n[39] 1 1 2 2 2 1 3 2 3 3 2 2 3 3 3 3 3 2 2 3 2 2 3 2 2 2 3 3 2 3 3 2 2\n\n\nWe can see that the graph has 71 vertices, and that there are various attributes associated with each vertex, like gender, age, seniority, status in the law firm, etc. We can query those attributes using the igraph function vertex_attr, which takes the graph object as input.\n\nSubsetting the Graph According to a Node Attribute\nTo keep things manageable, we will restrict our analysis to partners. To do that we need to select the subgraph that only includes the vertices with value of 1 in the “status” vertex attribute. From the data description, we know the first 36 nodes (with value of 1 in the status attribute) are the law firm’s partners (the rest are associates). In igraph we can do this as using the subgraph function:\n\n   d.g <- subgraph(d.g, 1:36)\n   V(d.g)\n\n+ 36/36 vertices, from 06b4657:\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30 31 32 33 34 35 36\n\n   V(d.g)$status\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n\nThe first line just tells igraph to generate the subgraph containing the first 36 nodes (the partners). The subgraph function thus takes two main inputs: The graph object, and then a vector of node ids (or node labels) telling the function which nodes to select to create the node-induced subgraph.\nOf course we already knew from the data description that the first 36 nodes where the partners. But let’s say we have a large data set and we don’t know which nodes are the partners. A smarter way of selecting a subgraph based on a node attribute is as follows:\n\n   partners <- which(V(law_advice)$status == 1)\n   d.g <- subgraph(law_advice, partners)\n   V(d.g)\n\n+ 36/36 vertices, from 06babc7:\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30 31 32 33 34 35 36\n\n\nThe first line using the native R vector function which allowing us to subset a vector based on a logical condition. The function takes a vector followed by a logical condition as input, and returns the position of the vector elements that meet that condition. In this case, we took the vector of values for the attribute of status and selected the node ids where status is equal to 1. We then fed that vector to the subgraph function in line 2.\nWe could do this with any other attribute:\n\n   older <- which(V(law_advice)$age > 50)\n   older\n\n [1]  1  2  3  4  5  6  7  8  9 10 12 13 14 38 44\n\n   og <- subgraph(law_advice, older)\n   V(og)\n\n+ 15/15 vertices, from 06c08a8:\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15\n\n\nHere we selected the subgraph (called “og”, get it, get it) formed by the subset of nodes over the age of 50 at the firm. The values of the vector older tell us which of the 71 members meet the relevant condition.\n\n\nComputing in and outdegree\nOK, going back to the partners subgraph, we can now create our (asymmetric) adjacency matrix and compute the row and column sums:\n\n   d.A <- as_adjacency_matrix(d.g)\n   d.A <- as.matrix(d.A)\n   rowSums(d.A)\n\n [1]  3  7  7 17  4  0  4  2  3  7  5 18 11 13 10 19 17  5 21 10  9 12  9 16  8\n[26] 22 18 22 13 15 16  9 15  6 15  7\n\n   colSums(d.A)\n\n [1] 18 17  8 14 10 17  4  8 10  6 11 14 14 12 15 13 21  7  7 17 11 10  2 14  7\n[26] 20  2 14 12 11  8 13  2 16  8  2\n\n\nNote that in contrast to the undirected case the row and column sums give you two different sets of numbers. The row sums provide the directed graph’s outdegree set (number of outgoing links incident to each node), and the column sums provide the graph’s indegree set (number of incoming links incident to each node). So if you are high in the first vector, you are an advice giver (perhaps indicating informal status or experience) and if you are high in the second you are advice taker.\nOf course igraph has a dedicated function for this, which is just our old friend degree with an extra option mode, indicating whether you want the in or outdegrees:\n\n   d.o <- degree(d.g, mode = \"out\")\n   d.i <- degree(d.g, mode = \"in\")\n   d.o\n\n [1]  3  7  7 17  4  0  4  2  3  7  5 18 11 13 10 19 17  5 21 10  9 12  9 16  8\n[26] 22 18 22 13 15 16  9 15  6 15  7\n\n   d.i\n\n [1] 18 17  8 14 10 17  4  8 10  6 11 14 14 12 15 13 21  7  7 17 11 10  2 14  7\n[26] 20  2 14 12 11  8 13  2 16  8  2\n\n\nNote that the graph attributes are just vectors of values, and can be accessed from the graph object using the $ operator attached to the V() function as we did above.\nSo if we wanted to figure out the correlation between some vertex attribute and in or out degree centrality, all we need to do is correlate the two vectors:\n\n   r <- cor(d.o, V(d.g)$age)\n   round(r, 2)\n\n[1] -0.43\n\n\nWhich tells us that at least in this case, younger partners are more sought after as sources of advice than older partners."
  },
  {
    "objectID": "handout2.html#closeness-centrality",
    "href": "handout2.html#closeness-centrality",
    "title": "Centrality",
    "section": "Closeness Centrality",
    "text": "Closeness Centrality\nRecall that the closeness centrality is defined as the inverse of the sum of the lengths of shortest paths from each node to every other node. That means that to compute it, we first need to calculate the shortest path matrix. Then, we sum the rows (or columns) and then we obtain the inverse to get the closeness of each node:\n\n   S <- distances(g) #length of shortest paths matrix\n   d.sum <- rowSums(S)\n   close1 <- round(1/d.sum, 4)\n   close1\n\n          BRETT           BUDDY           BUTCH      CAPT KOONS     ED SULLIVAN \n         0.0125          0.0108          0.0143          0.0125          0.0108 \n   ENGLISH DAVE       ESMARELDA        FABIENNE      FOURTH MAN       GAWKER #2 \n         0.0105          0.0070          0.0096          0.0104          0.0097 \n    HONEY BUNNY          JIMMIE            JODY           JULES           LANCE \n         0.0114          0.0093          0.0093          0.0132          0.0093 \n        MANAGER       MARSELLUS          MARVIN         MAYNARD             MIA \n         0.0111          0.0104          0.0103          0.0098          0.0115 \n         MOTHER          PATRON      PEDESTRIAN        PREACHER         PUMPKIN \n         0.0125          0.0111          0.0097          0.0097          0.0114 \n         RAQUEL           ROGER SPORTSCASTER #1 SPORTSCASTER #2        THE GIMP \n         0.0105          0.0125          0.0096          0.0057          0.0073 \n       THE WOLF         VINCENT        WAITRESS         WINSTON           WOMAN \n         0.0056          0.0139          0.0083          0.0105          0.0125 \n      YOUNG MAN     YOUNG WOMAN             ZED \n         0.0083          0.0083          0.0073 \n\n\nOf course, we could have just used the available function in igraph and computed the closeness centrality directly from the graph object using the function closeness:\n\n   close2 <- round(closeness(g), 4)\n   close2\n\n          BRETT           BUDDY           BUTCH      CAPT KOONS     ED SULLIVAN \n         0.0125          0.0108          0.0143          0.0125          0.0108 \n   ENGLISH DAVE       ESMARELDA        FABIENNE      FOURTH MAN       GAWKER #2 \n         0.0105          0.0070          0.0096          0.0104          0.0097 \n    HONEY BUNNY          JIMMIE            JODY           JULES           LANCE \n         0.0114          0.0093          0.0093          0.0132          0.0093 \n        MANAGER       MARSELLUS          MARVIN         MAYNARD             MIA \n         0.0111          0.0104          0.0103          0.0098          0.0115 \n         MOTHER          PATRON      PEDESTRIAN        PREACHER         PUMPKIN \n         0.0125          0.0111          0.0097          0.0097          0.0114 \n         RAQUEL           ROGER SPORTSCASTER #1 SPORTSCASTER #2        THE GIMP \n         0.0105          0.0125          0.0096          0.0057          0.0073 \n       THE WOLF         VINCENT        WAITRESS         WINSTON           WOMAN \n         0.0056          0.0139          0.0083          0.0105          0.0125 \n      YOUNG MAN     YOUNG WOMAN             ZED \n         0.0083          0.0083          0.0073 \n\n\nOnce we have the closeness centrality values, we are interested in who are the top nodes. The following code creates a table with the top five:\n\n   library(kableExtra)\n   close2 <- sort(close2, decreasing = TRUE)\n   close2 <- data.frame(close2[1:5])\n   kbl(close2, format = \"pipe\", align = c(\"l\", \"c\"),\n       col.names = c(\"Character\", \"Closeness\"),\n       caption = \"Top Five Closeness Characters in Pulp Fiction Network.\") %>% \n   kable_styling(bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\nTop Five Closeness Characters in Pulp Fiction Network.\n \n  \n    Character \n    Closeness \n  \n \n\n  \n    BUTCH \n    0.0143 \n  \n  \n    VINCENT \n    0.0139 \n  \n  \n    JULES \n    0.0132 \n  \n  \n    BRETT \n    0.0125 \n  \n  \n    CAPT KOONS \n    0.0125 \n  \n\n\n\n\n\nIt makes sense that the three main characters are also the ones that are at closest distances from everyone else!"
  },
  {
    "objectID": "handout2.html#edge-closeness",
    "href": "handout2.html#edge-closeness",
    "title": "Centrality",
    "section": "Edge Closeness",
    "text": "Edge Closeness\nBröhl and Lehnertz (2022) define the closeness of an edge as a function of the closeness of the two nodes incident to it. An edge \\(e_{jk}\\) linking vertex \\(v_j\\) to \\(v_k\\) has high closeness whenever vertices \\(v_j\\) and \\(v_k\\) also have high closeness.\nMore specifically, the closeness centrality of an edge is proportional to the ratio of the product of the closeness of the two nodes incident to it divided by their sum:\n\\[\nC(e_{jk}) = (E - 1)\\frac{C(v_j) \\times C(v_k)}{C(v_j)+C(v_k)}\n\\]\nNote that the equation normalizes the ratio of the product to the sum of the vertex closeness centralities by the number of edges minus one.\nTo compute edge closeness in a real network, we can use the same approach to data wrangling we used to compute the degree correlation in Handout 1. The goal is to create an edge list data frame containing five columns. The ids of the two nodes in the edge, the closeness centralities of the two nodes in the edge, and the closeness centrality of the edge calculated according to the above equation.\nIn the Pulp Fiction network this looks like:\n\n   library(dplyr)\n   g.el <- as_edgelist(g) #transforming graph to edgelist\n   c <- round(closeness(g), 3)  #closeness centrality vector\n   c.dat <- data.frame(name1 = names(c), name2 = names(c), c)\n   el.temp <- data.frame(name2 = g.el[, 2]) %>% \n      left_join(c.dat, by = \"name2\") %>% \n      dplyr::select(c(\"name2\", \"c\")) %>% \n      rename(c2 = c) \n   c.el <- data.frame(name1 = g.el[, 1]) %>% \n      left_join(c.dat, by = \"name1\") %>% \n      dplyr::select(c(\"name1\", \"c\")) %>% \n      rename(c1 = c) %>% \n      cbind(el.temp) %>% \n      mutate(e.clos = round((ecount(g)-1)*(c1*c2)/(c+c2), 3))\nhead(c.el)\n\n  name1    c1     name2    c2 e.clos\n1 BRETT 0.013 MARSELLUS 0.010  0.571\n2 BRETT 0.013    MARVIN 0.010  0.625\n3 BRETT 0.013     ROGER 0.013  0.632\n4 BRETT 0.013   VINCENT 0.014  0.681\n5 BUDDY 0.011       MIA 0.011  0.555\n6 BUDDY 0.011   VINCENT 0.014  0.622\n\n\nTo create a table of the top five closeness centrality edges, we just order the data frame by the last column and table it:\n\n   c.el <- c.el[order(c.el$e.clos, decreasing = TRUE), ] %>% \n      dplyr::select(c(\"name1\", \"name2\", \"e.clos\"))\n\n   kbl(c.el[1:5, ], format = \"pipe\", align = c(\"l\", \"l\", \"c\"),\n       col.names = c(\"i\", \"j\", \"Edge Clos.\"), row.names = FALSE,\n       caption = \"Edges Sorted by Closeness in the Pulp Fiction Network\") %>% \n   kable_styling(bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\nEdges Sorted by Closeness in the Pulp Fiction Network\n \n  \n    i \n    j \n    Edge Clos. \n  \n \n\n  \n    BUTCH \n    VINCENT \n    0.900 \n  \n  \n    BRETT \n    BUTCH \n    0.875 \n  \n  \n    MOTHER \n    VINCENT \n    0.875 \n  \n  \n    BUTCH \n    MIA \n    0.864 \n  \n  \n    JULES \n    PUMPKIN \n    0.850 \n  \n\n\n\n\n\nInterestingly, the top closeness edges tend to bring somewhat strange bedfellows, characters that themselves don’t spend much time together in the film (e.g., the Butch/Vincent interaction is relatively brief and somewhat embarrassing for Vincent) but who themselves can reach other character clusters in the film via relatively short paths."
  },
  {
    "objectID": "handout2.html#closeness-centrality-in-directed-graphs",
    "href": "handout2.html#closeness-centrality-in-directed-graphs",
    "title": "Centrality",
    "section": "Closeness Centrality in Directed Graphs",
    "text": "Closeness Centrality in Directed Graphs\nWhat about closeness centrality for a directed network? Let us see how this works using a subgraph of the advice network, this time selecting just women under the age of forty:\n\n   women <- which(V(law_advice)$gender == 2)\n   wg <- subgraph(law_advice, women)\n   young <- which(V(wg)$age < 40)\n   wg <- subgraph(wg, young)\n   V(wg)\n\n+ 12/12 vertices, from 07531b8:\n [1]  1  2  3  4  5  6  7  8  9 10 11 12\n\n\nThis network is small enough that a plot could be informative about its structure. Let us plot it using the package ggraph, a visualization package that follows the same principles as the ggplot grammar of graphics but for network graphs (see here).\n\n   #install.packages(\"ggraph\")\n   library(ggraph)\n    p <- ggraph(wg, layout = 'auto')\n    p <- p + geom_edge_parallel(color = \"steelblue\", edge_width = 0.5,\n                                arrow = arrow(length = unit(2.5, 'mm')),\n                                end_cap = circle(4, 'mm'), \n                                sep = unit(3, 'mm'))\n    p <- p + geom_node_point(aes(x = x, y = y), size = 8, color = \"tan2\") \n    p <- p + geom_node_text(aes(label = 1:vcount(wg)), size = 4, color = \"white\")\n    p <- p + theme_graph() \n    p\n\n\n\n\nWomen lawyers advice network\n\n\n\n\nNow a question we might ask is who has the greatest closeness centrality in this advice network. We could proceed as usual and compute the geodesic distances between actors:\n\n   S <- distances(wg)\n   S\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]\n [1,]    0    1    2    1    3    3    4    2    2     3     3     3\n [2,]    1    0    2    1    2    3    3    1    1     3     3     3\n [3,]    2    2    0    1    1    1    2    2    3     1     1     1\n [4,]    1    1    1    0    2    2    3    2    2     2     2     2\n [5,]    3    2    1    2    0    1    1    1    2     2     2     2\n [6,]    3    3    1    2    1    0    2    2    3     1     2     1\n [7,]    4    3    2    3    1    2    0    2    3     3     3     3\n [8,]    2    1    2    2    1    2    2    0    1     3     3     3\n [9,]    2    1    3    2    2    3    3    1    0     4     4     4\n[10,]    3    3    1    2    2    1    3    3    4     0     1     1\n[11,]    3    3    1    2    2    2    3    3    4     1     0     1\n[12,]    3    3    1    2    2    1    3    3    4     1     1     0\n\n\nNote that this is not quite right. In igraph the default settings of the distance function treats the graph as undirected. So it doesn’t use the strict directed paths, but it just treats them all as semi-paths ignoring direction. That is why, for instance, it counts node 1 as being “adjacent” to node 4 even though there is only one incoming link from 4 to 1 and why the whole matrix is symmetric, when we know from just eyeballing the network that there is a lot of asymmetry in terms of who can reach who via directed paths.\nTo get the actual directed distance matrix, we need to specify the “mode” option, asking whether we want in or out paths. Here, let’s select out-paths:\n\n   S <- distances(wg, mode = \"out\")\n   S\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]\n [1,]    0    1  Inf  Inf  Inf  Inf  Inf  Inf  Inf   Inf   Inf   Inf\n [2,]    1    0  Inf  Inf  Inf  Inf  Inf  Inf  Inf   Inf   Inf   Inf\n [3,]    2    2    0    1  Inf    1  Inf  Inf  Inf   Inf   Inf   Inf\n [4,]    1    1    1    0  Inf    2  Inf  Inf  Inf   Inf   Inf   Inf\n [5,]    3    2    1    2    0    1    1    1    2   Inf   Inf   Inf\n [6,]    3    3    1    2  Inf    0  Inf  Inf  Inf   Inf   Inf   Inf\n [7,]    4    3    2    3    1    2    0    2    3   Inf   Inf   Inf\n [8,]    2    1  Inf  Inf  Inf  Inf  Inf    0    1   Inf   Inf   Inf\n [9,]    2    1  Inf  Inf  Inf  Inf  Inf    1    0   Inf   Inf   Inf\n[10,]    3    3    1    2  Inf    1  Inf  Inf  Inf     0     1     2\n[11,]    3    3    1    2  Inf    2  Inf  Inf  Inf     1     0     1\n[12,]    3    3    1    2  Inf    1  Inf  Inf  Inf     1     1     0\n\n\nThis is better but introduces a problem. The directed graph is not strongly connected, so it means that some nodes cannot reach other ones via a directed path of any length. That means that the geodesic distances from a node to an unreachable node is coded as “infinite” (Inf). The problem with infinity is that it gets in the way of calculating sums of distances, a requirement for the closeness centrality.\n\n   S <- distances(wg, mode = \"out\")\n   rowSums(S)\n\n [1] Inf Inf Inf Inf Inf Inf Inf Inf Inf Inf Inf Inf\n\n\nAdding infinity to a number just returns infinity so all the rows with at least one “Inf” in the distance matrix get an Inf for the row sum. In this case that’s all of them. A bummer.\n\nHarmonic Centrality\nBut dont’ worry there’s a patch. It is called the harmonic centrality (Rochat 2009).1 This is a variation on the closeness centrality that works whether you are working with connected or disconnected graphs (or in the case of directed graphs regardless of whether the graph is strongly or weakly connected), and therefore regardless of whether the geodesic distance matrix contains Infs.2\nThe main difference between the harmonic and regular closeness centrality is that instead of calculating the inverse of the sum of the distances for each node, we calculate the sum of the inverses:\n\n   S <- distances(wg, mode = \"out\")\n   S = round(1/S, 2) \n   diag(S) <- 0 #setting diagonals to zero\n   S\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]\n [1,] 0.00 1.00  0.0 0.00    0  0.0    0  0.0 0.00     0     0   0.0\n [2,] 1.00 0.00  0.0 0.00    0  0.0    0  0.0 0.00     0     0   0.0\n [3,] 0.50 0.50  0.0 1.00    0  1.0    0  0.0 0.00     0     0   0.0\n [4,] 1.00 1.00  1.0 0.00    0  0.5    0  0.0 0.00     0     0   0.0\n [5,] 0.33 0.50  1.0 0.50    0  1.0    1  1.0 0.50     0     0   0.0\n [6,] 0.33 0.33  1.0 0.50    0  0.0    0  0.0 0.00     0     0   0.0\n [7,] 0.25 0.33  0.5 0.33    1  0.5    0  0.5 0.33     0     0   0.0\n [8,] 0.50 1.00  0.0 0.00    0  0.0    0  0.0 1.00     0     0   0.0\n [9,] 0.50 1.00  0.0 0.00    0  0.0    0  1.0 0.00     0     0   0.0\n[10,] 0.33 0.33  1.0 0.50    0  1.0    0  0.0 0.00     0     1   0.5\n[11,] 0.33 0.33  1.0 0.50    0  0.5    0  0.0 0.00     1     0   1.0\n[12,] 0.33 0.33  1.0 0.50    0  1.0    0  0.0 0.00     1     1   0.0\n\n\nNote that in this matrix of inverse distances, the closest (adjacent) nodes get the maximum score of one, and nodes farther apart when smaller scores (approaching zero). More importantly, those pesky Infs disappear (!) because unreachable directed pairs of nodes get the lowest score, corresponding to \\(1/\\infty = 0\\). Turns out the mathematics of infinity weren’t our enemy after all.\nAlso note that the reachability relation expressed in this matrix is asymmetric: So node 4 and reach node 1 (there is a directed tie from 4 to 1), but node 1 cannot reach 4. This is precisely what we want.\nOnce we have this matrix of inverse distances, we can then we can compute the harmonic centrality the same way as regular closeness by adding up the row scores for each node and dividing by the number of nodes minus one (to get the average):\n\n   d.harm <- rowSums(S)\n   d.harm <- d.harm/(vcount(wg) - 1)\n   d.harm <- round(d.harm, 2)\n   d.harm\n\n [1] 0.09 0.09 0.27 0.32 0.53 0.20 0.34 0.23 0.23 0.42 0.42 0.47\n\n\nWe can see that the highest harmonic closeness centrality node is 5, followed by 12. Here’s a plot of the network highlighting the highest harmonic (closeness) centrality node.\n\n   col <- rep(\"tan2\", vcount(wg)) #creating node color vector\n   col[which(d.harm == max(d.harm))] <- \"red\" #changing color of max centrality node to red\n   p <- p + geom_node_point(aes(x = x, y = y), size = 8, color = col)\n   p <- p + geom_node_text(aes(label = 1:vcount(wg)), size = 4, color = \"white\")\n   p\n\n\n\n\nWomen lawyers advice network with highest closeness centrality node in red\n\n\n\n\nOf course, igraph has a built in function to calculate the harmonic centrality called (you guessed it) harmonic_centrality:\n\n   d.harm <- harmonic_centrality(wg, normalized = TRUE)\n   d.harm <- round(d.harm, 2)\n   d.harm\n\n [1] 0.09 0.09 0.27 0.32 0.53 0.20 0.34 0.23 0.23 0.42 0.42 0.47\n\n\nWhich gives us the same results."
  },
  {
    "objectID": "handout2.html#betweenness",
    "href": "handout2.html#betweenness",
    "title": "Centrality",
    "section": "Betweenness",
    "text": "Betweenness\nWe finally come to betweenness centrality. Recall that the key conceptual distinction between closeness and betweenness according to Freeman (1979) is that between (pun intended) the capacity to reach others quickly (e.g., via the shortest paths) and the capacity to intermediate among those same paths. High betweenness nodes control the flow of information in the network between other nodes.\nThis is evident in the way betweenness is calculated. Recall that the betweenness of a node k relative to any pair of nodes i and j in the network is simply:\n\\[\n\\frac{\\sigma_{i(k)j}}{\\sigma_{ij}}\n\\]\nWhere the denominator of the fraction (\\(\\sigma_{ij}\\)) is a count of the total number of shortest paths that start and end with nodes i and j and the numerator of the fraction (\\(\\sigma_{i(k)j}\\)) is the subset of those paths that include node k as an inner node.\nAs Freeman (1979) also notes because this is a ratio, it can range from zero to one, with everything in between. As such the betweenness centrality of a node relative to any two others has an intuitive interpretation as a probability, namely the probability that if you send something from i to j it has to go through k. This probability is 1.0 if k stands in every shortest path between i and j and zero if they stand in none of the shortest paths indirectly connecting i and j.\nThe betweenness of a given node is just the sum all of these probabilities across every pair of nodes in the graph for each node:\n\\[\n\\sum_{i \\neq j, i \\neq n, j \\neq v} \\frac{\\sigma_{i(k)j}}{\\sigma_{ij}}\n\\]\nBelow we can see a point and line diagram of the undirectd Pulp Fiction network we have been working with.\n\n\n\n\n\nPulp Fiction character schene co-appearance network.\n\n\n\n\nWe should expect a character to have high betweenness in this network to the extent that they appear in scenes with characters who themselves don’t appear in any scenes together, thus inter-mediating between different parts of the story. Characters who only appear in one scene with some others (like The Wolf or The Gimp) are likely to be low in betweenness.\nLet’s create a top ten table of betweenness for the Pulp Fiction network. We use the igraph function betweenness to calculate the scores:\n\n   pulp.bet <- betweenness(g)\n   top.5.bet <- sort(pulp.bet, decreasing = TRUE)[1:10]\n   kbl(round(top.5.bet, 2), format = \"pipe\", align = c(\"l\", \"c\"),\n       col.names = c(\"Character\", \"Betweenness\"),\n       caption = \"Top Five Betweenness Characters in Pulp Fiction Network.\") %>% \n   kable_styling(bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\nTop Five Betweenness Characters in Pulp Fiction Network.\n \n  \n    Character \n    Betweenness \n  \n \n\n  \n    BUTCH \n    275.52 \n  \n  \n    VINCENT \n    230.19 \n  \n  \n    JULES \n    142.11 \n  \n  \n    MIA \n    76.68 \n  \n  \n    MAYNARD \n    70.00 \n  \n  \n    HONEY BUNNY \n    49.97 \n  \n  \n    PUMPKIN \n    49.97 \n  \n  \n    SPORTSCASTER #1 \n    36.00 \n  \n  \n    BRETT \n    29.85 \n  \n  \n    PREACHER \n    28.23 \n  \n\n\n\n\n\nUnsurprisingly, the top four characters are also the highest in betweenness. Somewhat surprisingly, the main antagonist of the story (the pawn shop owner) is also up there. After that we see a steep drop in the bottom five of the top ten.\nNow let us examine betweenness centrality in our directed women lawyers advice network:\n\n   w.bet <- betweenness(wg)\n   w.bet\n\n [1]  0.0000000  3.0000000 16.3333333 11.0000000  7.0000000  0.0000000\n [7]  0.0000000  5.0000000  0.0000000  0.3333333  1.0000000  0.3333333\n\n\nHere we see that node 3 is the highest in betweenness, pictured below:\n\n\n\n\n\nWomen lawyers advice network with highest closeness centrality node in blue and highest betweenness centrality node in red\n\n\n\n\nThis result makes sense. Node 3 intermediates all the connections linking the tightly knit group of nodes on the left side (6, 10, 11, 12) with the rest of the network. Also if nodes 5 and 7 need to pass something along to the rest, they have to use 3 at least half time. Node 4 also needs 3 to reach 6.\nThis result nicely illustrates the difference between closeness and betweenness."
  },
  {
    "objectID": "handout2.html#edge-betweenness",
    "href": "handout2.html#edge-betweenness",
    "title": "Centrality",
    "section": "Edge Betweenness",
    "text": "Edge Betweenness\nEdge betweenness is defined in similar fashion as node betweenness:\n\\[\n\\sum_{i \\neq j} \\frac{\\sigma_{i(e)j}}{\\sigma_{ij}}\n\\]\nWhere \\(\\sigma_{i(e)j}\\) is a count of the number of shortest paths between i and j that feature edge e as an intermediary link. This tells us that the betweenness of an edge e is the sum of the ratios of the number of times that edge appears in the middle of a shortest path connecting every pair of nodes in the graph i and j divided by the total number of shortest paths linking each pair of nodes.\nLike before, the edge betweenness with respect to a specific pair of nodes in the graph is a probability: Namely, that if you send something–using a shortest path–from any node i to any other node j it has to go through edge e. The resulting edge betweenness scores is the sum of these probabilities across every possible pair of nodes for each edge in the graph.\nFor this example, we will work with a simplified version of the women lawyers advice network, in which we transform it into an undirected graph. We use the igraph function as.undirected for that:\n\n   wg <- as.undirected(wg, mode = \"collapse\")\n\nThe “collapse” value in the “mode” argument tells as.undirected to link every connected dyad in the original directed graph using an undirected edge. It does that by removing the directional arrow of the single directed links and collapsing (hence the name) all the bi-directional links into a single undirected one.\nThe resulting undirected graph looks like this:\n\n\n\n\n\nLooking at this point and line plot of the women lawyers advice network, which edge do you think has the top betweenness?\nWell no need to figure that out via eyeballing! We can just use the igraph function edge_betweenness:\n\n   w.ebet <- edge_betweenness(wg)\n\nThe edge_betweenness function takes the igraph graph object as input and produces a vector of edge betweenness values of the same length as the number of edges in the graph, which happens to be 20 in this case.\nUsing this information, we can then create a table of the top ten edges ordered by betweenness:\n\n   edges <- as_edgelist(wg) #creating an edgelist\n   etab <- data.frame(edges, bet = round(w.ebet, 2)) #adding bet. scores to edgelist\n   etab <- etab[order(etab$bet, decreasing = TRUE), ] #ordering by bet.\n   kbl(etab[1:10, ], format = \"pipe\", align = c(\"l\", \"l\", \"c\"),\n       col.names = c(\"i\", \"j\", \"Edge Bet.\"), row.names = FALSE,\n       caption = \"Edges Sorted by Betweenness in the Women Lawyers Advice Network\") %>% \n   kable_styling(bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\nEdges Sorted by Betweenness in the Women Lawyers Advice Network\n \n  \n    i \n    j \n    Edge Bet. \n  \n \n\n  \n    3 \n    4 \n    19.17 \n  \n  \n    5 \n    8 \n    15.83 \n  \n  \n    3 \n    5 \n    13.67 \n  \n  \n    5 \n    7 \n    11.00 \n  \n  \n    2 \n    4 \n    9.17 \n  \n  \n    3 \n    11 \n    8.33 \n  \n  \n    5 \n    6 \n    8.17 \n  \n  \n    1 \n    4 \n    7.00 \n  \n  \n    2 \n    8 \n    6.50 \n  \n  \n    8 \n    9 \n    6.33 \n  \n\n\n\n\n\nNot surprisingly, the top edges are the ones linking nodes 3 and 4 and nodes 5 and 8.\n\nDisconnecting a Graph Via Bridge Removal\nHigh betweenness edges are likely to function as bridges being the only point of indirect connectivity between most nodes in the social structure. That means that an easy way to disconnect a connected graph is to remove the bridges (Girvan and Newman 2002).\nIn igraph we can produce an edge deleted subgraph of an original graph using the “minus” operator, along with the edge function like this:\n\n   del.g <- wg - edge(\"3|4\")\n   del.g <- del.g - edge(\"5|8\")\n\nThe first line creates a new graph object (a subgraph) which equals the original graph minus the edge linking nodes 3 and 4. The second line takes this last subgraph and further deletes the edge linking nodes 5 and 8.\nThe resulting subgraph, minus the top two high-betweenness edges, looks like:\n\n\n\n\n\nWhich is indeed disconnected!"
  },
  {
    "objectID": "handout2.html#induced-betweenness",
    "href": "handout2.html#induced-betweenness",
    "title": "Centrality",
    "section": "Induced Betweenness",
    "text": "Induced Betweenness\nIn the assigned handbook chapter reading, Borgatti and Everett argue that another way of thinking about centrality of a node (or edge) is to calculate the difference that removing that node makes for some graph property in the network. They further suggest that the sum of the centrality scores of each node is just such a property, proposing that betweenness is particularly interesting in this regard. Let’s see how this works.\nWe will use the undirected version of the women lawyers advice network for this example. Let’s say we are interested in the difference that node 10 makes for the betweenness centralities of everyone else. In that case we would proceed as follows:\n\n   bet <- betweenness(wg) #original centrality scores\n   Sbet <- sum(bet) #sum of original centrality scores\n   wg.d <- wg - vertex(\"10\") #removing vertex 10 from the graph\n   bet.d <- betweenness(wg.d) #centrality scores of node deleted subgraph\n   Sbet.d <- sum(bet.d) #sum of centrality scores of node deleted subgraph\n   total.c <- Sbet - Sbet.d #total centrality\n   indirect.c <- total.c - bet[10] #indirect centrality\n   indirect.c\n\n[1] 12.66667\n\n\nLine 1 just calculates the regular betweenness centrality vector for the graph. Line 2 sums up all of the entries of this vector. Line 3 creates a node deleted subgraph by removing node 10. This is done using the “minus” operator and the igraph function vertex, which works just like the edge function we used earlier to create an edge deleted subgraph, except it takes a node id or name as input.\nLines 4-5 just recalculate the sum of betweenness centralities in the subgraph that excludes node 10. Then in line 6 we subtract the sum of centralities of the node deleted subgraph from the sum of centralities of the original graph. If this number, which Borgatti and Everett call the “total” centrality, is large and positive then that means that node 10 makes a difference for the centrality of others.\nHowever, part of that difference is node 10’s own “direct” centrality, so to get a more accurate sense of node 10’s impact on other people’s centrality we need to subtract node 10’s direct centrality from the total number, which we do in line 7 to get node 10’s “indirect” centrality. The result is shown in the last line, which indicates that node 10 has a pretty big impact on other people’s betweenness centralities, net of their own (which is pretty small).\nNow all we need to do is do the same for each node to create a vector of indirect betweenness centralities. So we incorporate the code above into a short loop through all vertices:\n\n   total.c <- 0 #empty vector\n   indirect.c <- 0 #empty vector\n   for (i in 1:vcount(wg)) {\n      wg.d <- wg - vertex(i)\n      bet.d <- betweenness(wg.d) #centrality scores of node deleted subgraph\n      Sbet.d <- sum(bet.d) #sum of centrality scores of node deleted subgraph\n      total.c[i] <- Sbet - Sbet.d #total centrality\n   indirect.c[i] <- total.c[i] - bet[i] #total minus direct\n   }\n\nWe can now list the total, direct, and indirect betweenness centralities for the women lawyers graph using a nice table:\n\n   i.bet <- data.frame(n = 1:vcount(wg), total.c, round(betweenness(wg), 1), round(indirect.c, 1))\n   kbl(i.bet, format = \"pipe\", align = c(\"l\", \"c\", \"c\", \"c\"),\n       col.names = c(\"Node\", \"Total\", \"Direct\", \"Indirect\"), row.names = FALSE,\n       caption = \"Induced Betweenness Scores in the Women Lawyers Advice Network\") %>% \n   kable_styling(bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\nInduced Betweenness Scores in the Women Lawyers Advice Network\n \n  \n    Node \n    Total \n    Direct \n    Indirect \n  \n \n\n  \n    1 \n    16 \n    0.0 \n    16.0 \n  \n  \n    2 \n    4 \n    6.7 \n    -2.7 \n  \n  \n    3 \n    -24 \n    23.2 \n    -47.2 \n  \n  \n    4 \n    -4 \n    12.2 \n    -16.2 \n  \n  \n    5 \n    19 \n    18.8 \n    0.2 \n  \n  \n    6 \n    10 \n    3.7 \n    6.3 \n  \n  \n    7 \n    18 \n    0.0 \n    18.0 \n  \n  \n    8 \n    4 \n    8.8 \n    -4.8 \n  \n  \n    9 \n    18 \n    0.0 \n    18.0 \n  \n  \n    10 \n    13 \n    0.3 \n    12.7 \n  \n  \n    11 \n    14 \n    0.0 \n    14.0 \n  \n  \n    12 \n    13 \n    0.3 \n    12.7 \n  \n\n\n\n\n\nThis approach to decomposing betweenness centrality provides a new way to categorize actors in a network:\n\nOn the one hand, we have actors like nodes 3 and 4 who “hog” centrality from others. Perhaps these are the prototypical high betweenness actors who monopolize the flow through the network. Their own direct centrality is high, but their indirect centrality is negative, suggesting that others become more central when they are removed from the graph as they can now become intermediaries themselves.\nIn contrast, we also have actors like node 5 who are high centrality themselves, but who’s removal from the network does not affect anyone else’s centrality. These actors are high betweenness but themselves don’t monopolize the flow of information in the network.\nThen we have actors (like nodes 9-12) who have low centrality, but whose removal from the network makes a positive difference for other people’s centrality, which overall decreases when they are removed from the network.\nFinally, we have actors line nodes 2 and 8, who are not particularly central, but who also hog centrality from others, in that removing them from the network also increases other people’s centrality (although not such an extent as the hogs)."
  },
  {
    "objectID": "handout2.html#generalized-harmonic-centrality",
    "href": "handout2.html#generalized-harmonic-centrality",
    "title": "Centrality",
    "section": "Generalized Harmonic Centrality",
    "text": "Generalized Harmonic Centrality\nAgneessens, Borgatti, and Everett (2017) propose a “generalized” version of the harmonic centrality that yields plain old degree centrality and the regular harmonic centrality as special cases. The key is to introduce a parameter \\(\\delta\\) governing how much weight we give to shortest paths based on distance. Let’s see how this works.\nRecall that the harmonic centrality we defined earlier is given by:\n\\[\n\\frac{\\sum_{j \\neq i} (g_{ij})^{-1}}{n-1}\n\\]\nFor any node \\(i\\), where \\(g_{ij}\\) is the geodesic distance between \\(i\\) and every other node in the graph \\(j\\), which could be “infinite” if there is no path linking them.\nAgneessens et al’s tweak is to instead compute:\n\\[\n\\frac{\\sum_{j \\neq i} (g_{ij})^{-\\delta}}{n-1}\n\\]\nWhere \\(\\delta\\) is a free parameter chosen by the researcher with the restriction that \\(\\delta \\geq 0\\) (if you want to calculate a closeness measure as we will see below).\nWhen \\(\\delta = \\infty\\) the numerator element \\(1/(g_{ij})^{\\infty} = 1\\) only when nodes are adjacent and \\(g_{ij} = 1\\) (because \\(1^{\\infty} = 1\\)); otherwise, for \\(g_{ij} > 1\\) then \\(1/(g_{ij})^{\\infty} = 0\\), and therefore the generalized harmonic centrality just becomes a (normalized) version of degree centrality. Alternatively, when \\(\\delta = 1\\) we just get the plain old harmonic centrality we defined earlier.\nThe interesting cases come from \\(1 > \\delta < \\infty\\) and \\(0 > \\delta < 1\\). In the first case, nodes at shorter distances are weighted more (like in the standard harmonic centrality measure) as \\(\\delta\\) becomes bigger and bigger then the generalized harmonic centrality approximates degree. For values below one, as \\(\\delta\\) approaches zero, then indirect connections to nodes of greater length are discounted less, and thus count for “more” in defining your generalized harmonic centrality score.\nLet us see a real-world example of the generalized harmonic centrality in action:\nFirst, we create a custom function to compute the generalized harmonic centrality:\n\n   g.harm <- function(x, d) {\n      library(igraph)\n      S <- distances(x) #get distances from graph object\n      S <- 1/S^d #matrix of generalized inverse distances\n      diag(S) <- 0 #set diagonals to zero\n      c <- rowSums(S)/(vcount(x) - 1) #summing and averaging\n      return(c)\n   }\n\nSecond, we compute three versions of the harmonic centrality, with \\(\\delta = 5\\), \\(\\delta = 0.05\\), and \\(\\delta = -5\\), using the full (unrestricted by age) subgraph of the law_advice network composed of the women lawyers at the firm, with relations constrained to be undirected:\n\n   women <- which(V(law_advice)$gender == 2)\n   wg <- subgraph(law_advice, women)\n   wg <- as.undirected(wg)\n   c1 <- g.harm(wg, d = 5)\n   c2 <- g.harm(wg, d = 0.05)\n   c3 <- g.harm(wg, d = -5)\n\n\nThe first version of the harmonic centrality in line 5, with a positive value of \\(\\delta\\) above zero, will compute centrality scores emphasizing direct (one-step) connections, thus coming closer to degree.\nThe second version, in line 6, with a value of \\(\\delta\\) close to zero, will give comparatively more emphasis to indirect connections weighing longer paths almost as much as shorter paths (but always a little less), thus being more similar to closeness centrality.\nFinally, the last version, in line 7, with \\(\\delta < 0\\), will weigh longer paths more than shorter ones, serving as a measure of eccentricity (farness from others) not closeness.\n\n\n\n\n\n\nFull women lawyers advice network\n\n\n\n\nAbove is a plot of the women lawyers network showing the top node for each of the centralities:\n\nIn red we have node 3 who has the largest degree (\\(k(3) = 8\\)) and thus comes out on top using the generalized harmonic centrality version emphasizing direct connections (\\(\\delta > 1\\)).\nThen in blue we have node 9 who can reach the most others via the shortest paths, and thus comes out on top when the generalized harmonic centrality emphasizes indirect connectivity.\nFinally, in purple we have node 12, which is farthest from everyone else, and thus comes out on “top” when longer indirect connections count for more (\\(\\delta < 0)\\).\n\nAs we said earlier, both regular harmonic centrality and degree are special cases of the generalized measure. We can check this by setting \\(\\delta\\) to either one or infinity.\nWhen we set \\(\\delta=1\\) the generalized harmonic centrality is the same as (normalized by number of nodes minus one) the regular harmonic centrality:\n\n   g.harm(wg, d = 1)\n\n [1] 0.5980392 0.5343137 0.7058824 0.5980392 0.6568627 0.6274510 0.4264706\n [8] 0.5392157 0.6960784 0.6470588 0.6666667 0.4068627 0.5882353 0.5196078\n[15] 0.5833333 0.6029412 0.5588235 0.5441176\n\n   harmonic_centrality(wg, normalized = TRUE)\n\n [1] 0.5980392 0.5343137 0.7058824 0.5980392 0.6568627 0.6274510 0.4264706\n [8] 0.5392157 0.6960784 0.6470588 0.6666667 0.4068627 0.5882353 0.5196078\n[15] 0.5833333 0.6029412 0.5588235 0.5441176\n\n\nWhen we set \\(\\delta=\\infty\\) the generalized harmonic centrality is the same as (normalized by number of nodes minus one) degree centrality:\n\n   g.harm(wg, d = Inf)\n\n [1] 0.23529412 0.17647059 0.47058824 0.23529412 0.35294118 0.29411765\n [7] 0.05882353 0.17647059 0.41176471 0.35294118 0.41176471 0.05882353\n[13] 0.23529412 0.17647059 0.23529412 0.35294118 0.23529412 0.23529412\n\n   degree(wg, normalized = TRUE)\n\n [1] 0.23529412 0.17647059 0.47058824 0.23529412 0.35294118 0.29411765\n [7] 0.05882353 0.17647059 0.41176471 0.35294118 0.41176471 0.05882353\n[13] 0.23529412 0.17647059 0.23529412 0.35294118 0.23529412 0.23529412"
  },
  {
    "objectID": "handout2.html#appendix-selecting-a-subgraph-based-on-edge-conditions",
    "href": "handout2.html#appendix-selecting-a-subgraph-based-on-edge-conditions",
    "title": "Centrality",
    "section": "Appendix: Selecting a Subgraph Based on Edge Conditions",
    "text": "Appendix: Selecting a Subgraph Based on Edge Conditions\nA great question asked in class goes as follows: What if I want to create a subgraph based on selecting a subset of a nodes, and then the other nodes in the graph that are that set of node’s in-neighbors?\nLet’s see how that would work.\nFirst, we create a vector with the node ids of our focal nodes, which will be women under 40 in the law_advice network.\n\n   yw <- which(V(law_advice)$gender == 2 & V(law_advice)$age < 40)\n   yw\n\n [1] 29 34 39 48 51 57 59 60 61 67 69 71\n\n\nSecond, we need to collect the node ids of the people who point to these set of nodes; that is, each of these node’s in-neighbors. For that, we use the igraph function neighbors:\n\n   yw_in <- list() #creating empty list\n   k <- 1 #counter for list position\n   for (i in yw) {\n      nei <- neighbors(law_advice, i, mode = \"in\") \n      nei <- as.vector(nei) #vector of in-neighbors ids\n      yw_in[[k]] <- nei #adding to list\n      k <- k + 1 #incrementing counter\n   }\n\nLine one creates an (empty) list object in R. The beauty of a list object is that it is an object that can hold other objects (vectors, matrices, igraph graph objects, etc.) as members (it can also have other lists as members, with lists all the way down). For a primer on how to work with R lists see here\nThe idea is to populate this initially empty list with the vectors of the in-neighbor ids of each node listed in the vector yw. Lines 2-8 do that using a simple for loop starring the igraph command neighbors, a function which takes two inputs: an igraph graph object, and a node id. The argument “mode” (“in” or “out” for directed graphs), tells it which kid of neighbors you want (not necessary for undirected graphs). Here we want the in-neighbors, so mode = “in”.\nNow we have a list object in R of length equal to the number of younger women (12 in this case) with each entry equal to the ids of those women’s in-neighbors.\n\n   length(yw_in)\n\n[1] 12\n\n   head(yw_in)\n\n[[1]]\n [1]  4 10 12 15 16 17 19 26 27 28 34 36 41 42 45 48 70\n\n[[2]]\n [1]  7 10 12 13 14 15 16 17 19 22 26 27 28 29 33 36 42 44 45 46 48 56 60 61 64\n\n[[3]]\n [1] 13 17 30 40 41 42 48 51 52 55 56 57 65 66 67 69 71\n\n[[4]]\n[1] 16 17 39 42\n\n[[5]]\n[1] 58 59\n\n[[6]]\n [1] 27 39 41 51 55 56 62 65 66 67 71\n\n\nNow we need to create a vector of the unique ids of these nodes. To do this, we just to “unlist” all of the node ids to create a simple vector from the list object.\nThe unlist native R function does that for us, taking a list as input and returning all of the elements inside each of the separate objects stored in the list as output. Here we wrap that call in the unique native R function to eliminate repeats (common in-neighbors across women):\n\n   yw_in <- unique(unlist(yw_in))\n   yw_in\n\n [1]  4 10 12 15 16 17 19 26 27 28 34 36 41 42 45 48 70  7 13 14 22 29 33 44 46\n[26] 56 60 61 64 30 40 51 52 55 57 65 66 67 69 71 39 58 59 62 35\n\n\nOf course, because the younger women are their own in-neighbors, they are included in this vector, so we need to get rid of them:\n\n   yw_in <- setdiff(yw_in, yw)\n   yw_in\n\n [1]  4 10 12 15 16 17 19 26 27 28 36 41 42 45 70  7 13 14 22 33 44 46 56 64 30\n[26] 40 52 55 65 66 58 62 35\n\n\nWe use the native command setdiff to find the elements in vector yw_in that are not contained in the vector of young women ids yw or the difference between the set of nodes ids stored in yw_in and the set of node ids stored in yw.\nNow that we have the vector of ids of the focal nodes and the vector of ids of their in-neighbors, we are ready to create our subgraph! All we need to do is specify we want both the younger law firm women and their in-neighbors in our node-induced subgraph:\n\n   g <- subgraph(law_advice, c(yw, yw_in))\n\nWe can even specify a new vertex attribute, differentiating the focal network from the in-neighbor network.\n\n   V(g)$net_status <- c(rep(1, length(yw)), rep(2, length(c(yw, yw_in)) - length(yw)))\n   vertex_attr(g)\n\n$status\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[39] 2 2 2 2 2 2 2\n\n$gender\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 1 1 2 1 1 2 1 1 1 2 1 2 2 2 1 1 1 2 1 2 2 2 1\n[39] 2 1 1 2 2 1 2\n\n$office\n [1] 1 2 1 1 1 2 3 1 1 1 1 1 1 2 1 2 2 1 2 1 1 1 1 1 3 1 2 1 2 1 1 1 1 2 2 1 1 1\n[39] 1 1 1 1 1 1 1\n\n$seniority\n [1] 31 29 25 24 22  1 21 20 23 19  9 15 13 11 10  7  8  8  8  5  6  6  5  4  5\n[26]  3  3  1  4  4  3  3  3  3  2  2  2  2  2  2  1  1  1  1  1\n\n$age\n [1] 59 63 53 52 57 56 48 46 50 46 49 41 47 38 38 39 37 36 33 43 37 34 31 31 53\n[26] 38 42 35 29 29 34 38 33 33 30 31 34 32 45 28 43 35 38 31 26\n\n$practice\n [1] 2 2 2 2 1 2 2 2 2 2 1 1 1 2 2 1 1 2 2 1 1 1 1 2 2 2 2 2 1 1 1 1 1 1 1 2 2 2\n[39] 2 1 1 1 1 2 1\n\n$law_school\n [1] 3 3 3 2 2 1 3 1 1 1 3 3 1 2 3 3 3 2 3 3 1 1 2 2 1 3 2 3 3 3 3 2 2 3 2 2 3 2\n[39] 2 3 3 2 3 2 2\n\n$net_status\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[39] 2 2 2 2 2 2 2\n\n\nFinally, we create an edge deleted subgraph including only the incoming advice edges from nodes who are not younger women in the firm to younger women and deleting everything other link:\n\n   e.rem <- E(g)[V(g)[net_status==1] %->% V(g)[net_status==1]] \n   #selecting edges from younger women to younger women\n   g.r <- delete_edges(g, e.rem) #removing edges\n   e.rem <- E(g.r)[V(g.r)[net_status==1] %->% V(g.r)[net_status==2]] \n   #selecting edges from younger women to non-younger women\n   g.r <- delete_edges(g.r, e.rem) #removing edges\n   e.rem <- E(g.r)[V(g.r)[net_status==2] %->% V(g.r)[net_status==2]] \n   #selecting edges from non-younger women to non-younger women\n   g.r <- delete_edges(g.r, e.rem) #removing edges\n   Iso = which(degree(g.r)==0) #selecting isolates\n   g.r <- delete_vertices(g.r, Iso) #removing isolates\n\nHere we can see both the delete_edges and delete_vertices functions from igraph in action. Both take some graph object as input followed by either an edge sequence (in this case produced by E(g))or a vector of node ids respectively. In both cases those particular edges or nodes are removed from the graph.\nThe other neat functionality we see on display here is the igraph %->% operator for directed graph edges (the equivalent for undirected graphs is the double dash %–%). This allows us to select a set of edges according to a vertex condition (e.g., homophilous (same group) edges or edges that link a member from group a to a member from group b).\nSo the code chunk:\n\n   E(g)[V(g)[net_status==1] %->% V(g)[net_status==1]] \n\n+ 69/436 edges from 09c3363:\n [1]  1-> 4  1-> 5  1-> 6  1-> 9  1->10  1->11  1->12  2-> 3  3-> 8  3-> 9\n[11]  3->12  4-> 1  4-> 3  4-> 5  4-> 6  4-> 7  4-> 8  4-> 9  4->10  4->12\n[21]  5-> 4  5-> 7  5-> 8  5-> 9  5->12  6-> 1  6-> 4  6-> 7  6-> 8  6-> 9\n[31]  7-> 5  7-> 6  7-> 8  7->11  7->12  8-> 1  8-> 2  8-> 3  8-> 4  8-> 6\n[41]  8-> 7  8-> 9  8->10  8->12  9-> 1  9-> 4  9-> 6  9-> 8  9->11  9->12\n[51] 10-> 1 10-> 4 10-> 5 10-> 7 10-> 9 10->11 10->12 11-> 1 11-> 7 11-> 8\n[61] 11-> 9 11->10 12-> 1 12-> 4 12-> 5 12-> 7 12-> 8 12-> 9 12->11\n\n\nTakes the edge set of the graph g (E(g)) and gives us the subset of edges that go from a vertex with net_status equal to one to another vertex that also has net_status equal to one (in this case edges directed from one of our focal nodes to another one of our focal nodes). This, of course, happens to be all the directed edges linking nodes one through twelve in the network. The same go for the other calls to the same function using different logical combinations of values of net_status between nodes.\nFINALLY, can now plot the incoming advice network to younger women (in red):\n\n\n\n\n\nWomen lawyers advice network showing incoming links from outside the group (blue nodes) to younger women (red nodes)"
  },
  {
    "objectID": "handout2.html#note-handling-graph-objects-in-lists",
    "href": "handout2.html#note-handling-graph-objects-in-lists",
    "title": "Centrality",
    "section": "Note: Handling Graph Objects in Lists",
    "text": "Note: Handling Graph Objects in Lists\nSpeaking of lists, sometimes network data comes pre-stored as an R list. This is typical if you have a network with multiple kinds of ties recorded on the same set of actors (and thus multiple networks), or longitudinal network data, where we collect multiple “snapshots” of the same system (containing the same or more typically a different set of actors per time slice).\nThe networkdata package contains one such data set called atp. It’s a network of Tennis players who played in grand slam or official matches of the Association of Tennis Professionals (hence ATP) covering the years 1968-2021 (Radicchi 2011).\nIn the directed graph representing each network, a tie goes from the loser to the winner of each match. Accordingly, it can be interpreted as a directed “deference” network (it would be a dominance network if it was the other way around), where actor i “defers” to actor j by getting their ass kicked by them.\nLet’s see how this list of networks works:\n\n   g <- atp\n   head(g)\n\n[[1]]\nIGRAPH 08a202a DNW- 497 1213 -- ATP Season 1968\n+ attr: name (g/c), name (v/c), age (v/n), hand (v/c), country (v/c),\n| surface (e/c), weight (e/n)\n+ edges from 08a202a (vertex names):\n [1] U Unknown    ->Jose Mandarino     Alfredo Acuna->Alan Fox          \n [3] Andres Gimeno->Ken Rosewall       Andres Gimeno->Raymond Moore     \n [5] Juan Gisbert ->Tom Okker          Juan Gisbert ->Zeljko Franulovic \n [7] Onny Parun   ->Jan Kodes          Peter Curtis ->Tom Okker         \n [9] Premjit Lall ->Clark Graebner     Rod Laver    ->Ken Rosewall      \n[11] Thomas Lejus ->Nicola Pietrangeli Tom Okker    ->Arthur Ashe       \n[13] U Unknown    ->Jaidip Mukherjea   U Unknown    ->Jose Luis Arillla \n+ ... omitted several edges\n\n[[2]]\nIGRAPH c895c57 DNW- 446 1418 -- ATP Season 1969\n+ attr: name (g/c), name (v/c), age (v/n), hand (v/c), country (v/c),\n| surface (e/c), weight (e/n)\n+ edges from c895c57 (vertex names):\n [1] U Unknown           ->U Unknown        \n [2] Alejandro Olmedo    ->Ron Holmberg     \n [3] Arthur Ashe         ->Rod Laver        \n [4] Bob Carmichael      ->Jim Osborne      \n [5] Cliff Richey        ->Zeljko Franulovic\n [6] Francois Jauffret   ->Martin Mulligan  \n [7] Fred Stolle         ->John Newcombe    \n+ ... omitted several edges\n\n[[3]]\nIGRAPH d6709af DNW- 451 1650 -- ATP Season 1970\n+ attr: name (g/c), name (v/c), age (v/n), hand (v/c), country (v/c),\n| surface (e/c), weight (e/n)\n+ edges from d6709af (vertex names):\n [1] Mark Cox            ->Jan Kodes        \n [2] Charlie Pasarell    ->Stan Smith       \n [3] Cliff Richey        ->Arthur Ashe      \n [4] Francois Jauffret   ->Manuel Orantes   \n [5] Georges Goven       ->Jan Kodes        \n [6] Harald Elschenbroich->Zeljko Franulovic\n [7] Ilie Nastase        ->Zeljko Franulovic\n+ ... omitted several edges\n\n[[4]]\nIGRAPH a73a020 DNW- 459 2580 -- ATP Season 1971\n+ attr: name (g/c), name (v/c), age (v/n), hand (v/c), country (v/c),\n| surface (e/c), weight (e/n)\n+ edges from a73a020 (vertex names):\n [1] Andres Gimeno    ->Ken Rosewall   Arthur Ashe      ->Rod Laver     \n [3] Charlie Pasarell ->Cliff Drysdale Frank Froehling  ->Clark Graebner\n [5] Joaquin Loyo Mayo->Thomaz Koch    John Alexander   ->John Newcombe \n [7] John Newcombe    ->Marty Riessen  Nikola Pilic     ->Cliff Drysdale\n [9] Owen Davidson    ->Cliff Drysdale Robert Maud      ->Cliff Drysdale\n[11] Roger Taylor     ->Marty Riessen  Roy Emerson      ->Rod Laver     \n[13] Tom Okker        ->John Newcombe  Allan Stone      ->Bob Carmichael\n+ ... omitted several edges\n\n[[5]]\nIGRAPH 761ed05 DNW- 504 2767 -- ATP Season 1972\n+ attr: name (g/c), name (v/c), age (v/n), hand (v/c), country (v/c),\n| surface (e/c), weight (e/n)\n+ edges from 761ed05 (vertex names):\n [1] Jean Loup Rouyer->Stan Smith     Marty Riessen   ->Cliff Drysdale\n [3] Roy Emerson     ->Arthur Ashe    Roy Emerson     ->Arthur Ashe   \n [5] Tom Leonard     ->John Newcombe  Tom Okker       ->Arthur Ashe   \n [7] Tom Okker       ->Rod Laver      Adriano Panatta ->Andres Gimeno \n [9] Adriano Panatta ->Ilie Nastase   Allan Stone     ->John Alexander\n[11] Allan Stone     ->Marty Riessen  Andres Gimeno   ->Jan Kodes     \n[13] Andres Gimeno   ->Stan Smith     Andrew Pattison ->Ilie Nastase  \n+ ... omitted several edges\n\n[[6]]\nIGRAPH 92ff576 DNW- 592 3653 -- ATP Season 1973\n+ attr: name (g/c), name (v/c), age (v/n), hand (v/c), country (v/c),\n| surface (e/c), weight (e/n)\n+ edges from 92ff576 (vertex names):\n [1] Harold Solomon    ->Stan Smith       John Alexander    ->Stan Smith      \n [3] Patrice Dominguez ->Paolo Bertolucci Paul Gerken       ->Jimmy Connors   \n [5] Roy Emerson       ->Rod Laver        Adriano Panatta   ->Ilie Nastase    \n [7] Bjorn Borg        ->Adriano Panatta  Brian Gottfried   ->Cliff Richey    \n [9] Charlie Pasarell  ->John Alexander   Cliff Richey      ->Stan Smith      \n[11] Corrado Barazzutti->Bjorn Borg       Francois Jauffret ->Ilie Nastase    \n[13] Georges Goven     ->Manuel Orantes   Manuel Orantes    ->Ilie Nastase    \n+ ... omitted several edges\n\n\nWe create a graph object and then examine its contents, which we can see is a set of graph objects. In unnamed R lists each of the objects inside is indexed by a number in double brackets. So [[6]] just means the sixth network in the list object (corresponding to the year 1973).\nNow let’s say we wanted to compute a network statistic like density. One way to proceed would be:\n\n   edge_density(g)\n\nError in `ensure_igraph()`:\n! Must provide a graph object (provided wrong object type).\n\n\nWhich gives us a weird error about the wrong object type. The reason is that edge_density expects an igraph graph object as input, but g is not a graph object it is a list of such objects. For it to work you have to reference a particular element inside the list not the whole list.\nTo do that, we use the double bracket notation:\n\n   edge_density(g[[6]])\n\n[1] 0.01044096\n\n\nWhich gives us the density for the 1973 network.\n\nLooping Through Lists\nBut what if we wanted a table of network statistics for all the years or some subset of years? Of course, we could just type a million versions of the edge_density command or whatever, but that would be tedious. We could also write a for loop or something like that (less tedious). Even less tedious is to use the many apply functions in R that are designed to work with lists, which is a subject onto itself in R programming.\nBut here we can just use the simple version. Let’s say we wanted a vector of densities (or any other whole network statistic) for the whole 54 years. In that case, our friend sapply can do the job:\n\n   sapply(g, edge_density)\n\n [1] 0.004920653 0.007144657 0.008130081 0.012272740 0.010914671 0.010440961\n [7] 0.010567864 0.013315132 0.012088214 0.014019237 0.014135328 0.011649909\n[13] 0.011172821 0.011261426 0.012703925 0.012177336 0.012648755 0.012445937\n[19] 0.012034362 0.012351377 0.010174271 0.009772014 0.019526953 0.012236462\n[25] 0.014050245 0.015054181 0.013872832 0.014727924 0.014329906 0.013935502\n[31] 0.013962809 0.013870042 0.013665097 0.013818887 0.012551113 0.011571679\n[37] 0.012329090 0.012923683 0.011402945 0.012677988 0.012256963 0.013512884\n[43] 0.012543025 0.013661748 0.013786518 0.013679697 0.015052857 0.015075622\n[49] 0.015081206 0.014346468 0.015764351 0.020169225 0.011889114 0.016935400\n\n\nsapply is kind of a “meta” function that takes two inputs: A list, and the name of a function (which could be native, a package, or user defined); sapply then “applies” that function to each element inside the list. Here we asked R to apply the function edge_density to each element of the list of networks g and it obliged, creating a vector of length 54 containing the info.\nWe could use any igraph function, like number of nodes in the graph:\n\n   sapply(g, vcount)\n\n [1] 497 446 451 459 504 592 595 535 553 524 509 572 582 573 554 532 495 513 510\n[20] 523 596 597 405 542 509 498 520 496 502 499 497 480 486 479 497 517 505 492\n[39] 524 488 493 464 482 459 457 453 428 430 431 438 419 364 345 393\n\n\nWe could also select subset of elements inside the list. For instance this counts the number of nodes for the first five years:\n\n   sapply(g[1:5], vcount)\n\n[1] 497 446 451 459 504\n\n\nOr for years 2, 6, 8, and 12:\n\n   sapply(g[c(2, 6, 8, 12)], vcount)\n\n[1] 446 592 535 572\n\n\nNote the single bracket notation here to refer to subsets of elements in the list. Inside the brackets we could put any arbitrary vector, as long as the numbers in the vector do no exceed the length of the list.\nOf course, sometimes the functions we apply to elements of the list don’t return single numbers but vectors or other igraph objects. In that case it would be better to use lapply which is just like sapply but returns another list with the set of answers inside it.\nFor instance, let’s say we wanted the top five players for each year. In this deference network, a “top” player is one who beats many others, which means they have high indegree (lots of losers pointing at them).\nFirst we create a custom function to compute the indegree and return an ordered named vector of top 5 players:\n\n   top5 <- function(x) {\n      library(igraph)\n      t <- degree(x, mode = \"in\")\n      t <- sort(t, decreasing = TRUE)[1:5]\n      return(t)\n   }\n\nNow, we can just feed that function to lapply:\n\n   top.list <- lapply(g, top5)\n   head(top.list)\n\n[[1]]\n   Arthur Ashe      Rod Laver Clark Graebner   Ken Rosewall      Tom Okker \n            33             27             25             23             22 \n\n[[2]]\nJohn Newcombe     Tom Okker     Rod Laver    Tony Roche   Arthur Ashe \n           45            41            40            40            33 \n\n[[3]]\n      Arthur Ashe      Cliff Richey         Rod Laver        Stan Smith \n               51                49                48                45 \nZeljko Franulovic \n               42 \n\n[[4]]\n     Ilie Nastase         Tom Okker     Marty Riessen        Stan Smith \n               69                63                61                61 \nZeljko Franulovic \n               60 \n\n[[5]]\n  Ilie Nastase     Stan Smith Manuel Orantes  Jimmy Connors    Arthur Ashe \n            99             72             68             65             55 \n\n[[6]]\n Ilie Nastase     Tom Okker Jimmy Connors   Arthur Ashe    Stan Smith \n           96            81            68            63            63 \n\n\nWhich is a list of named vectors containing the number of victories of the top five players each year.\nBecause the object top.list is just a list, we can subset it just like before. Let’s say we wanted to see the top players for more recent years:\n\n   top.list[49:54]\n\n[[1]]\n   Andy Murray  Dominic Thiem  Kei Nishikori Novak Djokovic   David Goffin \n            63             55             53             50             47 \n\n[[2]]\n         Rafael Nadal          David Goffin      Alexander Zverev \n                   58                    55                    52 \nRoberto Bautista Agut         Dominic Thiem \n                   45                    43 \n\n[[3]]\n   Dominic Thiem Alexander Zverev   Novak Djokovic    Fabio Fognini \n              51               50               46               45 \n   Roger Federer \n              44 \n\n[[4]]\n   Daniil Medvedev     Novak Djokovic       Rafael Nadal Stefanos Tsitsipas \n                55                 52                 52                 49 \n     Roger Federer \n                47 \n\n[[5]]\n     Andrey Rublev     Novak Djokovic Stefanos Tsitsipas       Rafael Nadal \n                40                 36                 27                 26 \n   Daniil Medvedev \n                24 \n\n[[6]]\n   Daniil Medvedev Stefanos Tsitsipas        Casper Ruud   Alexander Zverev \n                54                 52                 52                 51 \n    Novak Djokovic \n                49 \n\n\nA series of names which make sense to you if you follow Tennis.\n\n\nNaming Lists\nFinally, sometimes it useful to name the elements of a list. In this case, for instance, having the year number would be easier to remember what’s what. For this, you can use the names command, which works via standard R assignment:\n\n   names(g) <- c(1968:2021)\n   head(g)\n\n$`1968`\nIGRAPH 08a202a DNW- 497 1213 -- ATP Season 1968\n+ attr: name (g/c), name (v/c), age (v/n), hand (v/c), country (v/c),\n| surface (e/c), weight (e/n)\n+ edges from 08a202a (vertex names):\n [1] U Unknown    ->Jose Mandarino     Alfredo Acuna->Alan Fox          \n [3] Andres Gimeno->Ken Rosewall       Andres Gimeno->Raymond Moore     \n [5] Juan Gisbert ->Tom Okker          Juan Gisbert ->Zeljko Franulovic \n [7] Onny Parun   ->Jan Kodes          Peter Curtis ->Tom Okker         \n [9] Premjit Lall ->Clark Graebner     Rod Laver    ->Ken Rosewall      \n[11] Thomas Lejus ->Nicola Pietrangeli Tom Okker    ->Arthur Ashe       \n[13] U Unknown    ->Jaidip Mukherjea   U Unknown    ->Jose Luis Arillla \n+ ... omitted several edges\n\n$`1969`\nIGRAPH c895c57 DNW- 446 1418 -- ATP Season 1969\n+ attr: name (g/c), name (v/c), age (v/n), hand (v/c), country (v/c),\n| surface (e/c), weight (e/n)\n+ edges from c895c57 (vertex names):\n [1] U Unknown           ->U Unknown        \n [2] Alejandro Olmedo    ->Ron Holmberg     \n [3] Arthur Ashe         ->Rod Laver        \n [4] Bob Carmichael      ->Jim Osborne      \n [5] Cliff Richey        ->Zeljko Franulovic\n [6] Francois Jauffret   ->Martin Mulligan  \n [7] Fred Stolle         ->John Newcombe    \n+ ... omitted several edges\n\n$`1970`\nIGRAPH d6709af DNW- 451 1650 -- ATP Season 1970\n+ attr: name (g/c), name (v/c), age (v/n), hand (v/c), country (v/c),\n| surface (e/c), weight (e/n)\n+ edges from d6709af (vertex names):\n [1] Mark Cox            ->Jan Kodes        \n [2] Charlie Pasarell    ->Stan Smith       \n [3] Cliff Richey        ->Arthur Ashe      \n [4] Francois Jauffret   ->Manuel Orantes   \n [5] Georges Goven       ->Jan Kodes        \n [6] Harald Elschenbroich->Zeljko Franulovic\n [7] Ilie Nastase        ->Zeljko Franulovic\n+ ... omitted several edges\n\n$`1971`\nIGRAPH a73a020 DNW- 459 2580 -- ATP Season 1971\n+ attr: name (g/c), name (v/c), age (v/n), hand (v/c), country (v/c),\n| surface (e/c), weight (e/n)\n+ edges from a73a020 (vertex names):\n [1] Andres Gimeno    ->Ken Rosewall   Arthur Ashe      ->Rod Laver     \n [3] Charlie Pasarell ->Cliff Drysdale Frank Froehling  ->Clark Graebner\n [5] Joaquin Loyo Mayo->Thomaz Koch    John Alexander   ->John Newcombe \n [7] John Newcombe    ->Marty Riessen  Nikola Pilic     ->Cliff Drysdale\n [9] Owen Davidson    ->Cliff Drysdale Robert Maud      ->Cliff Drysdale\n[11] Roger Taylor     ->Marty Riessen  Roy Emerson      ->Rod Laver     \n[13] Tom Okker        ->John Newcombe  Allan Stone      ->Bob Carmichael\n+ ... omitted several edges\n\n$`1972`\nIGRAPH 761ed05 DNW- 504 2767 -- ATP Season 1972\n+ attr: name (g/c), name (v/c), age (v/n), hand (v/c), country (v/c),\n| surface (e/c), weight (e/n)\n+ edges from 761ed05 (vertex names):\n [1] Jean Loup Rouyer->Stan Smith     Marty Riessen   ->Cliff Drysdale\n [3] Roy Emerson     ->Arthur Ashe    Roy Emerson     ->Arthur Ashe   \n [5] Tom Leonard     ->John Newcombe  Tom Okker       ->Arthur Ashe   \n [7] Tom Okker       ->Rod Laver      Adriano Panatta ->Andres Gimeno \n [9] Adriano Panatta ->Ilie Nastase   Allan Stone     ->John Alexander\n[11] Allan Stone     ->Marty Riessen  Andres Gimeno   ->Jan Kodes     \n[13] Andres Gimeno   ->Stan Smith     Andrew Pattison ->Ilie Nastase  \n+ ... omitted several edges\n\n$`1973`\nIGRAPH 92ff576 DNW- 592 3653 -- ATP Season 1973\n+ attr: name (g/c), name (v/c), age (v/n), hand (v/c), country (v/c),\n| surface (e/c), weight (e/n)\n+ edges from 92ff576 (vertex names):\n [1] Harold Solomon    ->Stan Smith       John Alexander    ->Stan Smith      \n [3] Patrice Dominguez ->Paolo Bertolucci Paul Gerken       ->Jimmy Connors   \n [5] Roy Emerson       ->Rod Laver        Adriano Panatta   ->Ilie Nastase    \n [7] Bjorn Borg        ->Adriano Panatta  Brian Gottfried   ->Cliff Richey    \n [9] Charlie Pasarell  ->John Alexander   Cliff Richey      ->Stan Smith      \n[11] Corrado Barazzutti->Bjorn Borg       Francois Jauffret ->Ilie Nastase    \n[13] Georges Goven     ->Manuel Orantes   Manuel Orantes    ->Ilie Nastase    \n+ ... omitted several edges\n\n\nNow instead of the useless one, two, three, etc. names, we have the actual year numbers as the names of the elements on each list.\nSo if we wanted to know the top five players for 1988 we could just type:\n\n   top5(g[[\"1988\"]])\n\n   Stefan Edberg     Andre Agassi     Boris Becker    Mats Wilander \n              63               59               52               49 \nAaron Krickstein \n              48 \n\n\nNote the double bracket notation and the fact that the name of the list is a character not a number (hence the scare quotes).\nIf we don’t want to remember the bracket business, we could also use the $ operator to refer to particular list elements:\n\n   top5(g$\"1988\")\n\n   Stefan Edberg     Andre Agassi     Boris Becker    Mats Wilander \n              63               59               52               49 \nAaron Krickstein \n              48 \n\n\nOf course, we can also use the names to subset the list. Let’s say we wanted the top five players for 1970, 1980, 1990, 2000, 2010, and 2020.\nAll we have to do is type:\n\n   decades <- c(\"1970\", \"1980\", \"1990\", \"2000\", \"2010\", \"2020\")\n   lapply(g[decades], top5)\n\n$`1970`\n      Arthur Ashe      Cliff Richey         Rod Laver        Stan Smith \n               51                49                48                45 \nZeljko Franulovic \n               42 \n\n$`1980`\n     Ivan Lendl    John Mcenroe Brian Gottfried      Bjorn Borg Eliot Teltscher \n             97              76              63              62              62 \n\n$`1990`\n  Boris Becker  Stefan Edberg     Ivan Lendl   Pete Sampras Emilio Sanchez \n            62             57             50             47             44 \n\n$`2000`\nYevgeny Kafelnikov        Marat Safin    Gustavo Kuerten      Magnus Norman \n                63                 61                 59                 58 \n    Lleyton Hewitt \n                53 \n\n$`2010`\n   Rafael Nadal   Roger Federer    David Ferrer Robin Soderling   Jurgen Melzer \n             63              54              53              53              51 \n\n$`2020`\n     Andrey Rublev     Novak Djokovic Stefanos Tsitsipas       Rafael Nadal \n                40                 36                 27                 26 \n   Daniil Medvedev \n                24 \n\n\nNote that we are back to the single bracket notation.\nWith a bit of practice, lists will become your friends!"
  },
  {
    "objectID": "handout3.html",
    "href": "handout3.html",
    "title": "Status and Prestige",
    "section": "",
    "text": "In the last handout, we saw how to compute the most popular centrality measures. Freeman’s “big three” have strong graph-theoretic foundation and do a good job of formalizing and quantifying the idea that a node is central if it is “well-placed” in the network, where being well-placed resolves into either being able to reach others (directly as with degree or indirectly as with closeness) or being able to intermediate between others (as with betweenness)."
  },
  {
    "objectID": "handout3.html#networks-as-prisms",
    "href": "handout3.html#networks-as-prisms",
    "title": "Status and Prestige",
    "section": "Networks as Prisms",
    "text": "Networks as Prisms\nThere is, however, another strong and well-motivated intuition as to what it means to be “well-placed” in a network. Here the ties in the network are seen less as “pipes” that transmit stuff and more like “prisms” that reflect on you (Podolny 2001). Another way of thinking about this second version of well-placedness is that what is transmitted through the network is the network itself, or more accurately, the importance, status, and prestige of the people you are connected to.\nUnder this interpretation, actors get status and prestige in the network from being connected to prestigious and high status others. Those others, in turn, get their status from being connected to high status others, and so ad infinitum.\nOne way of quantifying this idea goes like this. If \\(\\mathbf{x}\\) is a vector containing the desired status scores, then the status of each actor should be equal to:\n\\[\n   x_i = \\sum_{j} a_{ij}x_j\n\\tag{1}\\]\nWhere \\(a_{ij} = 1\\) if \\(i\\) is adjacent to \\(j\\) in the network. Note that this formula just sums up the status scores of all the others each actor is connected to.\nIn matrix notation, if \\(\\mathbf{x}\\) is a column vector of status scores then:\n\\[\n   \\mathbf{x} = A\\mathbf{x}\n\\]\nBecause \\(\\mathbf{x}\\) is an \\(n \\times n\\) matrix and \\(\\mathbf{x}\\) is \\(n \\times 1\\) column vector, the multiplication \\(A\\mathbf{x}\\) will return another column vector of dimensions \\(n \\times 1\\), in this case \\(\\mathbf{x}\\) itself!\nNote the problem that this formulation poses: \\(\\mathbf{x}\\) appears on both sides of the equation, which means that in order to know the status of any one node we would need to know the status of the others, but calculating the status of the others depends on knowing the status of the focal node, and so on. There’s a chicken and the egg problem here.\nNow, there is an obvious (to the math majors) mathematical solution to this problem, because there’s a class of solvable (under some mild conditions imposed on the matrix \\(\\mathbf{A}\\)) linear algebra problems that take the form:\n\\[\n   \\lambda\\mathbf{x} = A\\mathbf{x}\n\\]\nWhere \\(\\lambda\\) is just a plain old number (a scalar). Once again conditional of the aforementioned mild conditions being met, we can search for a value \\(\\lambda\\) while also filling up the \\(\\mathbf{x}\\) vector with another set of values until we make the above equality true.\nWhen we do that successfully, we say that the value of \\(\\lambda\\) we hit upon is an eigenvalue of the matrix \\(\\mathbf{A}\\) and the values of the vector \\(\\mathbf{x}\\) we came up with are an eigenvector of the same matrix (technically in the above equation a right eigenvector).\nEigenvalues and eigenvectors, like Don Quixote and Sancho Panza, come in pairs, because you need a unique combination of both to solve the equation. Typically, a given matrix (like an adjacency matrix) will have multiple \\(\\lambda/\\mathbf{x}\\) pairs that will solve the equation. Together the whole set \\(\\lambda/\\mathbf{x}\\) pairs that make the equation true are the eigenvalues and eigenvectors of the matrix.\nNote that all of this obscure talk about eigenvalues and eigenvectors is just matrix math stuff. It has nothing to do with networks and social structure.\nIn contrast, because the big three centrality measures have a direct foundation in graph theory, and graph theory is an isomorphic model of social structures (points map to actor and lines map to relations) the “math” we do with graph theory is directly meaningful as a model of networks (the counts of the number of edges incident to a node is the count of other actors they someone is directly connected to).\nEigenvalues and eigenvectors are not a model of social structure in the way graph theory is (their first scientific application was in Physics). They are just a mechanical math fix to a circular equation problem.\nThis is why it’s a mistake to introduce network measures of status and prestige by jumping directly to the machinery of linear algebra (or worse talk about the idea of eigenvector centrality which means nothing to most people).\nA better approach is to see if we can motivate the use of measures like the ones above using the simple model of the distribution of status and prestige we started with earlier. We will see that we can, and that doing that leads us back to solutions that are the mathematical equivalent of all the eigenvector stuff."
  },
  {
    "objectID": "handout3.html#distributing-centrality-to-others",
    "href": "handout3.html#distributing-centrality-to-others",
    "title": "Status and Prestige",
    "section": "Distributing Centrality to Others",
    "text": "Distributing Centrality to Others\nLet’s start with the simplest model of how people can get their status from the status of others in a network. It is the simplest because it is based on degree.\nImagine everyone has the same “quantum” of status to begin with (this can be stored in a vector containing the same number of length equals to number of actors in the network). Then, at each step, people “send” the same amount of status to all their alters in the network. At the end of each step, we compute people’s status scores using Equation 1. We stop doing this after the status scores of people stop changing across each iteration.\nLet us see a real-life example at work.\nWe will use a data set collected by David Krackhardt on the friendships of 21 managers in a high tech company in the West coast (see the description here). The data are reported as directed ties (\\(i\\) nominates \\(j\\) as a friend) but we will constrain ties to be undirected:\n\n   library(networkdata)\n   library(igraph)\n   g <- as.undirected(ht_friends)\n\nThis is what the network looks like:\n\n\n\n\n\nKrackhardt’s Manager Data.\n\n\n\n\nWe extract the adjacency matrix corresponding to this network:\n\n   A <- as.matrix(as_adjacency_matrix(g))\n\nAnd here’s a simple custom function using a while loop that exemplifies the process of status distribution through the network we talked about earlier:\n\n   status1 <- function(A) {\n      n <- nrow(A) #number of actors\n      x <- rep(1, n) #initial status vector set to all ones\n      w <- 1 \n      k <- 0 #initializing counter\n      while (w > 0.0001) {\n          o.x <- x #old status scores\n          x <- A %*% x #new scores a function of old scores and adjacency matrix\n          x <- x/norm(x, type = \"E\") #normalizing new status scores\n          w <- abs(sum(abs(x) - abs(o.x))) #diff. between new and old scores\n          k <- k + 1 #incrementing while counter\n      }\n   return(as.vector(x))\n   }\n\nLines 1-5 initialize various quantities, most importantly the initial status vector for each node to just a series of ones:\n\n   rep(1, nrow(A))\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n\nThen lines 6-12 implement a little loop of how status is distributed through the network, with the most important piece of code being line 8 where the current status scores for each node are just the sum of the status scores of its neighbors computed one iteration earlier. The program stops when the difference between the old and the new scores is negligible (\\(\\delta < 0.0001\\)) as checked in line 10.\nNote the normalization step on line 9, which is necessary to prevent the sum of status scores from getting bigger and bigger indefinitely. In base R, the type = \"E\" normalization implements the euclidean vector norm, by which we divide each value of the status scores by after each update.1\nAnd here’s the resulting (row) vector of status scores for each node:\n\n   round(status1(A), 3)\n\n [1] 0.256 0.262 0.184 0.202 0.260 0.178 0.085 0.157 0.183 0.193 0.336 0.227\n[13] 0.067 0.165 0.253 0.149 0.413 0.102 0.281 0.149 0.162\n\n\nWhat if I told you that this vector is the same as that given by the leading (first) eigenvector of the adjacency matrix?\n\n   eig <- eigen(A)\n   round(eig$vector[, 1], 3) * -1\n\n [1] 0.256 0.262 0.184 0.202 0.260 0.178 0.085 0.157 0.183 0.193 0.336 0.227\n[13] 0.067 0.165 0.253 0.149 0.413 0.102 0.281 0.149 0.162\n\n\nWhich is of course what is computed by the eigen_centrality function in igraph:\n\n   round(eigen_centrality(g, scale = FALSE)$vector, 3)\n\n [1] 0.256 0.262 0.184 0.202 0.260 0.178 0.085 0.157 0.183 0.193 0.336 0.227\n[13] 0.067 0.165 0.253 0.149 0.413 0.102 0.281 0.149 0.162\n\n\nSo, the eigenvector centralities are just the limit scores produced by the status distribution process implemented in the status1 function!\nMost other measures of status in networks are constructed using similar principles. What changes is the model of how status is distributed in the system. That’s why scary and non-intuitive stuff about eigenvectors or whatever is misleading.\nOther measures are designed such that they either change the quantum of status that is distributed through the network by making it dependent on some node characteristic (like degree) or differentiate between different routes of distribution in directed graphs, by for instance, differentiating status derived from outgoing links from that derived from incoming links.\nLet’s see some examples of these alternative cases."
  },
  {
    "objectID": "handout3.html#a-degree-dependent-model-of-status-aka-pagerank",
    "href": "handout3.html#a-degree-dependent-model-of-status-aka-pagerank",
    "title": "Status and Prestige",
    "section": "A Degree-Dependent Model of Status (AKA PageRank)",
    "text": "A Degree-Dependent Model of Status (AKA PageRank)\nNote that the model of status distribution implied by the traditional eigenvector centrality implies that each node distributes the same amount of status independently of the number of connection it has; status just replicates. Thus, a node with a 100 friends has 100 status units to distribute to each of them and a node with a 10 friends has 10 units.\nThis is why the eigenvector idea rewards nodes who are connected to popular others more. Even though everyone begins with a single unit of status, well-connected nodes by degree end up having more of it to distribute.\nBut what if status dissipated proportionately to the number of connections one had? For instance, if someone has 100 friends and they only had so much time or energy, they would only have a fraction of status to distribute to others than a person with 10 friends.\nIn that case, the node with a hundred friend would only have 1/100 of status unites to distribute to each of their connections while the node with 10 friends would have 1/10 units. Under this formulation, being connected to discerning others, that is people who only connect to a few, is better than being connected to others who connect to everyone else indiscriminately.\nHow would we implement this model? First, let’s create a variation of the adjacency matrix called the \\(\\mathbf{P}\\) matrix:\n\n   P <- A/rowSums(A)\n\nSo this is the original adjacency matrix, with each entry \\(a_{ij}\\) divided by the sum of the corresponding row, which, as you may recall, is equivalent to the degree of node \\(i\\).\nHere are the first 10 rows and columns of the new matrix:\n\n   round(P[1:10, 1:10], 2)\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n [1,] 0.00 0.11 0.00 0.11 0.00 0.00 0.00 0.11 0.00  0.00\n [2,] 0.10 0.00 0.00 0.10 0.10 0.10 0.00 0.00 0.00  0.00\n [3,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.17\n [4,] 0.14 0.14 0.00 0.00 0.00 0.00 0.00 0.14 0.00  0.00\n [5,] 0.00 0.10 0.00 0.00 0.00 0.00 0.00 0.00 0.10  0.10\n [6,] 0.00 0.14 0.00 0.00 0.00 0.00 0.14 0.00 0.14  0.00\n [7,] 0.00 0.00 0.00 0.00 0.00 0.33 0.00 0.00 0.00  0.00\n [8,] 0.20 0.00 0.00 0.20 0.00 0.00 0.00 0.00 0.00  0.20\n [9,] 0.00 0.00 0.00 0.00 0.17 0.17 0.00 0.00 0.00  0.17\n[10,] 0.00 0.00 0.12 0.00 0.12 0.00 0.00 0.12 0.12  0.00\n\n\nNote that the entries are now numbers between zero and one and the matrix is asymmetric that is \\(p_{ij}\\) is not necessarily equal to \\(p_{ji}\\). In fact \\(p_{ij}\\) will only be equal to \\(p_{ji}\\) when \\(k_i = k_j\\) (nodes have the same degree).\nMoreover the rows of \\(\\mathbf{P}\\) sum to one:\n\n   rowSums(P)\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n\nWhich means that the \\(\\mathbf{P}\\) matrix is row stochastic. That is the “outdegree” of each node in the matrix is forced to sum to a fixed number (which means that it is a useless quantity). However, the indegree is not:\n\n   round(colSums(P), 2)\n\n [1] 1.11 1.34 0.63 0.86 1.56 1.06 0.37 0.51 0.61 1.21 2.33 0.92 0.17 0.87 1.08\n[16] 0.53 2.73 0.54 1.21 0.60 0.77\n\n\nWhich means that inequalities in the system will be tied to the indegree of each node in the \\(\\mathbf{P}\\) matrix. This matrix has many interpretations, but here it just quantifies the idea that the amount of centrality each node can distribute is proportional to their degree, and that the larger the degree, the less there is to distribute (the smaller each cell \\(p_{ij}\\) will be). Meanwhile, it is clear that nodes that are pointed to by many other nodes who themselves don’t point to many others have a large indegree in \\(\\mathbf{P}\\).\nNow we can just write function implementing the model of status distribution we used for eigenvector centrality but this time using the \\(\\mathbf{P}\\) rather than the \\(\\mathbf{A}\\) matrix. Note that because we are interested in the status that comes into each node we use the transpose of \\(\\mathbf{P}\\) rather than \\(\\mathbf{P}\\). So at each step the status of a node is equivalent to the sum of the status scores of their in-neighbors, with more discerning in-neighbors passing along more status than less discerning ones:\n\n   round(status1(t(P)), 3)\n\n [1] 0.236 0.262 0.157 0.183 0.262 0.183 0.079 0.131 0.157 0.210 0.367 0.210\n[13] 0.052 0.157 0.236 0.131 0.472 0.105 0.262 0.131 0.157\n\n\nWhat if I told you that these numbers are the same as the leading eigenvector of \\(\\mathbf{P}^T\\)?\n\n   eig <- eigen(t(P))\n   round(eig$vector[, 1], 3) * -1\n\n [1] 0.236 0.262 0.157 0.183 0.262 0.183 0.079 0.131 0.157 0.210 0.367 0.210\n[13] 0.052 0.157 0.236 0.131 0.472 0.105 0.262 0.131 0.157\n\n\nAnd, of course, the (normalized) scores produced by this approach are identical to those computed by the page_rank function in igraph:\n\n   s2 <- status1(t(P))\n   pr <- page_rank(g, damping = 1, algo = \"arpack\")$vector\n   round(pr/max(pr), 3)\n\n [1] 0.500 0.556 0.333 0.389 0.556 0.389 0.167 0.278 0.333 0.444 0.778 0.444\n[13] 0.111 0.333 0.500 0.278 1.000 0.222 0.556 0.278 0.333\n\n   round(s2/max(s2), 3)\n\n [1] 0.500 0.556 0.333 0.389 0.556 0.389 0.167 0.278 0.333 0.444 0.778 0.444\n[13] 0.111 0.333 0.500 0.278 1.000 0.222 0.556 0.278 0.333\n\n\nSo the distributional model of status is the same one implemented in the PageRank algorithm!\nPageRank of course was designed to deal with directed graphs (like the World Wide Web). So let’s load up the version of the Krackhardt’s Managers data that contains the advice network which is an unambiguously directed relation.\n\n   g <- ht_advice\n\nWe then compute the \\(\\mathbf{P}\\) matrix:\n\n   A <- as.matrix(as_adjacency_matrix(g))\n   P <- A/rowSums(A)\n   round(P, 2)\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]\n [1,] 0.00 0.17 0.00 0.17 0.00 0.00 0.00 0.17 0.00  0.00  0.00  0.00  0.00\n [2,] 0.00 0.00 0.00 0.00 0.00 0.33 0.33 0.00 0.00  0.00  0.00  0.00  0.00\n [3,] 0.07 0.07 0.00 0.07 0.00 0.07 0.07 0.07 0.07  0.07  0.07  0.07  0.00\n [4,] 0.08 0.08 0.00 0.00 0.00 0.08 0.00 0.08 0.00  0.08  0.08  0.08  0.00\n [5,] 0.07 0.07 0.00 0.00 0.00 0.07 0.07 0.07 0.00  0.07  0.07  0.00  0.07\n [6,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.00  0.00  0.00  0.00\n [7,] 0.00 0.12 0.00 0.00 0.00 0.12 0.00 0.00 0.00  0.00  0.12  0.12  0.00\n [8,] 0.00 0.12 0.00 0.12 0.00 0.12 0.12 0.00 0.00  0.12  0.12  0.00  0.00\n [9,] 0.08 0.08 0.00 0.00 0.00 0.08 0.08 0.08 0.00  0.08  0.08  0.08  0.00\n[10,] 0.07 0.07 0.07 0.07 0.07 0.00 0.00 0.07 0.00  0.00  0.07  0.00  0.07\n[11,] 0.33 0.33 0.00 0.00 0.00 0.00 0.33 0.00 0.00  0.00  0.00  0.00  0.00\n[12,] 0.00 0.00 0.00 0.00 0.00 0.00 0.50 0.00 0.00  0.00  0.00  0.00  0.00\n[13,] 0.17 0.17 0.00 0.00 0.17 0.00 0.00 0.00 0.17  0.00  0.00  0.00  0.00\n[14,] 0.00 0.25 0.00 0.00 0.00 0.00 0.25 0.00 0.00  0.00  0.00  0.00  0.00\n[15,] 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05  0.05  0.05  0.05  0.05\n[16,] 0.25 0.25 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.25  0.00  0.00  0.00\n[17,] 0.20 0.20 0.00 0.20 0.00 0.00 0.20 0.00 0.00  0.00  0.00  0.00  0.00\n[18,] 0.06 0.06 0.06 0.06 0.06 0.00 0.06 0.06 0.06  0.06  0.06  0.00  0.06\n[19,] 0.09 0.09 0.09 0.00 0.09 0.00 0.09 0.00 0.00  0.09  0.09  0.00  0.00\n[20,] 0.08 0.08 0.00 0.00 0.00 0.08 0.00 0.08 0.00  0.00  0.08  0.08  0.00\n[21,] 0.00 0.09 0.09 0.09 0.00 0.09 0.09 0.09 0.00  0.00  0.00  0.09  0.00\n      [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21]\n [1,]  0.00  0.00  0.17  0.00  0.17  0.00  0.00  0.17\n [2,]  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.33\n [3,]  0.07  0.00  0.00  0.07  0.07  0.00  0.07  0.07\n [4,]  0.00  0.00  0.08  0.08  0.08  0.00  0.08  0.08\n [5,]  0.07  0.00  0.07  0.07  0.07  0.07  0.07  0.07\n [6,]  0.00  0.00  0.00  0.00  0.00  0.00  0.00  1.00\n [7,]  0.12  0.00  0.00  0.12  0.12  0.00  0.00  0.12\n [8,]  0.00  0.00  0.00  0.00  0.12  0.00  0.00  0.12\n [9,]  0.08  0.00  0.08  0.08  0.08  0.00  0.00  0.08\n[10,]  0.00  0.07  0.07  0.07  0.07  0.07  0.07  0.00\n[11,]  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n[12,]  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.50\n[13,]  0.17  0.00  0.00  0.00  0.17  0.00  0.00  0.00\n[14,]  0.00  0.00  0.00  0.00  0.25  0.00  0.00  0.25\n[15,]  0.05  0.00  0.05  0.05  0.05  0.05  0.05  0.05\n[16,]  0.00  0.00  0.00  0.00  0.25  0.00  0.00  0.00\n[17,]  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.20\n[18,]  0.06  0.06  0.06  0.00  0.00  0.06  0.06  0.06\n[19,]  0.09  0.09  0.00  0.00  0.09  0.00  0.09  0.00\n[20,]  0.08  0.08  0.08  0.08  0.08  0.00  0.00  0.08\n[21,]  0.09  0.00  0.00  0.09  0.09  0.00  0.09  0.00\n\n\nRemember how we said earlier that there are multiple ways of thinking about \\(\\mathbf{P}\\)? Another way of thinking about the \\(\\mathbf{P}\\) matrix is as characterizing the behavior of a random walker in the directed graph. At any time point \\(t\\) the walker (a piece of information, a virus, or status itself) sits on a node and the with probability \\(p_{ij}\\) it jumps to one of that node’s out-neighbors. The probabilities are stored in the matrix \\(\\mathbf{P}\\).\nOne issue that arises is that there could be nodes with no out-neighbors (so-called sink nodes) or like node 6 in the above matrix nodes, with just one out-neighbor, in which case the probability is 1.0 that if the random walker is at node 6 it will go to node 21.\nTo avoid this issue the original designers of the PageRank algorithm (Brin and Page 1998) added a “fudge” factor: That is, with probability \\(\\alpha\\) the random walker should hop from node to node following the directed links in the graph. But once in a while with probability \\(1-\\alpha\\) the walker should decide to “teleport” (with uniform probability) to any node in the graph.\nHow do we do that? Well we need to “fix” the \\(\\mathbf{P}\\) matrix to allow for such behavior. So instead of \\(\\mathbf{P}\\) we estimate our distributive status model on the matrix \\(\\mathbf{G}\\) (yes, for Google):\n\\[\n   \\mathbf{G} = \\alpha \\mathbf{P} + (1 - \\alpha) \\mathbf{E}\n\\]\nWhere \\(\\mathbf{E}\\) is a matrix of the same dimensions as \\(\\mathbf{P}\\) but containing \\(1/n\\) in every cell indicating that every node has an equal chance of being teleported to.\nSo fixing \\(\\alpha = 0.85\\) our \\(\\mathbf{G}\\) matrix would be:\n\n   E <- matrix(1/vcount(g), vcount(g), vcount(g))\n   G <- (0.85 * P) + (0.15 * E)\n\nAnd then we just play our status distribution game on the transpose of \\(\\mathbf{G}\\):\n\n   s3 <- round(status1(t(G)), 3)\n   round(s3/max(s3), 3)\n\n [1] 0.302 0.573 0.165 0.282 0.100 0.437 0.635 0.255 0.092 0.180 0.237 0.242\n[13] 0.087 0.268 0.097 0.170 0.258 0.440 0.087 0.200 1.000\n\n\nWhich is the same answer you would get from the igraph function page_rank by setting the “damping” parameter to 0.85:\n\n   pr <- page_rank(g, damping = 0.85, algo = \"arpack\")$vector\n   round(pr/max(pr), 3)\n\n [1] 0.302 0.573 0.165 0.281 0.100 0.437 0.635 0.256 0.091 0.180 0.237 0.242\n[13] 0.086 0.269 0.097 0.169 0.258 0.440 0.086 0.200 1.000"
  },
  {
    "objectID": "handout3.html#hubs-and-authorities",
    "href": "handout3.html#hubs-and-authorities",
    "title": "Status and Prestige",
    "section": "Hubs and Authorities",
    "text": "Hubs and Authorities\nRecall from previous discussions that everything doubles (sometimes quadruples like degree correlations) in directed graphs. The same goes for status as reflected in a distributive model through the network.\nConsider two ways of showing your status in a system governed by directed relations (like advice). You can be highly sought after by others (be an “authority”), or you can be discerning in who you seek advice from, preferably seeking out people who are also sought after (e.g., be a “hub” pointing to high-quality others).\nThese two forms of status are mutually defining. The top authorities are those who are sought after by the top hubs, and the top hubs are the ones who seek the top authorities!\nSo this leads to a doubling of Equation 1:\n\\[  \n   x^h_i = \\sum_j a_{ij} x^a_j\n\\]\n\\[\n   x^a_i = \\sum_i a_{ij} x^h_i\n\\]\nWhich says that the hub score \\(x^h\\) of a node is the sum of the authority scores \\(x^a\\) of the nodes they point to, and the authority score of a node is the sum of the hub scores of the nodes that point to it.\nSo we need to make our status game a bit more complicated (but not too much) to account for this duality:\n\n   status2 <- function(A) {\n     n <- nrow(A)\n     a <- rep(1, n)  #initializing authority scores\n     diff.a.h <- 1 #initializing diff.\n     while (diff.a.h >= 0.0001) {\n         o.a <- a #old authority scores\n         h <- A %*% o.a #new hub scores a function of previous authority scores of their out-neighbors\n         h <- h/norm(h, type = \"E\")\n         a <- t(A) %*% h #new authority scores a function of current hub scores of their in-neighbors\n         a <- a/norm(a, type = \"E\")\n         diff.a.h <- abs(sum(abs(o.a) - abs(a))) #diff. between old and new authority scores\n         }\n   return(list(h = as.vector(a), a = as.vector(h)))\n   }\n\nEverything is like our previous status1 function except now we are keeping track of two mutually defining scores a and h. As you may have guessed this is just an implementation of the “HITS” algorithm developed by Kleinberg (1999).\nThe results for the Krackhardt advice network are:\n\n   round(status2(A)$a/max(status2(A)$a), 3)\n\n [1] 0.370 0.176 0.841 0.709 0.835 0.065 0.492 0.490 0.773 0.672 0.206 0.122\n[13] 0.331 0.279 1.000 0.274 0.313 0.800 0.581 0.687 0.600\n\n   round(status2(A)$h/max(status2(A)$h), 3)\n\n [1] 0.782 1.000 0.356 0.496 0.330 0.644 0.684 0.711 0.290 0.615 0.769 0.498\n[13] 0.323 0.677 0.267 0.570 0.645 0.871 0.323 0.589 0.776\n\n\nWhich are equivalent to using the igraph functions authority_score and hub_score:\n\n   round(authority_score(g, scale = TRUE)$vector, 3)\n\n [1] 0.782 1.000 0.356 0.496 0.330 0.644 0.684 0.711 0.290 0.615 0.769 0.498\n[13] 0.323 0.677 0.267 0.570 0.645 0.871 0.323 0.589 0.776\n\n   round(hub_score(g, scale = TRUE)$vector, 3)\n\n [1] 0.370 0.176 0.841 0.709 0.835 0.065 0.492 0.490 0.773 0.672 0.206 0.122\n[13] 0.331 0.279 1.000 0.274 0.313 0.800 0.581 0.687 0.600\n\n\nRecall that both the eigenvector and PageRank status scores computed via the network status distribution game routine ended up being equivalent to the eigenvectors of a network proximity matrix (the adjacency matrix \\(\\mathbf{A}\\) and the probability matrix \\(\\mathbf{P}\\) respectively). It would be surprising if the same wasn’t true of the hub and authority status scores.\nLet’s find out which ones!\nConsider the matrices:\n\\[\n\\mathbf{M} = \\mathbf{A}\\mathbf{A}^T\n\\]\n\\[\n\\mathbf{N} = \\mathbf{A}^T\\mathbf{A}\n\\]\nLet’s see what they look like in the Krackhardt manager’s network:\n\n   M = A %*% t(A)\n   N = t(A) %*% A\n   M[1:10, 1:10]\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n [1,]    6    1    5    5    5    1    3    4    5     5\n [2,]    1    3    3    2    3    1    2    3    3     0\n [3,]    5    3   15   11   12    1    8    8   12     8\n [4,]    5    2   11   12   11    1    7    6   11     8\n [5,]    5    3   12   11   15    1    7    7   12    10\n [6,]    1    1    1    1    1    1    1    1    1     0\n [7,]    3    2    8    7    7    1    8    5    8     4\n [8,]    4    3    8    6    7    1    5    8    7     4\n [9,]    5    3   12   11   12    1    8    7   13     7\n[10,]    5    0    8    8   10    0    4    4    7    14\n\n   N[1:10, 1:10]\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n [1,]   13   13    4    5    5    6    8    8    4     8\n [2,]   13   18    5    8    5    9   11   10    4     9\n [3,]    4    5    5    4    4    2    4    4    2     3\n [4,]    5    8    4    8    3    4    6    6    3     4\n [5,]    5    5    4    3    5    1    3    3    3     3\n [6,]    6    9    2    4    1   10    7    7    2     6\n [7,]    8   11    4    6    3    7   13    6    3     7\n [8,]    8   10    4    6    3    7    6   10    3     6\n [9,]    4    4    2    3    3    2    3    3    4     3\n[10,]    8    9    3    4    3    6    7    6    3     9\n\n\nWhat’s in these matrices? Well let’s look at \\(\\mathbf{M}\\). The diagonals will look familiar because they happen to be the outdegree of each node:\n\n   degree(g, mode = \"out\")[1:10]\n\n [1]  6  3 15 12 15  1  8  8 13 14\n\n\nYou may have guessed that the diagonals of matrix \\(\\mathbf{N}\\) contain the indegrees:\n\n   degree(g, mode = \"in\")[1:10]\n\n [1] 13 18  5  8  5 10 13 10  4  9\n\n\nWhich means that the off-diagonals cells of each matrix \\(m_{ij}\\) and \\(n_{ij}\\), contain the common out-neighbors and common in-neighbors shared by nodes \\(i\\) and \\(j\\) in the graph, respectively.\nAs you may already be suspecting, the hub and authorities scores are the leading eigenvectors of the \\(\\mathbf{M}\\) and \\(\\mathbf{N}\\) matrices (Kleinberg 1999):\n\n   h <- eigen(M)$vector[,1] * -1\n   a <- eigen(N)$vector[,1] * -1\n   round(h/max(h), 3)\n\n [1] 0.370 0.176 0.841 0.709 0.835 0.065 0.492 0.490 0.773 0.672 0.206 0.122\n[13] 0.331 0.279 1.000 0.274 0.313 0.800 0.581 0.687 0.600\n\n   round(a/max(a), 3)\n\n [1] 0.782 1.000 0.356 0.496 0.330 0.644 0.684 0.711 0.290 0.615 0.769 0.498\n[13] 0.323 0.677 0.267 0.570 0.645 0.871 0.323 0.589 0.776\n\n\nOnce again demonstrating the equivalence between eigenvectors of proximity matrices in networks and our prismatic status distribution game!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Social Networks Sequence",
    "section": "",
    "text": "Website containing syllabi, reading schedules, and other instructional materials for the courses in the Social Network Analysis computational sequence (208A & 208B) at UCLA Sociology."
  },
  {
    "objectID": "inoutdegdist.html",
    "href": "inoutdegdist.html",
    "title": "Graph Degree Metrics in Directed Networks",
    "section": "",
    "text": "As we saw in the Basic Network Statistics Handout, the degree distribution and the degree correlation are two basic things we want to have a sense of when characterizing a network, and we showed examples for the undirected graph case.\nAs we also saw on the Centrality handout, the number of things you have to compute in terms of degrees “doubles” in the directed graph case; for instance, instead of a single degree set and sequence, now we have two: An out and an indegree set and sequence.\nThe same thing applies to the degree distribution and the degree correlation."
  },
  {
    "objectID": "inoutdegdist.html#in-and-out-degree-distributions",
    "href": "inoutdegdist.html#in-and-out-degree-distributions",
    "title": "Graph Degree Metrics in Directed Networks",
    "section": "In and Out Degree Distributions",
    "text": "In and Out Degree Distributions\nIn the case of the degree distribution, now we have two distributions: An outdegree distribution and an indegree distribution.\nLet’s see an example using the law_advice data.\n\n   library(networkdata)\n   library(igraph)\n   g <- law_advice\n   i.prop <- degree_distribution(g, mode = \"in\")\n   o.prop <- degree_distribution(g, mode = \"out\")\n\nSo the main complication is that now we have to specify a value for the “mode” argument; “in” for indegree and “out” for outdegree.\nThat also means that when plotting, we have to create two data frames and present two plots.\nFirst the data frames:\n\n   i.d <- degree(g, mode = \"in\")\n   o.d <- degree(g, mode = \"out\")\n   i.d.vals <- c(0:max(i.d))\n   o.d.vals <- c(0:max(o.d))\n   i.deg.dist <- data.frame(i.d.vals, i.prop)\n   o.deg.dist <- data.frame(o.d.vals, o.prop)\n   head(i.deg.dist)\n\n  i.d.vals     i.prop\n1        0 0.01408451\n2        1 0.02816901\n3        2 0.07042254\n4        3 0.02816901\n5        4 0.08450704\n6        5 0.02816901\n\n   head(o.deg.dist)\n\n  o.d.vals     o.prop\n1        0 0.01408451\n2        1 0.00000000\n3        2 0.01408451\n4        3 0.05633803\n5        4 0.04225352\n6        5 0.04225352\n\n\nNow, to plotting. To be effective, the resulting plot has to show the outdegree and indegree distribution side by side so as to allow the reader to compare. To do that, we first generate each plot separately:\n\n   library(ggplot2)\n   p <- ggplot(data = o.deg.dist, aes(x = o.d.vals, y = o.prop))\n   p <- p + geom_bar(stat = \"identity\", fill = \"red\", color = \"red\")\n   p <- p + theme_minimal()\n   p <- p + labs(x = \"\", y = \"Proportion\", \n                 title = \"Outdegree Distribution in Law Advice Network\") \n   p <- p + geom_vline(xintercept = mean(o.d), \n                       linetype = 2, linewidth = 0.75, color = \"blue\")\n   p1 <- p + scale_x_continuous(breaks = c(0, 5, 10, 15, 20, 25, 30, 35, 40)) + xlim(0, 40)\n\n   p <- ggplot(data = i.deg.dist, aes(x = i.d.vals, y = i.prop))\n   p <- p + geom_bar(stat = \"identity\", fill = \"red\", color = \"red\")\n   p <- p + theme_minimal()\n   p <- p + labs(x = \"\", y = \"Proportion\", \n                 title = \"Indegree Distribution in Law Advice Network\") \n   p <- p + geom_vline(xintercept = mean(i.d), \n                       linetype = 2, linewidth = 0.75, color = \"blue\")\n   p2 <- p + scale_x_continuous(breaks = c(0, 5, 10, 15, 20, 25, 30, 35, 40)) + xlim(0, 40)\n\nThen we use the magical package patchwork to combine the plots:\n\n   # install.packages(\"patchwork\")\n   library(patchwork)\n   p <- p1 / p2\n   p\n\n\n\n\nThe data clearly shows that while both distributions are skewed, the indegree distribution is more heterogeneous, with a larger proportion of nodes in the high end of receiving advice as compared to giving advice.\nNote also that since the mean degree is the same regardless of whether we use the out or indegree distribution, then the blue line pointing to the mean degree falls in the same spot on the x-axis for both plots."
  },
  {
    "objectID": "inoutdegdist.html#in-and-out-degree-correlations",
    "href": "inoutdegdist.html#in-and-out-degree-correlations",
    "title": "Graph Degree Metrics in Directed Networks",
    "section": "In and Out Degree Correlations",
    "text": "In and Out Degree Correlations\nThe same doubling (really quadrupling) happens to degree correlations in directed graphs. While in an undirected graph, there is a single degree correlation, in the directed case we have four quantities to compute: The out-out degree correlation, the in-in degree correlation, the out-in degree correlation, and the in-out degree correlation (see here, p. 38).\nTo proceed, we need to create an edge list data set with six columns: The node id of the “from” node, the node id of the “to” node, the indegree of the “from” node, the outdegree of the “from” node, the indegree of the “to” node, and the outdegree of the “to” node.\nWe can adapt the code we used for the undirected case for this purpose. First, we create an edge list data frame using the igraph function as_data_frame:\n\n   library(dplyr)\n   g.el <- igraph::as_data_frame(g) %>% \n      rename(fr = from)\n   head(g.el)\n\n  fr to\n1  1  2\n2  1 17\n3  1 20\n4  2  1\n5  2  6\n6  2 17\n\n\nNote that we have to specify that this is in an igraph function by typing igraph:: in front of the as_data_frame because there is an (older) dplyr function with the same name that was used for data wrangling.\nSecond, we create data frames containing the in and outdegrees of each node in the network:\n\n    deg.dat.fr <- data.frame(fr = 1:vcount(g), o.d, i.d)\n    deg.dat.to <- data.frame(to = 1:vcount(g), o.d, i.d)\n\nThird, we merge this info into the edge list data frame to get the in and outdegrees of the from and to nodes in the directed edge:\n\n   d.el <- g.el %>% \n      left_join(deg.dat.fr) %>% \n      rename(o.d.fr = o.d, i.d.fr = i.d) %>% \n      left_join(deg.dat.to, by = \"to\") %>% \n      rename(o.d.to = o.d, i.d.to = i.d) \n   head(d.el)\n\n  fr to o.d.fr i.d.fr o.d.to i.d.to\n1  1  2      3     22      7     23\n2  1 17      3     22     21     26\n3  1 20      3     22     11     22\n4  2  1      7     23      3     22\n5  2  6      7     23      0     21\n6  2 17      7     23     21     26\n\n\nNow we can compute the four different flavors of the degree correlation for directed graphs:\n\n   round(cor(d.el$o.d.fr, d.el$o.d.to), 4) #out-out correlation\n\n[1] -0.0054\n\n   round(cor(d.el$i.d.fr, d.el$i.d.to), 4) #in-in correlation\n\n[1] 0.187\n\n   round(cor(d.el$i.d.fr, d.el$o.d.to), 4) #in-out correlation\n\n[1] -0.0283\n\n   round(cor(d.el$o.d.fr, d.el$i.d.to), 4) #out-in correlation\n\n[1] -0.0843\n\n\nThese results tell us that there is not much degree assortativity going on in the law advice network, except for a slight tendency of people who receive advice from lots of others to give advice to people who also receive advice from a lot of other people (the “in-in” correlation)\nNote that by default, the assortativity_degree function in igraph only returns the out-in correlation for directed graphs:\n\n   round(assortativity_degree(g, directed = TRUE), 4)\n\n[1] -0.0843\n\n\nThat is, assortativity_degree checks if more active senders are more likely to send ties to people who are popular receivers of ties."
  },
  {
    "objectID": "schedule-208A-F24.html",
    "href": "schedule-208A-F24.html",
    "title": "SOCIOL 208A Reading Schedule (Fall 2024)",
    "section": "",
    "text": "Prell, C. & Schaefer, D. R. (2023). Introducing Social Network Analysis. In J. McLevey, J. Scott, P. J. Carrington (Eds.) The SAGE Handbook of Social Network Analysis. Sage Publications. link\nLight, R. & Moody, J. (2021). Network Basics: Points, Lines, and Positions. In R. Light, and J. Moody (Eds.) The Oxford Handbook of Social Networks Oxford University Press. link\nHarary, F. & Norman., R. Z. (1953). Graph Theory as a Mathematical Model in Social Science. Research Center for Group Dynamics, University of Michigan. link\n\n\n\n\n\nBasic Introduction to R\nThe Basics of the R Programming Language Short Intergraph Tutorial\n\nPackage networkdata"
  },
  {
    "objectID": "schedule-208A-F24.html#week-2-october-9-centrality",
    "href": "schedule-208A-F24.html#week-2-october-9-centrality",
    "title": "SOCIOL 208A Reading Schedule (Fall 2024)",
    "section": "Week 2, October 9: Centrality",
    "text": "Week 2, October 9: Centrality\n\nReadings\n\nMartin G. Everett & Steve P. Borgatti (2023). “Centrality.” In J. McLevey, J. Scott, P. J. Carrington (Eds.) The SAGE Handbook of Social Network Analysis. Sage Publications. link\nFreeman, L. C. (1978). Centrality in Social Networks Conceptual Clarification. Social Networks, 1(3), 215-239. pdf\nKoschützki, D., Lehmann, K.A., Peeters, L., Richter, S., Tenfelde-Podehl, D., Zlotowski, O. (2005). Centrality Indices. In: Brandes, U., Erlebach, T. (eds) Network Analysis. Lecture Notes in Computer Science, vol 3418. Springer, Berlin, Heidelberg (secs. 3.2, 3.3, and 3.4). link\nNeal, Z. P. (2014). A network perspective on the processes of empowered organizations. American Journal of Community Psychology, 53, 407-418. https://doi.org/10.1007/s10464-013-9623-1\nAgneessens, F., Borgatti, S. P., & Everett, M. G. (2017). Geodesic based centrality: Unifying the local and the global. Social Networks, 49, 12-26. link\n\n\n\nExplainers\n\nLizardo, O. (n.d). Centralities based on Degree. link\nLizardo, O. (n.d). Centralities based on the Geodesic Distance. link\nLizardo, O. (n.d). Centralities based on Shortest Paths. link\n\n\n\nOther Material & Further Reading\n\nBorgatti, S. P., & Everett, M. G. (2006). A graph-theoretic perspective on centrality. Social Networks, 28(4), 466-484. link\nBrandes, U., Borgatti, S. P., & Freeman, L. C. (2016). Maintaining the duality of closeness and betweenness centrality. Social networks, 44, 153-159. link\nRossman, G., Esparza, N., & Bonacich, P. (2010). I’d Like To Thank The Academy, Team Spillovers, and Network Centrality. American Sociological Review, 75(1), 31-51. link\nKoschützki, D., Lehmann, K.A., Tenfelde-Podehl, D., Zlotowski, O. (2005). Advanced Centrality Concepts. In: Brandes, U., Erlebach, T. (eds) Network Analysis. Lecture Notes in Computer Science, vol 3418. Springer, Berlin, Heidelberg. link\nComprehensive list of centrality measures with formulas and software"
  },
  {
    "objectID": "schedule-208A-F24.html#week-3-october-16-status-and-prestige",
    "href": "schedule-208A-F24.html#week-3-october-16-status-and-prestige",
    "title": "SOCIOL 208A Reading Schedule (Fall 2024)",
    "section": "Week 3, October 16: Status and Prestige",
    "text": "Week 3, October 16: Status and Prestige\n\nLizardo, O. (n.d.) Status link\nFranceschet, M. (2011). PageRank: standing on the shoulders of giants. Communications of the ACM, 54(6), 92-101. link\nMartin, J. L. & Murphy, J. P. (2021). Networks, Status, and Inequality. In R. Light, and J. Moody (Eds.) The Oxford Handbook of Social Networks Oxford University Press. link\nKoschützki, D., Lehmann, K.A., Peeters, L., Richter, S., Tenfelde-Podehl, D., Zlotowski, O. (2005). Centrality Indices. In: Brandes, U., Erlebach, T. (eds) Network Analysis. Lecture Notes in Computer Science, vol 3418. Springer, Berlin, Heidelberg (sec. 3.9). link\n\n\nFurther (Mathy) Reading\n\nVigna, S. (2016). Spectral ranking. Network Science, 4(4), 433-445. pdf\nBaltz, A., Kliemann, L. (2005). Spectral Analysis. In: Brandes, U., Erlebach, T. (eds) Network Analysis. Lecture Notes in Computer Science, vol 3418. Springer, Berlin, Heidelberg. link\nBonacich, P. (1972). Factoring and Weighting Approaches to Status Scores and Clique Identification. Journal of Mathematical Sociology, 2(1), 113-120. pdf\nKatz, L. (1953). A New Status Index Derived from Sociometric Analysis. Psychometrika, 18(1), 39-43. pdf"
  },
  {
    "objectID": "schedule-208A-F24.html#week-4-october-23",
    "href": "schedule-208A-F24.html#week-4-october-23",
    "title": "SOCIOL 208A Reading Schedule (Fall 2024)",
    "section": "Week 4, October 23:",
    "text": "Week 4, October 23:\n\nNo Class (Traveling)"
  },
  {
    "objectID": "schedule-208A-F24.html#week-5-october-30-similarity-roles-and-positions",
    "href": "schedule-208A-F24.html#week-5-october-30-similarity-roles-and-positions",
    "title": "SOCIOL 208A Reading Schedule (Fall 2024)",
    "section": "Week 5, October 30: Similarity, Roles, and Positions",
    "text": "Week 5, October 30: Similarity, Roles, and Positions\n\nReadings\n\nBreiger, R. L., Boorman, S. A., & Arabie, P. (1975). An algorithm for clustering relational data with applications to social network analysis and comparison with multidimensional scaling. Journal of Mathematical Psychology, 12(3), 328-383. link\nBorgatti, S. P., & Everett, M. G. (1992). Notions of position in social network analysis. Sociological Methodology, 1-35. link\nBrandes, U., & Lerner, J. (2010). Structural similarity: spectral methods for relaxed blockmodeling. Journal of Classification, 27(3), 279-306. link\nLeicht, E. A., Holme, P., & Newman, M. E. (2006). Vertex similarity in networks. Physical Review E—Statistical, Nonlinear, and Soft Matter Physics, 73(2), 026120. link"
  },
  {
    "objectID": "schedule-208A-F24.html#week-6-november-6-subgroups-and-communities",
    "href": "schedule-208A-F24.html#week-6-november-6-subgroups-and-communities",
    "title": "SOCIOL 208A Reading Schedule (Fall 2024)",
    "section": "Week 6, November 6: Subgroups and Communities",
    "text": "Week 6, November 6: Subgroups and Communities\n\nReadings\n\nMoody, J., & Mucha, P. J. (2023). Structural Cohesion and Cohesive Groups. In J. McLevey, J. Scott, P. J. Carrington (Eds.) The SAGE Handbook of Social Network Analysis. Sage Publications. link\nShai, S., Stanley, N., Granell, C., Taylor, D. & Mucha, P. J. (2021). Case Studies in Network Community Detection. In R. Light, and J. Moody (Eds.) The Oxford Handbook of Social Networks Oxford University Press. link\nNewman, M. E. (2006). Modularity and Community Structure in Networks. Proceedings of the National Academy of Sciences, 103(23), 8577-8582. link\nGirvan, M., & Newman, M. E. (2002). Community Structure in Social and Biological Networks. Proceedings of the National academy of Sciences, 99(12), 7821-7826. link\n\n\n\nFurther Reading\n\nFortunato, S., & Castellano, C. (2007). Community structure in graphs. arXiv preprint arXiv:0712.2716. link"
  },
  {
    "objectID": "schedule-208A-F24.html#week-7-november-13-analyzing-two-mode-networks",
    "href": "schedule-208A-F24.html#week-7-november-13-analyzing-two-mode-networks",
    "title": "SOCIOL 208A Reading Schedule (Fall 2024)",
    "section": "Week 7, November 13: Analyzing Two-Mode Networks",
    "text": "Week 7, November 13: Analyzing Two-Mode Networks\n\nReadings\n\nBreiger, R. L. (1974). The Duality of Persons and Groups. Social Forces, 53(2), 181–190. link\nBorgatti, S. P., & Everett, M. G. (1997). Network Analysis of 2-Mode Data. Social Networks, 19(3), 243-269. pdf\nEverett, M. G., & Borgatti, S. P. (2013). The Dual-Projection Approach for Two-Mode Networks. Social Networks, 35(2), 204-210. link\nNeal, Z. (2014). The backbone of bipartite projections: Inferring relationships from co-authorship, co-sponsorship, co-attendance and other co-behaviors. Social Networks, 39, 84-97. link\n\n\n\nOther Material\n\nDomagalski, R., Neal, Z. P., & Sagan, B. (2021). Backbone: An R package for extracting the backbone of bipartite projections. Plos one, 16(1), e0244363. link\nNeal, Z. P. (2022). backbone: An R package to extract network backbones. PloS one, 17(5), e0269137. link"
  },
  {
    "objectID": "schedule-208A-F24.html#week-8-november-20-ego-networks",
    "href": "schedule-208A-F24.html#week-8-november-20-ego-networks",
    "title": "SOCIOL 208A Reading Schedule (Fall 2024)",
    "section": "Week 8, November 20: Ego Networks",
    "text": "Week 8, November 20: Ego Networks\n\nReadings\n\nSmith, J. A. (2021). The Continued Relevance of Ego Network Data. In R. Light, and J. Moody (Eds.) The Oxford Handbook of Social Networks Oxford University Press. link"
  },
  {
    "objectID": "schedule-208A-F24.html#week-9-november-27-statistical-models-of-network-structure",
    "href": "schedule-208A-F24.html#week-9-november-27-statistical-models-of-network-structure",
    "title": "SOCIOL 208A Reading Schedule (Fall 2024)",
    "section": "Week 9, November 27: Statistical Models of Network Structure",
    "text": "Week 9, November 27: Statistical Models of Network Structure\n\nReadings\n\nLusher D., Wang, P., Brennecke, J., Brailly J., Faye, M., Gallagher, C. (2021). Advances in Exponential Random Graph Models. In R. Light, and J. Moody (Eds.) The Oxford Handbook of Social Networks Oxford University Press. link"
  },
  {
    "objectID": "schedule-208A-F24.html#week-10-december-4-dynamic-networks-and-relational-events",
    "href": "schedule-208A-F24.html#week-10-december-4-dynamic-networks-and-relational-events",
    "title": "SOCIOL 208A Reading Schedule (Fall 2024)",
    "section": "Week 10, December 4: Dynamic Networks and Relational Events",
    "text": "Week 10, December 4: Dynamic Networks and Relational Events\n\nTBA"
  },
  {
    "objectID": "schedule-208B-S22.html",
    "href": "schedule-208B-S22.html",
    "title": "SOCIOL 208B Reading Schedule (Spring 2022)",
    "section": "",
    "text": "Marsden, P. V., & Laumann, E. O. (1984). Mathematical Ideas In Social Structural Analysis. Journal of Mathematical Sociology, 10(3-4), 271-294. [pdf]\nWellman, B. (1988). Structural Analysis: From Method and Metaphor To Theory and Substance. In B. Wellman & S. D. Berkowitz (Eds.), Social Structures: A Network Approach. (Pp. 19–61). Cambridge University Press. [pdf]\nEmirbayer, M. (1997). Manifesto For A Relational Sociology. American Journal of Sociology, 103(2), 281–317. [pdf]\n\n\n\n\n\nErikson, E. (2013). Formalist and Relationalist Theory In Social Network Analysis. Sociological Theory, 31(3), 219–242. [pdf]\nEmirbayer, M., & Goodwin, J. (1994). Network Analysis, Culture, and The Problem of Agency. American Journal of Sociology, 99(6), 1411–1454. [pdf]\nMische, A. (2011). Relational Sociology, Culture, and Agency. In J. Scott & P. J. Carrington (Eds.), The Sage Handbook of Social Network Analysis (Pp. 80–97). Sage. [pdf]"
  },
  {
    "objectID": "schedule-208B-S22.html#week-2-the-greatest-hits",
    "href": "schedule-208B-S22.html#week-2-the-greatest-hits",
    "title": "SOCIOL 208B Reading Schedule (Spring 2022)",
    "section": "Week 2: The Greatest Hits",
    "text": "Week 2: The Greatest Hits\n\nTue., Apr. 5\n\nWhite, H. C., Boorman, S. A., & Breiger, R. L. (1976). Social Structure From Multiple Networks. I. Blockmodels of Roles and Positions. American Journal of Sociology, 81(4), 730–780. [pdf]\nBreiger, R. L. (1974). The Duality of Persons and Groups. Social Forces, 53(2), 181–190. [pdf]\nFreeman, L. C. (1978). Centrality in social networks conceptual clarification. Social networks, 1(3), 215-239. [pdf]\n\n\n\nThu., Apr. 7\n\nGranovetter, M. S. (1973). The Strength of Weak Ties. American Journal of Sociology, 78(3), 1360–1380. [pdf]\n\nRejection letter from American Sociological Review of the first (1969) version of the paper [pdf]\nGranovetter, M. S. (1969) ``Alienation Reconsidered: The Strength of Weak Ties.’’ Reprinted in Connections 5(2): 4-16. [pdf]\n\nCentola, D., & Macy, M. (2007). Complex Contagions and the Weakness of Long Ties. American Journal of Sociology, 113(3), 702-734. [pdf]\nMcPherson, M., Smith-Lovin, L., & Cook, J. M. (2001). Birds of a feather: Homophily in social networks. Annual Review of Sociology, 27(1), 415-444. [pdf]"
  },
  {
    "objectID": "schedule-208B-S22.html#week-3-brokerage-and-intermediation",
    "href": "schedule-208B-S22.html#week-3-brokerage-and-intermediation",
    "title": "SOCIOL 208B Reading Schedule (Spring 2022)",
    "section": "Week 3: Brokerage and Intermediation",
    "text": "Week 3: Brokerage and Intermediation\n\nTue., Apr. 12\n\nGould, R. V., & Fernandez, R. M. (1989). Structures of Mediation: A Formal Approach To Brokerage In Transaction Networks. Sociological Methodology, 89-126. [pdf]\nObstfeld, D., Borgatti, S.P. and Davis, J. (2014). Brokerage As a Process: Decoupling Third Party Action From Social Network Structure. Research In The Sociology of Organizations, 40), 135-159. [pdf]\nStovel, K., & Shaw, L. (2012). Brokerage. Annual Review of Sociology, 38, 139-158. [pdf]\n\n\n\nThu., Apr. 14\n\nBurt, R. S. (2004). Structural Holes and Good Ideas. American Journal of Sociology, 110(2), 349-399.[pdf]\nGoldberg, A., Srivastava, S. B., Manian, V. G., Monroe, W., & Potts, C. (2016). Fitting In Or Standing Out? The Tradeoffs of Structural and Cultural Embeddedness. American Sociological Review, 81(6), 1190-1222. [pdf]\nAral, S., & Van Alstyne, M. (2011). The diversity-bandwidth trade-off. American Journal of Sociology, 117(1), 90-171. [pdf]"
  },
  {
    "objectID": "schedule-208B-S22.html#week-4-networks-in-science",
    "href": "schedule-208B-S22.html#week-4-networks-in-science",
    "title": "SOCIOL 208B Reading Schedule (Spring 2022)",
    "section": "Week 4: Networks In Science",
    "text": "Week 4: Networks In Science\n\nTue., Apr. 19\n\nMoody, J. (2004). The Structure of A Social Scientific Collaboration Network. American Sociological Review, 69, 213-238. [pdf]\nShwed, U., & Bearman, P. S. (2010). The Temporal Structure of Scientific Consensus Formation. American Sociological Review, 75(6), 817-840. [pdf]\nMoody, J., & Light, R. (2006). A view from above: The evolving sociological landscape. The American Sociologist, 37(2), 67-86. [pdf]\n\n\n\nThu., Apr. 21\n\nFoster, J. G., Rzhetsky, A., & Evans, J. A. (2015). Tradition and Innovation In Scientists’ Research Strategies. American Sociological Review, 80(5), 875-908. [pdf]\nMcmahan, P., & Mcfarland, D. A. (2021). Creative Destruction: The Structural Consequences of Scientific Curation. American Sociological Review, 86(2), 341-376. [pdf]"
  },
  {
    "objectID": "schedule-208B-S22.html#week-5-collaboration-creativity-and-field-dynamics",
    "href": "schedule-208B-S22.html#week-5-collaboration-creativity-and-field-dynamics",
    "title": "SOCIOL 208B Reading Schedule (Spring 2022)",
    "section": "Week 5: Collaboration, Creativity, and Field Dynamics",
    "text": "Week 5: Collaboration, Creativity, and Field Dynamics\n\nTue., Apr. 26\n\nVedres, B., & Stark, D. (2010). Structural Folds: Generative Disruption In Overlapping Groups. American Journal of Sociology, 115(4), 1150-1190. [pdf]\nUzzi, B, & Spiro, J. (2005). Collaboration and Creativity: The Small World Problem. American Journal of Sociology, 111, 447-504. [pdf]\n\n\n\nThu., Apr. 28\n\nPowell, W. W., White, D. R., Koput, K. W., & Owen-Smith, J. (2005). Network Dynamics and Field Evolution: The Growth of Interorganizational Collaboration In The Life Sciences. American Journal of Sociology, 110(4), 1132-1205. [pdf]\nRossman, G., Esparza, N., & Bonacich, P. (2010). I’d Like To Thank The Academy, Team Spillovers, and Network Centrality. American Sociological Review, 75(1), 31-51. [pdf]"
  },
  {
    "objectID": "schedule-208B-S22.html#week-6-networks-and-culture-and-culture-in-networks",
    "href": "schedule-208B-S22.html#week-6-networks-and-culture-and-culture-in-networks",
    "title": "SOCIOL 208B Reading Schedule (Spring 2022)",
    "section": "Week 6: Networks and Culture and Culture in Networks",
    "text": "Week 6: Networks and Culture and Culture in Networks\n\nTue., May. 3\n\nBreiger, R. L. (2010). Dualities of culture and structure: Seeing through cultural holes. Pp. 37-47 in Relationale soziologie. VS Verlag für Sozialwissenschaften. [pdf]\nLizardo, 0. (2023). Culture and Networks. Pp. 188-201 in The SAGE Handbook of Social Network Analysis. SAGE Publications Ltd. [pdf]\nLewis, K., & Kaufman, J. (2018). The Conversion of Cultural Tastes Into Social Network Ties. American Journal of Sociology, 123(6), 1684-1742. [pdf]\n\n\n\nThu., May. 5\n\nFuhse, J. A. (2009). The Meaning Structure of Social Networks. Sociological Theory, 27(1), 51-73. [pdf]\nIkegami, E. (2000). A Sociological Theory of Publics: Identity and Culture As Emergent Properties In Networks. Social Research, 989-1029. [pdf]\nMützel, S., & Breiger, R. (2021). Duality Beyond Persons and Groups. In The Oxford Handbook of Social Networks. Oxford University Press. [pdf]"
  },
  {
    "objectID": "schedule-208B-S22.html#week-7-difussion-in-networks",
    "href": "schedule-208B-S22.html#week-7-difussion-in-networks",
    "title": "SOCIOL 208B Reading Schedule (Spring 2022)",
    "section": "Week 7: Difussion In Networks",
    "text": "Week 7: Difussion In Networks\n\nTue., May. 10\n\nDellaposta, D., Shi, Y., & Macy, M. (2015) Why Do Liberals Drink Lattes?. American Journal of Sociology, 120(5), 1473-1511. [pdf]\nCentola, D. (2015). The social origins of networks and diffusion. American Journal of Sociology, 120(5), 1295-1338. [pdf]\nBecker, S. O., Hsiao, Y., Pfaff, S., & Rubin, J. (2020). Multiplex Network Ties and the Spatial Diffusion of Radical Innovations: Martin Luther’s Leadership In The Early Reformation. American Sociological Review, 85(5), 857-894. [pdf]\n\n\n\nThu., May. 12\n\nGoldberg, A., & Stein, S. K. (2018). Beyond Social Contagion: Associative Diffusion and The Emergence of Cultural Variation. American Sociological Review, 83(5), 897-932. [pdf]\nBail, C. A., Brown, T. W., & Mann, M. (2017). Channeling hearts and minds: Advocacy organizations, cognitive-emotional currents, and public conversation. American Sociological Review, 82(6), 1188-1213. [pdf]"
  },
  {
    "objectID": "schedule-208B-S22.html#week-8-networks-in-history",
    "href": "schedule-208B-S22.html#week-8-networks-in-history",
    "title": "SOCIOL 208B Reading Schedule (Spring 2022)",
    "section": "Week 8: Networks In History",
    "text": "Week 8: Networks In History\n\nTue., May. 17\n\nPadgett, J. F., & Ansell, C. K. (1993). Robust Action and the Rise of the Medici, 1400-1434. American Journal of Sociology, 98(6), 1259–1319. [pdf]\nGould, R. V. (1991). Multiple Networks and Mobilization In The Paris Commune, 1871. American Sociological Review, 56(6), 716-729. [pdf]\nErikson, E., & Feltham, E. (2021). Historical Network Research. In The Oxford Handbook of Social Networks. Oxford University Press. [pdf]\n\n\n\nThu., May. 19\n\nErikson, E., & Bearman, P. (2006). Malfeasance and the Foundations For Global Trade: The Structure of English Trade In The East Indies, 1601–1833. American Journal of Sociology, 112(1), 195-230. [pdf]\nBearman, P., Faris, R., & Moody, J. (1999). Blocking The Future: New Solutions For Old Problems In Historical Social Science. Social Science History, 23(4), 501-533. [pdf]"
  },
  {
    "objectID": "schedule-208B-S22.html#week-9-networks-and-inequality",
    "href": "schedule-208B-S22.html#week-9-networks-and-inequality",
    "title": "SOCIOL 208B Reading Schedule (Spring 2022)",
    "section": "Week 9: Networks and Inequality",
    "text": "Week 9: Networks and Inequality\n\nTue., May. 24\n\nGould, R. V. (2002). The Origins of Status Hierarchies: A Formal Theory and Empirical Test. American Journal of Sociology, 107(5), 1143-1178. [pdf]\nGondal, N. (2015). Inequality preservation through uneven diffusion of Cultural materials across stratified groups. Social Forces, 93(3), 1109-1137. [pdf]\nThomas, R. J., & Mark, N. P. (2013). Population size, network density, and the emergence of inherited inequality. Social Forces, 92(2), 521-544. [pdf]\n\n\n\nThu., May. 26\n\nBurris, V. (2004). The Academic Caste System: Prestige Hierarchies In PhD Exchange Networks. American Sociological Review, 69, 239-264. [pdf]\nFowler, J. H., Grofman, B., & Masuoka, N. (2007). Social networks in political science: Hiring and placement of Ph.Ds, 1960–2002. PS: Political Science & Politics, 40(4), 729-739. [pdf]\nClauset, A., Arbesman, S., & Larremore, D. B. (2015). Systematic inequality and hierarchy in faculty hiring networks. Science Advances, 1(1), e1400005. [pdf]\nGondal, N. (2018). Duality of departmental specializations and PhD exchange: A Weberian analysis of status in interaction using multilevel exponential random graph models (mERGM). Social Networks, 55, 202-212. [pdf]"
  },
  {
    "objectID": "schedule-208B-S22.html#week-10-networks-and-the-micromacro-link",
    "href": "schedule-208B-S22.html#week-10-networks-and-the-micromacro-link",
    "title": "SOCIOL 208B Reading Schedule (Spring 2022)",
    "section": "Week 10: Networks and The Micro/Macro Link",
    "text": "Week 10: Networks and The Micro/Macro Link\n\nTue., May. 31\n\nMcfarland, D. A., Moody, J., Diehl, D., Smith, J. A., & Thomas, R. J. (2014). Network Ecology and Adolescent Social Structure. American Sociological Review, 79(6), 1088-1121. [pdf]\nBearman, P. S., Moody, J., & Stovel, K. (2004). Chains of Affection: The Structure of Adolescent Romantic and Sexual Networks. American Journal of Sociology, 110(1), 44-91. [pdf]\n\n\n\nThu., Jun. 2\n\nLevy, B. L., Phillips, N. E., & Sampson, R. J. (2020). Triple Disadvantage: Neighborhood Networks of Everyday Urban Mobility and Violence In US Cities. American Sociological Review, 85(6), 925-956. [pdf]\nPapachristos, A. V. (2009). Murder By Structure: Dominance Relations and the Social Structure of Gang Homicide. American Journal of Sociology, 115(1), 74-128. [pdf]"
  },
  {
    "objectID": "schedule-208B-S24.html",
    "href": "schedule-208B-S24.html",
    "title": "SOCIOL 208B Reading Schedule (Spring 2024)",
    "section": "",
    "text": "Wellman, B. (1988). Structural Analysis: From Method and Metaphor To Theory and Substance. In B. Wellman & S. D. Berkowitz (Eds.), Social Structures: A Network Approach. (Pp. 19–61). Cambridge University Press.\nFreeman, L. (2004). The Development of Social Network Analysis. Vancouver, BC: Empirical Press (Introduction, and Chap. 9)\n\n\n\n\n\nEmirbayer, M. (1997). Manifesto For A Relational Sociology. American Journal of Sociology, 103(2), 281–317.\nErikson, E. (2013). Formalist and Relationalist Theory In Social Network Analysis. Sociological Theory, 31(3), 219–242.\nMische, A. (2011). Relational Sociology, Culture, and Agency. In J. Scott & P. J. Carrington (Eds.), The Sage Handbook of Social Network Analysis (Pp. 80–97). Sage."
  },
  {
    "objectID": "schedule-208B-S24.html#week-2-the-greatest-hits",
    "href": "schedule-208B-S24.html#week-2-the-greatest-hits",
    "title": "SOCIOL 208B Reading Schedule (Spring 2024)",
    "section": "Week 2: The Greatest Hits",
    "text": "Week 2: The Greatest Hits\n\nTuesday, April 9\n\nWhite, H. C., Boorman, S. A., & Breiger, R. L. (1976). Social Structure From Multiple Networks. I. Blockmodels of Roles and Positions. American Journal of Sociology, 81(4), 730–780.\nBreiger, R. L. (1974). The Duality of Persons and Groups. Social Forces, 53(2), 181–190.\nFreeman, L. C. (1978). Centrality in social networks conceptual clarification. Social networks, 1(3), 215-239.\n\n\n\nThursday, April 11\n\nGranovetter, M. S. (1973). The Strength of Weak Ties. American Journal of Sociology, 78(3), 1360–1380.\n\nRejection letter from American Sociological Review of the first (1969) version of the paper\nGranovetter, M. S. (1969) “Alienation Reconsidered: The Strength of Weak Ties.” Reprinted in Connections 5(2): 4-16.\n\nWang, D., & Uzzi, B. (2022). Weak ties, failed tries, and success. Science, 377(6612), 1256-1258.\nMcPherson, M., Smith-Lovin, L., & Cook, J. M. (2001). Birds of a feather: Homophily in social networks. Annual Review of Sociology, 27(1), 415-444."
  },
  {
    "objectID": "schedule-208B-S24.html#week-3-brokerage",
    "href": "schedule-208B-S24.html#week-3-brokerage",
    "title": "SOCIOL 208B Reading Schedule (Spring 2024)",
    "section": "Week 3: Brokerage",
    "text": "Week 3: Brokerage\n\nTuesday, April 16 (Virtual Meeting)\n\nGould, R. V., & Fernandez, R. M. (1989). Structures of Mediation: A Formal Approach To Brokerage In Transaction Networks. Sociological Methodology, 89-126.\nObstfeld, D., Borgatti, S.P. and Davis, J. (2014). Brokerage As a Process: Decoupling Third Party Action From Social Network Structure. Research In The Sociology of Organizations, 40), 135-159.\nStovel, K., & Shaw, L. (2012). Brokerage. Annual Review of Sociology, 38, 139-158.\n\n\n\nThursday, April 18 (Virtual Meeting)\n\nBurt, R. S. (2004). Structural Holes and Good Ideas. American Journal of Sociology, 110(2), 349-399.\nGoldberg, A., Srivastava, S. B., Manian, V. G., Monroe, W., & Potts, C. (2016). Fitting In Or Standing Out? The Tradeoffs of Structural and Cultural Embeddedness. American Sociological Review, 81(6), 1190-1222.\nAral, S. (2016). The future of weak ties. American Journal of Sociology, 121(6), 1931-1939."
  },
  {
    "objectID": "schedule-208B-S24.html#week-4-science",
    "href": "schedule-208B-S24.html#week-4-science",
    "title": "SOCIOL 208B Reading Schedule (Spring 2024)",
    "section": "Week 4: Science",
    "text": "Week 4: Science\n\nTuesday, April 23\n\nMoody, J. (2004). The Structure of A Social Scientific Collaboration Network. American Sociological Review, 69, 213-238.\nClauset, A., Arbesman, S., & Larremore, D. B. (2015). Systematic inequality and hierarchy in faculty hiring networks. Science Advances, 1(1), e1400005.\n\n\n\nThursday, April 25\n\nGondal, N. (2018). Duality of departmental specializations and PhD exchange: A Weberian analysis of status in interaction using multilevel exponential random graph models (mERGM). Social Networks, 55, 202-212.\nMcmahan, P., & Mcfarland, D. A. (2021). Creative Destruction: The Structural Consequences of Scientific Curation. American Sociological Review, 86(2), 341-376.\nShwed, U., & Bearman, P. S. (2010). The Temporal Structure of Scientific Consensus Formation. American Sociological Review, 75(6), 817-840."
  },
  {
    "objectID": "schedule-208B-S24.html#week-5-collaboration-creativity",
    "href": "schedule-208B-S24.html#week-5-collaboration-creativity",
    "title": "SOCIOL 208B Reading Schedule (Spring 2024)",
    "section": "Week 5: Collaboration & Creativity",
    "text": "Week 5: Collaboration & Creativity\n\nTuesday, April 30\n\nUzzi, B, & Spiro, J. (2005). Collaboration and Creativity: The Small World Problem. American Journal of Sociology, 111, 447-504.\nDe Vaan, M., Stark, D., & Vedres, B. (2015). Game changer: The topology of creativity. American Journal of Sociology, 120(4), 1144-1194.\n\n\n\nThursday, May 2\n\nRossman, G., Esparza, N., & Bonacich, P. (2010). I’d Like To Thank The Academy, Team Spillovers, and Network Centrality. American Sociological Review, 75(1), 31-51.\nChoi, Y., Ingram, P., & Han, S. W. (2023). Cultural breadth and embeddedness: The individual adoption of organizational culture as a determinant of creativity. Administrative Science Quarterly, 68(2), 429-464.\nSilver, D., Childress, C., Lee, M., Slez, A., & Dias, F. (2022). Balancing categorical conventionality in music. American Journal of Sociology, 128(1), 224-286."
  },
  {
    "objectID": "schedule-208B-S24.html#week-6-difussion",
    "href": "schedule-208B-S24.html#week-6-difussion",
    "title": "SOCIOL 208B Reading Schedule (Spring 2024)",
    "section": "Week 6: Difussion",
    "text": "Week 6: Difussion\n\nTuesday, May 7\n\nDellaPosta, D., Shi, Y., & Macy, M. (2015). Why do liberals drink lattes?. American Journal of Sociology, 120(5), 1473-1511.\nDellaPosta, D. (2020). Pluralistic collapse: The “oil spill” model of mass opinion polarization. American Sociological Review, 85(3), 507-536.\nGoldberg, A., & Stein, S. K. (2018). Beyond Social Contagion: Associative Diffusion and The Emergence of Cultural Variation. American Sociological Review, 83(5), 897-932.\n\n\n\nThursday, May 9\n\nCentola, D., & Macy, M. (2007). Complex Contagions and the Weakness of Long Ties. American Journal of Sociology, 113(3), 702-734.\nGondal, N. (2023). Diffusion of innovations through social networks: Determinants and implications. Sociology Compass, 17(5), e13084.\nBail, C. A., Brown, T. W., & Wimmer, A. (2019). Prestige, proximity, and prejudice: how Google search terms diffuse across the world. American Journal of Sociology, 124(5), 1496-1548."
  },
  {
    "objectID": "schedule-208B-S24.html#week-7-culture",
    "href": "schedule-208B-S24.html#week-7-culture",
    "title": "SOCIOL 208B Reading Schedule (Spring 2024)",
    "section": "Week 7: Culture",
    "text": "Week 7: Culture\n\nTuesday, May 14\n\nLizardo, O. (2023). Culture and Networks. Pp. 188-201 in The SAGE Handbook of Social Network Analysis. SAGE Publications Ltd.\nLewis, K., & Kaufman, J. (2018). The Conversion of Cultural Tastes Into Social Network Ties. American Journal of Sociology, 123(6), 1684-1742.\nRawlings, C. M., & Childress, C. (2023). The Polarization of Popular Culture: Tracing the Size, Shape, and Depth of the “Oil Spill”. Social Forces, soad150.\n\n\n\nThursday, May 16 (No Meeting)"
  },
  {
    "objectID": "schedule-208B-S24.html#week-8-history",
    "href": "schedule-208B-S24.html#week-8-history",
    "title": "SOCIOL 208B Reading Schedule (Spring 2024)",
    "section": "Week 8: History",
    "text": "Week 8: History\n\nTuesday, May 21 (Virtual Meeting)\n\nPadgett, J. F., & Ansell, C. K. (1993). Robust Action and the Rise of the Medici, 1400-1434. American Journal of Sociology, 98(6), 1259–1319.\nGould, R. V. (1991). Multiple Networks and Mobilization In The Paris Commune, 1871. American Sociological Review, 56(6), 716-729.\nBearman, P., Moody, J., & Faris, R. (2002). Networks and History. Complexity, 8(1), 61-71.\n\n\n\nThursday, May 23\n\nErikson, E., & Feltham E., (2021) Historical Network Research, Pp. 432–442 in R Light, and J Moody (eds), The Oxford Handbook of Social Networks. Oxford University Press.\nErikson, E., & Bearman, P. (2006). Malfeasance and the Foundations For Global Trade: The Structure of English Trade In The East Indies, 1601–1833. American Journal of Sociology, 112(1), 195-230.\nBecker, S. O., Hsiao, Y., Pfaff, S., & Rubin, J. (2020). Multiplex Network Ties and the Spatial Diffusion of Radical Innovations: Martin Luther’s Leadership In The Early Reformation. American Sociological Review, 85(5), 857-894."
  },
  {
    "objectID": "schedule-208B-S24.html#week-9-inequality",
    "href": "schedule-208B-S24.html#week-9-inequality",
    "title": "SOCIOL 208B Reading Schedule (Spring 2024)",
    "section": "Week 9: Inequality",
    "text": "Week 9: Inequality\n\nTuesday, May 28\n\nDiMaggio, P., & Garip, F. (2012). Network effects and social inequality. Annual Review of Sociology, 38, 93-118.\nPedulla, D. S., & Pager, D. (2019). Race and networks in the job search process. American Sociological Review, 84(6), 983-1012.\n\n\n\nThursday, May 30\n\nErikson, E., & Occhiuto, N. (2017). Social networks and macrosocial change. Annual Review of Sociology, 43, 229-248.\nZhang, J., & Centola, D. (2019). Social networks and health: New developments in diffusion, online and offline. Annual Review of Sociology, 45, 91-109.\nHofstra, B., Corten, R., Van Tubergen, F., & Ellison, N. B. (2017). Sources of segregation in social networks: A novel approach using Facebook. American Sociological Review, 82(3), 625-656."
  },
  {
    "objectID": "schedule-208B-S24.html#week-10-linking-micro-and-macro",
    "href": "schedule-208B-S24.html#week-10-linking-micro-and-macro",
    "title": "SOCIOL 208B Reading Schedule (Spring 2024)",
    "section": "Week 10: Linking Micro and Macro",
    "text": "Week 10: Linking Micro and Macro\n\nTuesday, June 4\n\nMcfarland, D. A., Moody, J., Diehl, D., Smith, J. A., & Thomas, R. J. (2014). Network Ecology and Adolescent Social Structure. American Sociological Review, 79(6), 1088-1121.\nBearman, P. S., Moody, J., & Stovel, K. (2004). Chains of Affection: The Structure of Adolescent Romantic and Sexual Networks. American Journal of Sociology, 110(1), 44-91.\n\n\n\nThursday, June 6\n\nLevy, B. L., Phillips, N. E., & Sampson, R. J. (2020). Triple Disadvantage: Neighborhood Networks of Everyday Urban Mobility and Violence In US Cities. American Sociological Review, 85(6), 925-956.\nPapachristos, A. V., & Bastomski, S. (2018). Connected in crime: the enduring effect of neighborhood networks on the spatial patterning of violence. American Journal of Sociology, 124(2), 517-568."
  },
  {
    "objectID": "syllabus-208A.html",
    "href": "syllabus-208A.html",
    "title": "208A Syllabus",
    "section": "",
    "text": "THIS IS A LIVE DOCUMENT CHECK REGULARLY FOR CHANGES"
  },
  {
    "objectID": "syllabus-208A.html#class-description",
    "href": "syllabus-208A.html#class-description",
    "title": "208A Syllabus",
    "section": "Class Description",
    "text": "Class Description\nThis class is an introductory graduate-level seminar focused on techniques in Social Network Analysis (SNA). The seminar covers the most common data analytic tasks that people engage in when analyzing “network data.” What is network data? What counts as network data is itself a point of contention—as we will see, for some people all data is network data—but let us say for the sake of this class that network data is data in which the unit of analysis is the relation or the interaction between at least two actors or objects, and the data come typically arranged in this “dyadic” form. At the end of the course, you will be familiar with (and will have acquired some practice) the basic techniques used to analyze social network data."
  },
  {
    "objectID": "syllabus-208A.html#course-content",
    "href": "syllabus-208A.html#course-content",
    "title": "208A Syllabus",
    "section": "Course Content",
    "text": "Course Content\n\nBasic SNA\nSo, what are the things that people usually do when they have network data? Well, they typically want to figure out basic statistics about the interaction system formed by the set of dyads in the data, where a dyad is any two pairs of actors (whether they are connected or not). This task requires computing basic network quantities like the number of nodes and the number of links between entities as well as more advanced statistics based on representing the network as a graph (like the average path length, number of components, etc., all notions we will cover in the first week of class).\n\n\nCentrality and Prestige\nThen come the various things that almost everyone is interested in computing when using network data to answer social science questions. Primarily, this includes measures and indices of a node’s position in the network (e.g., differentiating between more or less central or more or less prestigious nodes), which we will cover in weeks 2 and 3.\n\n\nClasses and Communities\nAfter taking a break in Week 4, we move to the common case of people wanting to see if the nodes in the network fall into definable clusters or classes, where the criterion for being in the same cluster is based on how they connect to other nodes. Here, we want to find clusters of nodes that are similar to one another by some graph theoretic criterion and partition the graph into clusters based on that criterion.\nWeek 6 is dedicated to the next thing we may want to do, and that is to see if we can uncover clumps of densely connected nodes in the network indicating some natural partition into subgroups or communities, defined as nodes that interact more among themselves than they do among those outside the group, leading to the myriad of group and community detection techniques designed to partition a graph into clusters based on the underlying connectivity structure.\n\n\nTwo-Mode and Ego Networks\nThe next two weeks are dedicated to the analysis of some pretty common “non-standard” types of network data (e.g., data that doesn’t use the dyadic relation between objects of the same type as the analytic unit). The first is ego networks, where we first sample a set of units (egos), and then within each ego, we sample a subset of their contacts (e.g., by asking the people who are their most important friends or figuring out the most frequent interaction partners). These data come closest to the traditional data in social science (a rectangular matrix of cases by variables), so various standard techniques—like regression—apply (with some twists).\nThe second type of non-standard network data comes in a two-mode network form, in which some sets of objects are linked to objects of a different set, but there is no data on the links between objects of the same set. Standard cases by variables data in surveys are two-mode data (people connect to variables), as is any web or archival data collecting memberships or interactions between persons and objects (like attendance at events or people buying books on Amazon). We will see that due to a neat mathematical trick, we can transform two-mode into standard dyadic network data and thus deploy the whole panoply of techniques we learned in weeks 1-6 (which means that we can do SNA on all types of data, not just network data, and therefore all data is network data).\n\n\nProbabilistic Models of Networks\nThe bulk of SNA assumes that the ties exist as recorded in the data. Recently (e.g., over the last two decades or so) developed approaches to social network analysis make the ties the dependent variable and thus see the observed network data as a realization of some stochastic process governing the probability that two objects will be linked and thus one that can be modeled statistically. We analyze the theory and methods behind this approach to thinking of network structure from the bottom up and also cover some models designed to treat networks as composed of “relational events” and thus model how events that link entities in networks evolve."
  },
  {
    "objectID": "syllabus-208A.html#requirements",
    "href": "syllabus-208A.html#requirements",
    "title": "208A Syllabus",
    "section": "Requirements",
    "text": "Requirements\nThere are three main requirements in the class. Participation (mainly attendance and contributions made during our seminar meetings), a short weekly data exercise, and a longer data analysis paper due at the end of the quarter.\n\nClass Attendance and Class Discussion (25% of grade)\nAttendance is required not optional. If you need to miss a class meeting please let me know beforehand. It is part of your professional socialization to commit to attending class meetings and to inform me when that’s not possible (if only as a point of courtesy). The informal part of participation will be gauged by your contribution to our class discussion in the form of questions, comments, suggestions, wonderings, problems.\n\n\nWeekly Data Analysis Exercises (25% of grade)\nThese will be short weekly assignments in which I will ask you to take a (small) social network data set of your choice and compute some of the basic statistics or implement some of the techniques that we covered the week before. They will be due on Sunday at the end of each week. What you submit will take the form of a file containing the code and results from your analysis (typically an R Markdown file). These will not be graded, but will just be counted as submitted or not submitted.\n\n\nFinal Data Analysis Paper (50% of grade)\nThe basic goal here is for you to end up with something that could be useful to you at the end of the day. Hopefully a basic data exercise that can be the basis of a longer substantive paper or as a standalone research note.\nThis will be a 2500 to 5000 word (single-spaced, Times New Roman Font, 12pt, 1in margins) write-up of a data analysis, including some type of network data and/or some kind of network analytic technique (to be discussed and cleared by me). The data source can be obtained from a public repository of network data, or it could be network data that you already have access to or collected yourself.\nIn the paper, you will describe the data, provide key network metrics, describe the data-analytic approach that you will use, and provide a summary of the key empirical patterns that you found, along with a brief conclusion. The paper should be written in the style of a “research note” focused on key empirical findings (not long theory windup or literature review).\nYou will submit an extended abstract of your final project, outlining your main research idea (e.g., data source and type of analysis) due on Sunday of week 6. This will be a one-page, single-spaced document with 12pt Times New Roman Font and 1in margins."
  },
  {
    "objectID": "syllabus-208B.html",
    "href": "syllabus-208B.html",
    "title": "208B Syllabus",
    "section": "",
    "text": "THIS IS A LIVE DOCUMENT CHECK REGULARLY FOR CHANGES"
  },
  {
    "objectID": "syllabus-208B.html#participation-50-of-grade",
    "href": "syllabus-208B.html#participation-50-of-grade",
    "title": "208B Syllabus",
    "section": "Participation (50% of grade)",
    "text": "Participation (50% of grade)\n\nWeekly Analytic Memo\nThe formal side of participation will come in the form of you submitting a short memo (500 to 1000 words) where you try to put two or more of the readings for that week in conversation with one another. The first memo will be due starting on week 2.\nMemo specifications:\n\nYou should pick at least one reading from the Tuesday meeting and at least one reading from the Thursday meeting as the focus of your memo.\nIn your memo you should feel free to raise questions or issues the readings brought up for you as well as any questions, problems, or weaknesses you identify in the argument or the analyses.\nYou may also feel free to connect the readings to other work you are familiar with, pointing to key points of commonality and difference. The main point of the memo is for me to see evidence of you thinking thought the material, as well as providing fodder for discussion during our meeting.\nThe memo will be due by 5p the day before our first class meeting of the week (that’s Monday) so that I have a chance to read it and comment on it. You will submit it via the assignments tool on Canvas.\n\n\n\nClass Attendance\nAttendance is required not optional. If you need to miss a class meeting please let me know before hand. It is part of your professional socialization to commit to attending class meetings and to inform me when that’s not possible (if only as a point of courtesy).\n\n\nClass Discussion\nThe informal part of participation will be gauged by your contribution to our class discussion. You can use the thoughts developed in your analytic memo as a take-off point for framing your contribution."
  },
  {
    "objectID": "syllabus-208B.html#paper-50-of-grade",
    "href": "syllabus-208B.html#paper-50-of-grade",
    "title": "208B Syllabus",
    "section": "Paper (50% of grade)",
    "text": "Paper (50% of grade)\nThe basic goal here is for you to end up with something that could be useful to you at the end of the day. As such, I’ll give you a set of options here, but if you none of these work, we can talk about something that can be customized for your needs and goals. So by the end of the course you will submit one of the following:\n\nDraft of a research paper.- This will be a 3500 to 9000 word (double-spaced, Times New Roman Font, 12pt, 1in margins) draft of a research paper. This paper will contain some kind of data analysis, involving networks broadly defined. It will include an introduction reviewing literature and setting up a research problem or question. It will then move on to a methods section describing your data and analytic approach, and will close with a discussion section summarizing key findings, outlining implications for substantive research and theory, and describing potential future work and extensions. The goal here is to come up with a draft of a paper that could one day be submitted to the various social science and sociology-oriented journal focused on substantive research, whether “generalist” (e.g., American Sociological Review) or “specialist” (e.g., Social Networks).\nDraft of a Conceptual Paper.- This will be a 5000 to 10000 word page (double-spaced, Times New Roman Font, 12pt, 1in margins) draft of a research paper. The paper will focus on a set of concepts, theoretical ideas, or overall perspectives for approaching the study of social life that are based, inspired, extend, or incorporate network ideas, network thinking, or network concepts and techniques (broadly defined). The goal here is to come up with a draft of a paper that could one day be submitted to the various social science and sociology-oriented journal focused on “theory.”\nExtended Literature Review Draft.- This will be a 10-15 page (double-spaced, Times New Roman Font, 12pt, 1in Margins) draft of a literature review of work done from a social network perspective on a topic of your interest. The paper will cover what has been done in the field so far, what the strengths and limitations of previous is, and will note gaps or opportunities for future work addressing those limitations or extending the literature to new substantive domains, perhaps linking previous work to the some of the stuff we will be reading in class.\nDraft of a Research Proposal.- This will be a 10 page (single-spaced, Times New Roman Font, 12pt, 1in margins, including references) draft of a research proposal for a project incorporating either network thinking, theories, or techniques that you plan to start in the near future. The proposal will include a background section reviewing previous work, noting their strengths and limitations, and pointing to gaps in the literature. It will also include an “approach” section describing your research project, your main research questions, the data gathering procedures you will use, and the data-analytic techniques you plan to implement once your data is collected. It will close with an implication sections describing what the contributions of your project will be and why it is relevant and important.\nData Analysis Exercise.- This will be a 2500 to 5000 word (single-spaced, Times New Roman Font, 12pt, 1in margins) write-up of a data analysis, including some type of network data and/or some kind of network analytic technique (to be discussed and cleared by me). The data source can be obtained from a public repository of network data, or it could be network data that you already have access to or collected yourself. In the paper, you will describe the data, provide key network metrics, describe the data-analytic approach that you will use, and provide a summary of the key empirical patterns that you found, along with a brief conclusion. The paper should be written in the style of a “research note” focused on key empirical findings (not long theory windup or literature review).\n\nWhatever you decide, you will submit an extended abstract of your final project, due on Friday of week 6. This will be a one-page, single-spaced document with 12pt Times New Roman Font and 1in margins."
  }
]