{
  "hash": "c4cc7e09201682b3410a944e5bf46be5",
  "result": {
    "markdown": "---\ntitle: \"Status and Prestige\"\nexecute: \n  eval: true\n  echo: true\n  output: true\n  warning: false\n  message: false\nformat: \n   html:\n      code-line-numbers: true\n---\n\n\nIn the [the centrality lecture notes](centrality.qmd), we saw how to compute the most popular centrality measures. Freeman's \"big three\" have strong *graph-theoretic* foundation and do a good job of formalizing and quantifying the idea that a node is central if it is \"well-placed\" in the network, where being well-placed resolves into either being able to *reach* others (directly as with degree or indirectly as with closeness) or being able to *intermediate* between others (as with betweenness).\n\n## Networks as Prisms\n\nThere is, however, another strong and well-motivated intuition as to what it means to be \"well-placed\" in a network. Here the ties in the network are seen less as \"pipes\" that transmit stuff and more like \"prisms\" that *reflect* on you [@podolny01]. \n\nOne way to think about this second version of well-placedness is that what is transmitted through the network is the *network itself*, or more accurately, the *importance*, *status*, and *prestige* of the people you are connected to, preferably flowing *from* them (high status people) to *you*.\n\nUnder this interpretation, actors get status and prestige in the network from being connected to prestigious and high status others. Those others, in turn, get their status from being connected to high status others, and so on *ad infinitum*. \n\nOne way of quantifying this idea goes like this. If $\\mathbf{x}$ is a vector containing the desired status scores, then the status of actor $i$ should be equal to:\n\n$$\n   x_i = \\sum_{j} a_{ij}x_j\n$$ {#eq-status-sum}\n\nWhere $a_{ij} = 1$ if $i$ is adjacent to $j$ in the network. Note that this formula just sums up the status scores of all the others each actor is connected to.\n\nIn matrix notation, if $\\mathbf{x}$ is a column vector of status scores then:\n\n$$\n   \\mathbf{x} = \\mathbf{A} \\mathbf{x}\n$$\n\nBecause $\\mathbf{A}$ is an $n \\times n$ matrix and $\\mathbf{x}$ is $n \\times 1$ column vector, the (matrix) multiplication of $\\mathbf{A} times the vector \\mathbf{x}$ will return another column vector of dimensions $n \\times 1$, in this case $\\mathbf{x}$ itself!\n\nNote the problem that this formulation poses: $\\mathbf{x}$ appears on both sides of the equation, which means that in order to know the status of any one node we would need to know the status of the others, but calculating the status of the others depends on knowing the status of the focal node, and so on. There's a chicken and the egg problem here.\n\nNow, there is an obvious (to the math majors) *mathematical solution* to this problem, because there's a class of solvable (under some mild conditions imposed on the matrix $\\mathbf{A}$) linear algebra problems that take the form:\n\n$$\n   \\lambda\\mathbf{x} =  \\mathbf{A} \\mathbf{x}\n$$\n\nWhere $\\lambda$ is just a plain old number (a scalar). Once again conditional of the aforementioned mild conditions being met, we can iteratively search for a value $\\lambda$, fix it, then fill up the $\\mathbf{x}$ vector with another set of values, fix those, search for a new $\\lambda$, and continue until we have values of $\\lambda$ and $\\mathbf{x}$ that make the above equality true. \n\nWhen we do that successfully, we say that the value of $\\lambda$ we hit upon is an **eigenvalue** of the matrix $\\mathbf{A}$ and the values of the vector $\\mathbf{x}$ we came up with are an **eigenvector** of the same matrix (technically in the above equation a right eigenvector). \n\nEigenvalues and eigenvectors, like Don Quixote and Sancho Panza, come in pairs, because you need a unique combination of both to solve the equation. Typically, a given matrix (like an adjacency matrix) will have multiple $\\lambda/\\mathbf{x}$ pairs that will solve the equation. Together the whole set $\\lambda/\\mathbf{x}$ pairs that make the equation true are the **eigenvalues** and **eigenvectors** of the matrix. \n\n## Eigenvalues, Eigenvectors, Oh My!\n\nNote that all of this obscure talk about eigenvalues and eigenvectors is just matrix linear algebra stuff. It has nothing to do with networks and social structure.\n\nIn contrast, because the big three centrality measures have a direct foundation in graph theory, and graph theory is an isomorphic **model** of social structures (points map to actors/people and lines map to relations) the \"math\" we do with graph theory is **directly** meaningful as a model of networks (the counts of the number of edges incident to a node is the count of other actors they someone is directly connected to). \n\nEigenvalues and eigenvectors are not a model of social structure in the way graph theory is (their first scientific application was in Chemistry and Physics). They are just a mechanical math fix to a circular equation problem. \n\nThis is why it's a mistake to introduce network measures of status and prestige by jumping directly to the machinery of linear algebra (or worse talk about the idea of **eigenvector centrality** which means nothing to most people, and combines two obscure terms into one even more obscure compound term). \n\nA better approach is to see if we can *motivate* the use of measures like the ones above using the simple model of the distribution of status and prestige we started with earlier. We will see that we can, and that doing that leads us back to solutions that are the mathematical equivalent of all the eigenvector stuff. \n\n## Distributing Status to Others\n\nLet's start with the simplest model of how people can get their status from the status of others in a network. It is the simplest because it is based on degree. \n\nImagine everyone has the same \"quantum\" of status to begin with (this can be stored in a vector containing the same number of length equals to number of actors in the network). Then, at each step, people \"send\" the same amount of status to all their alters in the network. Then we repeat, with everyone sending the amount of status they *now* have after receiving it from others. We repeat this many times. At the end of each step, we compute people's new status scores using @eq-status-sum. We stop doing this after the status scores of people stop changing across each iteration.\n\nLet us see a real-life example at work. \n\nWe will use a data set collected by David Krackhardt on the friendships of 21 managers in a high tech company in the West coast (see the description [here](https://rdrr.io/github/schochastics/networkdata/man/ht_friends.html)). The data are reported as directed ties ($i$ nominates $j$ as a friend) but we will constrain ties to be undirected:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   library(networkdata)\n   library(igraph)\n   g <- as_undirected(ht_friends, mode = \"collapse\")\n```\n:::\n\n\nThe undirected friendship network is shown in @fig-krack.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Krackhardt's managers network](prestige_files/figure-html/fig-krack-1.png){#fig-krack width=768}\n:::\n:::\n\n\nWe then extract the adjacency matrix corresponding to this network:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   A <- as.matrix(as_adjacency_matrix(g))\n```\n:::\n\n\nAnd here's a simple custom function using a `while` loop that exemplifies the process of status distribution through the network we just talked about:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   status1 <- function(w) {\n      x <- rep(1, nrow(w)) #initial status vector set to all ones of length equal to the number of nodes\n      d <- 1 #initial delta\n      k <- 0 #initializing counter\n      while (d > 1e-10) {\n          o.x <- x #old status scores\n          x <- w %*% o.x #new scores a function of old scores and adjacency matrix\n          x <- x/norm(x, type = \"E\") #normalizing new status scoress\n          d <- abs(sum(abs(x) - abs(o.x))) #delta between new and old scores\n          k <- k + 1 #incrementing while counter\n      }\n   return(as.vector(x))\n   }\n```\n:::\n\n\nLines 2-4 initialize various quantities, most importantly the initial status vector for each node to just a series of ones:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   rep(1, nrow(A))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n```\n:::\n:::\n\n\nThen lines 5-12 implement a little looping algorithm of how status is distributed through the network, with the most important piece of code being line 7 where the *current* status scores for each node are just the sum of the status scores of its neighbors computed one iteration earlier. The program stops when the difference between the old and the new scores is negligible ($\\delta < 10^{-10}$) as checked in line 9. \n\nNote the normalization step on line 8, where we divide the each status score by a normalized sum of all of the scores. This is required in order to prevent the sum of status scores from getting bigger and bigger indefinitely (in mathese, this is referred to as the sum \"diverging\"). In base `R`, the `type = \"E\"` normalization implements the **Euclidean vector norm** (also sometimes confusingly called the **Frobenieus norm**), by which we divide each value of the status scores by after each update.^[For a vector of numbers $\\mathbf{x}$ the Euclidean vector norm $||\\mathbf{x}||_2$ is given by: $\\sqrt{\\sum x^2}$.]\n\nAnd here's the resulting (row) vector of status scores for each node:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   s <- status1(A)\n   s <- s/max(s) #normalizing by maximum\n   round(s, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 0.619 0.635 0.446 0.489 0.629 0.430 0.205 0.380 0.444 0.468 0.814 0.549\n[13] 0.162 0.401 0.613 0.360 1.000 0.247 0.680 0.360 0.392\n```\n:::\n:::\n\n\nWhat if I told you that this vector is the same as that given by the leading (first) eigenvector of the adjacency matrix?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   s.eig <- abs(eigen(A)$vector[, 1]) #computing the first eigenvector\n   s.eig <- s.eig/max(s.eig) #normalizing by maximum\n   round(s.eig, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 0.619 0.635 0.446 0.489 0.629 0.430 0.205 0.380 0.444 0.468 0.814 0.549\n[13] 0.162 0.401 0.613 0.360 1.000 0.247 0.680 0.360 0.392\n```\n:::\n:::\n\n\nWhich is of course what is computed by the `eigen_centrality` function in `igraph`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   round(eigen_centrality(g)$vector, 3) #igraph automatically normalizes the scores\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 0.619 0.635 0.446 0.489 0.629 0.430 0.205 0.380 0.444 0.468 0.814 0.549\n[13] 0.162 0.401 0.613 0.360 1.000 0.247 0.680 0.360 0.392\n```\n:::\n:::\n\n\nSo, the \"eigenvector centralities\" [@bonacich72] are just the limit scores produced by the status distribution process implemented in the `status1` function!\n\nWhen treated as a structural index of connectivity in a graph (i.e., a [centrality](centrality.qmd) measure) the eigenvector status scores induce an ordering of the nodes which we may be interested in looking at:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   nodes <- 1:vcount(g)\n   eig.dat <- data.frame(Nodes = nodes, Eigen.Cent = s, Deg.Cent = degree(g))\n   eig.dat <- eig.dat[order(eig.dat$Eigen.Cent, decreasing = TRUE), ]\n   library(kableExtra)\n   kbl(eig.dat[1:10, ], \n       format = \"html\", align = \"c\", row.names = FALSE,\n       caption = \"Top Ten Eigenvector Scores.\",\n       digits = 3) %>%    \n   kable_styling(bootstrap_options = \n                    c(\"hover\", \"condensed\", \"responsive\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-hover table-condensed table-responsive\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Top Ten Eigenvector Scores.</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:center;\"> Nodes </th>\n   <th style=\"text-align:center;\"> Eigen.Cent </th>\n   <th style=\"text-align:center;\"> Deg.Cent </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:center;\"> 17 </td>\n   <td style=\"text-align:center;\"> 1.000 </td>\n   <td style=\"text-align:center;\"> 18 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 11 </td>\n   <td style=\"text-align:center;\"> 0.814 </td>\n   <td style=\"text-align:center;\"> 14 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 19 </td>\n   <td style=\"text-align:center;\"> 0.680 </td>\n   <td style=\"text-align:center;\"> 10 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 2 </td>\n   <td style=\"text-align:center;\"> 0.635 </td>\n   <td style=\"text-align:center;\"> 10 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 5 </td>\n   <td style=\"text-align:center;\"> 0.629 </td>\n   <td style=\"text-align:center;\"> 10 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 1 </td>\n   <td style=\"text-align:center;\"> 0.619 </td>\n   <td style=\"text-align:center;\"> 9 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 15 </td>\n   <td style=\"text-align:center;\"> 0.613 </td>\n   <td style=\"text-align:center;\"> 9 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 12 </td>\n   <td style=\"text-align:center;\"> 0.549 </td>\n   <td style=\"text-align:center;\"> 8 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 4 </td>\n   <td style=\"text-align:center;\"> 0.489 </td>\n   <td style=\"text-align:center;\"> 7 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 10 </td>\n   <td style=\"text-align:center;\"> 0.468 </td>\n   <td style=\"text-align:center;\"> 8 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nAs we can see, for instance, there is a strong correlation between degree and the eigenvector score. But they are not the same. For instance, nodes 19, 2, 5 are tied according to degree, but 19 is highest in the eigenvector scoring, indicating that even though they all have the same number of friends, 19 is connected to better-connected others. \n\nAs we will see, most other measures of prestige/status rank for nodes in networks are constructed using similar principles as the ones just described. What changes is the *model assumptions* of how status is distributed in the system. That's why scary and non-intuitive stuff about eigenvectors or whatever is misleading.\n\nOther measures of node status and prestige are designed such that they either change the quantum of status that is distributed through the network by making it dependent on some node characteristic (like degree) or differentiate between different routes of distribution in **directed graphs**, by for instance, differentiating status derived from outgoing links from that derived from incoming links. \n\nLet's see some examples of these alternative cases.\n\n## Bonacich Prestige in Directed Graphs\n\nIn a classic paper, Philip @bonacich72 noted the above connection between different ways people conceptualized status and prestige in networks and the leading eigenvector of the adjacency matrix. He then noted that we can extend similar ideas to the directed case.\n\nHere, people get status from *receiving* nominations from high status others (i.e., those who receive a lot of nominations), whose partners also get status from receiving a lot of nominations from high status others, and so forth.\n\nThis means that in a directed system of relations, status distribution operates primarily via the **indegree** of each node, so that if $\\mathbf{A}$ is the asymmetric adjacency matrix corresponding to the directed graph, then if we play our status game on the *transpose* of this matrix $\\mathbf{A}^T$ we will get the scores we seek [@fouss_etal16, p. 204]. \n\nRecall that in transposing the matrix of a directed graph, we change it from being a *from/to* matrix (nodes in the rows send ties to nodes in the columns) to a *to/from* matrix: Nodes in the rows *receive* ties from nodes in the columns. So we want to play our status game in this matrix, because we want to rank nodes according to their receipt of ties from high-status others. \n\nLet's see a real-life example, this time using the *directed* version of the Krackhardt friendship nomination network among the high-tech managers:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   g.d <- ht_friends\n   A.d <- as.matrix(as_adjacency_matrix(g.d))\n   s <- status1(t(A.d))\n   s <- s/max(s)\n   round(s, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 0.922 1.000 0.379 0.639 0.396 0.190 0.240 0.531 0.411 0.117 0.413 0.769\n[13] 0.082 0.428 0.368 0.450 0.592 0.440 0.425 0.225 0.583\n```\n:::\n:::\n\n\nWhich are the same scores we would have gotten using the `eigen_centrality` function in `igraph` with the argument `directed` set to `TRUE`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   round(eigen_centrality(g.d, directed = TRUE)$vector, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 0.922 1.000 0.379 0.639 0.396 0.190 0.240 0.531 0.411 0.117 0.413 0.769\n[13] 0.082 0.428 0.368 0.450 0.592 0.440 0.425 0.225 0.583\n```\n:::\n:::\n\n\nAnd, like before, we can treat these scores as centrality measures and rank the nodes in the graph according to them. \n\nHere are the top ten nodes:\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-hover table-condensed table-responsive\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Top Ten Eigenvector Scores for a Directed Graph.</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:center;\"> Nodes </th>\n   <th style=\"text-align:center;\"> Eigen.Cent </th>\n   <th style=\"text-align:center;\"> In.Deg.Cent </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:center;\"> 2 </td>\n   <td style=\"text-align:center;\"> 1.000 </td>\n   <td style=\"text-align:center;\"> 10 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 1 </td>\n   <td style=\"text-align:center;\"> 0.922 </td>\n   <td style=\"text-align:center;\"> 8 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 12 </td>\n   <td style=\"text-align:center;\"> 0.769 </td>\n   <td style=\"text-align:center;\"> 8 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 4 </td>\n   <td style=\"text-align:center;\"> 0.639 </td>\n   <td style=\"text-align:center;\"> 5 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 17 </td>\n   <td style=\"text-align:center;\"> 0.592 </td>\n   <td style=\"text-align:center;\"> 6 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 21 </td>\n   <td style=\"text-align:center;\"> 0.583 </td>\n   <td style=\"text-align:center;\"> 5 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 8 </td>\n   <td style=\"text-align:center;\"> 0.531 </td>\n   <td style=\"text-align:center;\"> 5 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 16 </td>\n   <td style=\"text-align:center;\"> 0.450 </td>\n   <td style=\"text-align:center;\"> 4 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 18 </td>\n   <td style=\"text-align:center;\"> 0.440 </td>\n   <td style=\"text-align:center;\"> 4 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 14 </td>\n   <td style=\"text-align:center;\"> 0.428 </td>\n   <td style=\"text-align:center;\"> 5 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nWhile the top indegree centrality node (2) also gets the top Eigenvector Centrality scores, we see many cases of nodes with equal indegree centrality that get substantively different Eigenvector scores. So *who* you are connected matters in addition to *how many* incoming connections you have. \n\n## Combining Prestige and Similarity\n\nIn a neat paper, @alvarez-socorro_etal15 combine ideas of [node similarity analysis](similarity.qmd) with prestige ranking to derive an interesting twist on the usual Eigenvector-based prestige score. \n\nTheir idea is simple and intuitive, linking the \"getting status from others\" idea we just talked about, but *weighting* those others by their *similarity* to ego. Their point is that linking to others who are not similar to you should give you more status than linking to others that are similar, which is the principle behind such fundamental network theories as Strength of Weak Ties [@granovetter73] and Structural Holes [@burt92].\n\nHow do we do it? First, we just need to compute one of the [many similarity metrics](similarity.qmd), to come up with a **dissimilarity matrix** between nodes. We choose the Jaccard similarity, which can be computed using the following function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   jaccard.sim <- function(x) {\n      A <- as.matrix(as_adjacency_matrix(x))\n      A.p <- A %*% A \n      A.q <- A %*% (1 - A) \n      A.r <- (1 - A) %*% A \n      return(A.p / (A.p + A.q + A.r))\n      }\n```\n:::\n\n\nThe dissimilarity matrix for the Krackhardt managers friendship network is just:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   D <- 1 - jaccard.sim(g)\n```\n:::\n\n\nThen @alvarez-socorro_etal15 create a new matrix `W` which equals the **element wise** product between the adjacency matrix and the distance matrix:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   W <- A * D\n```\n:::\n\n\nAnd the (dis)similarity weighted status scores are obtained by playing our status game on the `W` matrix:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   s.diss <- status1(W)\n   s.diss <- s.diss/max(s.diss)\n```\n:::\n\n\nAnd here are the top ten nodes by this dissimilarity weighted prestige measure:\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-hover table-condensed table-responsive\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Top Ten Dissimilarity-Weighted Eigenvector Scores.</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:center;\"> Nodes </th>\n   <th style=\"text-align:center;\"> Eigen.Cent </th>\n   <th style=\"text-align:center;\"> Deg.Cent </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:center;\"> 17 </td>\n   <td style=\"text-align:center;\"> 1.000 </td>\n   <td style=\"text-align:center;\"> 18 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 11 </td>\n   <td style=\"text-align:center;\"> 0.821 </td>\n   <td style=\"text-align:center;\"> 14 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 5 </td>\n   <td style=\"text-align:center;\"> 0.667 </td>\n   <td style=\"text-align:center;\"> 10 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 2 </td>\n   <td style=\"text-align:center;\"> 0.656 </td>\n   <td style=\"text-align:center;\"> 10 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 19 </td>\n   <td style=\"text-align:center;\"> 0.637 </td>\n   <td style=\"text-align:center;\"> 10 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 12 </td>\n   <td style=\"text-align:center;\"> 0.619 </td>\n   <td style=\"text-align:center;\"> 8 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 10 </td>\n   <td style=\"text-align:center;\"> 0.608 </td>\n   <td style=\"text-align:center;\"> 8 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 15 </td>\n   <td style=\"text-align:center;\"> 0.579 </td>\n   <td style=\"text-align:center;\"> 9 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 1 </td>\n   <td style=\"text-align:center;\"> 0.563 </td>\n   <td style=\"text-align:center;\"> 9 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 6 </td>\n   <td style=\"text-align:center;\"> 0.514 </td>\n   <td style=\"text-align:center;\"> 7 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nNote that while the top two nodes (17 and 11) does not change, taking dissimilarity into account alters the rankings. Node 19 drops from third to fifth place and node 1 from fifth to ninth, and node 5 goes from sixth to third, indicating that node 5 connects to well-connected others who are themselves not connected to their neighbors.\n\nOf course, it is possible that in some settings, it might make sense to say that people receive more status points by being connected to others who are themselves connected to the people they are connected to (similar others). This situation would apply when status is generated not via *brokerage*, but by belonging to well-delimited *cohesive groups*.\n\nTo accommodate this possibility, we can generalize the above approach as follows. Recall that for @alvarez-socorro_etal15 the `W` matrix is given by:\n\n$$\n\\mathbf{W} = \\mathbf{A} \\odot (1 - \\mathbf{S})\n$$\n\nWhere $\\mathbf{A}$ is the adjacency matrix and $\\mathbf{S}$ is a similarity matrix (making ($1-\\mathbf{S}$ a *dis*similarity matrix)) and $\\odot$ is the element-wise matrix multiplication operator.\n\nA generalization that accommodates a wider range of dependencies between status and similarity goes like this:\n\n$$\n\\mathbf{W} = \\mathbf{A} \\odot \\left[\\mathbf{S}^\\delta \\odot (1 - \\mathbf{S})^\\gamma\\right]\n$$\n\nWith the restriction that $0 \\geq \\delta \\leq 1$ and $0 \\geq \\gamma \\leq 1$.\n\nWhen $\\delta = \\gamma = 0$, then  $\\mathbf{W} = \\mathbf{A}$ and we recover the standard Eigenvector scores we computed earlier. When $\\delta = 0$ and $\\gamma > 0$, then we recover the dissimilarity-weighted eigenvector scores of @alvarez-socorro_etal15. When $\\delta > 0$ and $\\gamma = 0$ then we compute scores that weigh the eigenvector scores by similarity rather than dissimilarity. \n\nHere is a function that packages everything above into the generalized version:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   prestige.sim <- function(x, delta = 0, gamma = 0) {\n      A <- as.matrix(as_adjacency_matrix(x))\n      A.p <- A %*% A \n      A.q <- A %*% (1 - A) \n      A.r <- (1 - A) %*% A \n      S <- A.p / (A.p + A.q + A.r)\n      D < - 1 - S\n      W <- A * S^delta * D^gamma\n      return(abs(eigen(W)$vectors[, 1]))\n   }\n```\n:::\n\n\nAnd now we can run through the various possibilities.\n\nTop-ten regular eigenvector:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   s <- prestige.sim(g)\n   names(s) <- 1:vcount(g)\n   round(sort(s/max(s), decreasing = TRUE)[1:10], 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   17    11    19     2     5     1    15    12     4    10 \n1.000 0.814 0.680 0.635 0.629 0.619 0.613 0.549 0.489 0.468 \n```\n:::\n:::\n\n\nTop-ten dissimilarity-weighted eigenvector:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   s <- prestige.sim(g, gamma = 1)\n   names(s) <- 1:vcount(g)\n   round(sort(s/max(s), decreasing = TRUE)[1:10], 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   17    11     5     2    19    12    10    15     1     6 \n1.000 0.821 0.667 0.656 0.637 0.619 0.608 0.579 0.563 0.514 \n```\n:::\n:::\n\n\nTop-ten similarity-weighted eigenvector:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   s <- prestige.sim(g, delta = 1)\n   names(s) <- 1:vcount(g)\n   round(sort(s/max(s), decreasing = TRUE)[1:10], 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   17    11    19     1    15     2     5     4    12     3 \n1.000 0.830 0.768 0.714 0.660 0.581 0.565 0.526 0.412 0.375 \n```\n:::\n:::\n\n\nNeat!\n\n## Connection Between Eigenvector Centrality and the Katz Similarity\n\nThere is of course an even more intimate connection between vertex similarity analysis and prestige scoring at the node in level in networks. The reason for this some forms of \"generalized\" vertex similarity analysis, like the **Katz similarity** [@katz53] discussed in the [vertex similarity lecture notes](similarity.qmd) rely on the same mathematical apparatus, and distributional network imagery as the prestige ranking scores like eigenvector centrality [@vigna16]. \n\nRecall from that lecture that the **Katz similarity matrix** for every pair of nodes is given by: \n\n$$\n\\mathbf{S} = (\\mathbf{I} - \\alpha \\mathbf{A})^{-1}\n$$\n\nAnd here's the function to compute the Katz similarity matrix we introduced there:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   katz.sim <- function(w) {\n      I <- diag(nrow(w)) #creating identity matrix\n      alpha <- 1/eigen(w)$values[1] - 1e-10 #reciprocal of first eigenvalue of adjacency matrix minus a tiny number\n      S <- solve(I - alpha * w) #katz formula\n      return(S)\n   }\n```\n:::\n\n\nLet's now compute the Katz similarity matrix for the Krackhardt managers in the undirected friendship network and take a peek:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   S <- katz.sim(A)\n   head(S[1:10, 1:10])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         [,1]     [,2]     [,3]     [,4]     [,5]     [,6]     [,7]     [,8]\n[1,] 73290109 75152421 52783592 57849885 74451096 50947633 24316040 44999840\n[2,] 75152421 77062057 54124832 59319860 76342911 52242221 24933914 46143293\n[3,] 52783592 54124832 38014785 41663532 53619737 36692525 17512431 32408920\n[4,] 57849885 59319860 41663532 45662496 58766285 40214359 19193314 35519604\n[5,] 74451096 76342911 53619737 58766285 75630477 51754695 24701230 45712683\n[6,] 50947633 52242221 36692525 40214359 51754695 35416259 16903300 31281648\n         [,9]    [,10]\n[1,] 52493498 55335846\n[2,] 53827366 56741939\n[3,] 37805858 39852919\n[4,] 41434552 43678096\n[5,] 53325047 56212421\n[6,] 36490866 38466725\n```\n:::\n:::\n\n\nRecall from the [vertex similarity lecture](similarity.qmd) that these (giant) numbers are the *expected number of paths* of *all* lengths between every pair of managers in the network (with diagonals indicating the expected number of *cycles* of all lengths that start and end in the same manager) with longer paths discounted by $\\alpha$ which in this case equals 0.112.\n\nIn the vertex similarity lecture, we treated the dyadic entries in this matrix as a pairwise similarity score. However, nothing is stopping us from computing the *row* (or column) *sums* of this matrix to obtain a node-level score. This score is larger for nodes that are able to reach other nodes (and themselves) via more paths in the network, with shorter paths counting for more than longer paths. \n\nThus, the row sums of the Katz-similarity matrix, can be used to obtain the **Katz centrality** of each node:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   s.katz <- rowSums(S)\n```\n:::\n\n\nThe big reveal is that normalized ranks of the Katz centrality scores are equivalent (up to rounding error) to the eigenvector centrality scores!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   round(s.katz/max(s.katz), 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 0.619 0.635 0.446 0.489 0.629 0.430 0.205 0.380 0.444 0.468 0.814 0.549\n[13] 0.162 0.401 0.613 0.360 1.000 0.247 0.680 0.360 0.392\n```\n:::\n\n```{.r .cell-code}\n   round(s.eig, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 0.619 0.635 0.446 0.489 0.629 0.430 0.205 0.380 0.444 0.468 0.814 0.549\n[13] 0.162 0.401 0.613 0.360 1.000 0.247 0.680 0.360 0.392\n```\n:::\n:::\n\n\nSo here's another way of understanding the \"eigenvector centralities\" computed by our status distribution game: Better connected nodes by this metric are those who are able to reach all nodes in the graph via direct or indirect paths, with those who are able to do so using the shorter paths being better placed. \n\nSo an alternative expression of the eigenvector centrality vector $\\mathbf{s}$ for each node using the \"Katz\" approach is:\n\n$$\n\\mathbf{s} = \\left[(\\mathbf{I} - \\alpha A)^{-1}\\right]\\mathbf{1}\n$${#eq-katz}\n\nWhere $\\mathbf{1}$ is a column vector full of ones, because post-multiplying a matrix times the all ones column vector produces the row sums of that matrix [@fouss_etal16]. \n\nHere's a wrapper function around the `katz.sim` function that implements this approach to computing the centrality scores using @eq-katz:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   katz.cent <- function(w) {\n      s <-  katz.sim(w) %*% matrix(1, nrow(w), 1)\n      names(s) <- rownames(w)\n      return(s/max(s))\n      }\n```\n:::\n\n\nAnd voila:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   s.katz <- katz.cent(A)\n   nodes <- 1:vcount(g)\n   eig.dat <- data.frame(Nodes = nodes, Katz.Cent = s.katz, Deg.Cent = degree(g))\n   eig.dat <- eig.dat[order(eig.dat$Katz.Cent, decreasing = TRUE), ]\n   kbl(eig.dat[1:10, ], \n       format = \"html\", align = \"c\", row.names = FALSE,\n       caption = \"Top Ten Katz Centrality Scores.\",\n       digits = 3) %>%    \n   kable_styling(bootstrap_options = \n                    c(\"hover\", \"condensed\", \"responsive\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-hover table-condensed table-responsive\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Top Ten Katz Centrality Scores.</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:center;\"> Nodes </th>\n   <th style=\"text-align:center;\"> Katz.Cent </th>\n   <th style=\"text-align:center;\"> Deg.Cent </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:center;\"> 17 </td>\n   <td style=\"text-align:center;\"> 1.000 </td>\n   <td style=\"text-align:center;\"> 18 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 11 </td>\n   <td style=\"text-align:center;\"> 0.814 </td>\n   <td style=\"text-align:center;\"> 14 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 19 </td>\n   <td style=\"text-align:center;\"> 0.680 </td>\n   <td style=\"text-align:center;\"> 10 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 2 </td>\n   <td style=\"text-align:center;\"> 0.635 </td>\n   <td style=\"text-align:center;\"> 10 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 5 </td>\n   <td style=\"text-align:center;\"> 0.629 </td>\n   <td style=\"text-align:center;\"> 10 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 1 </td>\n   <td style=\"text-align:center;\"> 0.619 </td>\n   <td style=\"text-align:center;\"> 9 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 15 </td>\n   <td style=\"text-align:center;\"> 0.613 </td>\n   <td style=\"text-align:center;\"> 9 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 12 </td>\n   <td style=\"text-align:center;\"> 0.549 </td>\n   <td style=\"text-align:center;\"> 8 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 4 </td>\n   <td style=\"text-align:center;\"> 0.489 </td>\n   <td style=\"text-align:center;\"> 7 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 10 </td>\n   <td style=\"text-align:center;\"> 0.468 </td>\n   <td style=\"text-align:center;\"> 8 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nOf course, there's *yet* another way of thinking about the link between the Katz and eigenvector centralities. Consider the matrix $\\mathbf{W}$ defined as:\n\n$$\n\\mathbf{W} = \\alpha \\mathbf{A}\n$$\n\nIn `R` we obtain this matrix as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   alpha <- 1/eigen(A)$values[1] - 1e-10\n   W <- alpha * A\n```\n:::\n\n\nAs you may already suspect, the Katz centrality scores are nothing but the scores we would obtain if we play our `status1` game on this matrix!\n\nLet's see for ourselves:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   s.katz <- status1(W)\n   round(s.katz/max(s.katz), 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 0.619 0.635 0.446 0.489 0.629 0.430 0.205 0.380 0.444 0.468 0.814 0.549\n[13] 0.162 0.401 0.613 0.360 1.000 0.247 0.680 0.360 0.392\n```\n:::\n:::\n\n\nIndeed! Note that in terms of our status game, this means that rather than having \"one\" status point to distribute to others, each node is now assigned a *fraction* of a status point, with this fraction being equivalent to (you guessed it!) $\\alpha$. Everything else works the same way; at each round each node receives the sum of $\\alpha$ points from their neighbors and delivers the same sum (according to their degree) to them. \n\nNote that because $\\alpha$ is a number that is less than one, status \"dissipates\" the longer it has to travel to get distributed to others because multiplying a number that is less than one by itself results in a smaller number (e.g., 0.25 X 0.25 = 0.062). \n\nThus, every node distributes $\\alpha^2$ status points to nodes that are two-steps away, $\\alpha^3$ status points to nodes three-steps away, and so forth up to $\\alpha^k$ where $k$ is the length of the longest path linking two nodes in the network. This is consistent with the idea that in the Katz accounting, longer chains count for less in determining prestige in the system.  \n\nThis also means that the eigenvector centrality scores are equivalent to the dominant eigenvector of the \"perturbed\" adjacency matrix $\\mathbf{W}$, where the perturbation involves multiplying the original adjacency matrix $\\mathbf{A}$ by $\\alpha$ to turn it into $\\mathbf{W} = \\alpha \\mathbf{A}$, as long as we maintain the restriction that $\\alpha$ has to be smaller (even by a teeny-tiny amount) than the reciprocal of the first eigenvalue of the adjacency matrix [@vigna16]. \n\nIn eigenvector decomposition formulese:\n\n$$\n\\lambda \\mathbf{s} = \\mathbf{W} \\mathbf{s}\n$${#eq-katz2}\n\nWe can check this eigenvector equivalence as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   s.katz <- abs(eigen(W)$vector[, 1])\n   round(s.katz/max(s.katz), 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 0.619 0.635 0.446 0.489 0.629 0.430 0.205 0.380 0.444 0.468 0.814 0.549\n[13] 0.162 0.401 0.613 0.360 1.000 0.247 0.680 0.360 0.392\n```\n:::\n:::\n\n\nIt all comes together!\n\n## Generalizing Katz: Beta Centrality\nIn another neat (and also hugely cited) paper, @bonacich87 recognized these linkages between the eigenvector and Katz approaches and proposed a more general way to assign prestige scores in networks which he referred to as **Beta Centrality**. \n\nThe basic idea is that the $\\alpha$ parameter in @eq-katz above has a more general interpretation and can be assigned values other than something close to the reciprocal of the largest eigenvalue of the adjacency matrix (which makes it the same as the regular eigenvector score and thus redundant). So we substitute it with a free parameter $\\beta$, and rewrite @eq-katz as follows:\n\n$$\n\\mathbf{s} = [(\\mathbf{I} - \\beta \\mathbf{A})^{-1}\\mathbf{A}]\\mathbf{1}\n$${#eq-bonacich}\n\nWith the restriction: $-\\frac{1}{\\lambda_1} > \\beta < \\frac{1}{\\lambda_1}$, where $\\lambda_1$ is the dominant eigenvalue of the adjacency matrix. Note that when $\\beta = 0$ the above equation reduces to $\\mathbf{s} = [\\mathbf{I} -\\mathbf{A}]\\mathbf{1}$ which is just each node's degree centrality.\n\nHere's a function to compute the Beta Centrality according to @eq-bonacich:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   beta.cent <- function(w, beta) {\n      I <- diag(nrow(w)) #creating identity matrix\n      alpha <- 1/eigen(w)$values[1] #beta limit\n      if (beta > 0 & beta > alpha) {\n         beta <- alpha - 1e-10\n         }\n      if (beta < 0 & beta < (alpha*-1)) {\n         beta <- (alpha * -1) + 1e-10\n         }\n      s <- (solve(I - beta * w) %*% w) %*% matrix(1, nrow(w), 1) #beta centrality formula\n      s <- (s-min(s))/(max(s) - min(s)) #normalizing to min zero and max one\n      return(s)\n   }\n```\n:::\n\n\nThis function takes the adjacency matrix (`w`) as input and returns the vector of Beta Centrality scores (`s`) as output normalized to fall in the zero to one interval. Note that if a value outside of the outlawed range is given for the parameter $\\beta$, the function computes scores close to the eigenvector centrality ordering. \n\nAnd now let's see different rankings of nodes based on different values of $\\beta$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   r1 <- beta.cent(A, beta = 0.1)\n   r2 <- beta.cent(A, beta = 0.02)\n   r3 <- beta.cent(A, beta = 0)\n   r4 <- beta.cent(A, beta = -0.02)\n   r5 <- beta.cent(A, beta = -0.1)\n   nodes <- 1:vcount(g)\n   eig.dat <- data.frame(Nodes = nodes, r1 = r1, r2 = r2, r3 = r3, r4 = r4, r5 = r5,  \n                         Deg = degree(g))\n   eig.dat <- eig.dat[order(eig.dat$Deg, decreasing = TRUE), ]\n   kbl(eig.dat[1:10, ], \n       format = \"html\", align = \"c\", row.names = FALSE,\n       caption = \"Top Ten Katz Centrality Scores.\",\n       digits = 2, col.names = c(\"Nodes\", \"Beta = 0.1\", \"Beta = 0.02\", \"Beta = 0\",\n                                 \"Beta = -0.02\", \"Beta = -0.1\", \"Degree\")) %>%    \n   kable_styling(bootstrap_options = \n                    c(\"hover\", \"condensed\", \"responsive\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-hover table-condensed table-responsive\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Top Ten Katz Centrality Scores.</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:center;\"> Nodes </th>\n   <th style=\"text-align:center;\"> Beta = 0.1 </th>\n   <th style=\"text-align:center;\"> Beta = 0.02 </th>\n   <th style=\"text-align:center;\"> Beta = 0 </th>\n   <th style=\"text-align:center;\"> Beta = -0.02 </th>\n   <th style=\"text-align:center;\"> Beta = -0.1 </th>\n   <th style=\"text-align:center;\"> Degree </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:center;\"> 17 </td>\n   <td style=\"text-align:center;\"> 1.00 </td>\n   <td style=\"text-align:center;\"> 1.00 </td>\n   <td style=\"text-align:center;\"> 1.00 </td>\n   <td style=\"text-align:center;\"> 1.00 </td>\n   <td style=\"text-align:center;\"> 1.00 </td>\n   <td style=\"text-align:center;\"> 18 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 11 </td>\n   <td style=\"text-align:center;\"> 0.77 </td>\n   <td style=\"text-align:center;\"> 0.75 </td>\n   <td style=\"text-align:center;\"> 0.75 </td>\n   <td style=\"text-align:center;\"> 0.75 </td>\n   <td style=\"text-align:center;\"> 0.75 </td>\n   <td style=\"text-align:center;\"> 14 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 2 </td>\n   <td style=\"text-align:center;\"> 0.56 </td>\n   <td style=\"text-align:center;\"> 0.51 </td>\n   <td style=\"text-align:center;\"> 0.50 </td>\n   <td style=\"text-align:center;\"> 0.49 </td>\n   <td style=\"text-align:center;\"> 0.41 </td>\n   <td style=\"text-align:center;\"> 10 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 5 </td>\n   <td style=\"text-align:center;\"> 0.55 </td>\n   <td style=\"text-align:center;\"> 0.51 </td>\n   <td style=\"text-align:center;\"> 0.50 </td>\n   <td style=\"text-align:center;\"> 0.49 </td>\n   <td style=\"text-align:center;\"> 0.42 </td>\n   <td style=\"text-align:center;\"> 10 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 19 </td>\n   <td style=\"text-align:center;\"> 0.61 </td>\n   <td style=\"text-align:center;\"> 0.52 </td>\n   <td style=\"text-align:center;\"> 0.50 </td>\n   <td style=\"text-align:center;\"> 0.48 </td>\n   <td style=\"text-align:center;\"> 0.41 </td>\n   <td style=\"text-align:center;\"> 10 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 1 </td>\n   <td style=\"text-align:center;\"> 0.53 </td>\n   <td style=\"text-align:center;\"> 0.46 </td>\n   <td style=\"text-align:center;\"> 0.44 </td>\n   <td style=\"text-align:center;\"> 0.42 </td>\n   <td style=\"text-align:center;\"> 0.36 </td>\n   <td style=\"text-align:center;\"> 9 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 15 </td>\n   <td style=\"text-align:center;\"> 0.53 </td>\n   <td style=\"text-align:center;\"> 0.45 </td>\n   <td style=\"text-align:center;\"> 0.44 </td>\n   <td style=\"text-align:center;\"> 0.42 </td>\n   <td style=\"text-align:center;\"> 0.35 </td>\n   <td style=\"text-align:center;\"> 9 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 10 </td>\n   <td style=\"text-align:center;\"> 0.37 </td>\n   <td style=\"text-align:center;\"> 0.37 </td>\n   <td style=\"text-align:center;\"> 0.38 </td>\n   <td style=\"text-align:center;\"> 0.38 </td>\n   <td style=\"text-align:center;\"> 0.40 </td>\n   <td style=\"text-align:center;\"> 8 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 12 </td>\n   <td style=\"text-align:center;\"> 0.45 </td>\n   <td style=\"text-align:center;\"> 0.39 </td>\n   <td style=\"text-align:center;\"> 0.38 </td>\n   <td style=\"text-align:center;\"> 0.36 </td>\n   <td style=\"text-align:center;\"> 0.27 </td>\n   <td style=\"text-align:center;\"> 8 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> 4 </td>\n   <td style=\"text-align:center;\"> 0.38 </td>\n   <td style=\"text-align:center;\"> 0.33 </td>\n   <td style=\"text-align:center;\"> 0.31 </td>\n   <td style=\"text-align:center;\"> 0.30 </td>\n   <td style=\"text-align:center;\"> 0.25 </td>\n   <td style=\"text-align:center;\"> 7 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nAs expected, when $\\beta = 0$, the rankings produced by Beta Centrality are the same as those given by the degree of each node (the score is the same as for nodes of the same degree). It deviates from degree when different values of $\\beta$ are used. \n\nFor positive values close to the reciprocal of the first eigenvalue (e.g., $\\beta = 0.1$) then the rankings are the same as those obtained from the usual eigenvector score. However, as $\\beta$ gets closer to zero (e.g., $\\beta = 0.02$) the rankings get closer to the degree based ordering. This means that values of $\\beta$ closer to to $\\frac{1}{\\lambda_1}$ reward more distant connections, while values of $\\beta$ closer to zero reward the more local connections in determining centrality. \n\n### Beta Centrality in Negatively Connected Networks\n\nBut why would we ever want the $\\beta$ parameter to be negative?\n\nConsider @fig-toy. According to the usual eigenvector centrality principle, we should expect node 1 to have the highest eigenvector score, as it is connected to well-connected others. \n\n\n::: {.cell}\n::: {.cell-output-display}\n![A toy network.](prestige_files/figure-html/fig-toy-1.png){#fig-toy width=768}\n:::\n:::\n\n\nThis is precisely what the Beta Centrality scoring with a positive $\\beta$ parameter tells us:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   round(beta.cent(as_adjacency_matrix(g1), beta = 0.35), 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n16 x 1 Matrix of class \"dgeMatrix\"\n       [,1]\n [1,] 1.000\n [2,] 0.855\n [3,] 0.855\n [4,] 0.855\n [5,] 0.000\n [6,] 0.000\n [7,] 0.000\n [8,] 0.000\n [9,] 0.000\n[10,] 0.000\n[11,] 0.000\n[12,] 0.000\n[13,] 0.000\n[14,] 0.000\n[15,] 0.000\n[16,] 0.000\n```\n:::\n:::\n\n\nHowever, imagine instead that the colors in @fig-toy stands for \"sellers\" (tan nodes) and \"buyers\" (blue nodes). Accordingly, node 1 is trying to get nodes 2, 3, and 4 (in light blue) to *buy* something from them (like a house). However, each of the light blue nodes have *other* prospective sellers (four each). \n\nThis means that being connected to well-connected others in this case puts node 1 at a *disadvantage*. Meanwhile, the blue nodes, by being connected to others who are *not* well-connected are at an *advantage*, because they can play the different prospective sellers against one another. \n\n@bonacich87 argues that in these kinds of **negatively connected networks** eigenvector centrality should work in the opposite direction, penalizing nodes connected to well-connected others and rewarding those who are connected to poorly connected others (like the blue nodes in @fig-toy).^[Negative connected networks are ones where a transaction in one relationship *precludes* a transaction in another, like when you can only buy a house from *one* seller and buying a house from that seller precludes you from buy a house from another one; note, that this also works for *monogamous* dating.] \n\nThis is precisely what the Beta Centrality with a negative $\\beta$ parameter does:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   beta.cent(as_adjacency_matrix(g1), beta = -0.35)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n16 x 1 Matrix of class \"dgeMatrix\"\n        [,1]\n [1,] 0.0000\n [2,] 1.0000\n [3,] 1.0000\n [4,] 1.0000\n [5,] 0.3125\n [6,] 0.3125\n [7,] 0.3125\n [8,] 0.3125\n [9,] 0.3125\n[10,] 0.3125\n[11,] 0.3125\n[12,] 0.3125\n[13,] 0.3125\n[14,] 0.3125\n[15,] 0.3125\n[16,] 0.3125\n```\n:::\n:::\n\n\nAs expected, in the Beta Centrality scoring with a negative $\\beta$ parameter, the blue nodes, who are connected to others who don't have many other options are now the most central and node 1 who is connected to the blue nodes with many options is now the least central!\n\n### Beta Centrality with igraph\n\nOf course, `igraph` has a dedicated function called `power_centrality` that does everything that we did earlier with our custom function. You just have to specify the `exponent` argument, which is the value of $\\beta$ above:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   b <- power_centrality(g1, exponent = 0.35) #positive beta\n   b <- (b-min(b))/(max(b) - min(b)) #normalizing scores\n   round(b, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 1.000 0.855 0.855 0.855 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000\n[13] 0.000 0.000 0.000 0.000\n```\n:::\n\n```{.r .cell-code}\n   b <- power_centrality(g1, exponent = -0.35) #negative beta\n   b <- (b-min(b))/(max(b) - min(b)) #normalizing scores\n   round(b, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 0.000 1.000 1.000 1.000 0.312 0.312 0.312 0.312 0.312 0.312 0.312 0.312\n[13] 0.312 0.312 0.312 0.312\n```\n:::\n:::\n\n\nWhich are the same scores we just calculated. ",
    "supporting": [
      "prestige_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\r\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}