---
title: "Graph Ensembles in Two-Mode Networks"
execute: 
  eval: true
  echo: true
  output: true
  warning: false
  message: false
format: 
   html:
      code-line-numbers: true
---

As we saw in the [graph ensemble lesson](swap.qmd), there are many approaches to randomizing the structure of one-mode networks when the aim is to create **graph ensembles** preserving selected properties. These ensembles, in turn, can be used to do **null hypothesis testing** in networks. 

Not surprisingly, a similar suite of techniques exist for the two-mode case, but until recently various approaches were scattered (and reduplicated) across a bunch of literatures in social network analysis, ecology, network physics, and  computer science [@neal_etal24]. 

## Two-Mode Erdos-Renyi Model

Like with one-mode networks, the simplest null model for two-mode networks is one that preserves the number of nodes and the number of edges. This model, like we saw before, also preserves anything that is a function of these two-quantities. In the two-mode case, this is the bipartite graph's **density** and the average degrees of the nodes in each mode (recall that two-mode networks have two average degrees). This is thus a two-mode version of the **Erdos-Renyi** null model. 

Let's load up the *Southern Women* (SW) data and see how it works:

```{r}
   library(igraph)
   library(networkdata)
   A <- as_biadjacency_matrix(southern_women)
```

Let's compute some basic network statistics:

```{r}
   d <- sum(A)/(nrow(A)*ncol(A)) #density
   ad.p <- mean(rowSums(A)) #average degree of people
   ad.g <- mean(colSums(A)) #average degree of groups
   d
   ad.p
   ad.g
```

We can see that the density of the SW network is $d=$ `r round(d, 2)`, the average degree of people $\bar{k_p}=$ `r round(ad.p, 2)` and the average degree of groups $\bar{k_g}=$ `r round(ad.g, 2)`.  

Now, let's compute something on this network, like the **degree correlation** between people and groups, answering  the question: Do people with lots of memberships tend to join larger groups?

First let's create an edge list data frame with the incident nodes' degrees and compute the correlation between the respective degrees:

```{r}
   v.a <- as.vector(A) #adjacency relations
   rd <- rep(rowSums(A), ncol(A)) #row degrees
   cd <- rep(colSums(A), each = nrow(A)) #column degrees
   d <- data.frame(e = v.a, rd =  rd, cd = cd)
   cor(d[d$e == 1, ]$rd, d[d$e == 1, ]$cd) #degree correlation among adjacent nodes
```

Which is negative, suggesting degree **anti-correlation** in this network (people with lots of memberships tend to belong to smaller groups). 

Is this value statistically significant net of the network density? To find out, we need to create a **two-mode graph ensemble** of networks with the same density, number of persons, and number of groups as SW. 

A simple approach goes like this. First, let's create a vectorized version of the adjacency matrix and assign a number from 1:$M$ (where $M = |P| \times |G|$, the number of cells in the biadjacency matrix) to each value:

```{r}
   v.a <- as.vector(A)
   M <- nrow(A)*ncol(A)
   names(v.a) <- 1:M
   v.a
```

Now we can just permute the labels of the vector, and  re-order them to generate a new biadjacency matrix `A.perm`

```{r}
   s <- sample(1:M)
   v.perm <- v.a
   names(v.perm) <- s
   v.perm <- v.perm[names(v.a)] #reordering vector
   A.perm <- matrix(v.perm, nrow = nrow(A)) #creating permuted biadjacency matrix
   rownames(A.perm) <- rownames(A)
   colnames(A.perm) <- colnames(A)
   A.perm
```

We can verify that `A.perm` has the same basic network statistics as `A`:

```{r}
   d <- sum(A.perm)/(nrow(A.perm)*ncol(A.perm)) #density
   ad.p <- mean(rowSums(A.perm)) #average degree of people
   ad.g <- mean(colSums(A.perm)) #average degree of groups
   d
   ad.p
   ad.g
```

But *not* the same degree distributions:

```{r}
   rowSums(A)
   rowSums(A.perm)
   colSums(A)
   colSums(A.perm)
```


We can now package the two-mode permutation steps into a function called `tm.perm`:

```{r}
   tm.perm <- function(x) {
      z <- nrow(x)*ncol(x)
      l <- as.character(1:z)
      s <- sample(1:z)
      v <- as.vector(x)
      names(v) <- s
      v <- v[l]
      w <- matrix(v, nrow = nrow(x))
      rownames(w) <- rownames(x)
      colnames(w) <- colnames(x)
      return(w)
   }
```

And generate an Erdos-Renyi graph ensemble for the SW data:

```{r}
   set.seed(4567)
   G <- replicate(500, tm.perm(A), simplify = FALSE)
```

We then package the steps above into a two-mode degree correlation function:

```{r}
   tm.deg.corr <- function(x) {
      d <- data.frame(e = as.vector(x), 
                      rd = rep(rowSums(x), ncol(x)), 
                      cd = rep(colSums(x), each = nrow(x)))
      return(cor(d[d$e == 1, ]$rd, d[d$e == 1, ]$cd))
   }
```

And compute it across our ensemble:

```{r}
   corrs <- sapply(G, tm.deg.corr)
   corrs[1:100] #first hundred entries
```

So let's see how our observed value stacks up in the grand scheme:

```{r}
   library(ggplot2)
   p <- ggplot(data = data.frame(round(corrs, 2)), aes(x = corrs))
   p <- p + geom_histogram(binwidth = 0.015, stat = "bin", fill = "darkblue")
   p <- p + geom_vline(xintercept = tm.deg.corr(A), 
                       color = "red", linetype = 1, linewidth = 1.5)
   p <- p + geom_vline(xintercept = 0, linetype = 1, 
                       color = "purple", linewidth = 1.5)
   p <- p + theme_minimal() + labs(x = "Q by Level", y = "Freq.")
   p <- p + theme(axis.text = element_text(size = 12))
   p <- p + annotate("text", x=-0.05, y=47, label= "Zero Point", color = "purple")
   p <- p + annotate("text", x=-0.23, y=47, label= "Obs. Value", color = "red")
   p
```

Which looks close to the tail end of the negative spectrum. We can compute the value that corresponds to the 1st percentile of the assortativity distribution from the ensemble and then see if what observe is below that value ($p < 0.01$). 

```{r}
   quantile(corrs, probs = 0.01)
   tm.deg.corr(A) < quantile(corrs, probs = 0.01)
```

Which is definitely true in this null graph ensemble, suggesting that degree anti-correlation is present in the SW data, at statistically significant levels, net of density. 

As before, if we wanted a more stringent two-tailed we would need to create a vector with the *absolute value* of the two-mode degree correlation: 

```{r}
   1 - ecdf(abs(corrs))(abs(tm.deg.corr(A)))
```

Which is still statistically significant at conventional levels. 

## Fixed Degree Models
As we already noted, the two-mode Erdos-Renyi model fixes the number of edges (and thus the density and average degrees) in the network, but does not preserve the original degree distributions. We might want to test our hypotheses by using a two-mode graph ensemble that "controls for" the node degrees. 

How do we do that? One complication is that we have two sets of degrees so we have more options than in the one mode case. We can fix the row (person) degree, or the column (group) degree or *both* degrees. 

Let's begin with the simplest case, in which we fix *either* the row or column degree but not both. 

### Fixing Row Degrees

To fix the row degrees, we need to randomize the entries in each row of the biadjacency matrix, while preserving the number of ones in that row. One way to do this is to write a function that takes an observed row of the matrix, randomizes it and then substitutes it for the observed row:

```{r}
   rand.row <- function(r) {
      return(r[sample(1:length(r))])
      }
```

Now we can just `apply` the `rand.row` function to each row of the biadjacency matrix `A` to generate a new matrix `A.r`:

```{r}
   A.r <- apply(A, 1, rand.row)
   A.r <- t(A.r)
   rownames(A.r) <- rownames(A)
   colnames(A.r) <- colnames(A)
```

Here's the original matrix `A`:

```{r}
   A
```

And the reshuffled matrix `A.r`

```{r}
   A.r
```

Note that the new matrix `A.r` preserves the person degrees of the original:

```{r}
   rowSums(A)
   rowSums(A.r)
```

But not the group degrees:

```{r}
   colSums(A)
   colSums(A.r)
```

All the other lower order statistics like density are preserved:

```{r}
   sum(A.r)/(nrow(A.r)*ncol(A.r))
```

Now we package everything into a function:

```{r}
   fix.deg <- function(x, mode = 1) {
      w <- apply(x, mode, function(r) {r[sample(1:length(r))]})
      if (mode == 1) {
         w <- t(w)
         }
      rownames(w) <- rownames(x)
      colnames(w) <- colnames(x)
      return(w)
      }
```

And generate a graph ensemble of reshuffled matrices that preserve the person degrees:

```{r}
   set.seed(4567)
   G <- replicate(500, fix.deg(A), simplify = FALSE)
```

Let's compute the degree correlations across this ensemble and see how our observed value stacks up in the grand scheme:

```{r}
   corrs <- sapply(G, tm.deg.corr)
   library(ggplot2)
   p <- ggplot(data = data.frame(round(corrs, 2)), aes(x = corrs))
   p <- p + geom_histogram(binwidth = 0.015, stat = "bin", fill = "darkblue")
   p <- p + geom_vline(xintercept = tm.deg.corr(A), 
                       color = "red", linetype = 1, linewidth = 1.5)
   p <- p + geom_vline(xintercept = 0, linetype = 1, 
                       color = "purple", linewidth = 1.5)
   p <- p + theme_minimal() + labs(x = "Q by Level", y = "Freq.")
   p <- p + theme(axis.text = element_text(size = 12))
   p <- p + annotate("text", x=-0.05, y=47, label= "Zero Point", color = "purple")
   p <- p + annotate("text", x=-0.23, y=47, label= "Obs. Value", color = "red")
   p
```

Looking pretty good! Let's check the p-value:

```{r}
   quantile(corrs, probs = 0.01)
   tm.deg.corr(A) < quantile(corrs, probs = 0.01)
   1 - ecdf(abs(corrs))(abs(tm.deg.corr(A)))
```

Note that our result is not longer statistically significant at the $p <0.01$ level, but it can be defended at the $p < 0.05$ level. Still a chance of getting published.

### Fixing Row Degrees

We can fix the column degrees using the same `fix.deg` function as earlier, but this time, we just change the `mode` to equal `2` argument to apply the functions to the columns and not the rows. 

For instance:

```{r}
   A.c <- fix.deg(A, mode = 2)
   A.c
```

Which generates a reshuffled adjacency matrix that preserves the group degrees:

```{r}
   colSums(A)
   colSums(A.c)
```

And now test our hypothesis on an ensemble of graphs with fixed group degrees:


```{r}
   set.seed(4567)
   G <- replicate(500, fix.deg(A, mode = 2), simplify = FALSE)
   corrs <- sapply(G, tm.deg.corr)
   library(ggplot2)
   p <- ggplot(data = data.frame(round(corrs, 2)), aes(x = corrs))
   p <- p + geom_histogram(binwidth = 0.015, stat = "bin", fill = "darkblue")
   p <- p + geom_vline(xintercept = tm.deg.corr(A), 
                       color = "red", linetype = 1, linewidth = 1.5)
   p <- p + geom_vline(xintercept = 0, linetype = 1, 
                       color = "purple", linewidth = 1.5)
   p <- p + theme_minimal() + labs(x = "Q by Level", y = "Freq.")
   p <- p + theme(axis.text = element_text(size = 12))
   p <- p + annotate("text", x=-0.05, y=47, label= "Zero Point", color = "purple")
   p <- p + annotate("text", x=-0.23, y=47, label= "Obs. Value", color = "red")
   p
```

Uh oh, adjusting for group degrees seems to have made our conclusions a bit more shaky. Let' see:

```{r}
   quantile(corrs, probs = 0.01)
   tm.deg.corr(A) < quantile(corrs, probs = 0.01)
   1 - ecdf(abs(corrs))(abs(tm.deg.corr(A)))
```

Still significant at $p < 0.05$! Regardless, it is clear that our earlier conclusions from the Erdos-Renyi model were a bit too optimistic. What happens when we try to fix *both* the row and column degrees?

