{
  "hash": "5f5ea9beb82ea43626846023d675162e",
  "result": {
    "markdown": "---\ntitle: \"Community Detection Using Spectral Clustering\"\nexecute: \n  eval: true\n  echo: true\n  output: true\n  warning: false\n  message: false\nformat: \n   html:\n      code-line-numbers: true\n---\n\n\n## Spectral Clustering\n\nIn the handout on communities, we saw how to use the leading (first) eigenvector of the modularity matrix to split a network in two (and then split those partitions in two and so on). \n\nIn the handout on CA, we saw that the eigendecomposition of a square matrix actually results in $p$ eigenvectors and eigenvalues (not just the leading or first). \n\nWe also learned from the eigendecomposition lesson that selecting some number $k < p$ of eigenvectors and eigenvalues helps us reconstruct the best approximation of the original matrix given that number of dimensions.\n\nPutting these three lessons together suggests a simple way to detect multiple communities in a network using the eigenvalues of a suitable matrix, along with standard clustering algorithms such as k-means clustering. This approach does not have to iterate between binary divisions based on a single (leading) eigenvector, but can instead use multiple eigenvectors at once to partition the data into any desired number of clusters. \n\nThis general approach to community detection is sometimes referred to as **spectral clustering**.^[Not as a reference to ghosts, but because the eigendecomposition of a matrix is sometimes also called the **spectral decomposition**.]\n\n## Spectral Clustering Using the Eigenvectors of the Laplacian\n\nThe issue then becomes, *which* matrix should we use the eigenvalues and eigenvectors of? As suggested by @vonluxburg07, an obvious candidate is what is called the **graph Laplacian** ($\\mathbf{L}$), which is given by:\n\n$$\n\\mathbf{L} = \\mathbf{D} - \\mathbf{A}\n$$\n\nWhere $\\mathbf{D}$ is the graph's **degree matrix** a matrix with the degree vector of the graph along the diagonals and zeros everywhere else. \n\nLike the modularity matrix, the $\\mathbf{L}$ matrix is doubly-centered (rows and columns sum to zero), which means, via some math other smarter people have worked out, that, if $\\mathbf{A}$ is connected, one of the eigenvalues of $\\mathbf{L}$ is guaranteed to be zero.^[If $\\mathbf{A}$ is not connected there will be as many zero eigenvalues in the eigendecomposition of $\\mathbf{L}$ as there are components in $\\mathbf{A}$.] \n\nThe other interesting thing is that the second smallest (non-zero) eigenvalue of $\\mathbf{L}$ (sometimes called the **Fiedler vector**) provides an optimal (e.g., minimizing the number of cross-community edges) two-community partition of the graph (just like we saw the leading eigenvector of the modularity matrix does).\n\nIf we want to check for the existence of multiple groups, therefore, what we need to do is to create a new $n \\times k$ matrix $\\mathbf{U}$ composed of the $k$ eigenvectors of $\\mathbf{L}$ associated with the $k$ *smallest* eigenvalues, arranged from smallest to biggest. \n\nWe then normalize the node-specific row-vector of values of the$\\mathbf{U}$ matrix (e.g., using the Euclidean norm), and the use normalized $\\mathbf{U}$ matrix, which is an embedding of the node set of the graph in an k-dimensional Euclidean space, as input to a k-means algorithm with a known number of clusters [@fouss_etal16, p. 320]. \n\nLet's see how that would work. Let's load the `law_friends` data from the `networkdata` package containing information on the friendship nominations of 68 lawyers in a firm. We have already analyzed these data using other community detection algorithms in a previous handout. The original data are directed, so we constrain them to be undirected and remove nodes with degree less than two:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   library(networkdata)\n   library(igraph)\n   g <- law_friends\n   g <- as_undirected(g, mode = \"collapse\")\n   g <- subgraph(g, degree(g)>=2) #removing low degree nodes\n```\n:::\n\n\nWe can then write a function called `ratio.cut` that accomplishes the eigendecomposition of the graph Laplacian matrix described earlier. It looks like this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   ratio.cut <- function(x, k = 6) {\n         A <- as.matrix(as_adjacency_matrix(x))\n         D <- diag(degree(x))\n         n <- vcount(x)\n         L <- D - A\n         eig.L <- eigen(L)\n         b <- n - k\n         e <- n - 1\n         U <- matrix(eig.L$vectors[, b:e], nrow = n, ncol = k)\n         if (k > 1) {\n            U <- U[, k:1] #re-ordering columns from smallest to biggest\n            }\n         for (i in 1:nrow(U)) {\n            U[i, ] <- U[i, ]/norm(as.matrix(U[i, ]), type = \"E\")\n            }\n      return(U)\n   }\n```\n:::\n\n\nThis function takes a graph as input and produces the $\\mathbf{U}$ matrix with $n$ rows and $k$ columns, with $k=6$ by default. It first computes the adjacency matrix (line 2), then the degree matrix (line 3), then the Laplacian matrix (line 5). It then computes the eigendecomposition of the Laplacian (line 6), row normalizes the values taken by the eigenvectors corresponding to the $k$ smallest eigenvalues in lines 10-13 (reverse-ordered from smallest to biggest in line 9), and returns the resulting matrix $\\mathbf{U}$ in line 14.\n\nLet's see the function at work:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   U <- ratio.cut(g)\n   round(U, 2)[1:10, ]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]\n [1,]  0.08  0.34  0.66  0.37 -0.54  0.13\n [2,]  0.07  0.29  0.60  0.37 -0.53  0.37\n [3,]  0.47  0.13  0.08 -0.87  0.03  0.04\n [4,]  0.34  0.38  0.71  0.16 -0.45  0.10\n [5,]  0.57  0.26  0.06 -0.78  0.00  0.01\n [6,]  0.58 -0.76 -0.21  0.20 -0.05  0.03\n [7,]  0.37  0.20 -0.01 -0.90  0.08  0.03\n [8,] -0.02  0.33  0.65  0.41 -0.55 -0.01\n [9,]  0.14  0.37  0.67  0.36 -0.52  0.04\n[10,]  0.05  0.36  0.66  0.42 -0.50  0.06\n```\n:::\n:::\n\n\nWhich shows the corresponding values of $\\mathbf{U}$ for each node across the six eigenvectors of the Laplacian we selected. \n\nNow, the basic idea is to treat each of the normalized scores along the six eigenvectors as if they were \"variables\" or \"features\" in a standard k-means clustering problem. The algorithm will then group nodes based on how similar their scores are on each six-dimensional vector. Similar nodes will correspond to communities (k-means clusters) in our data. \n\nK-means clustering requires knowing how many groups we want in advance (differently from hierarchical clustering). Since we don't know which is the best community partition beforehand, we instead compute a bunch of partitions and check the modularity of each. \n\nThe following function computes cluster assignments of the nodes in the graph up to ten partitions (starting from the minimum two):\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   k.cuts <- function(x, max = 9) {\n      clus <- list()\n      for (i in 1:max) {\n         set.seed(456) #setting seed because kmeans uses random starting nodes for cluster centroids\n         k <- i + 1\n         clus[[i]] <- kmeans(x, k)$cluster\n         }\n      return(clus)\n   }\n```\n:::\n\n\nThis function takes the $\\mathbf{U}$ matrix as input and returns a nine-element list (with vectors of length $n$ as its elements) of cluster assignments for each node in the graph, corresponding to partitions $k = \\{2, 3, \\ldots 10\\}$ respectively. \n\nLet's see the clustering function at work:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   clus <- k.cuts(U)\n   clus\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n [1] 2 2 1 2 1 1 1 2 2 2 2 2 2 1 2 2 2 1 1 2 2 2 2 2 2 2 2 1 2 2 1 1 1 2 1 2 2 2\n[39] 2 1 1 2 1 1 2 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n[[2]]\n [1] 3 3 1 3 1 1 1 3 3 3 3 3 3 1 3 3 3 1 1 3 3 3 3 3 3 3 3 1 3 3 1 1 1 3 1 3 3 3\n[39] 3 2 2 3 2 1 3 2 1 1 2 3 2 2 2 2 1 1 1 1 2 1 2 2 2 2 2 2 2 2\n\n[[3]]\n [1] 3 3 4 3 4 4 4 3 3 3 3 3 3 4 3 3 3 4 1 3 3 3 3 3 3 3 3 4 3 3 4 4 4 3 1 3 3 3\n[39] 3 2 2 3 2 1 3 2 1 1 2 3 2 2 2 1 1 1 1 1 2 4 2 2 2 2 2 2 2 2\n\n[[4]]\n [1] 3 3 5 3 5 5 5 3 3 3 3 3 3 5 3 3 3 5 1 3 3 3 3 3 3 3 3 5 3 4 4 4 4 3 1 3 3 3\n[39] 3 2 2 3 2 1 3 2 1 1 2 3 2 2 2 1 1 1 1 1 2 4 2 2 2 2 2 2 2 2\n\n[[5]]\n [1] 3 3 5 3 5 5 5 3 3 3 3 3 3 5 3 3 3 5 1 3 3 3 3 3 4 3 3 5 3 4 6 6 6 3 1 4 3 3\n[39] 3 2 2 3 2 1 3 2 1 1 2 3 2 2 2 1 1 1 1 1 2 6 2 2 2 2 2 2 2 2\n\n[[6]]\n [1] 3 3 5 3 5 5 5 3 3 3 3 3 3 5 3 3 3 5 1 3 3 3 3 3 4 3 3 5 3 4 6 6 6 3 1 4 3 3\n[39] 3 2 2 3 2 1 3 2 1 1 2 3 2 2 2 1 1 1 1 1 2 6 7 2 7 7 7 7 7 7\n\n[[7]]\n [1] 8 8 5 8 5 5 5 8 8 8 8 8 8 5 8 8 8 5 1 8 8 8 8 8 4 8 8 5 8 4 6 6 6 3 1 3 3 3\n[39] 3 2 2 3 2 1 3 2 1 1 2 8 2 2 2 1 1 1 1 1 2 6 7 2 7 7 7 7 7 7\n\n[[8]]\n [1] 8 8 5 8 5 5 5 8 8 8 8 8 8 5 8 8 8 5 1 8 8 8 8 8 4 8 8 5 8 4 6 6 6 3 1 3 3 3\n[39] 3 2 2 3 7 1 3 2 1 1 2 8 2 2 2 7 1 1 1 1 7 6 7 2 9 9 9 9 9 9\n\n[[9]]\n [1]  3  3  5  3  5  5  5  3  3  3  3  3  3  5  8  3  3  5  1  3  3  3  3  3  4\n[26]  3  3  5  3  4  6  6  6 10  1 10 10 10 10  2  2 10  7  1 10  2  1  1  2  8\n[51]  2  2  2  7  1  1  1  1  7  6  7  2  9  9  9  9  9  9\n```\n:::\n:::\n\n\nGreat! Now that we have our partitions, we need to check the modularity corresponding to each one of them. \n\nWe can do this with the following quick function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   mod.check <- function(x, c) {\n      k <- length(c)\n      m <- rep(0, k)\n      for (i in 1:k) {\n         m[i] <- modularity(x, c[[i]])\n         }\n      names(m) <- 2:(k+1)\n      return(m)\n      }\n```\n:::\n\n\nWhich takes an `igraph` graph object and the list of cluster assignments as input and produces a vector of the same length as the list of cluster assignments with the modularity corresponding to that assignment.\n\nLet's see this function at work in our running example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   mod.res <- mod.check(g, clus)\n   round(mod.res, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    2     3     4     5     6     7     8     9    10 \n0.305 0.356 0.328 0.319 0.316 0.300 0.302 0.293 0.286 \n```\n:::\n:::\n\n\nWhich suggests that the three-cluster partition of the network (shown in @fig-friends-2) does pretty well in separating densely connected subgraphs ($Q = 0.36$). \n\nNote that this partition looks a lot like the one we settled on using Newman's divisive leading eigenvector approach based on the modularity matrix: One core dense group of lawyers surrounded by a looser couple of communities.\n\n\n\n::: {#fig-friends .cell layout-ncol=\"2\"}\n::: {.cell-output-display}\n![Original Network](spectral_files/figure-html/fig-friends-1.png){#fig-friends-1 width=1152}\n:::\n\n::: {.cell-output-display}\n![Maximum Modularity Solution)](spectral_files/figure-html/fig-friends-2.png){#fig-friends-2 width=1152}\n:::\n\n::: {.cell-output-display}\n![Four Community Solution](spectral_files/figure-html/fig-friends-3.png){#fig-friends-3 width=1152}\n:::\n\n::: {.cell-output-display}\n![Five Community Solution](spectral_files/figure-html/fig-friends-4.png){#fig-friends-4 width=1152}\n:::\n\n::: {.cell-output-display}\n![Six Community Solution](spectral_files/figure-html/fig-friends-5.png){#fig-friends-5 width=1152}\n:::\n\n::: {.cell-output-display}\n![Seven Community Solution](spectral_files/figure-html/fig-friends-6.png){#fig-friends-6 width=1152}\n:::\n\nClustering of Nodes in the Law Firm Friendship Network Using the Ratio Cut\n:::\n\n\nNote also that the four, five, six and even seven-community partitions are not too shabby either. We can see those in @fig-friends-3-@fig-friends-6 as they reveal further insights into the group structure of the network beyond the main three-community division. \n\nNote that further subdivisions of the network split the more loosely structured community in the upper-right, while the more densely linked community in the lower-left remains largely undisturbed. \n\n\n## Spectral Clustering Using the Eigenvectors of the *Normalized* Laplacian\n\nWe saw how to cluster a network in a way that results in a good community partition using the Laplacian of the adjacency matrix $\\mathbf{L}$. Another approach is to use a (degree) normalized version of the same matrix $\\mathbf{\\hat{L}}$, defined as follows:\n\n$$\n\\mathbf{\\hat{L}} = \\mathbf{I} - \\mathbf{D}^{-\\frac{1}{2}}\\mathbf{A}\\mathbf{D}^{-\\frac{1}{2}}\n$$\n\nWhere everything else is as before and $\\mathbf{I}$ is the identity matrix (an $n \\times n$ matrix with ones along the diagonals and zero everywhere else), and $\\mathbf{D}^{-\\frac{1}{2}}$ is a matrix containing the inverse of the square root of degrees of each node ($1/\\sqrt{k_i}$) in the diagonals and zeros everywhere else. \n\nWe can just adapt our previous `ratio.cut` function code to perform this new job:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   norm.ratio.cut <- function(x, k = 6) {\n         A <- as.matrix(as_adjacency_matrix(x))\n         n <- vcount(x)\n         I <- diag(1, n, n)\n         D <- diag(1/sqrt(degree(x)))\n         L <- I - (D %*% A %*% D)\n         eig.L <- eigen(L)\n         b <- n - k\n         e <- n - 1\n         U <- matrix(eig.L$vectors[, b:e], nrow = n, ncol = k)\n         if (k > 1) {\n            U <- U[, k:1] #re-ordering columns from smallest to biggest\n            }         \n         for (i in 1:nrow(U)) {\n            U[i, ] <- U[i, ]/norm(as.matrix(U[i, ]), type = \"E\")\n            }\n      return(U)\n   }\n```\n:::\n\n\nWhere we just have to modify the way we define the $\\mathbf{D}$ and $\\mathbf{L}$ matrices in lines 5 and 6 respectively, after creating the $\\mathbf{I}$ matrix in line 4. \n\nNow let's see if the normalized cut can help us finds some communities:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   U <- norm.ratio.cut(g)\n   clus <- k.cuts(U)\n   mod.res <- mod.check(g, clus)\n   round(mod.res, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    2     3     4     5     6     7     8     9    10 \n0.250 0.316 0.320 0.352 0.350 0.331 0.293 0.284 0.246 \n```\n:::\n:::\n\n::: {#fig-friends2 .cell layout-ncol=\"2\"}\n::: {.cell-output-display}\n![Five Community Solution](spectral_files/figure-html/fig-friends2-1.png){#fig-friends2-1 width=1152}\n:::\n\n::: {.cell-output-display}\n![Six Community Solution](spectral_files/figure-html/fig-friends2-2.png){#fig-friends2-2 width=1152}\n:::\n\nClustering of Nodes in the Law Firm Friendship Network Using the Normalized Ratio Cut\n:::\n\n\nAs we can see, the normalized ratio cut approach performs almost as well as the ratio cut approach in terms of the maximum modularity it finds ($Q = 0.35$), but suggests a finer grained partition, with the maximum at either five or six communities. \n\nThe resulting node clusters are shown in @fig-friends2-1 and @fig-friends2-2. \n\n## Spectral Clustering Using the Eigenvectors of the *Degree-Normalized* Laplacian\n\nAn alternative version of the normalized Laplacian is given by:\n\n$$\n\\mathbf{\\bar{L}} = \\mathbf{D}^{-1}\\mathbf{L}\n$$\n\nWhich is just the original Laplacian as defined earlier with each entry divided by the degree of the corresponding node in that row. \n\nA function that extracts the relevant eigenvectors of this version of the normalized Laplacian goes as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   d.norm.ratio.cut <- function(x, k = 6) {\n         A <- as.matrix(as_adjacency_matrix(x))\n         n <- vcount(x)\n         I <- diag(1, n, n)\n         D <- diag(degree(x))\n         L <- solve(D) %*% (D - A)\n         eig.L <- eigen(L)\n         b <- n - k\n         e <- n - 1\n         U <- matrix(eig.L$vectors[, b:e], nrow = n, ncol = k)\n         if (k > 1) {\n            U <- U[, k:1] #re-ordering columns from smallest to biggest\n            }         \n         for (i in 1:nrow(U)) {\n            U[i, ] <- U[i, ]/norm(as.matrix(U[i, ]), type = \"E\")\n            }\n      return(U)\n   }\n```\n:::\n\n\nWhere, once again, we only need to modify a couple of lines (5 and 6) from before to compute the new version of $\\mathbf{L}$. \n\nTo see the quality of the partitions obtained via this method, we just type:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   clus <- k.cuts(d.norm.ratio.cut(g))\n   mod.res <- mod.check(g, clus)\n   round(mod.res, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    2     3     4     5     6     7     8     9    10 \n0.250 0.355 0.339 0.327 0.340 0.331 0.294 0.287 0.252 \n```\n:::\n:::\n\n\nThe degree-normalized Laplacian once again prefers the three-community partition, but also shows that the *six* community partition produces a high-quality clustering. Here's how those look like:\n\n\n::: {#fig-friends3 .cell layout-ncol=\"2\"}\n::: {.cell-output-display}\n![Three Community Solution](spectral_files/figure-html/fig-friends3-1.png){#fig-friends3-1 width=1152}\n:::\n\n::: {.cell-output-display}\n![Six Community Solution](spectral_files/figure-html/fig-friends3-2.png){#fig-friends3-2 width=1152}\n:::\n\nClustering of Nodes in the Law Firm Friendship Network Using the Degree-Normalized Ratio Cut\n:::\n\n\n## Clustering Using the Eigenvectors of the Modularity Matrix\n\nAs noted by @fender_etal17 [p. 1796], we can extend the spectral clustering approach based on the Laplacian and the normalized Laplacian to the modularity matrix $\\mathbf{B}$. That is, we cluster the graph by embedding the nodes in a set of dimensions defined by the eigendecomposition of $\\mathbf{B}$.\n\nThe main difference is that rather than using the eigenvectors corresponding to the *smallest* eigenvalues (as we did with $\\mathbf{L}$) we proceed in more typical fashion (as done with PCA and CA) and choose the eigenvectors corresponding to the *largest* ones. \n\nThis approach, once again, only requires small modifications to the one we used for the Laplacian:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   mod.mat.cut <- function(x, k = 2) {\n         A <- as.matrix(as_adjacency_matrix(x))\n         n <- vcount(x)\n         d <- as.matrix(degree(x))\n         B <- A - (d %*% t(d))/sum(A)\n         eig.B <- eigen(B)\n         U <- eig.B$vectors[, 1:k]\n         for (i in 1:nrow(U)) {\n            U[i, ] <- U[i, ]/norm(as.matrix(U[i, ]), type = \"E\")\n            }\n      return(U)\n   }\n```\n:::\n\n\nThe key difference here is that we compute the modularity matrix $\\mathbf{B}$ rather than $\\mathbf{L}$ from the adjacency matrix $\\mathbf{A}$ in line 5; we then plug $\\mathbf{B}$ into the `eigen` function and proceed with normalizing in the same way as before. \n\nAnother difference is that rather than using a large number of eigenvalues (e.g., $k = 6$), as we did when we were picking from the smallest ones, we now go for parsimony and pick a small rank (two-dimensional) representation of the original modularity matrix ($k = 2$).\n\nLet's see how this works:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   U <- mod.mat.cut(g)\n   clus <- k.cuts(U)\n   mod.res <- mod.check(g, clus)\n   round(mod.res, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    2     3     4     5     6     7     8     9    10 \n0.264 0.366 0.331 0.317 0.306 0.304 0.224 0.187 0.166 \n```\n:::\n:::\n\n\nWe can see that the three-cluster solution does really well modularity-wise ($Q = 0.37$), however, the four cluster solution also seems promising. The resulting communities are shown in @fig-friends3-1 and @fig-friends3-2. \n\n\n\n::: {#fig-friends4 .cell layout-ncol=\"2\"}\n::: {.cell-output-display}\n![Three Community Solution](spectral_files/figure-html/fig-friends4-1.png){#fig-friends4-1 width=1152}\n:::\n\n::: {.cell-output-display}\n![Four Community Solution](spectral_files/figure-html/fig-friends4-2.png){#fig-friends4-2 width=1152}\n:::\n\nClustering of Nodes in the Law Firm Friendship Network Using the Spectral Decomposition of the Modularity Matrix.\n:::\n\n\n## Spectral Clustering of Two-Mode Networks \n\nWe can use a variant of the spectral clustering approach to find multiple communities in two-mode networks [@wu_etal22]. This approach combines Correspondence Analysis (CA)---which we covered in the previous handout---and k-means clustering on multiple  dimensions. \n\nSo let's bring back our old friend, the Southern Women data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   g <- southern_women #southern women data\n   A <- as.matrix(as_biadjacency_matrix(g)) #bi-adjacency matrix\n   A2 <- as.matrix(as_adjacency_matrix(g)) #bipartite adjacency matrix\n```\n:::\n\n\nLet's also compute the bi-adjacency modularity matrix:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   dp <- as.matrix(rowSums(A))\n   dg <- as.matrix(colSums(A))\n   dpdg <- dp %*% t(dg) #person x group degree product matrix\n   B <- A - dpdg/sum(A)\n   round(B, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10  2/23\nEVELYN     0.73  0.73  0.46  0.64  0.28  0.28 -0.90 -0.26 -0.08 -0.45 -0.36\nLAURA      0.76  0.76  0.53 -0.31  0.37  0.37  0.21 -0.10 -0.94 -0.39 -0.31\nTHERESA   -0.27  0.73  0.46  0.64  0.28  0.28  0.10 -0.26 -0.08 -0.45 -0.36\nBRENDA     0.76 -0.24  0.53  0.69  0.37  0.37  0.21 -0.10 -0.94 -0.39 -0.31\nCHARLOTTE -0.13 -0.13  0.73  0.82  0.64 -0.36  0.55 -0.63 -0.54 -0.22 -0.18\nFRANCES   -0.13 -0.13  0.73 -0.18  0.64  0.64 -0.45  0.37 -0.54 -0.22 -0.18\nELEANOR   -0.13 -0.13 -0.27 -0.18  0.64  0.64  0.55  0.37 -0.54 -0.22 -0.18\nPEARL     -0.10 -0.10 -0.20 -0.13 -0.27  0.73 -0.34  0.53  0.60 -0.17 -0.13\nRUTH      -0.13 -0.13 -0.27 -0.18  0.64 -0.36  0.55  0.37  0.46 -0.22 -0.18\nVERNE     -0.13 -0.13 -0.27 -0.18 -0.36 -0.36  0.55  0.37  0.46 -0.22 -0.18\nMYRNA     -0.13 -0.13 -0.27 -0.18 -0.36 -0.36 -0.45  0.37  0.46  0.78 -0.18\nKATHERINE -0.20 -0.20 -0.40 -0.27 -0.54 -0.54 -0.67  0.06  0.19  0.66 -0.27\nSYLVIA    -0.24 -0.24 -0.47 -0.31 -0.63 -0.63  0.21 -0.10  0.06  0.61 -0.31\nNORA      -0.27 -0.27 -0.54 -0.36 -0.72  0.28  0.10 -1.26 -0.08  0.55  0.64\nHELEN     -0.17 -0.17 -0.34 -0.22 -0.45 -0.45  0.44  0.21 -0.67  0.72  0.78\nDOROTHY   -0.07 -0.07 -0.13 -0.09 -0.18 -0.18 -0.22  0.69  0.73 -0.11 -0.09\nOLIVIA    -0.07 -0.07 -0.13 -0.09 -0.18 -0.18 -0.22 -0.31  0.73 -0.11  0.91\nFLORA     -0.07 -0.07 -0.13 -0.09 -0.18 -0.18 -0.22 -0.31  0.73 -0.11  0.91\n            4/7 11/21   8/3\nEVELYN    -0.54 -0.27 -0.27\nLAURA     -0.47 -0.24 -0.24\nTHERESA   -0.54 -0.27 -0.27\nBRENDA    -0.47 -0.24 -0.24\nCHARLOTTE -0.27 -0.13 -0.13\nFRANCES   -0.27 -0.13 -0.13\nELEANOR   -0.27 -0.13 -0.13\nPEARL     -0.20 -0.10 -0.10\nRUTH      -0.27 -0.13 -0.13\nVERNE      0.73 -0.13 -0.13\nMYRNA      0.73 -0.13 -0.13\nKATHERINE  0.60  0.80  0.80\nSYLVIA     0.53  0.76  0.76\nNORA       0.46  0.73  0.73\nHELEN      0.66 -0.17 -0.17\nDOROTHY   -0.13 -0.07 -0.07\nOLIVIA    -0.13 -0.07 -0.07\nFLORA     -0.13 -0.07 -0.07\n```\n:::\n:::\n\n\nGreat! Now, from this information we can compute a version of the bipartite modularity matrix:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   n <- nrow(A) + ncol(A)\n   Np <- nrow(A)\n   names <- c(rownames(A), colnames(A))\n   B2 <- matrix(0, n, n) #all zeros matrix of dimensions (p + g) X (p + g)\n   B2[1:Np, (Np + 1):n] <- B #putting B in the top right block\n   B2[(Np + 1):n, 1:Np] <- t(B) #putting B transpose in the lower-left block\n   rownames(B2) <- names\n   colnames(B2) <- names\n```\n:::\n\n\nAnd now let's find the CA scores. This time will use the canned function `CA` from the the package `FactoMineR`\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   #install.packages(\"FactoMineR\")\n   library(FactoMineR)\n   CA.res <- CA(A, graph = FALSE, ncp = 10)\n```\n:::\n\n\nWhich computes CA directly on the bi-adjacency matrix (the argument `ncp` asks to keep the first ten dimensions). \n\nWe can now extract the CA scores for persons and groups from the resulting object:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   eig.vec.p <- CA.res$row$coord\n   eig.vec.g <- CA.res$col$coord\n   head(eig.vec.p)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n               Dim 1       Dim 2       Dim 3        Dim 4       Dim 5\nEVELYN    -0.7994396 -0.11278306  0.12856965  0.491615655  0.36993238\nLAURA     -0.8426887  0.03973055  0.11862978  0.286643489  0.10696165\nTHERESA   -0.6538505 -0.08107422  0.03285721  0.066653892  0.11835519\nBRENDA    -0.8552592  0.05084420  0.26039689 -0.082978126  0.05028130\nCHARLOTTE -0.9735517  0.03683948  0.66023345 -0.774105247  0.08510784\nFRANCES   -0.7973597  0.05794469 -0.20558235 -0.001077288 -0.59307835\n                 Dim 6        Dim 7       Dim 8         Dim 9      Dim 10\nEVELYN    -0.007414199 -0.007447136 -0.02256155 -0.0160061655  0.08340311\nLAURA      0.519058804  0.358983310  0.06243870  0.1954326939 -0.10193085\nTHERESA   -0.238792769  0.037114413  0.44547214 -0.1638696977  0.04645662\nBRENDA     0.060849392 -0.093559773 -0.53495958 -0.1262516982  0.02818219\nCHARLOTTE -0.721861078 -0.222554541  0.02152725 -0.0001022383 -0.04432666\nFRANCES    0.178454033 -0.648007177  0.12713961  0.3578859472 -0.29201035\n```\n:::\n\n```{.r .cell-code}\n   head(eig.vec.g)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          Dim 1        Dim 2       Dim 3        Dim 4       Dim 5      Dim 6\n6/27 -1.0510521 -0.013102803  0.40045025  0.624502007  0.53693139  0.6062955\n3/2  -0.9662871 -0.090934069  0.22094083  0.758901614  0.60626508  0.2889617\n4/12 -1.0357695 -0.002506996  0.39252639 -0.005949514  0.07005287 -0.1110437\n9/26 -1.0359803 -0.046981456  0.64023822 -0.201296127  0.47641399 -0.7205873\n2/25 -0.8840034  0.002527085  0.00616034 -0.304708001 -0.31330029 -0.1008760\n5/19 -0.5723848 -0.007363883 -0.15907563  0.336653678 -0.55713599  0.3565649\n            Dim 7       Dim 8       Dim 9       Dim 10\n6/27  0.341243814 -0.78585220  0.09303009  0.022211664\n3/2   0.514095893  0.77040262  0.02721689  0.064255000\n4/12 -0.380608237  0.07861703  0.21614264 -0.322353047\n9/26 -0.284177968 -0.10776494 -0.40181489  0.196215569\n2/25  0.009376108  0.04679332  0.31794275  0.121060750\n5/19 -0.215101435  0.05292157 -0.34808611  0.004513752\n```\n:::\n:::\n\n\nGreat! You can verify that these are the same scores we obtained in the last handout via a more elaborate route.\n\nNow, we can just create our `U` matrix by stacking the person and group scores using the first three dimensions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   U <- rbind(eig.vec.p, eig.vec.g)[, 1:3]\n   head(U)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n               Dim 1       Dim 2       Dim 3\nEVELYN    -0.7994396 -0.11278306  0.12856965\nLAURA     -0.8426887  0.03973055  0.11862978\nTHERESA   -0.6538505 -0.08107422  0.03285721\nBRENDA    -0.8552592  0.05084420  0.26039689\nCHARLOTTE -0.9735517  0.03683948  0.66023345\nFRANCES   -0.7973597  0.05794469 -0.20558235\n```\n:::\n\n```{.r .cell-code}\n   tail(U)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          Dim 1      Dim 2       Dim 3\n4/8   0.5140165 -0.4896890 -0.47959026\n6/10  1.1173628  0.5729577  0.23464730\n2/23  1.2219952 -2.0539686  0.69618973\n4/7   1.0223902  0.5159415 -0.04625319\n11/21 1.1742556  0.9078702  0.66121084\n8/3   1.1742556  0.9078702  0.66121084\n```\n:::\n:::\n\n\nNice! Now we can just feed `U` to our `k.cuts` function to place persons and groups into cluster assignments beginning with two and ending with ten:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   clus <- k.cuts(U)\n```\n:::\n\n\nOf course, we can't use the `mod.check` function we used earlier because that uses the standard method for checking the modularity in one-mode networks and doesn't take into account the structural zeros in the bipartite graph. \n\nSo we need to come up with a custom method to check the modularity for the bipartite case. \n\nFirst, we need a function that takes a cluster assignment vector containing numbers for each cluster $k = \\{1, 2, 3, \\ldots C\\}$ and turns it into a dummy coded cluster assignment matrix:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   make.dummies <- function(x) {\n      vals <- unique(x)\n      U <- matrix(0, length(x), length(vals))\n      for (k in vals) {\n         U[, k] <- as.numeric(x == k)\n      }\n   return(U)\n   }\n```\n:::\n\n\nLet's test it out:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   make.dummies(clus[[3]])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      [,1] [,2] [,3] [,4]\n [1,]    0    1    0    0\n [2,]    0    1    0    0\n [3,]    0    1    0    0\n [4,]    0    1    0    0\n [5,]    0    1    0    0\n [6,]    0    1    0    0\n [7,]    0    1    0    0\n [8,]    0    0    0    1\n [9,]    0    0    0    1\n[10,]    0    0    0    1\n[11,]    0    0    0    1\n[12,]    1    0    0    0\n[13,]    1    0    0    0\n[14,]    1    0    0    0\n[15,]    1    0    0    0\n[16,]    0    0    0    1\n[17,]    0    0    1    0\n[18,]    0    0    1    0\n[19,]    0    1    0    0\n[20,]    0    1    0    0\n[21,]    0    1    0    0\n[22,]    0    1    0    0\n[23,]    0    1    0    0\n[24,]    0    1    0    0\n[25,]    0    1    0    0\n[26,]    0    0    0    1\n[27,]    0    0    0    1\n[28,]    1    0    0    0\n[29,]    0    0    1    0\n[30,]    1    0    0    0\n[31,]    1    0    0    0\n[32,]    1    0    0    0\n```\n:::\n:::\n\n\nGreat! Looks like it works. \n\nFinally, we need to write a custom function for bipartite modularity checking across different assignments:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   mod.check2 <- function(x, c, w) {\n      k <- length(c)\n      m <- rep(0, k)\n      for (i in 1:k) {\n         u <- make.dummies(c[[i]])\n         m[i] <- sum(diag(t(u) %*% x %*% u))/sum(w)\n         }\n   names(m) <- 2:(k+1)\n   return(m)\n   }\n```\n:::\n\n\nThe function `mod.check2` needs three inputs: The bipartite modularity matrix, a list with different assignments of the nodes in the bipartite graph to different clusters, and the bipartite adjacency matrix. It returns a vector `m` with the modularities of each of the partitions in the list `c`. \n\nAnd, now, for the big reveal:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   round(mod.check2(B2, clus, A2), 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    2     3     4     5     6     7     8     9    10 \n0.318 0.306 0.338 0.257 0.248 0.224 0.190 0.184 0.155 \n```\n:::\n:::\n\n\nLooks like the spectral clustering results favor a four-community partition although the more parsimonious three and binary community partitions also look pretty good. \n\n@fig-women-1 and @fig-women-2 show a plot of the three and four community solutions according to the CA dimensions (since we already saw the binary partition in the CA handout).\n\nOf course, just like we did with one-mode networks, we can also obtain a spectral clustering directly from the eigenvectors of the bipartite modularity matrix in just a couple of lines:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   clusB <- k.cuts(eigen(B2)$vectors[, 1:2])\n   round(mod.check2(B2, clusB, A2), 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    2     3     4     5     6     7     8     9    10 \n0.319 0.334 0.271 0.216 0.179 0.144 0.133 0.101 0.057 \n```\n:::\n:::\n\n\nHere we create the `U` matrix from the first two dimensions of the eigendecomposition of the bipartite modularity matrix. The results suggest that the three community partition is optimal, although the two-community one also does well. The corresponding splits are shown in @fig-women-3 and @fig-women-4. \n\nNote that the main difference between CA and modularity based clustering in two-mode networks is that modularity seems to prefer evenly balanced communities (in terms of number of nodes), while CA does not mind grouping nodes into small communities (like $\\{Flora, Nora, 2/23\\}$)\n\n\n\n::: {#fig-women .cell layout-ncol=\"2\"}\n::: {.cell-output-display}\n![Three Community Solution (CA)](spectral_files/figure-html/fig-women-1.png){#fig-women-1 width=1152}\n:::\n\n::: {.cell-output-display}\n![Four Community Solution (CA)](spectral_files/figure-html/fig-women-2.png){#fig-women-2 width=1152}\n:::\n\n::: {.cell-output-display}\n![Two Community Solution (Bipartite Modularity)](spectral_files/figure-html/fig-women-3.png){#fig-women-3 width=1152}\n:::\n\n::: {.cell-output-display}\n![Three Community Solution (Bipartite Modularity)](spectral_files/figure-html/fig-women-4.png){#fig-women-4 width=1152}\n:::\n\nSpectral Clustering of Nodes in the Southern Women Data.\n:::\n\n\n## Bipartite Modularity Allowing People and Groups to Belong to Different Number of Communities\n\nOne limitation of Barber's approach to computing the modularity is that it can only be used under the assumption that the number of communities is the *same* for both persons and groups. \n\nHowever, it could be that the optimal partition is actually one in which the people node set is split into a different number of clusters than the group node set. \n\nSo we need a way to evaluate the modularity of a partition when we have different number of communities on the people and group side. \n\nHere's how to do it.\n\nFirst, we generate two separate candidate community assignments for people and groups via spectral clustering from CA using the first six eigenvectors:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   k.cuts.p <- k.cuts(CA.res$row$coord[, 1:6])\n   k.cuts.g <- k.cuts(CA.res$col$coord[, 1:6])\n```\n:::\n\n\nAs an example, let's pick the solution that partitions people into four communities and the groups into three communities:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   C.p <- k.cuts.p[[3]]\n   C.g <- k.cuts.g[[2]]\n```\n:::\n\n\nGiven this information, we can create a $4 \\times 3$ matrix recording the proportion of ties in the network that go from person-community $l$ to group-community $m$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   e <- matrix(0, 4, 3)\n   for (l in 1:4) {\n      for (m in 1:3) {\n         e[l, m] = sum(A[C.p == l, C.g == m]) * 1/sum(A)\n      }\n   }\n   round(e, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,] 0.19 0.02 0.17\n[2,] 0.00 0.00 0.04\n[3,] 0.00 0.02 0.02\n[4,] 0.00 0.00 0.53\n```\n:::\n:::\n\n\nFor instance, this matrix says that 53% of the ties in the Southern women data go from people in the fourth community to groups in the third community, according to the CA spectral partition. \n\n@suzuki_wakita09, building on work by @murata09, suggest using the `e` matrix above to compute the modularity of any pair of person/group community assignment according to the following formula:\n\n$$\nQ = \\frac{1}{2}\\sum_{l, m}\\frac{e_{lm}}{e_{l+}}\\left(e_{lm} - e_{l+}e_{+m}\\right)\n$$\n\nWhere $e_{lm}$ is the proportion of edges connecting people in the $l^{th}$ person-community to groups in the $m^{th}$ event-community, $e_{l+}$ is the proportion of edges originating from person-community $i$ (the corresponding entry of the row sum of `e`), and $e_{+m}$ is the proportion of edges originating from nodes in group-community $j$ (the corresponding entry of the column sum of `e`).\n\nSo the idea is that given a partition of the person nodes into $L$ communities and a partition of the group nodes into $M$ communities, we can generate an `e` matrix like the one above and compute the corresponding modularity of that person/group partition using the above equation.\n\nHere's a function that computes the `e` matrix from a pair of person/group partitions and then returns the $Q$ value corresponding to that matrix using the above formula:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   find.mod <- function(w, x, y) {\n      Cx <- max(x)\n      Cy <- max(y)\n      M <- sum(w)\n      e <- matrix(0, Cx, Cy)\n      for (l in 1:Cx) {\n         for (m in 1:Cy) {\n            e[l, m] = sum(w[x == l, y == m]) * 1/M\n         }\n      }\n      Q <- 0\n      a <- rowSums(e)\n      b <- colSums(e)\n      for (l in 1:Cx) {\n         for (m in 1:Cy) {\n            Q <- Q + (e[l, m]/a[l] * (e[l, m] - a[l]*b[m]))\n         }\n      }\n   return(Q/2)\n   }\n```\n:::\n\n\nSo for the `e` matrix from the above example, $Q$ would be:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   find.mod(A,  k.cuts.p[[3]],  k.cuts.g[[2]])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.07220939\n```\n:::\n:::\n\n\nWhich looks like a positive number. But is it the biggest of all the possible community partition combinations between people and groups?\n\nTo answer this question, we can use the `find.mod` function to compute the modularity between *every pair* of partitions between people and groups that we calculated earlier. Since we computed eight different partitions for people and groups this leads to $8 \\times 8 = 64$ pairs.\n\nHere's a wrapper function over `find.mod` that computes the corresponding modularity values for each partition pair:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   mod.mat <- function(d) {\n     q <- matrix(0, d, d)\n      for (i in 1:d) {\n         for (j in 1:d) {\n               q[i, j] <- find.mod(A,  k.cuts.p[[i]],  k.cuts.g[[j]])\n         }\n      }\n    return(q)\n   }\n   Q <- round(mod.mat(8), 3)\n   Q\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]\n[1,] 0.102 0.058 0.042 0.031 0.032 0.033 0.029 0.024\n[2,] 0.106 0.066 0.050 0.038 0.039 0.041 0.035 0.033\n[3,] 0.104 0.072 0.056 0.042 0.045 0.043 0.037 0.039\n[4,] 0.112 0.074 0.069 0.055 0.058 0.056 0.050 0.046\n[5,] 0.112 0.075 0.069 0.058 0.061 0.060 0.054 0.050\n[6,] 0.110 0.074 0.075 0.064 0.061 0.063 0.058 0.054\n[7,] 0.111 0.077 0.077 0.066 0.067 0.068 0.062 0.059\n[8,] 0.111 0.077 0.078 0.067 0.067 0.070 0.064 0.060\n```\n:::\n:::\n\n\nInterestingly, the analysis suggests that the maximum modularity $Q = 0.112$ is obtained with a partition of people into five communities and groups into two communities corresponding to cells $(4, 1)$ of the above matrix.\n\nHere's what this community assignment looks like in the Southern Women data:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Spectral Clustering of Nodes in the Southern Women Data with Optimal Community Assignment Obtained via the Suzuki Modularity, with Five Communities for People, Two Communities for Groups.](spectral_files/figure-html/unnamed-chunk-37-1.png){width=1152}\n:::\n:::\n\n\nThe analysis separates two groups of densely connected actors of size six and five, respectively, namely, $\\{Brenda, Theresa, Laura, Frances, Evelyn, Eleanor\\}$ and $\\{Katherine, Nora, Sylvia, Myrna, Helen\\}$ along with their corresponding events from the one another. In the same way, $\\{Pearl, Dorothy, Ruth, Verne\\}$ form a community of more peripheral actors who selectively attend the more popular events; $\\{Flora, Olivia\\}$ are a two-actor community occupying the most peripheral position. Among the core set of actors, $\\{Charlotte\\}$ occupies a singleton-community of her own. \n\nEvents are partitioned into two broad groups: One the one hand, we have those selectively selectively attended by the larger community of densely connected actors along with the most popular events; on the other hand, we have the events selectively attended by the smaller group of densely connected actors. \n\n\nIn this handout we have seen that spectral clustering via the Laplacian, normalized Laplacian or the modularity matrix (and CA or the modularity in the bipartite case) produces high-quality partitions, highlighting the core groups in the data. It is a simple and easy to implement option to add to your arsenal. \n",
    "supporting": [
      "spectral_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}