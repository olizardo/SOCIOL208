{
  "hash": "aaa9eb048d392d881e5e2f9c1f3ffff1",
  "result": {
    "markdown": "---\ntitle: \"Local Vertex Similarities\"\nexecute: \n  eval: true\n  echo: true\n  output: true\n  warning: false\n  message: false\nformat: \n   html:\n      code-line-numbers: true\n---\n\n\nAs we noted in the role equivalence handout, for odd reasons, classical approaches to structural similarity in networks used *distance* metrics but did not measure similarity directly. More recent approaches from network and information science prefer to define vertex similarity using direct similarity measures based on local structural characteristics, like node neighborhoods.\n\nMathematically, similarity is a less stringent (but also less well-defined compared to distances) relation between pairs of nodes in a graph than distance, so it can be easier to work with in most applications. \n\nFor instance, similarity is required to be symmetric ($s_{ij} = s_{ji}$ for all $i$ and $j$) and most metrics have reasonable bounds (e.g., 0 for least similar and 1.0 for maximally similar). Given such a bounded similarity we can get to *dissimilarity* by subtracting one: $d_{ij} = 1 - s_{ij}$\n\n## Basic Ingredients of Vertex Similarity Metrics\n\nConsider two nodes and the set of nodes that are the immediate neighbors to each. In this case, most vertex similarity measures will make use of three pieces of information:\n\n1. The number of common neighbors $p$.\n\n1. The number of actors $q$ who are connected to node $i$ but not to node $j$.\n\n1. The number of actors $r$ who are connected to node $j$ but not to node $i$.\n\nIn the simplest case of the binary undirected graph then these are given by:\n\n$$\n   p = \\sum_{k = 1}^{n} a_{ik} a_{jk}\n$$\n\n$$\n   q = \\sum_{k = 1}^{n} a_{ik} (1 - a_{jk})\n$$\n\n$$\n   r = \\sum_{k = 1}^{n} (1- a_{ik}) a_{jk}\n$$\n\nIn matrix form:\n\n$$\n   \\mathbf{A}(p) = \\mathbf{A} \\mathbf{A} = \\mathbf{A}^2\n$$\n\n$$\n   \\mathbf{A}(q) = \\mathbf{A} (1 - \\mathbf{A})\n$$\n\n$$\n   \\mathbf{A}(r) = (1 - \\mathbf{A}) \\mathbf{A}\n$$\n\nLet's look at an example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   library(networkdata)\n   library(igraph)\n   library(stringr) #using stringr to change names from all caps to title case\n   g <- movie_267\n   V(g)$name <- str_to_title(V(g)$name)\n   g <- delete_vertices(g, degree(g) <= 3) #deleting low degree vertices\n   A <- as.matrix(as_adjacency_matrix(g))\n   A.p <- A %*% A #common neighbors matrix\n   A.q <- A %*% (1 - A) #neighbors of i not connected to j\n   A.r <- (1 - A) %*% A #neighbors of j not connected to i\n   A.p[1:10, 1:10]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             Bam-Bam Barney Betty Feldspar Fred Headmistress Herdmaster Lava\nBam-Bam            6      4     5        2    5            2          2    5\nBarney             4     13     9        1   11            5          3    7\nBetty              5      9    13        2    9            3          4    8\nFeldspar           2      1     2        2    1            0          2    2\nFred               5     11     9        1   14            4          3    9\nHeadmistress       2      5     3        0    4            5          0    3\nHerdmaster         2      3     4        2    3            0          4    4\nLava               5      7     8        2    9            3          4   11\nLeach              2      3     3        1    2            2          1    2\nMorris             2      3     2        0    2            3          0    2\n             Leach Morris\nBam-Bam          2      2\nBarney           3      3\nBetty            3      2\nFeldspar         1      0\nFred             2      2\nHeadmistress     2      3\nHerdmaster       1      0\nLava             2      2\nLeach            3      1\nMorris           1      3\n```\n:::\n\n```{.r .cell-code}\n   A.q[1:10, 1:10]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             Bam-Bam Barney Betty Feldspar Fred Headmistress Herdmaster Lava\nBam-Bam            0      2     1        4    1            4          4    1\nBarney             9      0     4       12    2            8         10    6\nBetty              8      4     0       11    4           10          9    5\nFeldspar           0      1     0        0    1            2          0    0\nFred               9      3     5       13    0           10         11    5\nHeadmistress       3      0     2        5    1            0          5    2\nHerdmaster         2      1     0        2    1            4          0    0\nLava               6      4     3        9    2            8          7    0\nLeach              1      0     0        2    1            1          2    1\nMorris             1      0     1        3    1            0          3    1\n             Leach Morris\nBam-Bam          4      4\nBarney          10     10\nBetty           10     11\nFeldspar         1      2\nFred            12     12\nHeadmistress     3      2\nHerdmaster       3      4\nLava             9      9\nLeach            0      2\nMorris           2      0\n```\n:::\n\n```{.r .cell-code}\n   A.r[1:10, 1:10]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             Bam-Bam Barney Betty Feldspar Fred Headmistress Herdmaster Lava\nBam-Bam            0      9     8        0    9            3          2    6\nBarney             2      0     4        1    3            0          1    4\nBetty              1      4     0        0    5            2          0    3\nFeldspar           4     12    11        0   13            5          2    9\nFred               1      2     4        1    0            1          1    2\nHeadmistress       4      8    10        2   10            0          4    8\nHerdmaster         4     10     9        0   11            5          0    7\nLava               1      6     5        0    5            2          0    0\nLeach              4     10    10        1   12            3          3    9\nMorris             4     10    11        2   12            2          4    9\n             Leach Morris\nBam-Bam          1      1\nBarney           0      0\nBetty            0      1\nFeldspar         2      3\nFred             1      1\nHeadmistress     1      0\nHerdmaster       2      3\nLava             1      1\nLeach            0      2\nMorris           2      0\n```\n:::\n:::\n\n\nNote that while $\\mathbf{A}(p)$ is necessarily symmetric, neither $q$ nor $r$ have to be. Barney has many more neighbors that Bam-Bam is not connected to than vice versa. Also note that the $\\mathbf{A}(r)$ matrix is just the transpose of the $\\mathbf{A}(q)$ matrix in the undirected case. \n\nSo the most obvious measure of similarity between two nodes is simply the number of common neighbors [@leicht_etal06]:\n\n$$\n   s_{ij} = p_{ij}\n$$\n\nWe have already seen a version of this in the directed case when talking about the HITS algorithm [@kleinberg99], which computes a spectral (eigenvector-based) ranking based on the matrices of common in and out-neighbors in a directed graph. \n\n$$\n   p^{in}_{ij} = \\sum_{k = 1}^{n} a_{ki} a_{kj}\n$$\n\n$$\n   p^{out}_{ij} = \\sum_{k = 1}^{n} a_{ik} a_{jk}\n$$\n\nWhich in matrix form is:\n\n$$\n   \\mathbf{A}(p^{out}) = \\mathbf{A}^T \\mathbf{A}\n$$\n\n$$\n   \\mathbf{A}(p^{in}) = \\mathbf{A} \\mathbf{A}^T\n$$\n\nIn this case, similarity can be measured *either* by the number of common in-neighbors or the number of common out-neighbors. \n\nIf the network under consideration is a (directed) **citation network** with nodes being papers and links between papers defined as a citation from paper $i$ to paper $j$, then the number of *common in-neighbors* between two papers is their **co-citation** similarity (the number of other papers that cite both papers), and the number of *common out-neighbors* is their **bibliographic coupling** similarity (the overlap in their list of references).\n\nOne problem with using unbounded quantities like the sheer number of common (in or out) neighbors to define node similarity is that they are only limited by the number of nodes in the network [@leicht_etal06]. Thus, an actor with many neighbors will end up having lots of other neighbors in common with lots of other nodes, which will mean we would count them as \"similar\" to almost everyone.\n\n## Normalized Similarity Metrics\n\nNormalized similarity metrics deal with this issue by adjusting the raw similarity based on $p$ using the number of non-shared neighbors $q$ and $r$. \n\nThe two most popular versions of normalized vertex similarity scores are the **Jaccard index** and the **cosine similarity** (sometimes also referred to as the **Salton Index**). \n\nThe Jacccard index is given by:\n\n$$\n   s_{ij} = \\frac{p}{p + q + r}\n$$ {#eq-jac}\n\nWhich is the ratio of the size of the *intersection* of the neighborhoods of the two nodes (number of common neighbors) divided by the size of the *union* of the two neighborhoods. \n\nWhen the neighborhoods of the two nodes coincide (e.g., $q = 0$ and $r = 0$) then @eq-jac turns into $\\frac{p}{p} = 1.0$. \n\nIn our example, we can compute the Jaccard similarity as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   J <- A.p / (A.p + A.q + A.r)\n   round(J[1:10, 1:10], 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             Bam-Bam Barney Betty Feldspar Fred Headmistress Herdmaster Lava\nBam-Bam         1.00   0.27  0.36     0.33 0.33         0.22       0.25 0.42\nBarney          0.27   1.00  0.53     0.07 0.69         0.38       0.21 0.41\nBetty           0.36   0.53  1.00     0.15 0.50         0.20       0.31 0.50\nFeldspar        0.33   0.07  0.15     1.00 0.07         0.00       0.50 0.18\nFred            0.33   0.69  0.50     0.07 1.00         0.27       0.20 0.56\nHeadmistress    0.22   0.38  0.20     0.00 0.27         1.00       0.00 0.23\nHerdmaster      0.25   0.21  0.31     0.50 0.20         0.00       1.00 0.36\nLava            0.42   0.41  0.50     0.18 0.56         0.23       0.36 1.00\nLeach           0.29   0.23  0.23     0.25 0.13         0.33       0.17 0.17\nMorris          0.29   0.23  0.14     0.00 0.13         0.60       0.00 0.17\n             Leach Morris\nBam-Bam       0.29   0.29\nBarney        0.23   0.23\nBetty         0.23   0.14\nFeldspar      0.25   0.00\nFred          0.13   0.13\nHeadmistress  0.33   0.60\nHerdmaster    0.17   0.00\nLava          0.17   0.17\nLeach         1.00   0.20\nMorris        0.20   1.00\n```\n:::\n:::\n\n\nHere showing that Barney is more similar to Fred and Betty than he is to Bam-Bam.\n\nThe cosine similarity is given by:\n\n$$\n   s_{ij} = \\frac{p}{\\sqrt{p + q} \\sqrt{p + r}}\n$$ {#eq-cos}\n\nWhich is the ratio of the number of common neighbors divided by the product of the square root of the degrees of each node (or the square root of the product which is the same thing), since $p$ + $q$ is the degree of node $i$ and $p$ + $r$ is the degree of node $j$.\n\nWhen the neighborhoods of the two nodes coincide (e.g., $q = 0$ and $r = 0$) then @eq-cos turns into $\\frac{p}{(\\sqrt(p))^2} = \\frac{p}{p} = 1.0$. \n\nIn our example, we can compute the Cosine similarity as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   C <- A.p / (sqrt(A.p + A.q) * sqrt(A.p + A.r))\n   round(C[1:10, 1:10], 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             Bam-Bam Barney Betty Feldspar Fred Headmistress Herdmaster Lava\nBam-Bam         1.00   0.45  0.57     0.58 0.55         0.37       0.41 0.62\nBarney          0.45   1.00  0.69     0.20 0.82         0.62       0.42 0.59\nBetty           0.57   0.69  1.00     0.39 0.67         0.37       0.55 0.67\nFeldspar        0.58   0.20  0.39     1.00 0.19         0.00       0.71 0.43\nFred            0.55   0.82  0.67     0.19 1.00         0.48       0.40 0.73\nHeadmistress    0.37   0.62  0.37     0.00 0.48         1.00       0.00 0.40\nHerdmaster      0.41   0.42  0.55     0.71 0.40         0.00       1.00 0.60\nLava            0.62   0.59  0.67     0.43 0.73         0.40       0.60 1.00\nLeach           0.47   0.48  0.48     0.41 0.31         0.52       0.29 0.35\nMorris          0.47   0.48  0.32     0.00 0.31         0.77       0.00 0.35\n             Leach Morris\nBam-Bam       0.47   0.47\nBarney        0.48   0.48\nBetty         0.48   0.32\nFeldspar      0.41   0.00\nFred          0.31   0.31\nHeadmistress  0.52   0.77\nHerdmaster    0.29   0.00\nLava          0.35   0.35\nLeach         1.00   0.33\nMorris        0.33   1.00\n```\n:::\n:::\n\n\nShowing results similar (pun intended) to those obtained using the Jaccard index. \n\nA less commonly used option is the **Dice Coefficient** (sometimes called the **Sorensen Index**) given by:\n\n$$\n   s_{ij} = \\frac{2p}{2p + q + r}\n$$ {#eq-dice}\n\nWhich is given by the ratio of *twice* the number of common neighbors divided by twice the same quantity plus the sum of the non-shared neighbors (and thus a variation of the Jaccard measure).\n\nWhen the neighborhoods of the two nodes coincide (e.g., $q = 0$ and $r = 0$) then @eq-dice turns into $\\frac{2p}{2p} = 1.0$. \n\nIn our example, we can compute the Dice similarity as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   D <- (2 * A.p) / ((2 * A.p) + A.q + A.r)\n   round(D[1:10, 1:10], 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             Bam-Bam Barney Betty Feldspar Fred Headmistress Herdmaster Lava\nBam-Bam         1.00   0.42  0.53     0.50 0.50         0.36       0.40 0.59\nBarney          0.42   1.00  0.69     0.13 0.81         0.56       0.35 0.58\nBetty           0.53   0.69  1.00     0.27 0.67         0.33       0.47 0.67\nFeldspar        0.50   0.13  0.27     1.00 0.12         0.00       0.67 0.31\nFred            0.50   0.81  0.67     0.12 1.00         0.42       0.33 0.72\nHeadmistress    0.36   0.56  0.33     0.00 0.42         1.00       0.00 0.38\nHerdmaster      0.40   0.35  0.47     0.67 0.33         0.00       1.00 0.53\nLava            0.59   0.58  0.67     0.31 0.72         0.38       0.53 1.00\nLeach           0.44   0.38  0.38     0.40 0.24         0.50       0.29 0.29\nMorris          0.44   0.38  0.25     0.00 0.24         0.75       0.00 0.29\n             Leach Morris\nBam-Bam       0.44   0.44\nBarney        0.38   0.38\nBetty         0.38   0.25\nFeldspar      0.40   0.00\nFred          0.24   0.24\nHeadmistress  0.50   0.75\nHerdmaster    0.29   0.00\nLava          0.29   0.29\nLeach         1.00   0.33\nMorris        0.33   1.00\n```\n:::\n:::\n\n\nOnce again, showing results comparable to the previous.  \n\nNote, that all three of these pairwise measures of similarity are bounded between zero and one, with nodes being maximally similar to themselves and with pairs of distinct nodes being maximally similar when they have the same set of neighbors (e.g., they are structurally equivalent).\n\n@leicht_etal06 introduce a variation on the theme of normalized structural similarity scores. Their point is that maybe we should care about nodes that are *surprisingly* similar given some suitable null model. They propose the **configuration model** as such a null model. This model takes a graph with the same degree distribution as the original but with connections formed at random as reference. \n\nThe **LHN similarity index** (for Leicht, Holme, and Newman) is then given by: \n\n$$\ns_{ij} = \\frac{p}{(p + q)(p + r)}\n$$ {#eq-lhn}\n\nWhich can be seen as a variation of the cosine similarity defined earlier. Note that in contrast to the other similarities we have considered which reach a maximum of 1.0 when the two nodes have the same neighborhood ($q = 0$ and $r = 0$) the LHN similarity only reaches that limit for two nodes that are adjacent *and don't have any other neighbors* ($p=1$). \n\nFor nodes with identical neighborhoods, the maximum LHN score will always be $\\frac{p}{p^2} = \\frac{1}{p}$ which means that it will decrease as node degree increases, even when the two nodes share the same neighbors. This makes sense, since it is much more \"surprising\" for two nodes to share neighbors when they only have a few friends than we they have lots of friends. \n\nIn our example, we can compute the LHN similarity as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   D <- A.p / ((A.p + A.q) * (A.p + A.r))\n   round(D[1:10, 1:10], 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             Bam-Bam Barney Betty Feldspar Fred Headmistress Herdmaster Lava\nBam-Bam         0.17   0.05  0.06     0.17 0.06         0.07       0.08 0.08\nBarney          0.05   0.08  0.05     0.04 0.06         0.08       0.06 0.05\nBetty           0.06   0.05  0.08     0.08 0.05         0.05       0.08 0.06\nFeldspar        0.17   0.04  0.08     0.50 0.04         0.00       0.25 0.09\nFred            0.06   0.06  0.05     0.04 0.07         0.06       0.05 0.06\nHeadmistress    0.07   0.08  0.05     0.00 0.06         0.20       0.00 0.05\nHerdmaster      0.08   0.06  0.08     0.25 0.05         0.00       0.25 0.09\nLava            0.08   0.05  0.06     0.09 0.06         0.05       0.09 0.09\nLeach           0.11   0.08  0.08     0.17 0.05         0.13       0.08 0.06\nMorris          0.11   0.08  0.05     0.00 0.05         0.20       0.00 0.06\n             Leach Morris\nBam-Bam       0.11   0.11\nBarney        0.08   0.08\nBetty         0.08   0.05\nFeldspar      0.17   0.00\nFred          0.05   0.05\nHeadmistress  0.13   0.20\nHerdmaster    0.08   0.00\nLava          0.06   0.06\nLeach         0.33   0.11\nMorris        0.11   0.33\n```\n:::\n:::\n\n\nWhich, once again, produces similar results to what we found before. Note, however, that the LHN is not naturally maximal for self-similar nodes. \n\nAnother approach is to compute:\n\n$$\n1 - \\frac{(p + q + r) - p}{(p + q + r) + p} = 1 - \\frac{q + r}{2p + q + r}\n$$ {#eq-braun}\n\nWhich is equivalent to the ratio of the union of the neighborhood of the two nodes minus their intersection divided by the sum of the union plus the intersection.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   S <- 1 - ((A.q + A.r)/((2*A.p) + A.q + A.r))\n   round(S[1:10, 1:10], 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             Bam-Bam Barney Betty Feldspar Fred Headmistress Herdmaster Lava\nBam-Bam         1.00   0.42  0.53     0.50 0.50         0.36       0.40 0.59\nBarney          0.42   1.00  0.69     0.13 0.81         0.56       0.35 0.58\nBetty           0.53   0.69  1.00     0.27 0.67         0.33       0.47 0.67\nFeldspar        0.50   0.13  0.27     1.00 0.12         0.00       0.67 0.31\nFred            0.50   0.81  0.67     0.12 1.00         0.42       0.33 0.72\nHeadmistress    0.36   0.56  0.33     0.00 0.42         1.00       0.00 0.38\nHerdmaster      0.40   0.35  0.47     0.67 0.33         0.00       1.00 0.53\nLava            0.59   0.58  0.67     0.31 0.72         0.38       0.53 1.00\nLeach           0.44   0.38  0.38     0.40 0.24         0.50       0.29 0.29\nMorris          0.44   0.38  0.25     0.00 0.24         0.75       0.00 0.29\n             Leach Morris\nBam-Bam       0.44   0.44\nBarney        0.38   0.38\nBetty         0.38   0.25\nFeldspar      0.40   0.00\nFred          0.24   0.24\nHeadmistress  0.50   0.75\nHerdmaster    0.29   0.00\nLava          0.29   0.29\nLeach         1.00   0.33\nMorris        0.33   1.00\n```\n:::\n:::\n\n\nLike the others (except LHN), this similarity measure is at its maximum of 1.0 when the neighborhoods of the two nodes coincide ($q = 0$ and $r = 0$), and therefore @eq-braun turns into $1-\\frac{0}{2p} = 1-0 = 1$.\n\n## Similarity and Structural Equivalence\n\nAll normalized similarity measures bounded between zero and one (like Jaccard, Cosine, and Dice) also define a *distance* on each pair of nodes which is equal to one minus the similarity. So the **cosine distance** between two nodes is one minus the cosine similarity, and so on for the Jaccard and Dice indexes.^[Note that this are not technically *metric distances* as the triangle inequality (e.g., $d(x,z) \\leq d(x,y) + d(x,z)$) may not always be respected. They perform well however in most applied clustering applications.]\n\nBecause they define distances, this also means that these approaches can be used to find approximately structurally equivalent classes of nodes in a graph just like we did with the Euclidean and correlation distances. \n\nFor instance, consider our toy graph from before with four structurally equivalent sets of nodes:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![A toy graph demonstrating structural equivalence.](locsim_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nThe cosine similarity matrix for this graph is:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   W <- as.matrix(as_adjacency_matrix(h))\n   W.p <- W %*% W\n   W.q <- W %*% (1 - W) \n   W.r <- (1 - W) %*% W\n   C <- W.p / (sqrt(W.p + W.q) * sqrt(W.p + W.r))\n   round(C, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     A    B    C    D    E    F    G    H    I\nA 1.00 0.26 0.26 0.58 0.58 0.58 0.67 0.00 0.00\nB 0.26 1.00 1.00 0.00 0.00 0.00 0.26 0.67 0.67\nC 0.26 1.00 1.00 0.00 0.00 0.00 0.26 0.67 0.67\nD 0.58 0.00 0.00 1.00 1.00 1.00 0.58 0.25 0.25\nE 0.58 0.00 0.00 1.00 1.00 1.00 0.58 0.25 0.25\nF 0.58 0.00 0.00 1.00 1.00 1.00 0.58 0.25 0.25\nG 0.67 0.26 0.26 0.58 0.58 0.58 1.00 0.00 0.00\nH 0.00 0.67 0.67 0.25 0.25 0.25 0.00 1.00 0.75\nI 0.00 0.67 0.67 0.25 0.25 0.25 0.00 0.75 1.00\n```\n:::\n:::\n\n\nNote that structurally equivalent nodes have similarity scores equal to 1.0. In this case, the distance matrix is given by:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   D <- 1 - C\n```\n:::\n\n\nAnd a hierarchical clustering on this matrix reveals the structurally equivalent classes:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   D <- dist(D) \n   h.res <- hclust(D, method = \"ward.D2\")\n   plot(h.res)\n```\n\n::: {.cell-output-display}\n![](locsim_files/figure-html/unnamed-chunk-10-1.png){width=576}\n:::\n:::\n\n\nWe can package all that we said before into a handy function that takes a graph as input and returns all four normalized similarity metrics as output:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   vertex.sim <- function(x) {\n      A <- as.matrix(as_adjacency_matrix(x))\n      A.p <- A %*% A \n      A.q <- A %*% (1 - A) \n      A.r <- (1 - A) %*% A \n      J <- A.p / (A.p + A.q + A.r)\n      C <- A.p / (sqrt(A.p + A.q) * sqrt(A.p + A.r))\n      D <- (2 * A.p) / ((2 * A.p) + A.q + A.r)\n      L <- A.p / ((A.p + A.q) * (A.p + A.r))\n      return(list(J = J, C = C, D = D, L = L))\n      }\n```\n:::\n\n\nIn the *Flintstones* network, we could then find structurally equivalent blocks from the similarity analysis as follows (using Cosine):\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   D <- dist(1- vertex.sim(g)$C)\n   h.res <- hclust(D, method = \"ward.D2\") #hierarchical clustering\n   plot(h.res)\n```\n\n::: {.cell-output-display}\n![](locsim_files/figure-html/unnamed-chunk-12-1.png){width=960}\n:::\n:::\n\n\nWe can then extract our blocks just like we did with the Euclidean distance hierarchical clustering results in the previous handout:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   blocks  <- cutree(h.res, k = 6)\n   blocks\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        Bam-Bam          Barney           Betty        Feldspar            Fred \n              1               2               2               3               2 \n   Headmistress      Herdmaster            Lava           Leach          Morris \n              4               3               2               5               4 \n      Mrs Slate         Pebbles Pebbles Bam-Bam        Piltdown      Poindexter \n              2               1               5               6               5 \n         Pyrite           Slate           Wilma \n              6               2               2 \n```\n:::\n:::\n\n\nThis analysis now puts $\\{Barney, Betty,  Fred, Lava, Mrs. Slate, Slate, Wilma\\}$ in the second block of equivalent actors (shown as the right-most cluster of actors in the dendrogram). \n\n\n\n",
    "supporting": [
      "locsim_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}