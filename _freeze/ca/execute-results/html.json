{
  "hash": "410cd3ccfc45eb331e701742846bafa1",
  "result": {
    "markdown": "---\ntitle: \"Correspondence Analysis (CA) of Two-Mode Networks\"\nexecute: \n  eval: true\n  echo: true\n  output: true\n  warning: false\n  message: false\nformat: \n   html:\n      code-line-numbers: true\n---\n\n\n**Correspondence Analysis** (CA) a relatively simple way to analyze and visualize two-mode data. However, there are a few additional computational details to discuss. \n\n## The Eigendecomposition of a Square Matrix\n\nFirst, let us review the idea of an **eigendecomposition** of a square matrix. Let's say we have the following matrix $\\mathbf{B}$ of dimensions $3 \\times 3$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   set.seed(567)\n   B <- matrix(round(runif(9), 2), nrow = 3, ncol = 3)\n   B\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,] 0.74 0.49 0.07\n[2,] 0.88 0.26 0.51\n[3,] 0.63 0.24 0.59\n```\n:::\n:::\n\n\nMost matrices like this can be decomposed into two other matrices $\\mathbf{U}$ and $\\mathbf{\\lambda}$, such that the following matrix multiplication equation is true:\n\n$$\n\\mathbf{B} = \\mathbf{U}\\mathbf{\\lambda}\\mathbf{U}^{-1}\n$$\n\nBoth $\\mathbf{U}$ and $\\mathbf{\\lambda}$ are of the same dimensions as the original, with $\\mathbf{U}$ having numbers in each cell and $\\mathbf{\\lambda}$ being a matrix with values along the diagonals and zeros everywhere else. \n\nThe column values of $\\mathbf{U}$ are called the **eigenvectors** of $\\mathbf{B}$ and the diagonal values of $\\mathbf{\\lambda}$ are called the **eigenvalues** of $\\mathbf{B}$.\n\nIn `R` you can find the values that yield the eigendecomposition of any square matrix (if one exists) using the function `eigen`. \n\nSo in our case this would be:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   eig.res <- eigen(B)\n   eig.res\n```\n\n::: {.cell-output .cell-output-stdout}\n```\neigen() decomposition\n$values\n[1]  1.4256541  0.3195604 -0.1552145\n\n$vectors\n           [,1]       [,2]       [,3]\n[1,] -0.5148954 -0.4669390  0.4838633\n[2,] -0.6388257  0.2808667 -0.8653808\n[3,] -0.5716507  0.8384997 -0.1303551\n```\n:::\n:::\n\n\nThe function `eigen` returns a list with two components, one called `values` are the diagonal values of $\\mathbf{\\lambda}$, and the other one called `vectors` is the eigenvector matrix $\\mathbf{U}$.\n\nWe can check that these two elements can help us reconstruct the original matrix as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   lambda <- diag(eig.res$values)\n   U <- eig.res$vectors\n   B.rec <- U %*% lambda %*% solve(U)\n   B.rec\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,] 0.74 0.49 0.07\n[2,] 0.88 0.26 0.51\n[3,] 0.63 0.24 0.59\n```\n:::\n:::\n\n\nWhich are indeed the original values of $\\mathbf{B}$!\n\nNow imagine that the matrix $\\mathbf{B}$ is *symmetrical*:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   B[upper.tri(B)] <- B[lower.tri(B)]\n   B\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,] 0.74 0.88 0.63\n[2,] 0.88 0.26 0.24\n[3,] 0.63 0.24 0.59\n```\n:::\n:::\n\n\nAnd let's do the eigendecomposition of this matrix:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   eig.res <- eigen(B)\n   eig.res\n```\n\n::: {.cell-output .cell-output-stdout}\n```\neigen() decomposition\n$values\n[1]  1.7709909  0.2757779 -0.4567688\n\n$vectors\n           [,1]       [,2]       [,3]\n[1,] -0.7200185 -0.2389854  0.6515054\n[2,] -0.4963684 -0.4787344 -0.7241766\n[3,] -0.4849657  0.8448073 -0.2260728\n```\n:::\n:::\n\n\nThe interesting thing here is that now the reconstruction equation boils down to:\n\n$$\n\\mathbf{B} = \\mathbf{U}\\mathbf{\\lambda}\\mathbf{U}^T\n$$\n\nNote that now we just need to post-multiply $\\mathbf{U}\\mathbf{\\lambda}$ by the *transpose* of $\\mathbf{U}$ rather than the inverse, which is a much simpler matrix operation.\n\nWe can check that this is true as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   lambda <- diag(eig.res$values)\n   U <- eig.res$vectors\n   B.rec <- U %*% lambda %*% t(U)\n   B.rec\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,] 0.74 0.88 0.63\n[2,] 0.88 0.26 0.24\n[3,] 0.63 0.24 0.59\n```\n:::\n:::\n\n\nWhich are indeed the original values of the symmetric version of $\\mathbf{B}$!\n\nNow, the idea is that we can perform the asymmetric or symmetric eigendecomposition with any matrix, including a network adjacency matrix or a proximity matrix derived from it. \n\nIn fact, we have already done a partial version of the matrix eigendecomposition many times before, because the reflective status game is a way to compute the first column (leading eigenvector) of the $\\mathbf{U}$ matrix for any proximity or adjacency matrix you feed into it.\n\nThe more important thing is that, once you have the eigendecomposition of a matrix, and the full set of eigenvectors stored in $\\mathbf{U}$, the first few columns of $\\mathbf{U}$, gives us the best *low dimensional approximation* of the original matrix. \n\nFor instance, in the above case, the one-dimensional (also called \"rank one\") approximation of the original matrix is given by:\n\n$$\n\\mathbf{B}_{1-dim} = u_1\\lambda_1u_1^T\n$$\n\nWhere $u$ is just the first column (eigenvector) of $\\mathbf{U}$, and $\\lambda_1$ is just the first eigenvalue. \n\nIn `R` we can do this approximation as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   u.1 <- as.matrix(U[, 1]) #column vector\n   B.1dim <- u.1 %*% lambda[1, 1] %*% t(u.1)\n   round(B.1dim, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,] 0.92 0.63 0.62\n[2,] 0.63 0.44 0.43\n[3,] 0.62 0.43 0.42\n```\n:::\n:::\n\n\nWhich are not quite the same as the original values of $\\mathbf{B}$, but they are not wildly far off either. \n\nIf we wanted to be more accurate, however, we would use a *two-dimensional* approximation (rank two) and thus use more of the information:\n\n$$\n\\mathbf{B}_{2-dim} = u_1\\lambda_1u_1^T + u_2\\lambda_2u_2^T\n$$\n\nIn `R`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   u.2 <- as.matrix(U[, 2])\n   B.2dim <- u.1 %*% lambda[1, 1] %*% t(u.1) + u.2 %*% lambda[2, 2] %*% t(u.2)\n   round(B.2dim, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,] 0.93 0.66 0.56\n[2,] 0.66 0.50 0.31\n[3,] 0.56 0.31 0.61\n```\n:::\n:::\n\n\nWhich are not the same as the original, but are now a bit closer!\n\nOf course, if we wanted to reconstruct the original matrix, all we have to do is add the product of the eigenvector, eigenvalue, and transpose of the eigenvector across all three dimensions of the matrix:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   u.3 <- as.matrix(U[, 3])\n   B.3dim <- \n      u.1 %*% lambda[1, 1] %*% t(u.1) + \n      u.2 %*% lambda[2, 2] %*% t(u.2) + \n      u.3 %*% lambda[3, 3] %*% t(u.3)\n   round(B.3dim, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,] 0.74 0.88 0.63\n[2,] 0.88 0.26 0.24\n[3,] 0.63 0.24 0.59\n```\n:::\n:::\n\n\nWhich reconstructs the original values. \n\nSo in general, if you have a symmetric square matrix $\\mathbf{B}$ of dimensions $k \\times k$, and you obtain an eigenvalue decomposition of $\\mathbf{B}$ with eigenvectors stored in the columns of $\\mathbf{U}$ and eigenvalues in $\\lambda$, then the rank-$p$ approximation of the original matrix is given by:\n \n$$\n\\mathbf{B}_{rank-p} = \\sum_{m = 1}^p u_{m}\\lambda_mu_m^T\n$$\n\nWhen $p = k$, the equation above gives you the original matrix back. When $p<k$ you get the best guess as to what the original was, given $p$ dimensions.\n\n## Obtaining the Matrices We Need for CA\n\nAs you might have already guessed, two-mode CA boils down to the eigendecomposition of a suitable matrix, derived from the original affiliation (bi-adjacency) matrix of a two-mode network. \n\nThe goal is to come up with a low-rank (usually two-dimensional) approximation of the original affiliation network using the eigenvectors and eigenvalues obtained from the decomposition, as we did above with our toy example.\n\nSo which matrix should be use for CA? \n\nLet's find out:\n\nFirst we need to create **row stochastic** versions of the affiliation matrix and its transpose $\\mathbf{A}$ and $\\mathbf{A}^T$. Recall that a matrix is row stochastic if their rows sum to one. \n\nFor the people, we can do this by taking the original affiliation matrix, and pre-multiplying it by a **diagonal square matrix** $\\mathbf{D}_P^{-1}$ of dimensions $M \\times M$ containing the *inverse* of the degrees of each person in the affiliation network along the diagonals and zeros everywhere else, yielding the row-stochastic matrix $\\mathbf{P}_{PG}$ of dimensions $M \\times N$:\n\n$$\n\\mathbf{P}_{PG} = \\mathbf{D}_P^{-1}\\mathbf{A}\n$$\n\nAnd we can do the same with the groups, except that we pre-multiply the *transpose* of the original affiliation matrix by $\\mathbf{D}_G^{-1}$ which is an $N \\times N$ matrix containing the inverse of the size of each group along the diagonals and zero everywhere else, this yields the matrix $\\mathbf{P}_{GP}$ of dimensions $N \\times M$:\n\n$$\n\\mathbf{P}_{GP} = \\mathbf{D}_G^{-1}\\mathbf{A}^T\n$$\n\nIn `R` can compute $\\mathbf{P}_{PG}$ and $\\mathbf{P}_{GP}$, using the classic Southern Women two-mode data, as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   library(igraph)\n   library(networkdata)\n   g <- southern_women #southern women data\n   A <- as.matrix(as_biadjacency_matrix(g)) #bi-adjacency matrix\n   D.p <- diag(1/rowSums(A)) #inverse of degree matrix of persons\n   P.pg <- D.p %*% A\n   rownames(P.pg) <- rownames(A) \n   D.g <- diag(1/colSums(A)) #inverse of degree matrix of groups\n   P.gp <- D.g %*% t(A)\n   rownames(P.gp) <- colnames(A)\n```\n:::\n\n\nAnd we can check that both $\\mathbf{P}_{PG}$ and $\\mathbf{P}_{GP}$ are indeed row stochastic:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   rowSums(P.pg)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n        1         1         1         1         1         1         1         1 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n        1         1         1         1         1         1         1         1 \n   OLIVIA     FLORA \n        1         1 \n```\n:::\n\n```{.r .cell-code}\n   rowSums(P.gp)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n 6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10  2/23   4/7 11/21 \n    1     1     1     1     1     1     1     1     1     1     1     1     1 \n  8/3 \n    1 \n```\n:::\n:::\n\n\nAnd that they are of the predicted dimensions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   dim(P.pg)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 18 14\n```\n:::\n\n```{.r .cell-code}\n   dim(P.gp)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 14 18\n```\n:::\n:::\n\n\nGreat! Now, we can obtain the *degree-normalized projections* for people by multiplying $\\mathbf{P}_{PG}$ times $\\mathbf{P}_{GP}$:\n\n$$\n\\mathbf{P}_{PP} = \\mathbf{P}_{PG}\\mathbf{P}_{GP}\n$$\n\nWhich produces the matrix $\\mathbf{P}_{PP}$ a square $M \\times M$ matrix containing the *degree-normalized similarities* between each pair of people.\n\nWe then do the same for groups:\n\n$$\n\\mathbf{P}_{GG} = \\mathbf{P}_{GP}\\mathbf{P}_{PG}\n$$\n\nWhich produces the matrix $\\mathbf{P}_{GG}$ a square $N \\times N$ matrix containing the *degree-normalized similarities* between each pair of groups.\n\nIn `R` we obtain these matrices as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   P.pp <- P.pg %*% P.gp\n   P.gg <- P.gp %*% P.pg\n```\n:::\n\n\nWhich are still row stochastic--but now square--matrices:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   rowSums(P.pp)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n        1         1         1         1         1         1         1         1 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n        1         1         1         1         1         1         1         1 \n   OLIVIA     FLORA \n        1         1 \n```\n:::\n\n```{.r .cell-code}\n   rowSums(P.gg)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n 6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10  2/23   4/7 11/21 \n    1     1     1     1     1     1     1     1     1     1     1     1     1 \n  8/3 \n    1 \n```\n:::\n\n```{.r .cell-code}\n   dim(P.pp)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 18 18\n```\n:::\n\n```{.r .cell-code}\n   dim(P.gg)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 14 14\n```\n:::\n:::\n\n\nLet's peek inside one of these matrices:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   round(head(P.pp), 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          EVELYN LAURA THERESA BRENDA CHARLOTTE FRANCES ELEANOR PEARL  RUTH\nEVELYN     0.186 0.144   0.144  0.134     0.068   0.061   0.040 0.035 0.035\nLAURA      0.165 0.179   0.132  0.132     0.056   0.070   0.060 0.028 0.042\nTHERESA    0.144 0.115   0.157  0.105     0.080   0.061   0.053 0.035 0.047\nBRENDA     0.153 0.132   0.120  0.167     0.092   0.070   0.060 0.028 0.042\nCHARLOTTE  0.135 0.098   0.160  0.160     0.160   0.073   0.056 0.000 0.056\nFRANCES    0.122 0.122   0.122  0.122     0.073   0.122   0.080 0.049 0.049\n          VERNE MYRNA KATHERINE SYLVIA  NORA HELEN DOROTHY OLIVIA FLORA\nEVELYN    0.019 0.019     0.019  0.019 0.026 0.009   0.019   0.01  0.01\nLAURA     0.024 0.010     0.010  0.024 0.032 0.024   0.010   0.00  0.00\nTHERESA   0.032 0.019     0.019  0.032 0.039 0.021   0.019   0.01  0.01\nBRENDA    0.024 0.010     0.010  0.024 0.032 0.024   0.010   0.00  0.00\nCHARLOTTE 0.025 0.000     0.000  0.025 0.025 0.025   0.000   0.00  0.00\nFRANCES   0.018 0.018     0.018  0.018 0.031 0.018   0.018   0.00  0.00\n```\n:::\n:::\n\n\nWhat are these numbers? Well, they can be interpreted as *probabilities* that a random walker starting at the row node and, following any sequence of $person-group-person'-group'$ hops, will reach the column person. Thus, higher values indicate an *affinity* or *proximity* between the people (and the groups in the corresponding matrix).\n\n## Performing CA\n\nWe went through all these steps because CA is equivalent to the eigendecomposition of the last two square matrices we obtained, namely, $\\mathbf{P_{PP}}$ and $\\mathbf{P_{GG}}$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   CA.p <- eigen(P.pp)\n   CA.g <- eigen(P.gg)\n```\n:::\n\n\nLet's see what we have here:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   round(CA.p$values, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 1.00 0.63 0.32 0.18 0.14 0.11 0.10 0.06 0.04 0.04 0.02 0.01 0.01 0.00 0.00\n[16] 0.00 0.00 0.00\n```\n:::\n\n```{.r .cell-code}\n   round(CA.g$values, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 1.00 0.63 0.32 0.18 0.14 0.11 0.10 0.06 0.04 0.04 0.02 0.01 0.01 0.00\n```\n:::\n:::\n\n\nSo the two matrices have identical eigenvalues, and the first one is 1.0. \n\nLet's check out the first three eigenvectors:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   rownames(CA.p$vectors) <- rownames(A)\n   rownames(CA.g$vectors) <- colnames(A)\n   round(CA.p$vectors[, 1:3], 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           [,1]  [,2]  [,3]\nEVELYN    -0.24 -0.24  0.03\nLAURA     -0.24 -0.25 -0.01\nTHERESA   -0.24 -0.20  0.02\nBRENDA    -0.24 -0.26 -0.01\nCHARLOTTE -0.24 -0.29 -0.01\nFRANCES   -0.24 -0.24 -0.02\nELEANOR   -0.24 -0.15 -0.03\nPEARL     -0.24 -0.01  0.06\nRUTH      -0.24 -0.05  0.03\nVERNE     -0.24  0.13 -0.04\nMYRNA     -0.24  0.25 -0.10\nKATHERINE -0.24  0.31 -0.22\nSYLVIA    -0.24  0.26 -0.20\nNORA      -0.24  0.26 -0.03\nHELEN     -0.24  0.24  0.07\nDOROTHY   -0.24  0.09  0.09\nOLIVIA    -0.24  0.33  0.66\nFLORA     -0.24  0.33  0.66\n```\n:::\n\n```{.r .cell-code}\n   round(CA.g$vectors[, 1:3], 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      [,1]  [,2]  [,3]\n6/27  0.27 -0.30 -0.01\n3/2   0.27 -0.28 -0.04\n4/12  0.27 -0.30  0.00\n9/26  0.27 -0.30 -0.02\n2/25  0.27 -0.25  0.00\n5/19  0.27 -0.16  0.00\n3/15  0.27 -0.04  0.05\n9/16  0.27 -0.01  0.05\n4/8   0.27  0.15 -0.19\n6/10  0.27  0.32  0.22\n2/23  0.27  0.35 -0.79\n4/7   0.27  0.29  0.20\n11/21 0.27  0.34  0.35\n8/3   0.27  0.34  0.35\n```\n:::\n:::\n\n\nSo this is interesting. The first eigenvector of the decomposition of both $\\mathbf{P_{PP}}$ and $\\mathbf{P_{GG}}$ is just the same number for each person and group. Note that this is the eigenvector that is associated with the first eigenvalue which happens to be $\\lambda_1 = 1.0$.\n\nSo it looks like the first eigenvector is a pretty useless quantity (a constant) so we can discard it, keeping all the other ones. Now the old second eigenvector is the first, the old third is the second, and so on:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   eig.vec.p <- CA.p$vectors[, 2:ncol(CA.p$vectors)]\n   eig.vec.g <- CA.g$vectors[, 2:ncol(CA.g$vectors)]\n```\n:::\n\n\nNote that the rest of the eigenvalues (discarding the 1.0 one) are arranged in descending order:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   eig.vals <- CA.p$values[2:length(CA.p$values)]\n   round(eig.vals, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 0.627 0.319 0.179 0.138 0.107 0.099 0.064 0.044 0.036 0.021 0.012 0.005\n[13] 0.000 0.000 0.000 0.000 0.000\n```\n:::\n:::\n\n\nThe magnitude of the eigenvalue tells us how important is the related eigenvector in containing information about the original matrix. It looks like here, the first two eigenvectors contain a good chunk of the info. \n\nWe can check how much exactly by computing the ratio between the sum of the first two eigenvalues over the sum of all the eigenvalues:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   round(sum(eig.vals[1:2])/sum(eig.vals), 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.57\n```\n:::\n:::\n\n\nWhich tells us that the first two eigenvectors account for about 57% of the action (or more precisely we could reconstruct the original matrix with 57% accuracy using just these two eigenvectors and associated eigenvalues). \n\nBecause the magnitude of the CA eigenvectors don't have a natural scale, it is common to normalize them to have a variance of 1.0 [@fouss_etal16, p. 399], and the multiplying them by the square root of the eigenvalue corresponding to that dimension, so that the new variance is scaled to the importance of that dimension. \n\nWe can perform the normalization of the raw CA scores using the following function, which performs CA on the affiliation matrix:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   norm.CA.vec <- function(x, s = 0.000001) {\n      D.p <- diag(rowSums(x)) #degree matrix for persons\n      D.g <- diag(colSums(x)) #degree matrix for groups\n      i.Dp <- solve(D.p) #inverse of degree matrix for persons\n      i.Dg <- solve(D.g) #inverse of degree matrix for groups\n      CA.p <- eigen(i.Dp %*% x %*% i.Dg %*% t(x)) #person CA\n      CA.g <- eigen(i.Dg %*% t(x) %*% i.Dp %*% x) #group CA\n      ev <- CA.p$values[2:length(CA.p$values)]\n      ev <- ev[which(ev > s)]\n      m <- length(ev)\n      CA.p <- CA.p$vectors[, 2:ncol(CA.p$vectors)]\n      CA.g <- CA.g$vectors[, 2:ncol(CA.g$vectors)]\n      rownames(CA.p) <- rownames(A)\n      rownames(CA.g) <- colnames(A)\n      Z.u.p <- matrix(0, nrow(x), m)\n      Z.u.g <- matrix(0, ncol(x), m)\n      Z.v.p <- matrix(0, nrow(x), m)\n      Z.v.g <- matrix(0, ncol(x), m)\n      rownames(Z.u.p) <- rownames(x)\n      rownames(Z.u.g) <- colnames(x)\n      rownames(Z.v.p) <- rownames(x)\n      rownames(Z.v.g) <- colnames(x)\n      for (i in 1:m) {\n         ev.p <- as.matrix(CA.p[, i])\n         ev.g <- as.matrix(CA.g[, i])\n         norm.p <- as.numeric(t(ev.p) %*% D.p %*% ev.p) #person norm\n         Z.u.p[, i] <- ev.p * sqrt(sum(A)/norm.p) #normalizing to unit variance\n         Z.v.p[, i] <- Z.u.p[, i] * sqrt(ev[i]) #normalizing to square root of eigenvalue\n         norm.g <- as.numeric(t(ev.g) %*% D.g %*% ev.g) #group norm\n         Z.u.g[, i] <- ev.g * sqrt(sum(A)/norm.g) #normalizing to unit variance\n         Z.v.g[, i] <- Z.u.g[, i] * sqrt(ev[i]) #normalizing to square root of eigenvalue\n      }\n   return(list(Z.u.p = Z.u.p, Z.u.g = Z.u.g,\n               Z.v.p = Z.v.p, Z.v.g = Z.v.g))\n   }\n```\n:::\n\n\nThis function takes the bi-adjacency matrix as input and returns two new set of normalized CA scores for both persons and groups as output. The normalized CA scores are stored in four separate matrices: $\\mathbf{Z_P^U}, \\mathbf{Z_G^U}, \\mathbf{Z_P^V}, \\mathbf{Z_G^V}$. \n\nOne person-group set of scores is normalized to unit variance (`Z.u.p` and `Z.u.g`) and the other person-group set of scores is normalized to the scale of the eigenvalue corresponding to each CA dimension for both persons and groups (`Z.v.p` and `Z.v.g`).\n\nLet's see the normalization function at work, extracting the first two dimensions for persons and groups (the first two columns of each $\\mathbf{Z}$ matrix):\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   CA.res <- norm.CA.vec(A)\n   uni.p <- CA.res$Z.u.p[, 1:2]\n   uni.g <- CA.res$Z.u.g[, 1:2]\n   val.p <- CA.res$Z.v.p[, 1:2]\n   val.g <- CA.res$Z.v.g[, 1:2]\n   round(uni.p, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           [,1]  [,2]\nEVELYN    -1.01  0.20\nLAURA     -1.06 -0.07\nTHERESA   -0.83  0.14\nBRENDA    -1.08 -0.09\nCHARLOTTE -1.23 -0.07\nFRANCES   -1.01 -0.10\nELEANOR   -0.65 -0.21\nPEARL     -0.05  0.37\nRUTH      -0.21  0.17\nVERNE      0.55 -0.23\nMYRNA      1.04 -0.58\nKATHERINE  1.32 -1.33\nSYLVIA     1.10 -1.20\nNORA       1.10 -0.19\nHELEN      1.02  0.44\nDOROTHY    0.38  0.55\nOLIVIA     1.38  3.98\nFLORA      1.38  3.98\n```\n:::\n\n```{.r .cell-code}\n   round(uni.g, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       [,1]  [,2]\n6/27  -1.33 -0.02\n3/2   -1.22 -0.16\n4/12  -1.31  0.00\n9/26  -1.31 -0.08\n2/25  -1.12  0.00\n5/19  -0.72 -0.01\n3/15  -0.16  0.23\n9/16  -0.04  0.24\n4/8    0.65 -0.87\n6/10   1.41  1.01\n2/23   1.54 -3.64\n4/7    1.29  0.91\n11/21  1.48  1.61\n8/3    1.48  1.61\n```\n:::\n\n```{.r .cell-code}\n   round(val.p, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           [,1]  [,2]\nEVELYN    -0.80  0.11\nLAURA     -0.84 -0.04\nTHERESA   -0.65  0.08\nBRENDA    -0.86 -0.05\nCHARLOTTE -0.97 -0.04\nFRANCES   -0.80 -0.06\nELEANOR   -0.51 -0.12\nPEARL     -0.04  0.21\nRUTH      -0.17  0.10\nVERNE      0.43 -0.13\nMYRNA      0.83 -0.33\nKATHERINE  1.05 -0.75\nSYLVIA     0.87 -0.68\nNORA       0.87 -0.11\nHELEN      0.81  0.25\nDOROTHY    0.30  0.31\nOLIVIA     1.10  2.25\nFLORA      1.10  2.25\n```\n:::\n\n```{.r .cell-code}\n   round(val.g, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       [,1]  [,2]\n6/27  -1.05 -0.01\n3/2   -0.97 -0.09\n4/12  -1.04  0.00\n9/26  -1.04 -0.05\n2/25  -0.88  0.00\n5/19  -0.57 -0.01\n3/15  -0.13  0.13\n9/16  -0.03  0.14\n4/8    0.51 -0.49\n6/10   1.12  0.57\n2/23   1.22 -2.05\n4/7    1.02  0.52\n11/21  1.17  0.91\n8/3    1.17  0.91\n```\n:::\n:::\n\n\nGreat! Now we have two sets (unit variance versus eigenvalue variance) of normalized CA scores for persons and groups on the first two dimensions. \n\n## The Duality of CA Scores Between Persons and Groups\n\nJust like as we saw with the eigenvector centrality in the last handout, there is a *duality* between the CA scores assigned to the person and the groups on each dimension, such that the scores for each person are a weighted sum of the scores assigned to each group on that dimension and vice versa [@faust97, 171]. \n\nThe main difference is that this time we sum scores across the $\\mathbf{P_P}$ and $\\mathbf{P_G}$ matrices rather than the original affiliation matrix and its transpose, resulting in *degree-weighted* sums of scores for both persons and groups.\n\nSo for any given person, on any given dimension, let's say $EVELYN$, her CA score is given by the sum of the (unit variance normalized) CA scores of the groups she belongs to weighted by her degree (done by multiplying each CA score by the relevant cell in Evelyn's row of the $\\mathbf{P}_{PG}$ matrix):\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   sum(P.pg[\"EVELYN\", ] * uni.g[, 1]) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -0.7994396\n```\n:::\n:::\n\n\nWhich is the same as the (eigenvalue variance) normalized score we obtained via CA for $EVELYN$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   val.p[\"EVELYN\", 1]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    EVELYN \n-0.7994396 \n```\n:::\n:::\n\n\nA similar story applies to groups. Each group score is the group-size-weighted sum of the (unit variance normalized) CA scores of the people who join it:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   sum(P.gp[\"6/27\", ] * uni.p[, 1])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -1.051052\n```\n:::\n:::\n\n\nWhich is the same as the (eigenvalue variance normalized) score we obtained via CA:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n    val.g[\"6/27\", 1]  \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     6/27 \n-1.051052 \n```\n:::\n:::\n\n\nNeat! Duality at work.\n\n## Visualizing Two-Mode Networks Using CA\n\nAnd, finally, we can use the first two (eigenvalue variance) normalized CA scores to plot the persons and groups in a common two-dimensional space:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   val.g[, 2] <- val.g[, 2]*-1 #flippling sign of group scores on second dimension for plotting purposes\n   plot.dat <- data.frame(rbind(uni.p, val.g)) %>% \n      cbind(type = as.factor(c(rep(1, 18), rep(2, 14))))\n   library(ggplot2)\n   # install.packages(\"ggrepel\")\n   library(ggrepel)\n   p <- ggplot(data = plot.dat, aes(x = X1, y = X2, color = type))\n   p <- p + geom_hline(aes(yintercept = 0), color = \"gray\")\n   p <- p + geom_vline(aes(xintercept = 0), color = \"gray\")\n   p <- p + geom_text_repel(aes(label = rownames(plot.dat)), \n                            max.overlaps = 20, size = 2.75)\n   p <- p + theme_minimal()\n   p <- p + theme(legend.position = \"none\",\n                  axis.title = element_text(size = 14),\n                  axis.text = element_text(size = 12))\n   p <- p + scale_color_manual(values = c(\"red\", \"blue\"))\n   p <- p + labs(x = \"First Dimension\", y = \"Second Dimension\") \n   p <- p + xlim(-2, 2) + ylim(-2, 4)\n   p\n```\n\n::: {.cell-output-display}\n![](ca_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n:::\n\n\nIn this space, people with the most similar patterns of memberships to the most similar groups are placed close to one another. In the same way, groups with the most similar members are placed closed to one another. \n\nAlso like before, we can use the scores obtained from the CA analysis to re-arrange the rows and columns of the original matrix to reveal blocks of maximally similar persons and events:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   library(ggcorrplot)\n   p <- ggcorrplot(t(A[order(val.p[,1]), order(val.g[,1])]), \n                   colors = c(\"white\", \"white\", \"red\")) \n   p <- p + theme(legend.position = \"none\", \n                  axis.text.y = element_text(size = 8),\n                  axis.text.x = element_text(size = 8, angle = 0),\n                  )\n   p <- p + scale_x_discrete(position = \"top\") \n   p <- p + geom_hline(yintercept = 6.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_hline(yintercept = 11.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_hline(yintercept = 16.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_vline(xintercept = 9.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_vline(xintercept = 6.5, linewidth = 2, color = \"blue\")\n   p\n```\n\n::: {.cell-output-display}\n![](ca_files/figure-html/unnamed-chunk-29-1.png){width=672}\n:::\n:::\n\n\nHere CA seems to have detected two separate clusters of actors who preferentially attend two distinct clusters of events! \n\nThe three events in the middle $\\{3/15, 9/16, 4/8\\}$ don't seem to differentiate between participants in each cluster (everyone attends)--they thus appear near the origin in the CA diagram, indicating a weak association with either dimension. \n\nHowever, the events to the left (with clusters of participants in the lower-left) and to the right of the x-axis (with clusters of participants in the upper-right) are attended preferentially by distinct groups of participants; they thus appear at the extreme left and right positions of the first dimension of the CA diagram. \n\nIn the same way, the four people in the middle $\\{Ruth, Dorothy, Pearl, Verne\\}$ only attend the undifferentiated, popular events, so that means that they are not strongly associated with either cluster of actors (and thus appear near the origin in the CA diagram). The top and bottom participants, by contrast, appear to the extreme right and left in the CA diagram, indicating a strong association with the underlying dimensions.\n\nNote the similarity between this blocking and that obtained from the structural equivalence analysis in the previous handout.\n\n## Community Detection in Bipartite Networks\n\nYou may have noticed that the CA analysis of two-mode networks looks a lot like the identification of communities in one-mode networks. The main difference is that in a two-mode network, good communities are composed of clusters of persons and groups well-separated from other clusters of persons and groups. \n\nAs @barber07 noted, we can extend Newman's modularity approach to ascertain whether a given partition identifies a good \"community\" in the bipartite case. For that, we need a bi-adjacency analog of the modularity matrix $\\mathbf{B}$. This is given by:\n\n$$\n\\mathbf{B}_{(ij)} = \\mathbf{A}_{(ij)} - \\frac{k^p_ik^g_j}{|E|}\n$$\n\nWhere $k^p_i$ is the number of memberships of the $i^{th}$ person, $k^g_i$ is the number of members of the $j^{th}$ group, and $|E|$ is the number of edges in the bipartite graph.\n\nSo in our case, this would be:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   dp <- as.matrix(rowSums(A))\n   dg <- as.matrix(colSums(A))\n   dpdg <- dp %*% t(dg) #person x group degree product matrix\n   B <- A - dpdg/sum(A)\n   round(B, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10  2/23\nEVELYN     0.73  0.73  0.46  0.64  0.28  0.28 -0.90 -0.26 -0.08 -0.45 -0.36\nLAURA      0.76  0.76  0.53 -0.31  0.37  0.37  0.21 -0.10 -0.94 -0.39 -0.31\nTHERESA   -0.27  0.73  0.46  0.64  0.28  0.28  0.10 -0.26 -0.08 -0.45 -0.36\nBRENDA     0.76 -0.24  0.53  0.69  0.37  0.37  0.21 -0.10 -0.94 -0.39 -0.31\nCHARLOTTE -0.13 -0.13  0.73  0.82  0.64 -0.36  0.55 -0.63 -0.54 -0.22 -0.18\nFRANCES   -0.13 -0.13  0.73 -0.18  0.64  0.64 -0.45  0.37 -0.54 -0.22 -0.18\nELEANOR   -0.13 -0.13 -0.27 -0.18  0.64  0.64  0.55  0.37 -0.54 -0.22 -0.18\nPEARL     -0.10 -0.10 -0.20 -0.13 -0.27  0.73 -0.34  0.53  0.60 -0.17 -0.13\nRUTH      -0.13 -0.13 -0.27 -0.18  0.64 -0.36  0.55  0.37  0.46 -0.22 -0.18\nVERNE     -0.13 -0.13 -0.27 -0.18 -0.36 -0.36  0.55  0.37  0.46 -0.22 -0.18\nMYRNA     -0.13 -0.13 -0.27 -0.18 -0.36 -0.36 -0.45  0.37  0.46  0.78 -0.18\nKATHERINE -0.20 -0.20 -0.40 -0.27 -0.54 -0.54 -0.67  0.06  0.19  0.66 -0.27\nSYLVIA    -0.24 -0.24 -0.47 -0.31 -0.63 -0.63  0.21 -0.10  0.06  0.61 -0.31\nNORA      -0.27 -0.27 -0.54 -0.36 -0.72  0.28  0.10 -1.26 -0.08  0.55  0.64\nHELEN     -0.17 -0.17 -0.34 -0.22 -0.45 -0.45  0.44  0.21 -0.67  0.72  0.78\nDOROTHY   -0.07 -0.07 -0.13 -0.09 -0.18 -0.18 -0.22  0.69  0.73 -0.11 -0.09\nOLIVIA    -0.07 -0.07 -0.13 -0.09 -0.18 -0.18 -0.22 -0.31  0.73 -0.11  0.91\nFLORA     -0.07 -0.07 -0.13 -0.09 -0.18 -0.18 -0.22 -0.31  0.73 -0.11  0.91\n            4/7 11/21   8/3\nEVELYN    -0.54 -0.27 -0.27\nLAURA     -0.47 -0.24 -0.24\nTHERESA   -0.54 -0.27 -0.27\nBRENDA    -0.47 -0.24 -0.24\nCHARLOTTE -0.27 -0.13 -0.13\nFRANCES   -0.27 -0.13 -0.13\nELEANOR   -0.27 -0.13 -0.13\nPEARL     -0.20 -0.10 -0.10\nRUTH      -0.27 -0.13 -0.13\nVERNE      0.73 -0.13 -0.13\nMYRNA      0.73 -0.13 -0.13\nKATHERINE  0.60  0.80  0.80\nSYLVIA     0.53  0.76  0.76\nNORA       0.46  0.73  0.73\nHELEN      0.66 -0.17 -0.17\nDOROTHY   -0.13 -0.07 -0.07\nOLIVIA    -0.13 -0.07 -0.07\nFLORA     -0.13 -0.07 -0.07\n```\n:::\n:::\n\n\nNeat! Like before the numbers in this matrix represent the expected probability of observing a tie in a world in which people keep their number of memberships and groups keep their observed sizes, but otherwise, people and groups connect at random. \n\nWe can also create a bipartite matrix version of the bi-adjacency modularity, as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   n <- vcount(g)\n   Np <- nrow(A)\n   names <- c(rownames(A), colnames(A))\n   B2 <- matrix(0, n, n) #all zeros matrix of dimensions (p + g) X (p + g)\n   B2[1:Np, (Np + 1):n] <- B #putting B in the top right block\n   B2[(Np + 1):n, 1:Np] <- t(B) #putting B transpose in the lower-left block\n   rownames(B2) <- names\n   colnames(B2) <- names\n   round(B2, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          EVELYN LAURA THERESA BRENDA CHARLOTTE FRANCES ELEANOR PEARL  RUTH\nEVELYN      0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\nLAURA       0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\nTHERESA     0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\nBRENDA      0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\nCHARLOTTE   0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\nFRANCES     0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\nELEANOR     0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\nPEARL       0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\nRUTH        0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\nVERNE       0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\nMYRNA       0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\nKATHERINE   0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\nSYLVIA      0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\nNORA        0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\nHELEN       0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\nDOROTHY     0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\nOLIVIA      0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\nFLORA       0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\n6/27        0.73  0.76   -0.27   0.76     -0.13   -0.13   -0.13 -0.10 -0.13\n3/2         0.73  0.76    0.73  -0.24     -0.13   -0.13   -0.13 -0.10 -0.13\n4/12        0.46  0.53    0.46   0.53      0.73    0.73   -0.27 -0.20 -0.27\n9/26        0.64 -0.31    0.64   0.69      0.82   -0.18   -0.18 -0.13 -0.18\n2/25        0.28  0.37    0.28   0.37      0.64    0.64    0.64 -0.27  0.64\n5/19        0.28  0.37    0.28   0.37     -0.36    0.64    0.64  0.73 -0.36\n3/15       -0.90  0.21    0.10   0.21      0.55   -0.45    0.55 -0.34  0.55\n9/16       -0.26 -0.10   -0.26  -0.10     -0.63    0.37    0.37  0.53  0.37\n4/8        -0.08 -0.94   -0.08  -0.94     -0.54   -0.54   -0.54  0.60  0.46\n6/10       -0.45 -0.39   -0.45  -0.39     -0.22   -0.22   -0.22 -0.17 -0.22\n2/23       -0.36 -0.31   -0.36  -0.31     -0.18   -0.18   -0.18 -0.13 -0.18\n4/7        -0.54 -0.47   -0.54  -0.47     -0.27   -0.27   -0.27 -0.20 -0.27\n11/21      -0.27 -0.24   -0.27  -0.24     -0.13   -0.13   -0.13 -0.10 -0.13\n8/3        -0.27 -0.24   -0.27  -0.24     -0.13   -0.13   -0.13 -0.10 -0.13\n          VERNE MYRNA KATHERINE SYLVIA  NORA HELEN DOROTHY OLIVIA FLORA  6/27\nEVELYN     0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00  0.73\nLAURA      0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00  0.76\nTHERESA    0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00 -0.27\nBRENDA     0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00  0.76\nCHARLOTTE  0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00 -0.13\nFRANCES    0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00 -0.13\nELEANOR    0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00 -0.13\nPEARL      0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00 -0.10\nRUTH       0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00 -0.13\nVERNE      0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00 -0.13\nMYRNA      0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00 -0.13\nKATHERINE  0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00 -0.20\nSYLVIA     0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00 -0.24\nNORA       0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00 -0.27\nHELEN      0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00 -0.17\nDOROTHY    0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00 -0.07\nOLIVIA     0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00 -0.07\nFLORA      0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00 -0.07\n6/27      -0.13 -0.13     -0.20  -0.24 -0.27 -0.17   -0.07  -0.07 -0.07  0.00\n3/2       -0.13 -0.13     -0.20  -0.24 -0.27 -0.17   -0.07  -0.07 -0.07  0.00\n4/12      -0.27 -0.27     -0.40  -0.47 -0.54 -0.34   -0.13  -0.13 -0.13  0.00\n9/26      -0.18 -0.18     -0.27  -0.31 -0.36 -0.22   -0.09  -0.09 -0.09  0.00\n2/25      -0.36 -0.36     -0.54  -0.63 -0.72 -0.45   -0.18  -0.18 -0.18  0.00\n5/19      -0.36 -0.36     -0.54  -0.63  0.28 -0.45   -0.18  -0.18 -0.18  0.00\n3/15       0.55 -0.45     -0.67   0.21  0.10  0.44   -0.22  -0.22 -0.22  0.00\n9/16       0.37  0.37      0.06  -0.10 -1.26  0.21    0.69  -0.31 -0.31  0.00\n4/8        0.46  0.46      0.19   0.06 -0.08 -0.67    0.73   0.73  0.73  0.00\n6/10      -0.22  0.78      0.66   0.61  0.55  0.72   -0.11  -0.11 -0.11  0.00\n2/23      -0.18 -0.18     -0.27  -0.31  0.64  0.78   -0.09   0.91  0.91  0.00\n4/7        0.73  0.73      0.60   0.53  0.46  0.66   -0.13  -0.13 -0.13  0.00\n11/21     -0.13 -0.13      0.80   0.76  0.73 -0.17   -0.07  -0.07 -0.07  0.00\n8/3       -0.13 -0.13      0.80   0.76  0.73 -0.17   -0.07  -0.07 -0.07  0.00\n            3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10  2/23   4/7\nEVELYN     0.73  0.46  0.64  0.28  0.28 -0.90 -0.26 -0.08 -0.45 -0.36 -0.54\nLAURA      0.76  0.53 -0.31  0.37  0.37  0.21 -0.10 -0.94 -0.39 -0.31 -0.47\nTHERESA    0.73  0.46  0.64  0.28  0.28  0.10 -0.26 -0.08 -0.45 -0.36 -0.54\nBRENDA    -0.24  0.53  0.69  0.37  0.37  0.21 -0.10 -0.94 -0.39 -0.31 -0.47\nCHARLOTTE -0.13  0.73  0.82  0.64 -0.36  0.55 -0.63 -0.54 -0.22 -0.18 -0.27\nFRANCES   -0.13  0.73 -0.18  0.64  0.64 -0.45  0.37 -0.54 -0.22 -0.18 -0.27\nELEANOR   -0.13 -0.27 -0.18  0.64  0.64  0.55  0.37 -0.54 -0.22 -0.18 -0.27\nPEARL     -0.10 -0.20 -0.13 -0.27  0.73 -0.34  0.53  0.60 -0.17 -0.13 -0.20\nRUTH      -0.13 -0.27 -0.18  0.64 -0.36  0.55  0.37  0.46 -0.22 -0.18 -0.27\nVERNE     -0.13 -0.27 -0.18 -0.36 -0.36  0.55  0.37  0.46 -0.22 -0.18  0.73\nMYRNA     -0.13 -0.27 -0.18 -0.36 -0.36 -0.45  0.37  0.46  0.78 -0.18  0.73\nKATHERINE -0.20 -0.40 -0.27 -0.54 -0.54 -0.67  0.06  0.19  0.66 -0.27  0.60\nSYLVIA    -0.24 -0.47 -0.31 -0.63 -0.63  0.21 -0.10  0.06  0.61 -0.31  0.53\nNORA      -0.27 -0.54 -0.36 -0.72  0.28  0.10 -1.26 -0.08  0.55  0.64  0.46\nHELEN     -0.17 -0.34 -0.22 -0.45 -0.45  0.44  0.21 -0.67  0.72  0.78  0.66\nDOROTHY   -0.07 -0.13 -0.09 -0.18 -0.18 -0.22  0.69  0.73 -0.11 -0.09 -0.13\nOLIVIA    -0.07 -0.13 -0.09 -0.18 -0.18 -0.22 -0.31  0.73 -0.11  0.91 -0.13\nFLORA     -0.07 -0.13 -0.09 -0.18 -0.18 -0.22 -0.31  0.73 -0.11  0.91 -0.13\n6/27       0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n3/2        0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n4/12       0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n9/26       0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n2/25       0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n5/19       0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n3/15       0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n9/16       0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n4/8        0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n6/10       0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n2/23       0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n4/7        0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n11/21      0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n8/3        0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n          11/21   8/3\nEVELYN    -0.27 -0.27\nLAURA     -0.24 -0.24\nTHERESA   -0.27 -0.27\nBRENDA    -0.24 -0.24\nCHARLOTTE -0.13 -0.13\nFRANCES   -0.13 -0.13\nELEANOR   -0.13 -0.13\nPEARL     -0.10 -0.10\nRUTH      -0.13 -0.13\nVERNE     -0.13 -0.13\nMYRNA     -0.13 -0.13\nKATHERINE  0.80  0.80\nSYLVIA     0.76  0.76\nNORA       0.73  0.73\nHELEN     -0.17 -0.17\nDOROTHY   -0.07 -0.07\nOLIVIA    -0.07 -0.07\nFLORA     -0.07 -0.07\n6/27       0.00  0.00\n3/2        0.00  0.00\n4/12       0.00  0.00\n9/26       0.00  0.00\n2/25       0.00  0.00\n5/19       0.00  0.00\n3/15       0.00  0.00\n9/16       0.00  0.00\n4/8        0.00  0.00\n6/10       0.00  0.00\n2/23       0.00  0.00\n4/7        0.00  0.00\n11/21      0.00  0.00\n8/3        0.00  0.00\n```\n:::\n:::\n\n\nWhich is a bipartite version of modularity matrix ($\\mathbf{\\hat{B}}$) with the same block structure as the bipartite adjacency matrix:\n\n$$\n\\mathbf{\\hat{B}} = \\left[\n\\begin{matrix}\n\\mathbf{O}_{M \\times M} & \\mathbf{B}_{M \\times N} \\\\\n\\mathbf{B}^T_{N \\times M} & \\mathbf{O}_{N \\times N}\n\\end{matrix}\n\\right]\n$$\n\nNote that in $\\mathbf{\\hat{B}}$ the modularity (expected number of edges) is set to zero for nodes of the same set (people and people; groups and groups), and to non-zero values for nodes of different sets (persons and groups).\n\nNow, we can use the same approach we used in the unipartite case to check the modularity of some hypothetical partition of the nodes in the graph. \n\nTake for instance, the CA scores in the first dimension that we obtained earlier. They do seem to divide the persons and groups into distinct communities. So let's transform them into membership vectors (using dummy coding):\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   u1 <- rep(0, n)\n   u2 <- rep(0, n)\n   d <- c(eig.vec.p[, 1], eig.vec.g[, 1]) #original CA scores\n   u1[which(d > 0)] <- 1\n   u2[which(d <= 0)] <- 1\n   U <- cbind(u1, u2)\n   rownames(U) <- rownames(B2)\n   U\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          u1 u2\nEVELYN     0  1\nLAURA      0  1\nTHERESA    0  1\nBRENDA     0  1\nCHARLOTTE  0  1\nFRANCES    0  1\nELEANOR    0  1\nPEARL      0  1\nRUTH       0  1\nVERNE      1  0\nMYRNA      1  0\nKATHERINE  1  0\nSYLVIA     1  0\nNORA       1  0\nHELEN      1  0\nDOROTHY    1  0\nOLIVIA     1  0\nFLORA      1  0\n6/27       0  1\n3/2        0  1\n4/12       0  1\n9/26       0  1\n2/25       0  1\n5/19       0  1\n3/15       0  1\n9/16       0  1\n4/8        1  0\n6/10       1  0\n2/23       1  0\n4/7        1  0\n11/21      1  0\n8/3        1  0\n```\n:::\n:::\n\n\nRecall that we can check the modularity of a partition coded in a dummy matrix like `U` using the formula:\n\n$$\n\\frac{tr(U^TBU)}{\\sum_i\\sum_ja_{ij}}\n$$\n\nWhere $tr$ is the trace matrix operation (sum of the diagonals).\n\nLet's check it out:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   A2 <- as.matrix(as_adjacency_matrix(g))\n   round(sum(diag(t(U) %*% B2 %*% U))/sum(A2), 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.318\n```\n:::\n:::\n\n\nWhich looks pretty good!\n\nHere's a plot of the bipartite graph with nodes colored by the CA induced community bipartition:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   V(g)$type <- bipartite_mapping(g)$type\n   V(g)$shape <- ifelse(V(g)$type, \"square\", \"circle\")\n   V(g)$color <- (U[, 1] - U[, 2]) + 2\n   set.seed(123)\n   plot(g, \n     vertex.size=7, vertex.frame.color=\"lightgray\", \n     vertex.label.dist = 1.5, edge.curved=0.2, \n     vertex.label.cex = 1.35)\n```\n\n::: {.cell-output-display}\n![Southern Women's bipartite graph by the best two-community partition according to CA.](ca_files/figure-html/unnamed-chunk-34-1.png){width=1152}\n:::\n:::\n",
    "supporting": [
      "ca_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}