[
  {
    "objectID": "blondel.html",
    "href": "blondel.html",
    "title": "Role Similarity Across Graphs",
    "section": "",
    "text": "Sometimes we may want to figure out how similar a given node’s position in one social network is to that of another node in a different network. This calls for a method that could allow us to compare how similar a node in one graph is to other nodes in another graph.\nA particularly interesting version of this problem arises when we have information on the same set of nodes across different set of relations. In that case, we may be interested in answering the question as to whether nodes occupy similar or dissimilar positions across the networks defined by the different relations.\nBlondel et al. (2004) describe an approach that can help us make headway on this problem. They use a similar iterative procedure that we saw can be used to compute status scores from directed graphs (like PageRank and HITS) but this time to compute similarity scores between pairs of nodes across graphs.\nThe idea, just like with the status scores, is that the two set of nodes in each graph start with the same similarity scores, and then we update them as we traverse the connectivity structure of the two graphs.\nSo let’s say the adjacency matrix of the first graph is \\(\\mathbf{A}\\) and that of the second graph is \\(\\mathbf{B}\\). The first graph has \\(n_A\\) number of nodes and the corresponding quantity in the second graph is \\(n_B\\) our target similarity matrix \\(\\mathbf{Z}\\), comparing the node sets in the two graphs, will therefore be of dimensions \\(n_A \\times n_B\\).\nWe initialize \\(z_{ij}(0) = 1\\) for all \\(i\\) and \\(j\\); that is, \\(\\mathbf{Z}(0)\\) is a matrix full of ones. At each time step subsequent to that, we fill up the \\(\\mathbf{Z}\\) matrix with new values according to:\n\\[\n   \\mathbf{Z}(t + 1) = \\mathbf{B}\\mathbf{Z(t)}\\mathbf{A}^T + \\mathbf{B}^T\\mathbf{Z(t)}\\mathbf{A}\n\\]\nTo ensure convergence, we then normalize the \\(\\mathbf{Z}\\) matrix after every update using our trusty Euclidean norm:\n\\[\n\\mathbf{Z}(t > 0) = \\frac{\\mathbf{Z}}{||\\mathbf{Z}||_2}\n\\]\nLet us see how this would work with real data. Let us start with the case of the same set of nodes measured across two different relational networks. Here, we will use Krackhardt’s high tech manager’s data which contains directed adjacency relations based on both friendship and advice nominations.\n\n\n\n\n\n\nOlder Men Partner graph\n\n\n\n\n\n\n\n\n\nWomen graph\n\n\n\n\n\nA function to compute the Blondel similarity can be written as:\n\n   blondel.sim <- function(A, B) {\n      K <- matrix(1, nrow(B), nrow(A))\n      if (is.null(rownames(A)) == TRUE) {\n         rownames(A) <- 1:nrow(A)\n         colnames(A) <- 1:nrow(A)\n         }\n      if (is.null(rownames(B)) == TRUE) {\n         rownames(B) <- 1:nrow(B)\n         colnames(B) <- 1:nrow(B)\n         }\n      k <- 1\n      diff <- 1\n      old.diff <- 2\n      while (diff != old.diff | k %% 2 == 0) {\n         old.diff <- diff\n         K.old <- K\n         K <- (B %*% K.old %*% t(A)) + (t(B) %*% K.old %*% A)\n         K <- K/norm(K, type = \"F\")\n         diff <- abs(sum(abs(K)) - sum(abs(K.old)))\n         k <- k + 1\n      }\n   for (j in 1:ncol(K)) {\n      K[, j] <- K[, j]/max(K[, j])\n      }\n   rownames(K) <- rownames(B)\n   colnames(K) <- rownames(A)\n   return(list(K = K, k = k, diff = diff))\n   }\n\n\n   library(kableExtra)\n   A <- as.matrix(as_adjacency_matrix(g1))\n   B <- as.matrix(as_adjacency_matrix(g2))\n   K <- round(blondel.sim(A, B)[[1]], 2)\n   kbl(K, align = \"c\", format = \"html\", row.names = TRUE) %>% \n      column_spec(1, bold = TRUE) %>% \n      kable_styling(full_width = TRUE,\n                     bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\n \n  \n      \n    1 \n    2 \n    3 \n    4 \n    5 \n    6 \n    7 \n    8 \n    9 \n    10 \n    11 \n    12 \n    13 \n  \n \n\n  \n    1 \n    0.25 \n    0.22 \n    0.41 \n    0.48 \n    0.45 \n    0.11 \n    0.47 \n    0.24 \n    0.20 \n    0.30 \n    0.47 \n    0.25 \n    0.36 \n  \n  \n    2 \n    0.39 \n    0.38 \n    0.38 \n    0.27 \n    0.16 \n    0.37 \n    0.15 \n    0.38 \n    0.34 \n    0.38 \n    0.36 \n    0.34 \n    0.34 \n  \n  \n    3 \n    0.64 \n    0.63 \n    0.54 \n    0.33 \n    0.09 \n    0.67 \n    0.10 \n    0.63 \n    0.56 \n    0.58 \n    0.51 \n    0.55 \n    0.51 \n  \n  \n    4 \n    0.65 \n    0.66 \n    0.52 \n    0.26 \n    0.00 \n    0.71 \n    0.00 \n    0.63 \n    0.60 \n    0.59 \n    0.46 \n    0.58 \n    0.50 \n  \n  \n    5 \n    1.00 \n    1.00 \n    0.99 \n    0.73 \n    0.28 \n    1.00 \n    0.34 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n  \n  \n    6 \n    0.66 \n    0.67 \n    0.56 \n    0.33 \n    0.04 \n    0.71 \n    0.06 \n    0.65 \n    0.64 \n    0.62 \n    0.53 \n    0.62 \n    0.57 \n  \n  \n    7 \n    0.05 \n    0.04 \n    0.12 \n    0.14 \n    0.16 \n    0.00 \n    0.15 \n    0.04 \n    0.04 \n    0.08 \n    0.12 \n    0.05 \n    0.09 \n  \n  \n    8 \n    0.32 \n    0.31 \n    0.37 \n    0.32 \n    0.24 \n    0.26 \n    0.24 \n    0.30 \n    0.29 \n    0.34 \n    0.37 \n    0.31 \n    0.34 \n  \n  \n    9 \n    0.49 \n    0.44 \n    0.88 \n    1.00 \n    1.00 \n    0.20 \n    1.00 \n    0.47 \n    0.40 \n    0.63 \n    0.96 \n    0.50 \n    0.71 \n  \n  \n    10 \n    0.31 \n    0.28 \n    0.73 \n    0.89 \n    0.89 \n    0.04 \n    0.91 \n    0.29 \n    0.30 \n    0.47 \n    0.82 \n    0.39 \n    0.60 \n  \n  \n    11 \n    0.86 \n    0.86 \n    1.00 \n    0.82 \n    0.58 \n    0.76 \n    0.57 \n    0.86 \n    0.79 \n    0.92 \n    0.97 \n    0.82 \n    0.86 \n  \n  \n    12 \n    0.15 \n    0.15 \n    0.18 \n    0.16 \n    0.04 \n    0.15 \n    0.08 \n    0.15 \n    0.20 \n    0.17 \n    0.21 \n    0.20 \n    0.23 \n  \n  \n    13 \n    0.34 \n    0.34 \n    0.42 \n    0.37 \n    0.26 \n    0.29 \n    0.28 \n    0.33 \n    0.34 \n    0.38 \n    0.43 \n    0.36 \n    0.40 \n  \n  \n    14 \n    0.18 \n    0.17 \n    0.32 \n    0.35 \n    0.33 \n    0.09 \n    0.34 \n    0.17 \n    0.16 \n    0.23 \n    0.35 \n    0.19 \n    0.27 \n  \n  \n    15 \n    0.40 \n    0.41 \n    0.41 \n    0.28 \n    0.16 \n    0.39 \n    0.15 \n    0.40 \n    0.38 \n    0.41 \n    0.37 \n    0.38 \n    0.36 \n  \n  \n    16 \n    0.53 \n    0.51 \n    0.92 \n    0.99 \n    0.87 \n    0.29 \n    0.91 \n    0.52 \n    0.54 \n    0.69 \n    1.00 \n    0.61 \n    0.80 \n  \n  \n    17 \n    0.46 \n    0.44 \n    0.70 \n    0.74 \n    0.49 \n    0.33 \n    0.58 \n    0.46 \n    0.53 \n    0.55 \n    0.81 \n    0.57 \n    0.71 \n  \n  \n    18 \n    0.34 \n    0.31 \n    0.67 \n    0.81 \n    0.63 \n    0.15 \n    0.72 \n    0.34 \n    0.41 \n    0.46 \n    0.82 \n    0.47 \n    0.67 \n  \n\n\n\n\n\n\n   g <- make_empty_graph(2)\n   g <- add_edges(g, c(1,2))\n   V(g)$name <- c(\"Hub\", \"Authority\")\n   plot(g, \n        edge.arrow.size=1, \n        vertex.color=\"tan2\", \n        vertex.size=12, vertex.frame.color=\"lightgray\", \n        vertex.label.color=\"black\", vertex.label.cex=1.5, \n        vertex.label.dist=-3)\n\n\n\n\n\n   A <- as.matrix(as_adjacency_matrix(g))\n   B <- as.matrix(as_adjacency_matrix(g2))\n   K <- round(blondel.sim(A, B)[[1]], 4)\n   tab <- cbind(K, Hub.Score = round(hits_scores(g2)$hub, 4), \n         Auth.Score = round(hits_scores(g2)$authority, 4))\n   kbl(tab, align = \"c\", format = \"html\", row.names = TRUE) %>% \n      column_spec(1, bold = TRUE) %>% \n      kable_styling(full_width = TRUE,\n                     bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\n \n  \n      \n    Hub \n    Authority \n    Hub.Score \n    Auth.Score \n  \n \n\n  \n    1 \n    0.3810 \n    0.0000 \n    0.3810 \n    0.0000 \n  \n  \n    2 \n    0.1434 \n    0.3415 \n    0.1434 \n    0.3415 \n  \n  \n    3 \n    0.0813 \n    0.6021 \n    0.0813 \n    0.6021 \n  \n  \n    4 \n    0.0000 \n    0.7826 \n    0.0000 \n    0.7826 \n  \n  \n    5 \n    0.1654 \n    1.0000 \n    0.1654 \n    1.0000 \n  \n  \n    6 \n    0.0000 \n    0.7826 \n    0.0000 \n    0.7826 \n  \n  \n    7 \n    0.1434 \n    0.0000 \n    0.1434 \n    0.0000 \n  \n  \n    8 \n    0.2235 \n    0.2858 \n    0.2235 \n    0.2858 \n  \n  \n    9 \n    1.0000 \n    0.0386 \n    1.0000 \n    0.0386 \n  \n  \n    10 \n    0.8973 \n    0.0000 \n    0.8973 \n    0.0000 \n  \n  \n    11 \n    0.6110 \n    0.6559 \n    0.6110 \n    0.6559 \n  \n  \n    12 \n    0.0000 \n    0.2095 \n    0.0000 \n    0.2095 \n  \n  \n    13 \n    0.2239 \n    0.3364 \n    0.2239 \n    0.3364 \n  \n  \n    14 \n    0.3197 \n    0.0523 \n    0.3197 \n    0.0523 \n  \n  \n    15 \n    0.1434 \n    0.4036 \n    0.1434 \n    0.4036 \n  \n  \n    16 \n    0.8432 \n    0.2174 \n    0.8432 \n    0.2174 \n  \n  \n    17 \n    0.4089 \n    0.3188 \n    0.4089 \n    0.3188 \n  \n  \n    18 \n    0.5222 \n    0.0955 \n    0.5222 \n    0.0955 \n  \n\n\n\n\n\n\n   g <- make_empty_graph(3)\n   g <- add_edges(g, c(1,2, 2,3))\n   plot(g, \n        edge.arrow.size=1, \n        vertex.color=\"tan2\", \n        vertex.size=12, vertex.frame.color=\"lightgray\", \n        vertex.label.color=\"black\", vertex.label.cex=1.5, \n        vertex.label.dist=2)\n\n\n\n\n\n   A <- as.matrix(as_adjacency_matrix(g))\n   B <- as.matrix(as_adjacency_matrix(g2))\n   K <- round(blondel.sim(A, B)[[1]], 4)\n   kbl(K, align = \"c\", format = \"html\", row.names = TRUE) %>% \n      column_spec(1, bold = TRUE) %>% \n      kable_styling(full_width = TRUE,\n                     bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\n \n  \n      \n    1 \n    2 \n    3 \n  \n \n\n  \n    1 \n    0.5190 \n    0.3321 \n    0.1135 \n  \n  \n    2 \n    0.1318 \n    0.3472 \n    0.3600 \n  \n  \n    3 \n    0.0998 \n    0.4586 \n    0.6923 \n  \n  \n    4 \n    0.0000 \n    0.4888 \n    0.7547 \n  \n  \n    5 \n    0.5241 \n    0.8968 \n    1.0000 \n  \n  \n    6 \n    0.0954 \n    0.5091 \n    0.7547 \n  \n  \n    7 \n    0.1318 \n    0.1234 \n    0.0000 \n  \n  \n    8 \n    0.2501 \n    0.3584 \n    0.2755 \n  \n  \n    9 \n    1.0000 \n    0.8236 \n    0.2000 \n  \n  \n    10 \n    0.9913 \n    0.6859 \n    0.0318 \n  \n  \n    11 \n    0.5445 \n    1.0000 \n    0.7371 \n  \n  \n    12 \n    0.1971 \n    0.1427 \n    0.1530 \n  \n  \n    13 \n    0.3175 \n    0.4117 \n    0.2970 \n  \n  \n    14 \n    0.3710 \n    0.2875 \n    0.0918 \n  \n  \n    15 \n    0.1318 \n    0.4205 \n    0.3780 \n  \n  \n    16 \n    0.9998 \n    0.8747 \n    0.2453 \n  \n  \n    17 \n    0.7783 \n    0.5841 \n    0.3102 \n  \n  \n    18 \n    0.9644 \n    0.5162 \n    0.1302 \n  \n\n\n\n\n\n\n\n\n\nReferences\n\nBlondel, Vincent D, Anahı́ Gajardo, Maureen Heymans, Pierre Senellart, and Paul Van Dooren. 2004. “A Measure of Similarity Between Graph Vertices: Applications to Synonym Extraction and Web Searching.” SIAM Review 46 (4): 647–66."
  },
  {
    "objectID": "cube.html",
    "href": "cube.html",
    "title": "The Cube",
    "section": "",
    "text": "Brandes, Borgatti, and Freeman (2016) discuss the centrality “cube,” an interesting and intuitive way to understand the way betweenness centrality works, as well as the dual connection between closeness and betweenness.\nLet us illustrate using a simple example. We begin by creating an Erdos-Renyi graph with eight nodes and connection probability \\(p = 0.5\\):\nThe resulting graph looks like:\nThe basic innovation behind the centrality cube is to store the intermediation information among every node triplet in the graph \\(s\\), \\(r\\), \\(b\\) (standing for “sender,” receiver,” and “broker”) in a three dimensional array rather than the usual two dimensional matrix.\nThe three dimensional array can be thought of as a “cube” by stacking multiple reachability matrices between every pair \\(s\\) and \\(r\\) along a three dimensional dimension \\(b\\). So each “b-slice” of the cube will contain the number of times node \\(b\\) stands in a shortest path between \\(s\\) and \\(r\\) divided by the total number of paths between \\(s\\) and \\(r\\) which as you recall computes the pairwise betweenness of \\(b\\) with respect to \\(s\\) and \\(r\\).\nLet’s see how that works."
  },
  {
    "objectID": "cube.html#building-the-cube",
    "href": "cube.html#building-the-cube",
    "title": "The Cube",
    "section": "Building the Cube",
    "text": "Building the Cube\nWe begin by writing a simple user-defined function to count the total number of shortest paths between each pair of nodes:\n\n   nsp <- function(x) {\n      n <- vcount(g)\n      S <- matrix(0, n, n)\n      for (i in 1:n) {\n            for (j in 1:n) {\n                  if (j %in% neighbors(x, i) == FALSE) {\n                     S[i, j] <- length(all_shortest_paths(x, i, j)$vpaths)\n               }\n            }\n         }\n      return(S)\n   }\n\nThe function is called nsp and takes a graph as input and returns and matrix called \\(\\mathbf{S}\\) with entries \\(s_{ij}\\) equal to the total number of shortest paths between \\(i\\) and \\(j\\). This is done by computing the length of the list returned by the all_shortest_paths function in igraph for each pair of non-adjacent nodes. This is done in two steps.\n\nFirst, we check whether \\(j\\) is a neighbor of \\(i\\) using the neighbors function in igraph. The neighbors function takes a graph and a node id as input and returns a vector of that node’s neighbors in the graph. We want the function to update the \\(S\\) matrix only when \\(i\\) and \\(j\\) are not adjacent (indirectly connected).\nSecond, we use the all_shortest_paths function to actually compute the number of shortest paths between \\(i\\) and \\(j\\). This function takes three inputs: (1) A graph object, (2) a sender node id, and (3) a receiver node id (which can be a vector of receiver nodes), and returns a list of the paths between the sender and receiver nodes in the form of vectors of node ids defining each path as elements of a list called “vpaths.”\n\nNow we are ready to write a user defined function to build the cube. Here’s a not-so-efficient (programming wise) but working example:\n\n   cube <- function(g) {\n      n <- vcount(g)\n      c <- array(rep(n^3, 0), c(n, n, n))\n      S <- nsp(g)\n      for (b in 1:n) {\n         for (s in 1:n) {\n            for (r in 1:n) {\n               if (s != r & r %in% neighbors(g, s) == FALSE) {\n                  p.sr <- all_shortest_paths(g, s, r)$vpaths\n                  b.sr <- lapply(p.sr, function(x) {x[-c(1, length(x))]}) \n                  c[s, r, b] <- sum(as.numeric(sapply(b.sr, function(x) {b %in% x})))\n                  c[s, r, b] <- c[s, r, b]/S[s, r]\n               }\n            }\n         }\n      }\n   c[is.na(c)] <- 0\n   return(c)\n   }\n\nIn line 1, we name the function cube. Line 3 initializes the array in R. It takes a string of zeros of length \\(n^3\\) where \\(n\\) is the number of nodes and crams them into an \\(n \\times n \\times n\\) array. In this case, since \\(n = 8\\), this means eight empty matrices of dimensions \\(8 \\times 8\\) stacked together to form our cube full of zeros. The \\(ijk^{th}\\) cell of the array corresponds to sender \\(i\\), receiver \\(j\\) and broker node \\(k\\). Line 4 computes the matrix \\(S\\) containing the number of shortest paths between every sender and receiver node in the graph.\nLines 5-15 populate the cube with the required information using an (inefficient) triple for loop. As noted some, useful igraph functions come into play here:\n\nIn line 8 the if conditional inside the triple loop uses the neighbors function in igraph and checks that node \\(r\\) is not a neighbor of \\(s\\) (if they are directly connected then node \\(b\\) cannot be a broker).\nAfter we check that \\(s\\) and \\(r\\) are not neighbors, we use the all_shortest_paths function in igraph to get all the shortest paths between \\(s\\) and \\(r\\) in line 9.\nLine 10 uses some lapply magic to drop the source and receiver nodes from the list of node ids vectors returned by the all_shortest_paths function.\nLine 11 uses additional sapply magic and the base R function %in% to check how many times each broker node \\(b\\) shows up in that list of shortest paths as an inner node between \\(s\\) and \\(r\\); we put that number in the \\(ijk^{th}\\) cell of the array, and loop through all triplets until we are done.\nLine 12 takes the number computed in line 11 and divides by the total number of shortest paths between \\(s\\) and \\(r\\) which is the betweenness ratio we are seeking."
  },
  {
    "objectID": "cube.html#exploring-the-cube",
    "href": "cube.html#exploring-the-cube",
    "title": "The Cube",
    "section": "Exploring the Cube",
    "text": "Exploring the Cube\nOnce we have our array, we can create all kinds of interesting sub-matrices containing the intermediation information in the graph by summing rows and columns of the array along different dimensions.\nFirst, let us see what’s in the cube. We can query specific two-dimensional sub-matrices using an extension of the usual format for querying matrices in R for three-dimensional arrays. For instance this:\n\n   srb <- cube(g)\n   round(srb[, , 2], 2)\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n[1,]  0.0    0 0.00    0 0.00  0.5    0 0.00\n[2,]  0.0    0 0.00    0 0.00  0.0    0 0.00\n[3,]  0.0    0 0.00    0 0.00  0.0    0 0.33\n[4,]  0.0    0 0.00    0 0.00  0.0    0 0.00\n[5,]  0.0    0 0.00    0 0.00  0.0    0 0.33\n[6,]  0.5    0 0.00    0 0.00  0.0    0 1.00\n[7,]  0.0    0 0.00    0 0.00  0.0    0 0.00\n[8,]  0.0    0 0.33    0 0.33  1.0    0 0.00\n\n\nCreates a three-dimensional matrix of pairwise betweenness probabilities and assigns it to the srb object in line 1, and looks at the \\(s_{\\bullet} \\times r_{\\bullet} \\times b_2\\) entry in line 2.\nEach entry in the matrix is the probability that node 2 stands on a shortest path between the row sender and the column receiver node. For instance, the 0.33 in the entry corresponding to row 5 and column 8 tells us that node 2 stands in one third of the shortest paths between nodes 5 and 8 (there are 3 distinct shortest paths between 5 and 8).\nBecause each sub-matrix in the cube is a matrix, we can do the usual matrix operations on them. For instance, let’s take the row sums of the \\(s\\) to \\(r\\) matrix corresponding to node 3 as the broker. This can be done like this:\n\n   d.3 <- rowSums(srb[ , , 3])\n   names(d.3) <- 1:vcount(g)\n   d.3\n\n  1   2   3   4   5   6   7   8 \n1.5 2.0 0.0 3.0 6.0 3.5 3.0 1.0 \n\n\nAs Brandes, Borgatti, and Freeman (2016), note this vector gives us the dependence of each node in the graph on node 3. Obviously node 3 doesn’t depend on itself so there is a zero on the third spot in the vector. As is clear from the plot, node 5 is the most dependent on 3 for intermediation with the rest of the nodes in the graph.\nWe can also pick a particular sender and receiver node and sum all their dyadic entries in the cube across the third (broker) dimension:\n\n   sum(srb[1, 6, ])\n\n[1] 2\n\n\nThis number is equivalent to the geodesic distance between the nodes minus one:\n\n   distances(g)[1, 6] - 1\n\n[1] 2"
  },
  {
    "objectID": "cube.html#betweenness-and-closeness-in-the-cube",
    "href": "cube.html#betweenness-and-closeness-in-the-cube",
    "title": "The Cube",
    "section": "Betweenness and Closeness in the Cube",
    "text": "Betweenness and Closeness in the Cube\nThe betweenness centrality of each node is encoded in the cube, because we already computed the main ratio that the measure depends on. For instance, let’s look at the matrix composed by taking the slice of cube that corresponds to node 3 as a broker:\n\n   srb[, , 3]\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n[1,]  0.0  0.0    0  0.0    1  0.5  0.0    0\n[2,]  0.0  0.0    0  0.5    1  0.0  0.5    0\n[3,]  0.0  0.0    0  0.0    0  0.0  0.0    0\n[4,]  0.0  0.5    0  0.0    1  1.0  0.5    0\n[5,]  1.0  1.0    0  1.0    0  1.0  1.0    1\n[6,]  0.5  0.0    0  1.0    1  0.0  1.0    0\n[7,]  0.0  0.5    0  0.5    1  1.0  0.0    0\n[8,]  0.0  0.0    0  0.0    1  0.0  0.0    0\n\n\nThe sum of the all the cells in this matrix (divided by two) correspond to node 3’s betweenness centrality:\n\n   sum(srb[, , 3])/2\n\n[1] 10\n\n   betweenness(g)[3]\n\n[1] 10\n\n\nSo to get each node’s betweenness we just can just sum up the entries in each of the cube’s sub-matrices:\n\n   b.cube <- round(colSums(srb, dims = 2)/2, 2)\n   b.igraph <- round(betweenness(g), 2)\n   names(b.cube) <- 1:vcount(g)\n   names(b.igraph) <- 1:vcount(g)\n   b.cube\n\n    1     2     3     4     5     6     7     8 \n 0.00  2.17 10.00  0.67  0.00  0.00  3.17  4.00 \n\n   b.igraph\n\n    1     2     3     4     5     6     7     8 \n 0.00  2.17 10.00  0.67  0.00  0.00  3.17  4.00 \n\n\nNote the neat trick of using the argument dims = 2 in the usual colSums command. This tells colSums that we are dealing with a three dimensional matrix, and that what we want is the sum of the columns across the cube’s third dimension (the brokers). Note also that we divide the cube betweenness by two because we are summing identical entries across the upper and lower triangle of the symmetric dyadic brokerage matrices inside the cube (not surprisingly, node 3 is the top betweenness centrality node).\nAs Brandes, Borgatti, and Freeman (2016) point out, using the cube info, we can build a matrix of dependencies between each pair of nodes. In this matrix, the rows correspond to a sender (or receiver) node, the columns to a broker node and the \\(sb^{th}\\) entry contains the sum of the proportion of paths containing the broker nodes that starts with the sender node and end with some other node in the graph.\nHere’s a function that uses the cube info to build the dependency matrix that Brandes, Borgatti, and Freeman (2016) talk about using the cube as input:\n\n   dep.ij <- function(c) {\n      n <- nrow(c)\n      dep.ij <- rowSums(c[, , 1])\n      for (i in 2:n) {\n         dep.ij <- cbind(dep.ij,  rowSums(c[, , i]))\n         \n         }\n      rownames(dep.ij) <- 1:n\n      colnames(dep.ij) <- 1:n\n      return(dep.ij)\n   }\n\nThis function just takes the various vectors formed by the row sums of the sender-receiver matrix across each value of the third dimension (which is just each node in the graph when playing the broker role). It then returns a regular old \\(n \\times n\\) containing the info.\nHere’s the result when applied to our little example:\n\n   library(kableExtra)\n   kbl(round(dep.ij(srb), 2), \n       format = \"html\", align = \"c\", row.names = TRUE,\n       caption = \"Dependence Matrix.\") %>% \n      column_spec(1, bold = TRUE) %>%\n   kable_styling(bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\nDependence Matrix.\n \n  \n      \n    1 \n    2 \n    3 \n    4 \n    5 \n    6 \n    7 \n    8 \n  \n \n\n  \n    1 \n    0 \n    0.50 \n    1.5 \n    0.00 \n    0 \n    0 \n    2.50 \n    2.5 \n  \n  \n    2 \n    0 \n    0.00 \n    2.0 \n    0.00 \n    0 \n    0 \n    0.00 \n    2.0 \n  \n  \n    3 \n    0 \n    0.33 \n    0.0 \n    0.33 \n    0 \n    0 \n    1.33 \n    0.0 \n  \n  \n    4 \n    0 \n    0.00 \n    3.0 \n    0.00 \n    0 \n    0 \n    0.00 \n    2.0 \n  \n  \n    5 \n    0 \n    0.33 \n    6.0 \n    0.33 \n    0 \n    0 \n    1.33 \n    0.0 \n  \n  \n    6 \n    0 \n    1.50 \n    3.5 \n    0.00 \n    0 \n    0 \n    0.50 \n    0.5 \n  \n  \n    7 \n    0 \n    0.00 \n    3.0 \n    0.00 \n    0 \n    0 \n    0.00 \n    1.0 \n  \n  \n    8 \n    0 \n    1.67 \n    1.0 \n    0.67 \n    0 \n    0 \n    0.67 \n    0.0 \n  \n\n\n\n\n\nNote that this is valued matrix that is also asymmetric. Take for instance, node 3. Every node in the graph depends on node 3 for access to other nodes, but node 3 does not depend on nodes 1, 5, 6, or 8.\nInterestingly, as Brandes, Borgatti, and Freeman (2016) also show, the betweenness centrality also can be calculated from the dependency matrix! All we need to do is compute the column sums, equivalent to in-degree in the directed dependence network:\n\n   round(colSums(dep.ij(srb))/2, 2)\n\n    1     2     3     4     5     6     7     8 \n 0.00  2.17 10.00  0.67  0.00  0.00  3.17  4.00 \n\n\nEven more interestingly, closeness centrality is also in the dependence matrix! It is given by the outdegree of each actor in the directed dependence network, corresponding to the row sums of the matrix (shifted by a constant given by \\(n-1\\)).\n\n   c.c <- rowSums(distances(g))\n   c.d <- rowSums(dep.ij(srb)) + (vcount(g) - 1)\n   names(c.c) <- 1:vcount(g)\n   names(c.d) <- 1:vcount(g)\n   round(1/c.c, 3)\n\n    1     2     3     4     5     6     7     8 \n0.071 0.091 0.111 0.083 0.067 0.077 0.091 0.091 \n\n   round(1/c.d, 3)\n\n    1     2     3     4     5     6     7     8 \n0.071 0.091 0.111 0.083 0.067 0.077 0.091 0.091 \n\n\nHere we see that node 3 is also the top in closeness, followed closely (pun intended) by nodes 2, 7, and 8. This makes sense because an actor with high closeness is one that has low dependence on key nodes to be able to reach others.\nOf course, closeness is also in the cube because of the mathematical relationship we saw earlier between the sum of entries between senders and receivers across brokers in the cube and the geodesic distance.\nFor instance, let’s get the matrix corresponding to node 3’s role as sender across all brokers and receivers:\n\n   round(srb[3, , ], 2)\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n[1,]    0 0.00    0 0.00    0    0 1.00    0\n[2,]    0 0.00    0 0.00    0    0 0.00    0\n[3,]    0 0.00    0 0.00    0    0 0.00    0\n[4,]    0 0.00    0 0.00    0    0 0.00    0\n[5,]    0 0.00    0 0.00    0    0 0.00    0\n[6,]    0 0.00    0 0.00    0    0 0.00    0\n[7,]    0 0.00    0 0.00    0    0 0.00    0\n[8,]    0 0.33    0 0.33    0    0 0.33    0\n\n\nThe entries of this matrix give us the probability that node 3 is the sender, whenever the row node is the is the broker (an inner node in the path) and the column node is the receiver.\nFor instance, the value 0.3 in row 8 and column 1 tells us that node 3 is the sender node in one third of the paths that end in node 1 and feature node 8 as a broker.\nInterestingly, the sums of the entries in this matrix are equivalent to the sum of the geodesic distances between node 3 and every other node in the graph shifted by a constant (\\(n- 1\\)):\n\n   sum(srb[3, , ]) + (vcount(g) - 1)\n\n[1] 9\n\n   sum(distances(g)[3, ])\n\n[1] 9\n\n\nSo the closeness centrality can be computed from the cube as follows:\n\n   c <- 0\n   n <- vcount(g)\n   for (i in 1:n) {\n      c[i] <- 1/(sum(srb[i, , ]) + (n - 1))\n   }\n   round(c, 3)\n\n[1] 0.071 0.091 0.111 0.083 0.067 0.077 0.091 0.091\n\n\nWhich is the same as:\n\n   round(closeness(g), 3)\n\n[1] 0.071 0.091 0.111 0.083 0.067 0.077 0.091 0.091"
  },
  {
    "objectID": "handout1.html",
    "href": "handout1.html",
    "title": "Basic Network Statistics",
    "section": "",
    "text": "Here we will analyze a small network and computer some basic statistics of interest. The first thing we need to do is get some data! For this purpose, we will use the package networkdata (available here). To install the package, use the following code:\n\n   #install.packages(\"remotes\") \n   remotes::install_github(\"schochastics/networkdata\")\n\nTo load the network datasets in the networkdata just type:\n\n   library(networkdata)\n\nThe package contains a bunch of human and animal social networks to browse through them, type:\n\n   data(package = \"networkdata\")\n\nWe will pick one of the movies for this analysis, namely, Pulp Fiction. This is movie_559. In the movie network two characters are linked by an edge if they appear in a scene together. The networkdata data sets come in igraph format, so we need to load that package (or install it using install.packages if you haven’t done that yet).\n\n   #install.packages(\"igraph\") \n   library(igraph)\n   g <- movie_559"
  },
  {
    "objectID": "handout1.html#number-of-nodes-and-edges",
    "href": "handout1.html#number-of-nodes-and-edges",
    "title": "Basic Network Statistics",
    "section": "Number of Nodes and Edges",
    "text": "Number of Nodes and Edges\nNow we are ready to compute some basic network statistics. As with any network, we want to know what the number of nodes and the number of edges (links) are. Since this is a relatively small network, we can begin by listing the actors.\n\n   V(g)\n\n+ 38/38 vertices, named, from 9e7cc7a:\n [1] BRETT           BUDDY           BUTCH           CAPT KOONS     \n [5] ED SULLIVAN     ENGLISH DAVE    ESMARELDA       FABIENNE       \n [9] FOURTH MAN      GAWKER #2       HONEY BUNNY     JIMMIE         \n[13] JODY            JULES           LANCE           MANAGER        \n[17] MARSELLUS       MARVIN          MAYNARD         MIA            \n[21] MOTHER          PATRON          PEDESTRIAN      PREACHER       \n[25] PUMPKIN         RAQUEL          ROGER           SPORTSCASTER #1\n[29] SPORTSCASTER #2 THE GIMP        THE WOLF        VINCENT        \n[33] WAITRESS        WINSTON         WOMAN           YOUNG MAN      \n[37] YOUNG WOMAN     ZED            \n\n\nThe function V takes the igraph network object as input and returns an igraph.vs object as output (short for “igraph vertex sequence”), listing the names (if given as a graph attribute) of each node. The first line also tells us that there are 38 nodes in this network.\nThe igraph.vs object operates much like an R character vector, so we can query its length to figure out the number of nodes:\n\n   length(V(g))\n\n[1] 38\n\n\nThe analogue function for edges in igraph is E which also takes the network object as input and returns an object of class igraph.es (“igraph edge sequence”) as output:\n\n   E(g)\n\n+ 102/102 edges from 9e7cc7a (vertex names):\n [1] BRETT      --MARSELLUS       BRETT      --MARVIN         \n [3] BRETT      --ROGER           BRETT      --VINCENT        \n [5] BUDDY      --MIA             BUDDY      --VINCENT        \n [7] BRETT      --BUTCH           BUTCH      --CAPT KOONS     \n [9] BUTCH      --ESMARELDA       BUTCH      --GAWKER #2      \n[11] BUTCH      --JULES           BUTCH      --MARSELLUS      \n[13] BUTCH      --PEDESTRIAN      BUTCH      --SPORTSCASTER #1\n[15] BUTCH      --ENGLISH DAVE    BRETT      --FABIENNE       \n[17] BUTCH      --FABIENNE        FABIENNE   --JULES          \n[19] FOURTH MAN --JULES           FOURTH MAN --VINCENT        \n+ ... omitted several edges\n\n\nThis tells us that there are 102 edges (connected dyads) in the network. Some of these include Brett and Marsellus and Fabienne and Jules, but not all can be listed for reasons of space.\nigraph also has two dedicated functions that return the number of nodes and edges in the graph in one fell swoop. They are called vcount and ecount and take the graph object as input:\n\n   vcount(g)\n\n[1] 38\n\n   ecount(g)\n\n[1] 102"
  },
  {
    "objectID": "handout1.html#graph-density",
    "href": "handout1.html#graph-density",
    "title": "Basic Network Statistics",
    "section": "Graph Density",
    "text": "Graph Density\nOnce we have the number of edges and nodes, we can calculate the most basic derived statistic in a network, which is the density. Since the movie network is an undirected graph, the density is given by:\n\\[\n   \\frac{2m}{n(n-1)}\n\\]\nWhere \\(m\\) is the number of edges and \\(n\\) is the number of nodes, or in our case:\n\n   (2 * 102) / (38 * (38 - 1))\n\n[1] 0.1450925\n\n\nOf course, igraph has a dedicated function called edge_density to compute the density too, which takes the igraph object as input:\n\n   edge_density(g)\n\n[1] 0.1450925"
  },
  {
    "objectID": "handout1.html#degree",
    "href": "handout1.html#degree",
    "title": "Basic Network Statistics",
    "section": "Degree",
    "text": "Degree\nThe next set of graph metrics are based on the degree of the graph. We can list the graph’s degree set using the igraph function degree:\n\n   degree(g)\n\n          BRETT           BUDDY           BUTCH      CAPT KOONS     ED SULLIVAN \n              7               2              17               5               2 \n   ENGLISH DAVE       ESMARELDA        FABIENNE      FOURTH MAN       GAWKER #2 \n              4               1               3               2               3 \n    HONEY BUNNY          JIMMIE            JODY           JULES           LANCE \n              8               3               4              16               4 \n        MANAGER       MARSELLUS          MARVIN         MAYNARD             MIA \n              5              10               6               3              11 \n         MOTHER          PATRON      PEDESTRIAN        PREACHER         PUMPKIN \n              5               5               3               3               8 \n         RAQUEL           ROGER SPORTSCASTER #1 SPORTSCASTER #2        THE GIMP \n              3               6               2               1               2 \n       THE WOLF         VINCENT        WAITRESS         WINSTON           WOMAN \n              3              25               4               3               5 \n      YOUNG MAN     YOUNG WOMAN             ZED \n              4               4               2 \n\n\nThe degree function takes the igraph network object as input and returns a plain old R named vector as output with the names being the names attribute of vertices in the network object.\nUsually we are interested in who are the “top nodes” in the network by degree (a kind of centrality). To figure that out, all we need to do is sort the degree set (to generate the graph’s degree sequence) and list the top entries:\n\n   d <- degree(g)\n   d.sort <- sort(d, decreasing = TRUE)\n   d.sort[1:8]\n\n    VINCENT       BUTCH       JULES         MIA   MARSELLUS HONEY BUNNY \n         25          17          16          11          10           8 \n    PUMPKIN       BRETT \n          8           7 \n\n\nLine 1 stores the degrees in an object “d”, line 2 creates a “sorted” version of the same object (from bigger to smaller) and line 3 shows the first eight entries of the sorted degree sequence.\nBecause the degree vector “d” is just a regular old vector we can use native R mathematical operations to figure out things like the sum, maximum, minimum, and average degree of the graph:\n\n   sum(d)\n\n[1] 204\n\n   max(d)\n\n[1] 25\n\n   min(d)\n\n[1] 1\n\n   mean(d)\n\n[1] 5.368421\n\n\nSo the sum of degrees is 204, the maximum degree is 25 (belonging to Vincent), the minimum is one, and the average is about 5.4.\nNote that these numbers recreate some well-known equalities in graph theory:\n\nThe sum of degrees is twice the number of edges (the first theorem of graph theory):\n\n\n   2 * ecount(g)\n\n[1] 204\n\n\n\nThe average degree is just the sum of degrees divided by the number of nodes:\n\n\n   sum(d)/vcount(g)\n\n[1] 5.368421\n\n\n\nThe density is just the average degree divided by the number of nodes minus one, as explained here:\n\n\n   mean(d)/(vcount(g) - 1)\n\n[1] 0.1450925\n\n\nSome people also consider the degree variance of the graph as a measure of inequality of connectivity in the system. It is equal to the average sum of square deviations of each node’s degree from the average:\n\\[\n  \\mathcal{v}(G) = \\frac{\\sum_i (k_i - \\bar{k})^2}{n}\n\\]\n\n   sum((d - mean(d))^2)/vcount(g)\n\n[1] 22.96953\n\n\nThis tells us that there is a lot of inequality in the distribution of degrees in the graph (a graph with all nodes equal degree would have variance zero)."
  },
  {
    "objectID": "handout1.html#the-degree-distribution",
    "href": "handout1.html#the-degree-distribution",
    "title": "Basic Network Statistics",
    "section": "The Degree Distribution",
    "text": "The Degree Distribution\nAnother way of looking at inequalities of degrees in a graph is to examine its degree distribution. This gives us the probability of observing a node with a given degree k in the graph.\n\n   deg.dist <- degree_distribution(g)\n   deg.dist <- round(deg.dist, 3)\n   deg.dist\n\n [1] 0.000 0.053 0.158 0.237 0.158 0.132 0.053 0.026 0.053 0.000 0.026 0.026\n[13] 0.000 0.000 0.000 0.000 0.026 0.026 0.000 0.000 0.000 0.000 0.000 0.000\n[25] 0.000 0.026\n\n\nThe igraph function degree_distribution just returns a numeric vector of the same length as the maximum degree of the graph plus one. In this case that’s a vector of length 25 + 1 = 26. The first entry gives us the proportion of nodes with degree zero (isolates), the second the proportion of nodes of degree one, and so on up to the graph’s maximum degree.\nSince there are no isolates in the network, we can ignore the first element of this vector, to get the proportion of nodes of each degree in the Pulp Fiction network. To that, we fist create a two-column data.frame with the degrees in the first column and the proportions in the second:\n\n   degree <- c(1:max(d))\n   prop <- deg.dist\n   prop <- prop[-1]\n   deg.dist <- data.frame(degree, prop)\n   deg.dist\n\n   degree  prop\n1       1 0.053\n2       2 0.158\n3       3 0.237\n4       4 0.158\n5       5 0.132\n6       6 0.053\n7       7 0.026\n8       8 0.053\n9       9 0.000\n10     10 0.026\n11     11 0.026\n12     12 0.000\n13     13 0.000\n14     14 0.000\n15     15 0.000\n16     16 0.026\n17     17 0.026\n18     18 0.000\n19     19 0.000\n20     20 0.000\n21     21 0.000\n22     22 0.000\n23     23 0.000\n24     24 0.000\n25     25 0.026\n\n\nOf course, a better way to display the degree distribution of a graph is via some kind of data visualization, particularly for large networks where a long table of numbers is just not feasible. To do that, we can call on our good friend ggplot:\n\n   # install.packages(ggplot2)\n   library(ggplot2)\n   p <- ggplot(data = deg.dist, aes(x = degree, y = prop))\n   p <- p + geom_bar(stat = \"identity\", fill = \"red\", color = \"red\")\n   p <- p + theme_minimal()\n   p <- p + labs(x = \"\", y = \"Proportion\", \n                 title = \"Degree Distribution in Pulp Fiction Network\") \n   p <- p + geom_vline(xintercept = mean(d), \n                       linetype = 2, linewidth = 0.5, color = \"blue\")\n   p <- p + scale_x_continuous(breaks = c(1, 5, 10, 15, 20, 25))\n   p\n\n\n\n\nThe plot clearly shows that the Pulp Fiction network degree distribution is skewed with a small number of characters having a large degree \\(k \\geq 15\\) while most other characters in the movie have a small degree \\(k \\leq 5\\) indicating inequality of connectivity in the system."
  },
  {
    "objectID": "handout1.html#the-degree-correlation",
    "href": "handout1.html#the-degree-correlation",
    "title": "Basic Network Statistics",
    "section": "The Degree Correlation",
    "text": "The Degree Correlation\nAnother overall network statistic we may want to know is the degree correlation (Newman 2002). How do we compute it? Imagine taking each edge in the network and creating two degree vectors, one based on the degree of the node in one end and the degre of the node in another. Then the degree assortativity coefficient is just the Pearson product moment correlation between these two vectors.\nLet’s see how this would work for the Pulp Fiction network. First we need to extract an edge list from the graph:\n\n   g.el <- as_edgelist(g) #transforming graph to edgelist\n   head(g.el)\n\n     [,1]    [,2]       \n[1,] \"BRETT\" \"MARSELLUS\"\n[2,] \"BRETT\" \"MARVIN\"   \n[3,] \"BRETT\" \"ROGER\"    \n[4,] \"BRETT\" \"VINCENT\"  \n[5,] \"BUDDY\" \"MIA\"      \n[6,] \"BUDDY\" \"VINCENT\"  \n\n\nWe can see that the as_edgelist function takes the igraph network object as input and returns an \\(E \\times 2\\) matrix, with \\(E = 102\\) being the number of rows. Each column of the matrix records the name of the node on each end of the edge. So the first row of the edge list with entries “BRETT” and “MARSELLUS” tells us that there is an edge linking Brett and Marsellus, and so forth for each row.\nTo compute the correlation between the degrees of each node, all we need to do is attach the corresponding degrees to each name for each of the columns of the edge list, which can be done via data wrangling magic from the dplyr package (part of the tidyverse):\n\n   # install.packages(dplyr)\n   library(dplyr)\n   deg.dat <- data.frame(name1 = names(d), name2 = names(d), d)\n   el.temp <- data.frame(name2 = g.el[, 2]) %>% \n      left_join(deg.dat, by = \"name2\") %>% \n      dplyr::select(c(\"name2\", \"d\")) %>% \n      rename(d2 = d) \n   d.el <- data.frame(name1 = g.el[, 1]) %>% \n      left_join(deg.dat, by = \"name1\") %>% \n      dplyr::select(c(\"name1\", \"d\")) %>% \n      rename(d1 = d) %>% \n      cbind(el.temp)\nhead(d.el)\n\n  name1 d1     name2 d2\n1 BRETT  7 MARSELLUS 10\n2 BRETT  7    MARVIN  6\n3 BRETT  7     ROGER  6\n4 BRETT  7   VINCENT 25\n5 BUDDY  2       MIA 11\n6 BUDDY  2   VINCENT 25\n\n\nLine 3 creates a two-column data frame called “deg.dat” with as many rows as there are nodes in the network. The first two columns contain the names of each node (identically listed with different names) and the third columns contains the corresponding node’s degree.\nLines 4-7 use dplyr functions to create a new object “el.temp” joining the degree information to each of the node names listed in the second position in the original edge list “g.el,” and rename the imported column of degrees “d2.”\nLines 8-12 do the same for the nodes listed in the first position in the original edge list, renames the imported columns of degrees “d1,” and the binds the columns of the “el.temp” object to the new object “d.el.” The resulting object has four columns: Two for the names of the nodes incident to each edge on the edge list (columns 1 and 3), and two other ones corresponding to the degrees of the corresponding nodes (columns 2 and 4).\nWe can see from the output of the first few rows of the “d.el” object that indeed “BRETT” is assigned a degree of 7 in each row of the edge list, “BUDDY” a degree of 2, “MARSELLUS” a degree of 10, “VINCENT” a degree of 25 and so forth.\nNow to compute the degree correlation in the network all we need to do is call the native R function cor on the two columns from “d.el” that containing the degree information. Note that because each degree appears twice at the end of each edge in an undirected graph (as both “sender” and “receiver”), we need to double each column by appending the other degree column at the end. So the first degree column is the vector:\n\n   d1 <- c(d.el$d1, d.el$d2)\n\nAnd the second degree column is the vector:\n\n   d2 <- c(d.el$d2, d.el$d1)\n\nAnd the graph’s degree correlation (Newman 2003) is just the Pearson correlation between these two degree vectors:\n\n   cor(d1, d2)\n\n[1] -0.2896427\n\n\nThe result \\(r_{deg} = -0.29\\) tells us that there is anti-correlation by degree in the Pulp Fiction network. That is high-degree characters tend to appear with low degree characters, or conversely, high-degree characters (like Marsellus and Jules) don’t appear together very often.\nOf course, igraph has a function called assortativity_degree that does all the work for us:\n\n   assortativity_degree(g)\n\n[1] -0.2896427"
  },
  {
    "objectID": "handout1.html#the-average-shortest-path-length",
    "href": "handout1.html#the-average-shortest-path-length",
    "title": "Basic Network Statistics",
    "section": "The Average Shortest Path Length",
    "text": "The Average Shortest Path Length\nThe final statistic people use to characterize networks is the average shortest path length. In a network, even non-adjacent nodes, could be indirectly connected to other nodes via a path of some length (\\(l > 1\\)) So it is useful to know what the average of this quantity is across all dyads in the network.\nTo do that, we first need to compute the length of the shortest path \\(l\\) for each pair of nodes in the network (also known as the geodesic distance). Adjacent nodes get an automatic score of \\(l = 1\\). In igraph this is done as follows:\n\n   S <- distances(g)\n   S[1:7, 1:7]\n\n             BRETT BUDDY BUTCH CAPT KOONS ED SULLIVAN ENGLISH DAVE ESMARELDA\nBRETT            0     2     1          2           2            2         3\nBUDDY            2     0     2          2           2            2         4\nBUTCH            1     2     0          1           2            1         2\nCAPT KOONS       2     2     1          0           2            2         3\nED SULLIVAN      2     2     2          2           0            2         4\nENGLISH DAVE     2     2     1          2           2            0         3\nESMARELDA        3     4     2          3           4            3         0\n\n\nThe igraph function distances takes the network object as input and returns the desired shortest path matrix. So for instance, Brett is directly connected to Butch (they appear in a scene together) but indirectly connected to Buddy via a path of length two (they both appear in scenes with common neighbors even if they don’t appear together).\nThe maximum distance between two nodes in the graph (the longest shortest path to put it confusingly) is called the graph diameter. We can find this out simply by using the native R function for the maximum on the shortest paths matrix:\n\n   max(S)\n\n[1] 8\n\n\nThis means that in the Pulp Fiction network the maximum degree of separation between two characters is a path of length 8.\nOf course, we cann also call the igraph function diammeter:\n\n   diameter(g)\n\n[1] 8\n\n\nOnce we have the geodesic distance matrix, it is easy to calculate the average path length of the graph:\n\n   rs.S <- rowSums(S)\n   rm.S <- rs.S/(vcount(g) - 1)\n   mean(rm.S)\n\n[1] 2.769559\n\n\n\nFirst (line 1) we sum all the rows (or columns) of the geodesic distance matrix. This vector (of the same length as the number of nodes) gives us the sum of the geodesic distance of each node to each of the nodes (we will use this to compute closeness centrality later).\nThen (line 2) we divide this vector by the number of nodes minus one (to exclude the focal node) to create a vector of the average distance of each node to each of the other nodes.\nFinally (line 3) we take the average across all nodes of this average distance vector to get the graph’s average shortest path length, which in this case equals L = 2.8.\n\nThis means that, on average, each character in Pulp Fiction is separated by little less than three contacts in the co-appearance network (a fairly small world).\nOf course this can also be done in just one step on igraph:\n\n   mean_distance(g)\n\n[1] 2.769559"
  },
  {
    "objectID": "handout1.html#putting-it-all-together",
    "href": "handout1.html#putting-it-all-together",
    "title": "Basic Network Statistics",
    "section": "Putting it all Together",
    "text": "Putting it all Together\nNow we can put together all the basic network statistics that we have computed into some sort of summary table, like the ones here. We first create a vector with the names of each statistic:\n\n   Stat <- c(\"Nodes\", \"Edges\", \"Min. Degree\", \"Max. Degree\", \"Avg. Degree\", \"Degree Corr.\", \"Diameter\", \"Avg. Shortest Path Length\")\n\nThen we create a vector with the values:\n\n   Value <- c(vcount(g), ecount(g), min(d), max(d), round(mean(d), 2), round(assortativity_degree(g), 2), max(S), round(mean_distance(g), 2))\n\nWe can then put these two vector together into a data frame:\n\n   net.stats <- data.frame(Stat, Value)\n\nWe can then use the package kableExtra (a nice table maker) to create a nice html table:\n\n   # intall.packages(kableExtra)\n   library(kableExtra)\n   kbl(net.stats, format = \"pipe\", align = c(\"l\", \"c\"),\n       caption = \"Key Statistics for Pulp Fiction Network.\") %>% \n   kable_styling(bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\nKey Statistics for Pulp Fiction Network.\n \n  \n    Stat \n    Value \n  \n \n\n  \n    Nodes \n    38.00 \n  \n  \n    Edges \n    102.00 \n  \n  \n    Min. Degree \n    1.00 \n  \n  \n    Max. Degree \n    25.00 \n  \n  \n    Avg. Degree \n    5.37 \n  \n  \n    Degree Corr. \n    -0.29 \n  \n  \n    Diameter \n    8.00 \n  \n  \n    Avg. Shortest Path Length \n    2.77"
  },
  {
    "objectID": "handout1.html#appendix-loading-network-data-from-a-file",
    "href": "handout1.html#appendix-loading-network-data-from-a-file",
    "title": "Basic Network Statistics",
    "section": "Appendix: Loading Network Data from a File",
    "text": "Appendix: Loading Network Data from a File\nWhen get network data from an archival source, and it will be in the form of a matrix or an edge list, typically in some kind of comma separated value (csv) format. Here will show how to input that into R to create an igraph network object from an outside file.\nFirst we will write the Pulp Fiction data into an edge list and save it to disk. We already did that earlier with the “g.el” object. So all we have to do is save it to your local folder as a csv file:\n\n   #install.packages(here)\n   library(here)\n   write.csv(d.el[c(\"name1\", \"name2\")], here(\"pulp.csv\"))\n\nThe write.csv function just saves an R object into a .csv file. Here the R object is “g.el” and we asked it to save just the columns which contain the name of each character. This represents the adjacency relations in the network as an edge list. We use the package here to keep track of our working directory. See here (pun intended) for details.\nNow suppose that’s the network we want to work with and it’s saved in our hard drive. To load it, we just type:\n\n   g.el <- read.csv(here(\"pulp.csv\"), \n                    col.names = c(\"name1\", \"name2\"))\n   head(g.el)\n\n  name1     name2\n1 BRETT MARSELLUS\n2 BRETT    MARVIN\n3 BRETT     ROGER\n4 BRETT   VINCENT\n5 BUDDY       MIA\n6 BUDDY   VINCENT\n\n\nWhich gives us the edge list we want now saved into an R object of class data.frame. So all we need is to convert that into an igraph object. To do that we use one of the many graph_from... functions in the igraph package. In this case, we want graph_from_edgelist because our network is stored as an edge list:\n\n   g.el <- as.matrix(g.el)\n   g <- graph_from_edgelist(g.el, directed = FALSE)\n   V(g)\n\n+ 38/38 vertices, named, from 2b8d900:\n [1] BRETT           MARSELLUS       MARVIN          ROGER          \n [5] VINCENT         BUDDY           MIA             BUTCH          \n [9] CAPT KOONS      ESMARELDA       GAWKER #2       JULES          \n[13] PEDESTRIAN      SPORTSCASTER #1 ENGLISH DAVE    FABIENNE       \n[17] FOURTH MAN      HONEY BUNNY     MANAGER         JIMMIE         \n[21] JODY            PATRON          PUMPKIN         RAQUEL         \n[25] WINSTON         LANCE           MAYNARD         THE GIMP       \n[29] ZED             ED SULLIVAN     MOTHER          WOMAN          \n[33] PREACHER        SPORTSCASTER #2 THE WOLF        WAITRESS       \n[37] YOUNG MAN       YOUNG WOMAN    \n\n   E(g)\n\n+ 102/102 edges from 2b8d900 (vertex names):\n [1] BRETT      --MARSELLUS       BRETT      --MARVIN         \n [3] BRETT      --ROGER           BRETT      --VINCENT        \n [5] BUDDY      --MIA             VINCENT    --BUDDY          \n [7] BRETT      --BUTCH           BUTCH      --CAPT KOONS     \n [9] BUTCH      --ESMARELDA       BUTCH      --GAWKER #2      \n[11] BUTCH      --JULES           MARSELLUS  --BUTCH          \n[13] BUTCH      --PEDESTRIAN      BUTCH      --SPORTSCASTER #1\n[15] BUTCH      --ENGLISH DAVE    BRETT      --FABIENNE       \n[17] BUTCH      --FABIENNE        JULES      --FABIENNE       \n[19] JULES      --FOURTH MAN      VINCENT    --FOURTH MAN     \n+ ... omitted several edges\n\n\nWhich gives us back the original igraph object we have been working with. Note that first we converted the data.frame object into a matrix object. We also specified that the graph is undirected by setting the option directed to false."
  },
  {
    "objectID": "handout2.html",
    "href": "handout2.html",
    "title": "Centrality",
    "section": "",
    "text": "In this handout we will go through the basic centrality metrics. Particularly, the “big three” according to Freeman (1979), namely, degree, closeness (in two flavors) and betweenness.\nWe first load our trusty Pulp Fiction data set from the networkdata package, which is an undirected graph of character scene co-appearances in the film:"
  },
  {
    "objectID": "handout2.html#degree-centrality",
    "href": "handout2.html#degree-centrality",
    "title": "Centrality",
    "section": "Degree Centrality",
    "text": "Degree Centrality\nDegree centrality is the simplest and most straightforward measure. In fact, we are already computed in handout 1 since it is the same as obtaining the graph’s degree sequence. So the igraph function degree would do it as we already saw.\nHere we follow a different approach using the row (or column) sums of the graph’s adjacency matrix:\n\n   A <- as_adjacency_matrix(g)\n   A <- as.matrix(A)\n   rowSums(A)\n\n          BRETT           BUDDY           BUTCH      CAPT KOONS     ED SULLIVAN \n              7               2              17               5               2 \n   ENGLISH DAVE       ESMARELDA        FABIENNE      FOURTH MAN       GAWKER #2 \n              4               1               3               2               3 \n    HONEY BUNNY          JIMMIE            JODY           JULES           LANCE \n              8               3               4              16               4 \n        MANAGER       MARSELLUS          MARVIN         MAYNARD             MIA \n              5              10               6               3              11 \n         MOTHER          PATRON      PEDESTRIAN        PREACHER         PUMPKIN \n              5               5               3               3               8 \n         RAQUEL           ROGER SPORTSCASTER #1 SPORTSCASTER #2        THE GIMP \n              3               6               2               1               2 \n       THE WOLF         VINCENT        WAITRESS         WINSTON           WOMAN \n              3              25               4               3               5 \n      YOUNG MAN     YOUNG WOMAN             ZED \n              4               4               2 \n\n\nThe igraph function as_adjancency_matrix doesn’t quite return a regular R matrix object, so we have to further coerce the resulting object into a numerical matrix containing zeroes and ones using the as.matrix function in line 2. Then we can apply the native rowSums function to obtain each node’s degree. Note that this is same output we got using the degree function before."
  },
  {
    "objectID": "handout2.html#indegree-and-outdegree",
    "href": "handout2.html#indegree-and-outdegree",
    "title": "Centrality",
    "section": "Indegree and Outdegree",
    "text": "Indegree and Outdegree\nThe movie network is based on the relationship of co-appearance in a scene which by nature lacks any natural directionality (it’s a symmetric relation) and can therefore be represented in an undirected graph. The concepts of in and outdegree, by contrast, are only applicable to directed relations. So to illustrate them, we need to switch to a different source of data.\nWe pick an advice network which is a classical directed kind of (asymmetric) relation. I can give advice to you, but that doesn’t necessarily mean you can give advice to me. The networkdata package contains one such data set collected in the late 80s early 1990s in a New England law firm (see the description here), called law_advice:\n\n   d.g <- law_advice\n   V(d.g)\n\n+ 71/71 vertices, from d1a9da7:\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50\n[51] 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n\n   vertex_attr(d.g)\n\n$status\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2\n[39] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n\n$gender\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 1 1 1 1 2 1 1 1 2\n[39] 2 1 1 1 2 2 1 2 1 2 1 1 2 1 1 1 1 1 2 1 2 2 2 1 1 2 1 1 2 1 2 1 2\n\n$office\n [1] 1 1 2 1 2 2 2 1 1 1 1 1 1 2 3 1 1 2 1 1 1 1 1 1 2 1 1 2 1 2 2 2 2 1 2 1 3 1\n[39] 1 1 1 1 1 3 1 2 3 1 1 2 2 1 1 1 1 1 1 2 2 1 1 1 2 1 1 1 1 1 1 1 1\n\n$seniority\n [1] 31 32 13 31 31 29 29 28 25 25 23 24 22  1 21 20 23 18 19 19 17  9 16 15 15\n[26] 15 13 11 10  7  8  8  8  8  8  5  5  7  6  6  5  4  5  5  3  3  3  1  4  3\n[51]  4  4 10  3  3  3  3  3  2  2  2  2  2  2  2  1  1  1  1  1  1\n\n$age\n [1] 64 62 67 59 59 55 63 53 53 53 50 52 57 56 48 46 50 45 46 49 43 49 45 44 43\n[26] 41 47 38 38 39 34 33 37 36 33 43 44 53 37 34 31 31 47 53 38 42 38 35 36 31\n[51] 29 29 38 29 34 38 33 33 30 31 34 32 29 45 28 43 35 26 38 31 26\n\n$practice\n [1] 1 2 1 2 1 1 2 1 2 2 1 2 1 2 2 2 2 1 2 1 1 1 1 1 2 1 1 2 2 1 1 1 1 2 2 1 2 1\n[39] 1 1 1 2 1 2 2 2 1 2 1 2 1 1 2 1 1 1 1 1 1 2 2 2 2 2 1 1 1 1 1 2 1\n\n$law_school\n [1] 1 1 1 3 2 1 3 3 1 3 1 2 2 1 3 1 1 2 1 1 2 3 2 2 2 3 1 2 3 3 2 3 3 2 3 3 3 2\n[39] 1 1 2 2 2 1 3 2 3 3 2 2 3 3 3 3 3 2 2 3 2 2 3 2 2 2 3 3 2 3 3 2 2\n\n\nWe can see that the graph has 71 vertices, and that there are various attributes associated with each vertex, like gender, age, seniority, status in the law firm, etc. We can query those attributes using the igraph function vertex_attr, which takes the graph object as input.\n\nSubsetting the Graph According to a Node Attribute\nTo keep things manageable, we will restrict our analysis to partners. To do that we need to select the subgraph that only includes the vertices with value of 1 in the “status” vertex attribute. From the data description, we know the first 36 nodes (with value of 1 in the status attribute) are the law firm’s partners (the rest are associates). In igraph we can do this as using the subgraph function:\n\n   d.g <- subgraph(d.g, 1:36)\n   V(d.g)\n\n+ 36/36 vertices, from 2dfd527:\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30 31 32 33 34 35 36\n\n   V(d.g)$status\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n\nThe first line just tells igraph to generate the subgraph containing the first 36 nodes (the partners). The subgraph function thus takes two main inputs: The graph object, and then a vector of node ids (or node labels) telling the function which nodes to select to create the node-induced subgraph.\nOf course we already knew from the data description that the first 36 nodes where the partners. But let’s say we have a large data set and we don’t know which nodes are the partners. A smarter way of selecting a subgraph based on a node attribute is as follows:\n\n   partners <- which(V(law_advice)$status == 1)\n   d.g <- subgraph(law_advice, partners)\n   V(d.g)\n\n+ 36/36 vertices, from 2e035c0:\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30 31 32 33 34 35 36\n\n\nThe first line using the native R vector function which allowing us to subset a vector based on a logical condition. The function takes a vector followed by a logical condition as input, and returns the position of the vector elements that meet that condition. In this case, we took the vector of values for the attribute of status and selected the node ids where status is equal to 1. We then fed that vector to the subgraph function in line 2.\nWe could do this with any other attribute:\n\n   older <- which(V(law_advice)$age > 50)\n   older\n\n [1]  1  2  3  4  5  6  7  8  9 10 12 13 14 38 44\n\n   og <- subgraph(law_advice, older)\n   V(og)\n\n+ 15/15 vertices, from 2e08d4f:\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15\n\n\nHere we selected the subgraph (called “og”, get it, get it) formed by the subset of nodes over the age of 50 at the firm. The values of the vector older tell us which of the 71 members meet the relevant condition.\n\n\nComputing in and outdegree\nOK, going back to the partners subgraph, we can now create our (asymmetric) adjacency matrix and compute the row and column sums:\n\n   d.A <- as_adjacency_matrix(d.g)\n   d.A <- as.matrix(d.A)\n   rowSums(d.A)\n\n [1]  3  7  7 17  4  0  4  2  3  7  5 18 11 13 10 19 17  5 21 10  9 12  9 16  8\n[26] 22 18 22 13 15 16  9 15  6 15  7\n\n   colSums(d.A)\n\n [1] 18 17  8 14 10 17  4  8 10  6 11 14 14 12 15 13 21  7  7 17 11 10  2 14  7\n[26] 20  2 14 12 11  8 13  2 16  8  2\n\n\nNote that in contrast to the undirected case the row and column sums give you two different sets of numbers. The row sums provide the directed graph’s outdegree set (number of outgoing links incident to each node), and the column sums provide the graph’s indegree set (number of incoming links incident to each node). So if you are high in the first vector, you are an advice giver (perhaps indicating informal status or experience) and if you are high in the second you are advice taker.\nOf course igraph has a dedicated function for this, which is just our old friend degree with an extra option mode, indicating whether you want the in or outdegrees:\n\n   d.o <- degree(d.g, mode = \"out\")\n   d.i <- degree(d.g, mode = \"in\")\n   d.o\n\n [1]  3  7  7 17  4  0  4  2  3  7  5 18 11 13 10 19 17  5 21 10  9 12  9 16  8\n[26] 22 18 22 13 15 16  9 15  6 15  7\n\n   d.i\n\n [1] 18 17  8 14 10 17  4  8 10  6 11 14 14 12 15 13 21  7  7 17 11 10  2 14  7\n[26] 20  2 14 12 11  8 13  2 16  8  2\n\n\nNote that the graph attributes are just vectors of values, and can be accessed from the graph object using the $ operator attached to the V() function as we did above.\nSo if we wanted to figure out the correlation between some vertex attribute and in or out degree centrality, all we need to do is correlate the two vectors:\n\n   r <- cor(d.o, V(d.g)$age)\n   round(r, 2)\n\n[1] -0.43\n\n\nWhich tells us that at least in this case, younger partners are more sought after as sources of advice than older partners."
  },
  {
    "objectID": "handout2.html#closeness-centrality",
    "href": "handout2.html#closeness-centrality",
    "title": "Centrality",
    "section": "Closeness Centrality",
    "text": "Closeness Centrality\nRecall that the closeness centrality is defined as the inverse of the sum of the lengths of shortest paths from each node to every other node. That means that to compute it, we first need to calculate the geodesic distance matrix. This is matrix in which each entry \\(g_{ij}\\) records the length of the shortest path(s) between row node \\(i\\) and column node \\(j\\). Then, we sum the rows (or columns) of this symmetric matrix and then we obtain the inverse to get the closeness of each node:\n\n   S <- distances(g) #length of shortest paths matrix\n   d.sum <- rowSums(S)\n   close1 <- round(1/d.sum, 4)\n   close1\n\n          BRETT           BUDDY           BUTCH      CAPT KOONS     ED SULLIVAN \n         0.0125          0.0108          0.0143          0.0125          0.0108 \n   ENGLISH DAVE       ESMARELDA        FABIENNE      FOURTH MAN       GAWKER #2 \n         0.0105          0.0070          0.0096          0.0104          0.0097 \n    HONEY BUNNY          JIMMIE            JODY           JULES           LANCE \n         0.0114          0.0093          0.0093          0.0132          0.0093 \n        MANAGER       MARSELLUS          MARVIN         MAYNARD             MIA \n         0.0111          0.0104          0.0103          0.0098          0.0115 \n         MOTHER          PATRON      PEDESTRIAN        PREACHER         PUMPKIN \n         0.0125          0.0111          0.0097          0.0097          0.0114 \n         RAQUEL           ROGER SPORTSCASTER #1 SPORTSCASTER #2        THE GIMP \n         0.0105          0.0125          0.0096          0.0057          0.0073 \n       THE WOLF         VINCENT        WAITRESS         WINSTON           WOMAN \n         0.0056          0.0139          0.0083          0.0105          0.0125 \n      YOUNG MAN     YOUNG WOMAN             ZED \n         0.0083          0.0083          0.0073 \n\n\nOf course, we could have just used the available function in igraph and computed the closeness centrality directly from the graph object using the function closeness:\n\n   close2 <- round(closeness(g), 4)\n   close2\n\n          BRETT           BUDDY           BUTCH      CAPT KOONS     ED SULLIVAN \n         0.0125          0.0108          0.0143          0.0125          0.0108 \n   ENGLISH DAVE       ESMARELDA        FABIENNE      FOURTH MAN       GAWKER #2 \n         0.0105          0.0070          0.0096          0.0104          0.0097 \n    HONEY BUNNY          JIMMIE            JODY           JULES           LANCE \n         0.0114          0.0093          0.0093          0.0132          0.0093 \n        MANAGER       MARSELLUS          MARVIN         MAYNARD             MIA \n         0.0111          0.0104          0.0103          0.0098          0.0115 \n         MOTHER          PATRON      PEDESTRIAN        PREACHER         PUMPKIN \n         0.0125          0.0111          0.0097          0.0097          0.0114 \n         RAQUEL           ROGER SPORTSCASTER #1 SPORTSCASTER #2        THE GIMP \n         0.0105          0.0125          0.0096          0.0057          0.0073 \n       THE WOLF         VINCENT        WAITRESS         WINSTON           WOMAN \n         0.0056          0.0139          0.0083          0.0105          0.0125 \n      YOUNG MAN     YOUNG WOMAN             ZED \n         0.0083          0.0083          0.0073 \n\n\nOnce we have the closeness centrality values, we are interested in who are the top nodes. The following code creates a table with the top five:\n\n   library(kableExtra)\n   close2 <- sort(close2, decreasing = TRUE)\n   close2 <- data.frame(close2[1:5])\n   kbl(close2, format = \"pipe\", align = c(\"l\", \"c\"),\n       col.names = c(\"Character\", \"Closeness\"),\n       caption = \"Top Five Closeness Characters in Pulp Fiction Network.\") %>% \n   kable_styling(bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\nTop Five Closeness Characters in Pulp Fiction Network.\n \n  \n    Character \n    Closeness \n  \n \n\n  \n    BUTCH \n    0.0143 \n  \n  \n    VINCENT \n    0.0139 \n  \n  \n    JULES \n    0.0132 \n  \n  \n    BRETT \n    0.0125 \n  \n  \n    CAPT KOONS \n    0.0125 \n  \n\n\n\n\n\nIt makes sense that the three main characters are also the ones that are at closest distances from everyone else!"
  },
  {
    "objectID": "handout2.html#edge-closeness",
    "href": "handout2.html#edge-closeness",
    "title": "Centrality",
    "section": "Edge Closeness",
    "text": "Edge Closeness\nBröhl and Lehnertz (2022) define the closeness of an edge as a function of the closeness of the two nodes incident to it. An edge \\(e_{jk}\\) linking vertex \\(v_j\\) to \\(v_k\\) has high closeness whenever vertices \\(v_j\\) and \\(v_k\\) also have high closeness.\nMore specifically, the closeness centrality of an edge is proportional to the ratio of the product of the closeness of the two nodes incident to it divided by their sum:\n\\[\nC(e_{jk}) = (E - 1)\\frac{C(v_j) \\times C(v_k)}{C(v_j)+C(v_k)}\n\\]\nNote that the equation normalizes the ratio of the product to the sum of the vertex closeness centralities by the number of edges minus one.\nTo compute edge closeness in a real network, we can use the same approach to data wrangling we used to compute the degree correlation in Handout 1. The goal is to create an edge list data frame containing five columns. The ids of the two nodes in the edge, the closeness centralities of the two nodes in the edge, and the closeness centrality of the edge calculated according to the above equation.\nIn the Pulp Fiction network this looks like:\n\n   library(dplyr)\n   g.el <- as_edgelist(g) #transforming graph to edgelist\n   c <- round(closeness(g), 3)  #closeness centrality vector\n   c.dat <- data.frame(name1 = names(c), name2 = names(c), c)\n   el.temp <- data.frame(name2 = g.el[, 2]) %>% \n      left_join(c.dat, by = \"name2\") %>% \n      dplyr::select(c(\"name2\", \"c\")) %>% \n      rename(c2 = c) \n   c.el <- data.frame(name1 = g.el[, 1]) %>% \n      left_join(c.dat, by = \"name1\") %>% \n      dplyr::select(c(\"name1\", \"c\")) %>% \n      rename(c1 = c) %>% \n      cbind(el.temp) %>% \n      mutate(e.clos = round((ecount(g)-1)*(c1*c2)/(c+c2), 3))\nhead(c.el)\n\n  name1    c1     name2    c2 e.clos\n1 BRETT 0.013 MARSELLUS 0.010  0.571\n2 BRETT 0.013    MARVIN 0.010  0.625\n3 BRETT 0.013     ROGER 0.013  0.632\n4 BRETT 0.013   VINCENT 0.014  0.681\n5 BUDDY 0.011       MIA 0.011  0.555\n6 BUDDY 0.011   VINCENT 0.014  0.622\n\n\nTo create a table of the top five closeness centrality edges, we just order the data frame by the last column and table it:\n\n   c.el <- c.el[order(c.el$e.clos, decreasing = TRUE), ] %>% \n      dplyr::select(c(\"name1\", \"name2\", \"e.clos\"))\n\n   kbl(c.el[1:5, ], format = \"pipe\", align = c(\"l\", \"l\", \"c\"),\n       col.names = c(\"i\", \"j\", \"Edge Clos.\"), row.names = FALSE,\n       caption = \"Edges Sorted by Closeness in the Pulp Fiction Network\") %>% \n   kable_styling(bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\nEdges Sorted by Closeness in the Pulp Fiction Network\n \n  \n    i \n    j \n    Edge Clos. \n  \n \n\n  \n    BUTCH \n    VINCENT \n    0.900 \n  \n  \n    BRETT \n    BUTCH \n    0.875 \n  \n  \n    MOTHER \n    VINCENT \n    0.875 \n  \n  \n    BUTCH \n    MIA \n    0.864 \n  \n  \n    JULES \n    PUMPKIN \n    0.850 \n  \n\n\n\n\n\nInterestingly, the top closeness edges tend to bring somewhat strange bedfellows together, characters that themselves don’t spend much time together in the film (e.g., the Butch/Vincent interaction is relatively brief and somewhat embarrassing for Vincent) but who themselves can reach other character clusters in the film via relatively short paths."
  },
  {
    "objectID": "handout2.html#closeness-centrality-in-directed-graphs",
    "href": "handout2.html#closeness-centrality-in-directed-graphs",
    "title": "Centrality",
    "section": "Closeness Centrality in Directed Graphs",
    "text": "Closeness Centrality in Directed Graphs\nWhat about closeness centrality for a directed network? Let us see how this works using a subgraph of the advice network, this time selecting just women under the age of forty:\n\n   women <- which(V(law_advice)$gender == 2)\n   wg <- subgraph(law_advice, women)\n   young <- which(V(wg)$age < 40)\n   wg <- subgraph(wg, young)\n   V(wg)\n\n+ 12/12 vertices, from 2e9903e:\n [1]  1  2  3  4  5  6  7  8  9 10 11 12\n\n\nThis network is small enough that a plot could be informative about its structure. Let us plot it using the package ggraph, a visualization package that follows the same principles as the ggplot grammar of graphics but for network graphs (see here).\n\n   #install.packages(\"ggraph\")\n   library(ggraph)\n    p <- ggraph(wg, layout = 'auto')\n    p <- p + geom_edge_parallel(color = \"steelblue\", edge_width = 0.5,\n                                arrow = arrow(length = unit(2.5, 'mm')),\n                                end_cap = circle(4, 'mm'), \n                                sep = unit(3, 'mm'))\n    p <- p + geom_node_point(aes(x = x, y = y), size = 8, color = \"tan2\") \n    p <- p + geom_node_text(aes(label = 1:vcount(wg)), size = 4, color = \"white\")\n    p <- p + theme_graph() \n    p\n\n\n\n\nWomen lawyers advice network\n\n\n\n\nNow a question we might ask is who has the greatest closeness centrality in this advice network. We could proceed as usual and compute the geodesic distances between actors:\n\n   S <- distances(wg)\n   S\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]\n [1,]    0    1    2    1    3    3    4    2    2     3     3     3\n [2,]    1    0    2    1    2    3    3    1    1     3     3     3\n [3,]    2    2    0    1    1    1    2    2    3     1     1     1\n [4,]    1    1    1    0    2    2    3    2    2     2     2     2\n [5,]    3    2    1    2    0    1    1    1    2     2     2     2\n [6,]    3    3    1    2    1    0    2    2    3     1     2     1\n [7,]    4    3    2    3    1    2    0    2    3     3     3     3\n [8,]    2    1    2    2    1    2    2    0    1     3     3     3\n [9,]    2    1    3    2    2    3    3    1    0     4     4     4\n[10,]    3    3    1    2    2    1    3    3    4     0     1     1\n[11,]    3    3    1    2    2    2    3    3    4     1     0     1\n[12,]    3    3    1    2    2    1    3    3    4     1     1     0\n\n\nNote that this is not quite right. In igraph the default settings of the distance function treats the graph as undirected. So it doesn’t use the strict directed paths, but it just treats them all as semi-paths ignoring direction. That is why, for instance, it counts node 1 as being “adjacent” to node 4 even though there is only one incoming link from 4 to 1 and why the whole matrix is symmetric, when we know from just eyeballing the network that there is a lot of asymmetry in terms of who can reach who via directed paths.\nTo get the actual directed distance matrix, we need to specify the “mode” option, asking whether we want in or out paths. Here, let’s select out-paths:\n\n   S <- distances(wg, mode = \"out\")\n   S\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]\n [1,]    0    1  Inf  Inf  Inf  Inf  Inf  Inf  Inf   Inf   Inf   Inf\n [2,]    1    0  Inf  Inf  Inf  Inf  Inf  Inf  Inf   Inf   Inf   Inf\n [3,]    2    2    0    1  Inf    1  Inf  Inf  Inf   Inf   Inf   Inf\n [4,]    1    1    1    0  Inf    2  Inf  Inf  Inf   Inf   Inf   Inf\n [5,]    3    2    1    2    0    1    1    1    2   Inf   Inf   Inf\n [6,]    3    3    1    2  Inf    0  Inf  Inf  Inf   Inf   Inf   Inf\n [7,]    4    3    2    3    1    2    0    2    3   Inf   Inf   Inf\n [8,]    2    1  Inf  Inf  Inf  Inf  Inf    0    1   Inf   Inf   Inf\n [9,]    2    1  Inf  Inf  Inf  Inf  Inf    1    0   Inf   Inf   Inf\n[10,]    3    3    1    2  Inf    1  Inf  Inf  Inf     0     1     2\n[11,]    3    3    1    2  Inf    2  Inf  Inf  Inf     1     0     1\n[12,]    3    3    1    2  Inf    1  Inf  Inf  Inf     1     1     0\n\n\nThis is better but introduces a problem. The directed graph is not strongly connected, so it means that some nodes cannot reach other ones via a directed path of any length. That means that the geodesic distances from a node to an unreachable node is coded as “infinite” (Inf). The problem with infinity is that it gets in the way of calculating sums of distances, a requirement for the closeness centrality.\n\n   S <- distances(wg, mode = \"out\")\n   rowSums(S)\n\n [1] Inf Inf Inf Inf Inf Inf Inf Inf Inf Inf Inf Inf\n\n\nAdding infinity to a number just returns infinity so all the rows with at least one “Inf” in the distance matrix get an Inf for the row sum. In this case that’s all of them. A bummer.\n\nHarmonic Centrality\nBut dont’ worry there’s a patch. It is called the harmonic centrality (Rochat 2009).1 This is a variation on the closeness centrality that works whether you are working with connected or disconnected graphs (or in the case of directed graphs regardless of whether the graph is strongly or weakly connected), and therefore regardless of whether the geodesic distance matrix contains Infs.2\nThe main difference between the harmonic and regular closeness centrality is that instead of calculating the inverse of the sum of the distances for each node, we calculate the sum of the inverses:\n\n   S <- distances(wg, mode = \"out\")\n   S = round(1/S, 2) \n   diag(S) <- 0 #setting diagonals to zero\n   S\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]\n [1,] 0.00 1.00  0.0 0.00    0  0.0    0  0.0 0.00     0     0   0.0\n [2,] 1.00 0.00  0.0 0.00    0  0.0    0  0.0 0.00     0     0   0.0\n [3,] 0.50 0.50  0.0 1.00    0  1.0    0  0.0 0.00     0     0   0.0\n [4,] 1.00 1.00  1.0 0.00    0  0.5    0  0.0 0.00     0     0   0.0\n [5,] 0.33 0.50  1.0 0.50    0  1.0    1  1.0 0.50     0     0   0.0\n [6,] 0.33 0.33  1.0 0.50    0  0.0    0  0.0 0.00     0     0   0.0\n [7,] 0.25 0.33  0.5 0.33    1  0.5    0  0.5 0.33     0     0   0.0\n [8,] 0.50 1.00  0.0 0.00    0  0.0    0  0.0 1.00     0     0   0.0\n [9,] 0.50 1.00  0.0 0.00    0  0.0    0  1.0 0.00     0     0   0.0\n[10,] 0.33 0.33  1.0 0.50    0  1.0    0  0.0 0.00     0     1   0.5\n[11,] 0.33 0.33  1.0 0.50    0  0.5    0  0.0 0.00     1     0   1.0\n[12,] 0.33 0.33  1.0 0.50    0  1.0    0  0.0 0.00     1     1   0.0\n\n\nNote that in this matrix of inverse distances, the closest (adjacent) nodes get the maximum score of one, and nodes farther apart when smaller scores (approaching zero). More importantly, those pesky Infs disappear (!) because unreachable directed pairs of nodes get the lowest score, corresponding to \\(1/\\infty = 0\\). Turns out the mathematics of infinity weren’t our enemy after all.\nAlso note that the reachability relation expressed in this matrix is asymmetric: So node 4 and reach node 1 (there is a directed tie from 4 to 1), but node 1 cannot reach 4. This is precisely what we want.\nOnce we have this matrix of inverse distances, we can then we can compute the harmonic centrality the same way as regular closeness by adding up the row scores for each node and dividing by the number of nodes minus one (to get the average):\n\n   d.harm <- rowSums(S)\n   d.harm <- d.harm/(vcount(wg) - 1)\n   d.harm <- round(d.harm, 2)\n   d.harm\n\n [1] 0.09 0.09 0.27 0.32 0.53 0.20 0.34 0.23 0.23 0.42 0.42 0.47\n\n\nWe can see that the highest harmonic closeness centrality node is 5, followed by 12. Here’s a plot of the network highlighting the highest harmonic (closeness) centrality node.\n\n   col <- rep(\"tan2\", vcount(wg)) #creating node color vector\n   col[which(d.harm == max(d.harm))] <- \"red\" #changing color of max centrality node to red\n   p <- p + geom_node_point(aes(x = x, y = y), size = 8, color = col)\n   p <- p + geom_node_text(aes(label = 1:vcount(wg)), size = 4, color = \"white\")\n   p\n\n\n\n\nWomen lawyers advice network with highest closeness centrality node in red\n\n\n\n\nOf course, igraph has a built in function to calculate the harmonic centrality called (you guessed it) harmonic_centrality:\n\n   d.harm <- harmonic_centrality(wg, normalized = TRUE)\n   d.harm <- round(d.harm, 2)\n   d.harm\n\n [1] 0.09 0.09 0.27 0.32 0.53 0.20 0.34 0.23 0.23 0.42 0.42 0.47\n\n\nWhich gives us the same results."
  },
  {
    "objectID": "handout2.html#betweenness",
    "href": "handout2.html#betweenness",
    "title": "Centrality",
    "section": "Betweenness",
    "text": "Betweenness\nWe finally come to betweenness centrality. Recall that the key conceptual distinction between closeness and betweenness according to Freeman (1979) is that between (pun intended) the capacity to reach others quickly (e.g., via the shortest paths) and the capacity to intermediate among those same paths. High betweenness nodes control the flow of information in the network between other nodes.\nThis is evident in the way betweenness is calculated. Recall that the betweenness of a node k relative to any pair of nodes i and j in the network is simply:\n\\[\n\\frac{\\sigma_{i(k)j}}{\\sigma_{ij}}\n\\]\nWhere the denominator of the fraction (\\(\\sigma_{ij}\\)) is a count of the total number of shortest paths that start and end with nodes i and j and the numerator of the fraction (\\(\\sigma_{i(k)j}\\)) is the subset of those paths that include node k as an inner node.\nAs Freeman (1979) also notes because this is a ratio, it can range from zero to one, with everything in between. As such the betweenness centrality of a node relative to any two others has an intuitive interpretation as a probability, namely the probability that if you send something from i to j it has to go through k. This probability is 1.0 if k stands in every shortest path between i and j and zero if they stand in none of the shortest paths indirectly connecting i and j.\nThe betweenness of a given node is just the sum all of these probabilities across every pair of nodes in the graph for each node:\n\\[\n\\sum_{i \\neq j, i \\neq n, j \\neq v} \\frac{\\sigma_{i(k)j}}{\\sigma_{ij}}\n\\]\nBelow we can see a point and line diagram of the undirectd Pulp Fiction network we have been working with.\n\n\n\n\n\nPulp Fiction character schene co-appearance network.\n\n\n\n\nWe should expect a character to have high betweenness in this network to the extent that they appear in scenes with characters who themselves don’t appear in any scenes together, thus inter-mediating between different parts of the story. Characters who only appear in one scene with some others (like The Wolf or The Gimp) are likely to be low in betweenness.\nLet’s create a top ten table of betweenness for the Pulp Fiction network. We use the igraph function betweenness to calculate the scores:\n\n   pulp.bet <- betweenness(g)\n   top.5.bet <- sort(pulp.bet, decreasing = TRUE)[1:10]\n   kbl(round(top.5.bet, 2), format = \"pipe\", align = c(\"l\", \"c\"),\n       col.names = c(\"Character\", \"Betweenness\"),\n       caption = \"Top Five Betweenness Characters in Pulp Fiction Network.\") %>% \n   kable_styling(bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\nTop Five Betweenness Characters in Pulp Fiction Network.\n \n  \n    Character \n    Betweenness \n  \n \n\n  \n    BUTCH \n    275.52 \n  \n  \n    VINCENT \n    230.19 \n  \n  \n    JULES \n    142.11 \n  \n  \n    MIA \n    76.68 \n  \n  \n    MAYNARD \n    70.00 \n  \n  \n    HONEY BUNNY \n    49.97 \n  \n  \n    PUMPKIN \n    49.97 \n  \n  \n    SPORTSCASTER #1 \n    36.00 \n  \n  \n    BRETT \n    29.85 \n  \n  \n    PREACHER \n    28.23 \n  \n\n\n\n\n\nUnsurprisingly, the top four characters are also the highest in betweenness. Somewhat surprisingly, the main antagonist of the story (the pawn shop owner) is also up there. After that we see a steep drop in the bottom five of the top ten.\nNow let us examine betweenness centrality in our directed women lawyers advice network:\n\n   w.bet <- betweenness(wg)\n   w.bet\n\n [1]  0.0000000  3.0000000 16.3333333 11.0000000  7.0000000  0.0000000\n [7]  0.0000000  5.0000000  0.0000000  0.3333333  1.0000000  0.3333333\n\n\nHere we see that node 3 is the highest in betweenness, pictured below:\n\n\n\n\n\nWomen lawyers advice network with highest closeness centrality node in blue and highest betweenness centrality node in red\n\n\n\n\nThis result makes sense. Node 3 intermediates all the connections linking the tightly knit group of nodes on the left side (6, 10, 11, 12) with the rest of the network. Also if nodes 5 and 7 need to pass something along to the rest, they have to use 3 at least half time. Node 4 also needs 3 to reach 6.\nThis result nicely illustrates the difference between closeness and betweenness."
  },
  {
    "objectID": "handout2.html#edge-betweenness",
    "href": "handout2.html#edge-betweenness",
    "title": "Centrality",
    "section": "Edge Betweenness",
    "text": "Edge Betweenness\nEdge betweenness is defined in similar fashion as node betweenness:\n\\[\n\\sum_{i \\neq j} \\frac{\\sigma_{i(e)j}}{\\sigma_{ij}}\n\\]\nWhere \\(\\sigma_{i(e)j}\\) is a count of the number of shortest paths between i and j that feature edge e as an intermediary link. This tells us that the betweenness of an edge e is the sum of the ratios of the number of times that edge appears in the middle of a shortest path connecting every pair of nodes in the graph i and j divided by the total number of shortest paths linking each pair of nodes.\nLike before, the edge betweenness with respect to a specific pair of nodes in the graph is a probability: Namely, that if you send something–using a shortest path–from any node i to any other node j it has to go through edge e. The resulting edge betweenness scores is the sum of these probabilities across every possible pair of nodes for each edge in the graph.\nFor this example, we will work with a simplified version of the women lawyers advice network, in which we transform it into an undirected graph. We use the igraph function as.undirected for that:\n\n   wg <- as.undirected(wg, mode = \"collapse\")\n\nThe “collapse” value in the “mode” argument tells as.undirected to link every connected dyad in the original directed graph using an undirected edge. It does that by removing the directional arrow of the single directed links and collapsing (hence the name) all the bi-directional links into a single undirected one.\nThe resulting undirected graph looks like this:\n\n\n\n\n\nLooking at this point and line plot of the women lawyers advice network, which edge do you think has the top betweenness?\nWell no need to figure that out via eyeballing! We can just use the igraph function edge_betweenness:\n\n   w.ebet <- edge_betweenness(wg)\n\nThe edge_betweenness function takes the igraph graph object as input and produces a vector of edge betweenness values of the same length as the number of edges in the graph, which happens to be 20 in this case.\nUsing this information, we can then create a table of the top ten edges ordered by betweenness:\n\n   edges <- as_edgelist(wg) #creating an edgelist\n   etab <- data.frame(edges, bet = round(w.ebet, 2)) #adding bet. scores to edgelist\n   etab <- etab[order(etab$bet, decreasing = TRUE), ] #ordering by bet.\n   kbl(etab[1:10, ], format = \"pipe\", align = c(\"l\", \"l\", \"c\"),\n       col.names = c(\"i\", \"j\", \"Edge Bet.\"), row.names = FALSE,\n       caption = \"Edges Sorted by Betweenness in the Women Lawyers Advice Network\") %>% \n   kable_styling(bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\nEdges Sorted by Betweenness in the Women Lawyers Advice Network\n \n  \n    i \n    j \n    Edge Bet. \n  \n \n\n  \n    3 \n    4 \n    19.17 \n  \n  \n    5 \n    8 \n    15.83 \n  \n  \n    3 \n    5 \n    13.67 \n  \n  \n    5 \n    7 \n    11.00 \n  \n  \n    2 \n    4 \n    9.17 \n  \n  \n    3 \n    11 \n    8.33 \n  \n  \n    5 \n    6 \n    8.17 \n  \n  \n    1 \n    4 \n    7.00 \n  \n  \n    2 \n    8 \n    6.50 \n  \n  \n    8 \n    9 \n    6.33 \n  \n\n\n\n\n\nNot surprisingly, the top edges are the ones linking nodes 3 and 4 and nodes 5 and 8.\n\nDisconnecting a Graph Via Bridge Removal\nHigh betweenness edges are likely to function as bridges being the only point of indirect connectivity between most nodes in the social structure. That means that an easy way to disconnect a connected graph is to remove the bridges (Girvan and Newman 2002).\nIn igraph we can produce an edge deleted subgraph of an original graph using the “minus” operator, along with the edge function like this:\n\n   del.g <- wg - edge(\"3|4\")\n   del.g <- del.g - edge(\"5|8\")\n\nThe first line creates a new graph object (a subgraph) which equals the original graph minus the edge linking nodes 3 and 4. The second line takes this last subgraph and further deletes the edge linking nodes 5 and 8.\nThe resulting subgraph, minus the top two high-betweenness edges, looks like:\n\n\n\n\n\nWhich is indeed disconnected!"
  },
  {
    "objectID": "handout2.html#induced-betweenness",
    "href": "handout2.html#induced-betweenness",
    "title": "Centrality",
    "section": "Induced Betweenness",
    "text": "Induced Betweenness\nIn the assigned handbook chapter reading, Borgatti and Everett argue that another way of thinking about centrality of a node (or edge) is to calculate the difference that removing that node makes for some graph property in the network. They further suggest that the sum of the centrality scores of each node is just such a property, proposing that betweenness is particularly interesting in this regard. Let’s see how this works.\nWe will use the undirected version of the women lawyers advice network for this example. Let’s say we are interested in the difference that node 10 makes for the betweenness centralities of everyone else. In that case we would proceed as follows:\n\n   bet <- betweenness(wg) #original centrality scores\n   Sbet <- sum(bet) #sum of original centrality scores\n   wg.d <- wg - vertex(\"10\") #removing vertex 10 from the graph\n   bet.d <- betweenness(wg.d) #centrality scores of node deleted subgraph\n   Sbet.d <- sum(bet.d) #sum of centrality scores of node deleted subgraph\n   total.c <- Sbet - Sbet.d #total centrality\n   indirect.c <- total.c - bet[10] #indirect centrality\n   indirect.c\n\n[1] 12.66667\n\n\nLine 1 just calculates the regular betweenness centrality vector for the graph. Line 2 sums up all of the entries of this vector. Line 3 creates a node deleted subgraph by removing node 10. This is done using the “minus” operator and the igraph function vertex, which works just like the edge function we used earlier to create an edge deleted subgraph, except it takes a node id or name as input.\nLines 4-5 just recalculate the sum of betweenness centralities in the subgraph that excludes node 10. Then in line 6 we subtract the sum of centralities of the node deleted subgraph from the sum of centralities of the original graph. If this number, which Borgatti and Everett call the “total” centrality, is large and positive then that means that node 10 makes a difference for the centrality of others.\nHowever, part of that difference is node 10’s own “direct” centrality, so to get a more accurate sense of node 10’s impact on other people’s centrality we need to subtract node 10’s direct centrality from the total number, which we do in line 7 to get node 10’s “indirect” centrality. The result is shown in the last line, which indicates that node 10 has a pretty big impact on other people’s betweenness centralities, net of their own (which is pretty small).\nNow all we need to do is do the same for each node to create a vector of indirect betweenness centralities. So we incorporate the code above into a short loop through all vertices:\n\n   total.c <- 0 #empty vector\n   indirect.c <- 0 #empty vector\n   for (i in 1:vcount(wg)) {\n      wg.d <- wg - vertex(i)\n      bet.d <- betweenness(wg.d) #centrality scores of node deleted subgraph\n      Sbet.d <- sum(bet.d) #sum of centrality scores of node deleted subgraph\n      total.c[i] <- Sbet - Sbet.d #total centrality\n   indirect.c[i] <- total.c[i] - bet[i] #total minus direct\n   }\n\nWe can now list the total, direct, and indirect betweenness centralities for the women lawyers graph using a nice table:\n\n   i.bet <- data.frame(n = 1:vcount(wg), total.c, round(betweenness(wg), 1), round(indirect.c, 1))\n   kbl(i.bet, format = \"pipe\", align = c(\"l\", \"c\", \"c\", \"c\"),\n       col.names = c(\"Node\", \"Total\", \"Direct\", \"Indirect\"), row.names = FALSE,\n       caption = \"Induced Betweenness Scores in the Women Lawyers Advice Network\") %>% \n   kable_styling(bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\nInduced Betweenness Scores in the Women Lawyers Advice Network\n \n  \n    Node \n    Total \n    Direct \n    Indirect \n  \n \n\n  \n    1 \n    16 \n    0.0 \n    16.0 \n  \n  \n    2 \n    4 \n    6.7 \n    -2.7 \n  \n  \n    3 \n    -24 \n    23.2 \n    -47.2 \n  \n  \n    4 \n    -4 \n    12.2 \n    -16.2 \n  \n  \n    5 \n    19 \n    18.8 \n    0.2 \n  \n  \n    6 \n    10 \n    3.7 \n    6.3 \n  \n  \n    7 \n    18 \n    0.0 \n    18.0 \n  \n  \n    8 \n    4 \n    8.8 \n    -4.8 \n  \n  \n    9 \n    18 \n    0.0 \n    18.0 \n  \n  \n    10 \n    13 \n    0.3 \n    12.7 \n  \n  \n    11 \n    14 \n    0.0 \n    14.0 \n  \n  \n    12 \n    13 \n    0.3 \n    12.7 \n  \n\n\n\n\n\nThis approach to decomposing betweenness centrality provides a new way to categorize actors in a network:\n\nOn the one hand, we have actors like nodes 3 and 4 who “hog” centrality from others. Perhaps these are the prototypical high betweenness actors who monopolize the flow through the network. Their own direct centrality is high, but their indirect centrality is negative, suggesting that others become more central when they are removed from the graph as they can now become intermediaries themselves.\nIn contrast, we also have actors like node 5 who are high centrality themselves, but who’s removal from the network does not affect anyone else’s centrality. These actors are high betweenness but themselves don’t monopolize the flow of information in the network.\nThen we have actors (like nodes 9-12) who have low centrality, but whose removal from the network makes a positive difference for other people’s centrality, which overall decreases when they are removed from the network.\nFinally, we have actors line nodes 2 and 8, who are not particularly central, but who also hog centrality from others, in that removing them from the network also increases other people’s centrality (although not such an extent as the hogs)."
  },
  {
    "objectID": "handout2.html#generalized-harmonic-centrality",
    "href": "handout2.html#generalized-harmonic-centrality",
    "title": "Centrality",
    "section": "Generalized Harmonic Centrality",
    "text": "Generalized Harmonic Centrality\nAgneessens, Borgatti, and Everett (2017) propose a “generalized” version of the harmonic centrality that yields plain old degree centrality and the regular harmonic centrality as special cases. The key is to introduce a parameter \\(\\delta\\) governing how much weight we give to shortest paths based on distance. Let’s see how this works.\nRecall that the harmonic centrality we defined earlier is given by:\n\\[\n\\frac{\\sum_{j \\neq i} (g_{ij})^{-1}}{n-1}\n\\]\nFor any node \\(i\\), where \\(g_{ij}\\) is the geodesic distance between \\(i\\) and every other node in the graph \\(j\\), which could be “infinite” if there is no path linking them.\nAgneessens et al’s tweak is to instead compute:\n\\[\n\\frac{\\sum_{j \\neq i} (g_{ij})^{-\\delta}}{n-1}\n\\]\nWhere \\(\\delta\\) is a free parameter chosen by the researcher with the restriction that \\(\\delta \\geq 0\\) (if you want to calculate a closeness measure as we will see below).\nWhen \\(\\delta = \\infty\\) the numerator element \\(1/(g_{ij})^{\\infty} = 1\\) only when nodes are adjacent and \\(g_{ij} = 1\\) (because \\(1^{\\infty} = 1\\)); otherwise, for \\(g_{ij} > 1\\) then \\(1/(g_{ij})^{\\infty} = 0\\), and therefore the generalized harmonic centrality just becomes a (normalized) version of degree centrality. Alternatively, when \\(\\delta = 1\\) we just get the plain old harmonic centrality we defined earlier.\nThe interesting cases come from \\(1 > \\delta < \\infty\\) and \\(0 > \\delta < 1\\). In the first case, nodes at shorter distances are weighted more (like in the standard harmonic centrality measure) as \\(\\delta\\) becomes bigger and bigger then the generalized harmonic centrality approximates degree. For values below one, as \\(\\delta\\) approaches zero, then indirect connections to nodes of greater length are discounted less, and thus count for “more” in defining your generalized harmonic centrality score.\nLet us see a real-world example of the generalized harmonic centrality in action:\nFirst, we create a custom function to compute the generalized harmonic centrality:\n\n   g.harm <- function(x, d) {\n      library(igraph)\n      S <- distances(x) #get distances from graph object\n      S <- 1/S^d #matrix of generalized inverse distances\n      diag(S) <- 0 #set diagonals to zero\n      c <- rowSums(S)/(vcount(x) - 1) #summing and averaging\n      return(c)\n   }\n\nSecond, we compute three versions of the harmonic centrality, with \\(\\delta = 5\\), \\(\\delta = 0.05\\), and \\(\\delta = -5\\), using the full (unrestricted by age) subgraph of the law_advice network composed of the women lawyers at the firm, with relations constrained to be undirected:\n\n   women <- which(V(law_advice)$gender == 2)\n   wg <- subgraph(law_advice, women)\n   wg <- as.undirected(wg)\n   c1 <- g.harm(wg, d = 5)\n   c2 <- g.harm(wg, d = 0.05)\n   c3 <- g.harm(wg, d = -5)\n\n\nThe first version of the harmonic centrality in line 5, with a positive value of \\(\\delta\\) above zero, will compute centrality scores emphasizing direct (one-step) connections, thus coming closer to degree.\nThe second version, in line 6, with a value of \\(\\delta\\) close to zero, will give comparatively more emphasis to indirect connections weighing longer paths almost as much as shorter paths (but always a little less), thus being more similar to closeness centrality.\nFinally, the last version, in line 7, with \\(\\delta < 0\\), will weigh longer paths more than shorter ones, serving as a measure of eccentricity (farness from others) not closeness.\n\n\n\n\n\n\nFull women lawyers advice network\n\n\n\n\nAbove is a plot of the women lawyers network showing the top node for each of the centralities:\n\nIn red we have node 3 who has the largest degree (\\(k(3) = 8\\)) and thus comes out on top using the generalized harmonic centrality version emphasizing direct connections (\\(\\delta > 1\\)).\nThen in blue we have node 9 who can reach the most others via the shortest paths, and thus comes out on top when the generalized harmonic centrality emphasizes indirect connectivity.\nFinally, in purple we have node 12, which is farthest from everyone else, and thus comes out on “top” when longer indirect connections count for more (\\(\\delta < 0)\\).\n\nAs we said earlier, both regular harmonic centrality and degree are special cases of the generalized measure. We can check this by setting \\(\\delta\\) to either one or infinity.\nWhen we set \\(\\delta=1\\) the generalized harmonic centrality is the same as (normalized by number of nodes minus one) the regular harmonic centrality:\n\n   g.harm(wg, d = 1)\n\n [1] 0.5980392 0.5343137 0.7058824 0.5980392 0.6568627 0.6274510 0.4264706\n [8] 0.5392157 0.6960784 0.6470588 0.6666667 0.4068627 0.5882353 0.5196078\n[15] 0.5833333 0.6029412 0.5588235 0.5441176\n\n   harmonic_centrality(wg, normalized = TRUE)\n\n [1] 0.5980392 0.5343137 0.7058824 0.5980392 0.6568627 0.6274510 0.4264706\n [8] 0.5392157 0.6960784 0.6470588 0.6666667 0.4068627 0.5882353 0.5196078\n[15] 0.5833333 0.6029412 0.5588235 0.5441176\n\n\nWhen we set \\(\\delta=\\infty\\) the generalized harmonic centrality is the same as (normalized by number of nodes minus one) degree centrality:\n\n   g.harm(wg, d = Inf)\n\n [1] 0.23529412 0.17647059 0.47058824 0.23529412 0.35294118 0.29411765\n [7] 0.05882353 0.17647059 0.41176471 0.35294118 0.41176471 0.05882353\n[13] 0.23529412 0.17647059 0.23529412 0.35294118 0.23529412 0.23529412\n\n   degree(wg, normalized = TRUE)\n\n [1] 0.23529412 0.17647059 0.47058824 0.23529412 0.35294118 0.29411765\n [7] 0.05882353 0.17647059 0.41176471 0.35294118 0.41176471 0.05882353\n[13] 0.23529412 0.17647059 0.23529412 0.35294118 0.23529412 0.23529412"
  },
  {
    "objectID": "handout2.html#appendix-selecting-a-subgraph-based-on-edge-conditions",
    "href": "handout2.html#appendix-selecting-a-subgraph-based-on-edge-conditions",
    "title": "Centrality",
    "section": "Appendix: Selecting a Subgraph Based on Edge Conditions",
    "text": "Appendix: Selecting a Subgraph Based on Edge Conditions\nA great question asked in class goes as follows: What if I want to create a subgraph based on selecting a subset of a nodes, and then the other nodes in the graph that are that set of node’s in-neighbors?\nLet’s see how that would work.\nFirst, we create a vector with the node ids of our focal nodes, which will be women under 40 in the law_advice network.\n\n   yw <- which(V(law_advice)$gender == 2 & V(law_advice)$age < 40)\n   yw\n\n [1] 29 34 39 48 51 57 59 60 61 67 69 71\n\n\nSecond, we need to collect the node ids of the people who point to these set of nodes; that is, each of these node’s in-neighbors. For that, we use the igraph function neighbors:\n\n   yw_in <- list() #creating empty list\n   k <- 1 #counter for list position\n   for (i in yw) {\n      nei <- neighbors(law_advice, i, mode = \"in\") \n      nei <- as.vector(nei) #vector of in-neighbors ids\n      yw_in[[k]] <- nei #adding to list\n      k <- k + 1 #incrementing counter\n   }\n\nLine one creates an (empty) list object in R. The beauty of a list object is that it is an object that can hold other objects (vectors, matrices, igraph graph objects, etc.) as members (it can also have other lists as members, with lists all the way down). For a primer on how to work with R lists see here\nThe idea is to populate this initially empty list with the vectors of the in-neighbor ids of each node listed in the vector yw. Lines 2-8 do that using a simple for loop starring the igraph command neighbors, a function which takes two inputs: an igraph graph object, and a node id. The argument “mode” (“in” or “out” for directed graphs), tells it which kid of neighbors you want (not necessary for undirected graphs). Here we want the in-neighbors, so mode = “in”.\nNow we have a list object in R of length equal to the number of younger women (12 in this case) with each entry equal to the ids of those women’s in-neighbors.\n\n   length(yw_in)\n\n[1] 12\n\n   head(yw_in)\n\n[[1]]\n [1]  4 10 12 15 16 17 19 26 27 28 34 36 41 42 45 48 70\n\n[[2]]\n [1]  7 10 12 13 14 15 16 17 19 22 26 27 28 29 33 36 42 44 45 46 48 56 60 61 64\n\n[[3]]\n [1] 13 17 30 40 41 42 48 51 52 55 56 57 65 66 67 69 71\n\n[[4]]\n[1] 16 17 39 42\n\n[[5]]\n[1] 58 59\n\n[[6]]\n [1] 27 39 41 51 55 56 62 65 66 67 71\n\n\nNow we need to create a vector of the unique ids of these nodes. To do this, we just to “unlist” all of the node ids to create a simple vector from the list object.\nThe unlist native R function does that for us, taking a list as input and returning all of the elements inside each of the separate objects stored in the list as output. Here we wrap that call in the unique native R function to eliminate repeats (common in-neighbors across women):\n\n   yw_in <- unique(unlist(yw_in))\n   yw_in\n\n [1]  4 10 12 15 16 17 19 26 27 28 34 36 41 42 45 48 70  7 13 14 22 29 33 44 46\n[26] 56 60 61 64 30 40 51 52 55 57 65 66 67 69 71 39 58 59 62 35\n\n\nOf course, because the younger women are their own in-neighbors, they are included in this vector, so we need to get rid of them:\n\n   yw_in <- setdiff(yw_in, yw)\n   yw_in\n\n [1]  4 10 12 15 16 17 19 26 27 28 36 41 42 45 70  7 13 14 22 33 44 46 56 64 30\n[26] 40 52 55 65 66 58 62 35\n\n\nWe use the native command setdiff to find the elements in vector yw_in that are not contained in the vector of young women ids yw or the difference between the set of nodes ids stored in yw_in and the set of node ids stored in yw.\nNow that we have the vector of ids of the focal nodes and the vector of ids of their in-neighbors, we are ready to create our subgraph! All we need to do is specify we want both the younger law firm women and their in-neighbors in our node-induced subgraph:\n\n   g <- subgraph(law_advice, c(yw, yw_in))\n\nWe can even specify a new vertex attribute, differentiating the focal network from the in-neighbor network.\n\n   V(g)$net_status <- c(rep(1, length(yw)), rep(2, length(c(yw, yw_in)) - length(yw)))\n   vertex_attr(g)\n\n$status\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[39] 2 2 2 2 2 2 2\n\n$gender\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 1 1 2 1 1 2 1 1 1 2 1 2 2 2 1 1 1 2 1 2 2 2 1\n[39] 2 1 1 2 2 1 2\n\n$office\n [1] 1 2 1 1 1 2 3 1 1 1 1 1 1 2 1 2 2 1 2 1 1 1 1 1 3 1 2 1 2 1 1 1 1 2 2 1 1 1\n[39] 1 1 1 1 1 1 1\n\n$seniority\n [1] 31 29 25 24 22  1 21 20 23 19  9 15 13 11 10  7  8  8  8  5  6  6  5  4  5\n[26]  3  3  1  4  4  3  3  3  3  2  2  2  2  2  2  1  1  1  1  1\n\n$age\n [1] 59 63 53 52 57 56 48 46 50 46 49 41 47 38 38 39 37 36 33 43 37 34 31 31 53\n[26] 38 42 35 29 29 34 38 33 33 30 31 34 32 45 28 43 35 38 31 26\n\n$practice\n [1] 2 2 2 2 1 2 2 2 2 2 1 1 1 2 2 1 1 2 2 1 1 1 1 2 2 2 2 2 1 1 1 1 1 1 1 2 2 2\n[39] 2 1 1 1 1 2 1\n\n$law_school\n [1] 3 3 3 2 2 1 3 1 1 1 3 3 1 2 3 3 3 2 3 3 1 1 2 2 1 3 2 3 3 3 3 2 2 3 2 2 3 2\n[39] 2 3 3 2 3 2 2\n\n$net_status\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[39] 2 2 2 2 2 2 2\n\n\nFinally, we create an edge deleted subgraph including only the incoming advice edges from nodes who are not younger women in the firm to younger women and deleting everything other link:\n\n   e.rem <- E(g)[V(g)[net_status==1] %->% V(g)[net_status==1]] \n   #selecting edges from younger women to younger women\n   g.r <- delete_edges(g, e.rem) #removing edges\n   e.rem <- E(g.r)[V(g.r)[net_status==1] %->% V(g.r)[net_status==2]] \n   #selecting edges from younger women to non-younger women\n   g.r <- delete_edges(g.r, e.rem) #removing edges\n   e.rem <- E(g.r)[V(g.r)[net_status==2] %->% V(g.r)[net_status==2]] \n   #selecting edges from non-younger women to non-younger women\n   g.r <- delete_edges(g.r, e.rem) #removing edges\n   Iso = which(degree(g.r)==0) #selecting isolates\n   g.r <- delete_vertices(g.r, Iso) #removing isolates\n\nHere we can see both the delete_edges and delete_vertices functions from igraph in action. Both take some graph object as input followed by either an edge sequence (in this case produced by E(g))or a vector of node ids respectively. In both cases those particular edges or nodes are removed from the graph.\nThe other neat functionality we see on display here is the igraph %->% operator for directed graph edges (the equivalent for undirected graphs is the double dash %–%). This allows us to select a set of edges according to a vertex condition (e.g., homophilous (same group) edges or edges that link a member from group a to a member from group b).\nSo the code chunk:\n\n   E(g)[V(g)[net_status==1] %->% V(g)[net_status==1]] \n\n+ 69/436 edges from 318283d:\n [1]  1-> 4  1-> 5  1-> 6  1-> 9  1->10  1->11  1->12  2-> 3  3-> 8  3-> 9\n[11]  3->12  4-> 1  4-> 3  4-> 5  4-> 6  4-> 7  4-> 8  4-> 9  4->10  4->12\n[21]  5-> 4  5-> 7  5-> 8  5-> 9  5->12  6-> 1  6-> 4  6-> 7  6-> 8  6-> 9\n[31]  7-> 5  7-> 6  7-> 8  7->11  7->12  8-> 1  8-> 2  8-> 3  8-> 4  8-> 6\n[41]  8-> 7  8-> 9  8->10  8->12  9-> 1  9-> 4  9-> 6  9-> 8  9->11  9->12\n[51] 10-> 1 10-> 4 10-> 5 10-> 7 10-> 9 10->11 10->12 11-> 1 11-> 7 11-> 8\n[61] 11-> 9 11->10 12-> 1 12-> 4 12-> 5 12-> 7 12-> 8 12-> 9 12->11\n\n\nTakes the edge set of the graph g (E(g)) and gives us the subset of edges that go from a vertex with net_status equal to one to another vertex that also has net_status equal to one (in this case edges directed from one of our focal nodes to another one of our focal nodes). This, of course, happens to be all the directed edges linking nodes one through twelve in the network. The same go for the other calls to the same function using different logical combinations of values of net_status between nodes.\nFINALLY, can now plot the incoming advice network to younger women (in red):\n\n\n\n\n\nWomen lawyers advice network showing incoming links from outside the group (blue nodes) to younger women (red nodes)"
  },
  {
    "objectID": "handout2.html#note-handling-graph-objects-in-lists",
    "href": "handout2.html#note-handling-graph-objects-in-lists",
    "title": "Centrality",
    "section": "Note: Handling Graph Objects in Lists",
    "text": "Note: Handling Graph Objects in Lists\nSpeaking of lists, sometimes network data comes pre-stored as an R list. This is typical if you have a network with multiple kinds of ties recorded on the same set of actors (and thus multiple networks), or longitudinal network data, where we collect multiple “snapshots” of the same system (containing the same or more typically a different set of actors per time slice).\nThe networkdata package contains one such data set called atp. It’s a network of Tennis players who played in grand slam or official matches of the Association of Tennis Professionals (hence ATP) covering the years 1968-2021 (Radicchi 2011).\nIn the directed graph representing each network, a tie goes from the loser to the winner of each match. Accordingly, it can be interpreted as a directed “deference” network (it would be a dominance network if it was the other way around), where actor i “defers” to actor j by getting their ass kicked by them.\nLet’s see how this list of networks works:\n\n   g <- atp\n   head(g)\n\n[[1]]\nIGRAPH 08a202a DNW- 497 1213 -- ATP Season 1968\n+ attr: name (g/c), name (v/c), age (v/n), hand (v/c), country (v/c),\n| surface (e/c), weight (e/n)\n+ edges from 08a202a (vertex names):\n [1] U Unknown    ->Jose Mandarino     Alfredo Acuna->Alan Fox          \n [3] Andres Gimeno->Ken Rosewall       Andres Gimeno->Raymond Moore     \n [5] Juan Gisbert ->Tom Okker          Juan Gisbert ->Zeljko Franulovic \n [7] Onny Parun   ->Jan Kodes          Peter Curtis ->Tom Okker         \n [9] Premjit Lall ->Clark Graebner     Rod Laver    ->Ken Rosewall      \n[11] Thomas Lejus ->Nicola Pietrangeli Tom Okker    ->Arthur Ashe       \n[13] U Unknown    ->Jaidip Mukherjea   U Unknown    ->Jose Luis Arillla \n+ ... omitted several edges\n\n[[2]]\nIGRAPH c895c57 DNW- 446 1418 -- ATP Season 1969\n+ attr: name (g/c), name (v/c), age (v/n), hand (v/c), country (v/c),\n| surface (e/c), weight (e/n)\n+ edges from c895c57 (vertex names):\n [1] U Unknown           ->U Unknown        \n [2] Alejandro Olmedo    ->Ron Holmberg     \n [3] Arthur Ashe         ->Rod Laver        \n [4] Bob Carmichael      ->Jim Osborne      \n [5] Cliff Richey        ->Zeljko Franulovic\n [6] Francois Jauffret   ->Martin Mulligan  \n [7] Fred Stolle         ->John Newcombe    \n+ ... omitted several edges\n\n[[3]]\nIGRAPH d6709af DNW- 451 1650 -- ATP Season 1970\n+ attr: name (g/c), name (v/c), age (v/n), hand (v/c), country (v/c),\n| surface (e/c), weight (e/n)\n+ edges from d6709af (vertex names):\n [1] Mark Cox            ->Jan Kodes        \n [2] Charlie Pasarell    ->Stan Smith       \n [3] Cliff Richey        ->Arthur Ashe      \n [4] Francois Jauffret   ->Manuel Orantes   \n [5] Georges Goven       ->Jan Kodes        \n [6] Harald Elschenbroich->Zeljko Franulovic\n [7] Ilie Nastase        ->Zeljko Franulovic\n+ ... omitted several edges\n\n[[4]]\nIGRAPH a73a020 DNW- 459 2580 -- ATP Season 1971\n+ attr: name (g/c), name (v/c), age (v/n), hand (v/c), country (v/c),\n| surface (e/c), weight (e/n)\n+ edges from a73a020 (vertex names):\n [1] Andres Gimeno    ->Ken Rosewall   Arthur Ashe      ->Rod Laver     \n [3] Charlie Pasarell ->Cliff Drysdale Frank Froehling  ->Clark Graebner\n [5] Joaquin Loyo Mayo->Thomaz Koch    John Alexander   ->John Newcombe \n [7] John Newcombe    ->Marty Riessen  Nikola Pilic     ->Cliff Drysdale\n [9] Owen Davidson    ->Cliff Drysdale Robert Maud      ->Cliff Drysdale\n[11] Roger Taylor     ->Marty Riessen  Roy Emerson      ->Rod Laver     \n[13] Tom Okker        ->John Newcombe  Allan Stone      ->Bob Carmichael\n+ ... omitted several edges\n\n[[5]]\nIGRAPH 761ed05 DNW- 504 2767 -- ATP Season 1972\n+ attr: name (g/c), name (v/c), age (v/n), hand (v/c), country (v/c),\n| surface (e/c), weight (e/n)\n+ edges from 761ed05 (vertex names):\n [1] Jean Loup Rouyer->Stan Smith     Marty Riessen   ->Cliff Drysdale\n [3] Roy Emerson     ->Arthur Ashe    Roy Emerson     ->Arthur Ashe   \n [5] Tom Leonard     ->John Newcombe  Tom Okker       ->Arthur Ashe   \n [7] Tom Okker       ->Rod Laver      Adriano Panatta ->Andres Gimeno \n [9] Adriano Panatta ->Ilie Nastase   Allan Stone     ->John Alexander\n[11] Allan Stone     ->Marty Riessen  Andres Gimeno   ->Jan Kodes     \n[13] Andres Gimeno   ->Stan Smith     Andrew Pattison ->Ilie Nastase  \n+ ... omitted several edges\n\n[[6]]\nIGRAPH 92ff576 DNW- 592 3653 -- ATP Season 1973\n+ attr: name (g/c), name (v/c), age (v/n), hand (v/c), country (v/c),\n| surface (e/c), weight (e/n)\n+ edges from 92ff576 (vertex names):\n [1] Harold Solomon    ->Stan Smith       John Alexander    ->Stan Smith      \n [3] Patrice Dominguez ->Paolo Bertolucci Paul Gerken       ->Jimmy Connors   \n [5] Roy Emerson       ->Rod Laver        Adriano Panatta   ->Ilie Nastase    \n [7] Bjorn Borg        ->Adriano Panatta  Brian Gottfried   ->Cliff Richey    \n [9] Charlie Pasarell  ->John Alexander   Cliff Richey      ->Stan Smith      \n[11] Corrado Barazzutti->Bjorn Borg       Francois Jauffret ->Ilie Nastase    \n[13] Georges Goven     ->Manuel Orantes   Manuel Orantes    ->Ilie Nastase    \n+ ... omitted several edges\n\n\nWe create a graph object and then examine its contents, which we can see is a set of graph objects. In unnamed R lists each of the objects inside is indexed by a number in double brackets. So [[6]] just means the sixth network in the list object (corresponding to the year 1973).\nNow let’s say we wanted to compute a network statistic like density. One way to proceed would be:\n\n   edge_density(g)\n\nError in `ensure_igraph()`:\n! Must provide a graph object (provided wrong object type).\n\n\nWhich gives us a weird error about the wrong object type. The reason is that edge_density expects an igraph graph object as input, but g is not a graph object it is a list of such objects. For it to work you have to reference a particular element inside the list not the whole list.\nTo do that, we use the double bracket notation:\n\n   edge_density(g[[6]])\n\n[1] 0.01044096\n\n\nWhich gives us the density for the 1973 network.\n\nLooping Through Lists\nBut what if we wanted a table of network statistics for all the years or some subset of years? Of course, we could just type a million versions of the edge_density command or whatever, but that would be tedious. We could also write a for loop or something like that (less tedious). Even less tedious is to use the many apply functions in R that are designed to work with lists, which is a subject onto itself in R programming.\nBut here we can just use the simple version. Let’s say we wanted a vector of densities (or any other whole network statistic) for the whole 54 years. In that case, our friend sapply can do the job:\n\n   sapply(g, edge_density)\n\n [1] 0.004920653 0.007144657 0.008130081 0.012272740 0.010914671 0.010440961\n [7] 0.010567864 0.013315132 0.012088214 0.014019237 0.014135328 0.011649909\n[13] 0.011172821 0.011261426 0.012703925 0.012177336 0.012648755 0.012445937\n[19] 0.012034362 0.012351377 0.010174271 0.009772014 0.019526953 0.012236462\n[25] 0.014050245 0.015054181 0.013872832 0.014727924 0.014329906 0.013935502\n[31] 0.013962809 0.013870042 0.013665097 0.013818887 0.012551113 0.011571679\n[37] 0.012329090 0.012923683 0.011402945 0.012677988 0.012256963 0.013512884\n[43] 0.012543025 0.013661748 0.013786518 0.013679697 0.015052857 0.015075622\n[49] 0.015081206 0.014346468 0.015764351 0.020169225 0.011889114 0.016935400\n\n\nsapply is kind of a “meta” function that takes two inputs: A list, and the name of a function (which could be native, a package, or user defined); sapply then “applies” that function to each element inside the list. Here we asked R to apply the function edge_density to each element of the list of networks g and it obliged, creating a vector of length 54 containing the info.\nWe could use any igraph function, like number of nodes in the graph:\n\n   sapply(g, vcount)\n\n [1] 497 446 451 459 504 592 595 535 553 524 509 572 582 573 554 532 495 513 510\n[20] 523 596 597 405 542 509 498 520 496 502 499 497 480 486 479 497 517 505 492\n[39] 524 488 493 464 482 459 457 453 428 430 431 438 419 364 345 393\n\n\nWe could also select subset of elements inside the list. For instance this counts the number of nodes for the first five years:\n\n   sapply(g[1:5], vcount)\n\n[1] 497 446 451 459 504\n\n\nOr for years 2, 6, 8, and 12:\n\n   sapply(g[c(2, 6, 8, 12)], vcount)\n\n[1] 446 592 535 572\n\n\nNote the single bracket notation here to refer to subsets of elements in the list. Inside the brackets we could put any arbitrary vector, as long as the numbers in the vector do no exceed the length of the list.\nOf course, sometimes the functions we apply to elements of the list don’t return single numbers but vectors or other igraph objects. In that case it would be better to use lapply which is just like sapply but returns another list with the set of answers inside it.\nFor instance, let’s say we wanted the top five players for each year. In this deference network, a “top” player is one who beats many others, which means they have high indegree (lots of losers pointing at them).\nFirst we create a custom function to compute the indegree and return an ordered named vector of top 5 players:\n\n   top5 <- function(x) {\n      library(igraph)\n      t <- degree(x, mode = \"in\")\n      t <- sort(t, decreasing = TRUE)[1:5]\n      return(t)\n   }\n\nNow, we can just feed that function to lapply:\n\n   top.list <- lapply(g, top5)\n   head(top.list)\n\n[[1]]\n   Arthur Ashe      Rod Laver Clark Graebner   Ken Rosewall      Tom Okker \n            33             27             25             23             22 \n\n[[2]]\nJohn Newcombe     Tom Okker     Rod Laver    Tony Roche   Arthur Ashe \n           45            41            40            40            33 \n\n[[3]]\n      Arthur Ashe      Cliff Richey         Rod Laver        Stan Smith \n               51                49                48                45 \nZeljko Franulovic \n               42 \n\n[[4]]\n     Ilie Nastase         Tom Okker     Marty Riessen        Stan Smith \n               69                63                61                61 \nZeljko Franulovic \n               60 \n\n[[5]]\n  Ilie Nastase     Stan Smith Manuel Orantes  Jimmy Connors    Arthur Ashe \n            99             72             68             65             55 \n\n[[6]]\n Ilie Nastase     Tom Okker Jimmy Connors   Arthur Ashe    Stan Smith \n           96            81            68            63            63 \n\n\nWhich is a list of named vectors containing the number of victories of the top five players each year.\nBecause the object top.list is just a list, we can subset it just like before. Let’s say we wanted to see the top players for more recent years:\n\n   top.list[49:54]\n\n[[1]]\n   Andy Murray  Dominic Thiem  Kei Nishikori Novak Djokovic   David Goffin \n            63             55             53             50             47 \n\n[[2]]\n         Rafael Nadal          David Goffin      Alexander Zverev \n                   58                    55                    52 \nRoberto Bautista Agut         Dominic Thiem \n                   45                    43 \n\n[[3]]\n   Dominic Thiem Alexander Zverev   Novak Djokovic    Fabio Fognini \n              51               50               46               45 \n   Roger Federer \n              44 \n\n[[4]]\n   Daniil Medvedev     Novak Djokovic       Rafael Nadal Stefanos Tsitsipas \n                55                 52                 52                 49 \n     Roger Federer \n                47 \n\n[[5]]\n     Andrey Rublev     Novak Djokovic Stefanos Tsitsipas       Rafael Nadal \n                40                 36                 27                 26 \n   Daniil Medvedev \n                24 \n\n[[6]]\n   Daniil Medvedev Stefanos Tsitsipas        Casper Ruud   Alexander Zverev \n                54                 52                 52                 51 \n    Novak Djokovic \n                49 \n\n\nA series of names which make sense to you if you follow Tennis.\n\n\nNaming Lists\nFinally, sometimes it useful to name the elements of a list. In this case, for instance, having the year number would be easier to remember what’s what. For this, you can use the names command, which works via standard R assignment:\n\n   names(g) <- c(1968:2021)\n   head(g)\n\n$`1968`\nIGRAPH 08a202a DNW- 497 1213 -- ATP Season 1968\n+ attr: name (g/c), name (v/c), age (v/n), hand (v/c), country (v/c),\n| surface (e/c), weight (e/n)\n+ edges from 08a202a (vertex names):\n [1] U Unknown    ->Jose Mandarino     Alfredo Acuna->Alan Fox          \n [3] Andres Gimeno->Ken Rosewall       Andres Gimeno->Raymond Moore     \n [5] Juan Gisbert ->Tom Okker          Juan Gisbert ->Zeljko Franulovic \n [7] Onny Parun   ->Jan Kodes          Peter Curtis ->Tom Okker         \n [9] Premjit Lall ->Clark Graebner     Rod Laver    ->Ken Rosewall      \n[11] Thomas Lejus ->Nicola Pietrangeli Tom Okker    ->Arthur Ashe       \n[13] U Unknown    ->Jaidip Mukherjea   U Unknown    ->Jose Luis Arillla \n+ ... omitted several edges\n\n$`1969`\nIGRAPH c895c57 DNW- 446 1418 -- ATP Season 1969\n+ attr: name (g/c), name (v/c), age (v/n), hand (v/c), country (v/c),\n| surface (e/c), weight (e/n)\n+ edges from c895c57 (vertex names):\n [1] U Unknown           ->U Unknown        \n [2] Alejandro Olmedo    ->Ron Holmberg     \n [3] Arthur Ashe         ->Rod Laver        \n [4] Bob Carmichael      ->Jim Osborne      \n [5] Cliff Richey        ->Zeljko Franulovic\n [6] Francois Jauffret   ->Martin Mulligan  \n [7] Fred Stolle         ->John Newcombe    \n+ ... omitted several edges\n\n$`1970`\nIGRAPH d6709af DNW- 451 1650 -- ATP Season 1970\n+ attr: name (g/c), name (v/c), age (v/n), hand (v/c), country (v/c),\n| surface (e/c), weight (e/n)\n+ edges from d6709af (vertex names):\n [1] Mark Cox            ->Jan Kodes        \n [2] Charlie Pasarell    ->Stan Smith       \n [3] Cliff Richey        ->Arthur Ashe      \n [4] Francois Jauffret   ->Manuel Orantes   \n [5] Georges Goven       ->Jan Kodes        \n [6] Harald Elschenbroich->Zeljko Franulovic\n [7] Ilie Nastase        ->Zeljko Franulovic\n+ ... omitted several edges\n\n$`1971`\nIGRAPH a73a020 DNW- 459 2580 -- ATP Season 1971\n+ attr: name (g/c), name (v/c), age (v/n), hand (v/c), country (v/c),\n| surface (e/c), weight (e/n)\n+ edges from a73a020 (vertex names):\n [1] Andres Gimeno    ->Ken Rosewall   Arthur Ashe      ->Rod Laver     \n [3] Charlie Pasarell ->Cliff Drysdale Frank Froehling  ->Clark Graebner\n [5] Joaquin Loyo Mayo->Thomaz Koch    John Alexander   ->John Newcombe \n [7] John Newcombe    ->Marty Riessen  Nikola Pilic     ->Cliff Drysdale\n [9] Owen Davidson    ->Cliff Drysdale Robert Maud      ->Cliff Drysdale\n[11] Roger Taylor     ->Marty Riessen  Roy Emerson      ->Rod Laver     \n[13] Tom Okker        ->John Newcombe  Allan Stone      ->Bob Carmichael\n+ ... omitted several edges\n\n$`1972`\nIGRAPH 761ed05 DNW- 504 2767 -- ATP Season 1972\n+ attr: name (g/c), name (v/c), age (v/n), hand (v/c), country (v/c),\n| surface (e/c), weight (e/n)\n+ edges from 761ed05 (vertex names):\n [1] Jean Loup Rouyer->Stan Smith     Marty Riessen   ->Cliff Drysdale\n [3] Roy Emerson     ->Arthur Ashe    Roy Emerson     ->Arthur Ashe   \n [5] Tom Leonard     ->John Newcombe  Tom Okker       ->Arthur Ashe   \n [7] Tom Okker       ->Rod Laver      Adriano Panatta ->Andres Gimeno \n [9] Adriano Panatta ->Ilie Nastase   Allan Stone     ->John Alexander\n[11] Allan Stone     ->Marty Riessen  Andres Gimeno   ->Jan Kodes     \n[13] Andres Gimeno   ->Stan Smith     Andrew Pattison ->Ilie Nastase  \n+ ... omitted several edges\n\n$`1973`\nIGRAPH 92ff576 DNW- 592 3653 -- ATP Season 1973\n+ attr: name (g/c), name (v/c), age (v/n), hand (v/c), country (v/c),\n| surface (e/c), weight (e/n)\n+ edges from 92ff576 (vertex names):\n [1] Harold Solomon    ->Stan Smith       John Alexander    ->Stan Smith      \n [3] Patrice Dominguez ->Paolo Bertolucci Paul Gerken       ->Jimmy Connors   \n [5] Roy Emerson       ->Rod Laver        Adriano Panatta   ->Ilie Nastase    \n [7] Bjorn Borg        ->Adriano Panatta  Brian Gottfried   ->Cliff Richey    \n [9] Charlie Pasarell  ->John Alexander   Cliff Richey      ->Stan Smith      \n[11] Corrado Barazzutti->Bjorn Borg       Francois Jauffret ->Ilie Nastase    \n[13] Georges Goven     ->Manuel Orantes   Manuel Orantes    ->Ilie Nastase    \n+ ... omitted several edges\n\n\nNow instead of the useless one, two, three, etc. names, we have the actual year numbers as the names of the elements on each list.\nSo if we wanted to know the top five players for 1988 we could just type:\n\n   top5(g[[\"1988\"]])\n\n   Stefan Edberg     Andre Agassi     Boris Becker    Mats Wilander \n              63               59               52               49 \nAaron Krickstein \n              48 \n\n\nNote the double bracket notation and the fact that the name of the list is a character not a number (hence the scare quotes).\nIf we don’t want to remember the bracket business, we could also use the $ operator to refer to particular list elements:\n\n   top5(g$\"1988\")\n\n   Stefan Edberg     Andre Agassi     Boris Becker    Mats Wilander \n              63               59               52               49 \nAaron Krickstein \n              48 \n\n\nOf course, we can also use the names to subset the list. Let’s say we wanted the top five players for 1970, 1980, 1990, 2000, 2010, and 2020.\nAll we have to do is type:\n\n   decades <- c(\"1970\", \"1980\", \"1990\", \"2000\", \"2010\", \"2020\")\n   lapply(g[decades], top5)\n\n$`1970`\n      Arthur Ashe      Cliff Richey         Rod Laver        Stan Smith \n               51                49                48                45 \nZeljko Franulovic \n               42 \n\n$`1980`\n     Ivan Lendl    John Mcenroe Brian Gottfried      Bjorn Borg Eliot Teltscher \n             97              76              63              62              62 \n\n$`1990`\n  Boris Becker  Stefan Edberg     Ivan Lendl   Pete Sampras Emilio Sanchez \n            62             57             50             47             44 \n\n$`2000`\nYevgeny Kafelnikov        Marat Safin    Gustavo Kuerten      Magnus Norman \n                63                 61                 59                 58 \n    Lleyton Hewitt \n                53 \n\n$`2010`\n   Rafael Nadal   Roger Federer    David Ferrer Robin Soderling   Jurgen Melzer \n             63              54              53              53              51 \n\n$`2020`\n     Andrey Rublev     Novak Djokovic Stefanos Tsitsipas       Rafael Nadal \n                40                 36                 27                 26 \n   Daniil Medvedev \n                24 \n\n\nNote that we are back to the single bracket notation.\nWith a bit of practice, lists will become your friends!"
  },
  {
    "objectID": "handout3.html",
    "href": "handout3.html",
    "title": "Status and Prestige",
    "section": "",
    "text": "In the last handout, we saw how to compute the most popular centrality measures. Freeman’s “big three” have strong graph-theoretic foundation and do a good job of formalizing and quantifying the idea that a node is central if it is “well-placed” in the network, where being well-placed resolves into either being able to reach others (directly as with degree or indirectly as with closeness) or being able to intermediate between others (as with betweenness)."
  },
  {
    "objectID": "handout3.html#networks-as-prisms",
    "href": "handout3.html#networks-as-prisms",
    "title": "Status and Prestige",
    "section": "Networks as Prisms",
    "text": "Networks as Prisms\nThere is, however, another strong and well-motivated intuition as to what it means to be “well-placed” in a network. Here the ties in the network are seen less as “pipes” that transmit stuff and more like “prisms” that reflect on you (Podolny 2001). Another way of thinking about this second version of well-placedness is that what is transmitted through the network is the network itself, or more accurately, the importance, status, and prestige of the people you are connected to.\nUnder this interpretation, actors get status and prestige in the network from being connected to prestigious and high status others. Those others, in turn, get their status from being connected to high status others, and so ad infinitum.\nOne way of quantifying this idea goes like this. If \\(\\mathbf{x}\\) is a vector containing the desired status scores, then the status of each actor should be equal to:\n\\[\n   x_i = \\sum_{j} a_{ij}x_j\n\\tag{1}\\]\nWhere \\(a_{ij} = 1\\) if \\(i\\) is adjacent to \\(j\\) in the network. Note that this formula just sums up the status scores of all the others each actor is connected to.\nIn matrix notation, if \\(\\mathbf{x}\\) is a column vector of status scores then:\n\\[\n   \\mathbf{x} = A\\mathbf{x}\n\\]\nBecause \\(\\mathbf{x}\\) is an \\(n \\times n\\) matrix and \\(\\mathbf{x}\\) is \\(n \\times 1\\) column vector, the multiplication \\(A\\mathbf{x}\\) will return another column vector of dimensions \\(n \\times 1\\), in this case \\(\\mathbf{x}\\) itself!\nNote the problem that this formulation poses: \\(\\mathbf{x}\\) appears on both sides of the equation, which means that in order to know the status of any one node we would need to know the status of the others, but calculating the status of the others depends on knowing the status of the focal node, and so on. There’s a chicken and the egg problem here.\nNow, there is an obvious (to the math majors) mathematical solution to this problem, because there’s a class of solvable (under some mild conditions imposed on the matrix \\(\\mathbf{A}\\)) linear algebra problems that take the form:\n\\[\n   \\lambda\\mathbf{x} = A\\mathbf{x}\n\\]\nWhere \\(\\lambda\\) is just a plain old number (a scalar). Once again conditional of the aforementioned mild conditions being met, we can search for a value \\(\\lambda\\) while also filling up the \\(\\mathbf{x}\\) vector with another set of values until we make the above equality true.\nWhen we do that successfully, we say that the value of \\(\\lambda\\) we hit upon is an eigenvalue of the matrix \\(\\mathbf{A}\\) and the values of the vector \\(\\mathbf{x}\\) we came up with are an eigenvector of the same matrix (technically in the above equation a right eigenvector).\nEigenvalues and eigenvectors, like Don Quixote and Sancho Panza, come in pairs, because you need a unique combination of both to solve the equation. Typically, a given matrix (like an adjacency matrix) will have multiple \\(\\lambda/\\mathbf{x}\\) pairs that will solve the equation. Together the whole set \\(\\lambda/\\mathbf{x}\\) pairs that make the equation true are the eigenvalues and eigenvectors of the matrix.\nNote that all of this obscure talk about eigenvalues and eigenvectors is just matrix math stuff. It has nothing to do with networks and social structure.\nIn contrast, because the big three centrality measures have a direct foundation in graph theory, and graph theory is an isomorphic model of social structures (points map to actor and lines map to relations) the “math” we do with graph theory is directly meaningful as a model of networks (the counts of the number of edges incident to a node is the count of other actors they someone is directly connected to).\nEigenvalues and eigenvectors are not a model of social structure in the way graph theory is (their first scientific application was in Physics). They are just a mechanical math fix to a circular equation problem.\nThis is why it’s a mistake to introduce network measures of status and prestige by jumping directly to the machinery of linear algebra (or worse talk about the idea of eigenvector centrality which means nothing to most people).\nA better approach is to see if we can motivate the use of measures like the ones above using the simple model of the distribution of status and prestige we started with earlier. We will see that we can, and that doing that leads us back to solutions that are the mathematical equivalent of all the eigenvector stuff."
  },
  {
    "objectID": "handout3.html#distributing-centrality-to-others",
    "href": "handout3.html#distributing-centrality-to-others",
    "title": "Status and Prestige",
    "section": "Distributing Centrality to Others",
    "text": "Distributing Centrality to Others\nLet’s start with the simplest model of how people can get their status from the status of others in a network. It is the simplest because it is based on degree.\nImagine everyone has the same “quantum” of status to begin with (this can be stored in a vector containing the same number of length equals to number of actors in the network). Then, at each step, people “send” the same amount of status to all their alters in the network. At the end of each step, we compute people’s status scores using Equation 1. We stop doing this after the status scores of people stop changing across each iteration.\nLet us see a real-life example at work.\nWe will use a data set collected by David Krackhardt on the friendships of 21 managers in a high tech company in the West coast (see the description here). The data are reported as directed ties (\\(i\\) nominates \\(j\\) as a friend) but we will constrain ties to be undirected:\n\n   library(networkdata)\n   library(igraph)\n   g <- as.undirected(ht_friends)\n\nThis is what the network looks like:\n\n\n\n\n\nKrackhardt’s Manager Data.\n\n\n\n\nWe extract the adjacency matrix corresponding to this network:\n\n   A <- as.matrix(as_adjacency_matrix(g))\n\nAnd here’s a simple custom function using a while loop that exemplifies the process of status distribution through the network we talked about earlier:\n\n   status1 <- function(A) {\n      n <- nrow(A) #number of actors\n      x <- rep(1, n) #initial status vector set to all ones\n      w <- 1 \n      k <- 0 #initializing counter\n      while (w > 0.0001) {\n          o.x <- x #old status scores\n          x <- A %*% x #new scores a function of old scores and adjacency matrix\n          x <- x/norm(x, type = \"E\") #normalizing new status scores\n          w <- abs(sum(abs(x) - abs(o.x))) #diff. between new and old scores\n          k <- k + 1 #incrementing while counter\n      }\n   return(as.vector(x))\n   }\n\nLines 1-5 initialize various quantities, most importantly the initial status vector for each node to just a series of ones:\n\n   rep(1, nrow(A))\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n\nThen lines 6-12 implement a little loop of how status is distributed through the network, with the most important piece of code being line 8 where the current status scores for each node are just the sum of the status scores of its neighbors computed one iteration earlier. The program stops when the difference between the old and the new scores is negligible (\\(\\delta < 0.0001\\)) as checked in line 10.\nNote the normalization step on line 9, which is necessary to prevent the sum of status scores from getting bigger and bigger indefinitely. In base R, the type = \"E\" normalization implements the euclidean vector norm, by which we divide each value of the status scores by after each update.1\nAnd here’s the resulting (row) vector of status scores for each node:\n\n   s <- status1(A)\n   s <- s/max(s) #normalizing by maximum\n   round(s, 3)\n\n [1] 0.619 0.635 0.446 0.489 0.629 0.430 0.205 0.380 0.444 0.468 0.814 0.549\n[13] 0.162 0.401 0.613 0.360 1.000 0.247 0.680 0.360 0.392\n\n\nWhat if I told you that this vector is the same as that given by the leading (first) eigenvector of the adjacency matrix?\n\n   s <- abs(eigen(A)$vector[, 1])#computing the first eigenvector\n   s <- s/max(s) #normalizing by maximum\n   round(s, 3)\n\n [1] 0.619 0.635 0.446 0.489 0.629 0.430 0.205 0.380 0.444 0.468 0.814 0.549\n[13] 0.162 0.401 0.613 0.360 1.000 0.247 0.680 0.360 0.392\n\n\nWhich is of course what is computed by the eigen_centrality function in igraph:\n\n   round(eigen_centrality(g)$vector, 3) #igraph automatically normalizes\n\n [1] 0.619 0.635 0.446 0.489 0.629 0.430 0.205 0.380 0.444 0.468 0.814 0.549\n[13] 0.162 0.401 0.613 0.360 1.000 0.247 0.680 0.360 0.392\n\n\nSo, the “eigenvector centralities” are just the limit scores produced by the status distribution process implemented in the status1 function!\nWhen treated as a structural index of connectivity in a graph (aka a centrality measure) the eigenvector status scores induce an ordering of the nodes which we may be interested in looking at:\n\n   nodes <- 1:vcount(g)\n   eig.cent <- round(status1(A), 3)\n   eig.dat <- data.frame(Nodes = nodes, Eigen.Cent = eig.cent, Deg.Cent = degree(g))\n   eig.dat <- eig.dat[order(eig.dat$Eigen.Cent, decreasing = TRUE), ]\n   library(kableExtra)\n   kbl(eig.dat[1:10, ], \n       format = \"html\", align = \"c\", row.names = FALSE,\n       caption = \"Top Ten Eigenvector Scores.\") %>%    kable_styling(bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\nTop Ten Eigenvector Scores.\n \n  \n    Nodes \n    Eigen.Cent \n    Deg.Cent \n  \n \n\n  \n    17 \n    0.413 \n    18 \n  \n  \n    11 \n    0.336 \n    14 \n  \n  \n    19 \n    0.281 \n    10 \n  \n  \n    2 \n    0.262 \n    10 \n  \n  \n    5 \n    0.260 \n    10 \n  \n  \n    1 \n    0.256 \n    9 \n  \n  \n    15 \n    0.253 \n    9 \n  \n  \n    12 \n    0.227 \n    8 \n  \n  \n    4 \n    0.202 \n    7 \n  \n  \n    10 \n    0.193 \n    8 \n  \n\n\n\n\n\nMost other measures of status in networks are constructed using similar principles. What changes is the model of how status is distributed in the system. That’s why scary and non-intuitive stuff about eigenvectors or whatever is misleading.\nOther measures are designed such that they either change the quantum of status that is distributed through the network by making it dependent on some node characteristic (like degree) or differentiate between different routes of distribution in directed graphs, by for instance, differentiating status derived from outgoing links from that derived from incoming links.\nLet’s see some examples of these alternative cases."
  },
  {
    "objectID": "handout3.html#a-degree-dependent-model-of-status-aka-pagerank",
    "href": "handout3.html#a-degree-dependent-model-of-status-aka-pagerank",
    "title": "Status and Prestige",
    "section": "A Degree-Dependent Model of Status (AKA PageRank)",
    "text": "A Degree-Dependent Model of Status (AKA PageRank)\nNote that the model of status distribution implied by the traditional eigenvector centrality implies that each node distributes the same amount of status independently of the number of connection it has; status just replicates. Thus, a node with a 100 friends has 100 status units to distribute to each of them and a node with a 10 friends has 10 units.\nThis is why the eigenvector idea rewards nodes who are connected to popular others more. Even though everyone begins with a single unit of status, well-connected nodes by degree end up having more of it to distribute.\nBut what if status dissipated proportionately to the number of connections one had? For instance, if someone has 100 friends and they only had so much time or energy, they would only have a fraction of status to distribute to others than a person with 10 friends.\nIn that case, the node with a hundred friend would only have 1/100 of status unites to distribute to each of their connections while the node with 10 friends would have 1/10 units. Under this formulation, being connected to discerning others, that is people who only connect to a few, is better than being connected to others who connect to everyone else indiscriminately.\nHow would we implement this model? First, let’s create a variation of the adjacency matrix called the \\(\\mathbf{P}\\) matrix:\n\n   P <- A/rowSums(A)\n\nSo this is the original adjacency matrix, with each entry \\(a_{ij}\\) divided by the sum of the corresponding row, which, as you may recall, is equivalent to the degree of node \\(i\\).\nHere are the first 10 rows and columns of the new matrix:\n\n   round(P[1:10, 1:10], 2)\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n [1,] 0.00 0.11 0.00 0.11 0.00 0.00 0.00 0.11 0.00  0.00\n [2,] 0.10 0.00 0.00 0.10 0.10 0.10 0.00 0.00 0.00  0.00\n [3,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.17\n [4,] 0.14 0.14 0.00 0.00 0.00 0.00 0.00 0.14 0.00  0.00\n [5,] 0.00 0.10 0.00 0.00 0.00 0.00 0.00 0.00 0.10  0.10\n [6,] 0.00 0.14 0.00 0.00 0.00 0.00 0.14 0.00 0.14  0.00\n [7,] 0.00 0.00 0.00 0.00 0.00 0.33 0.00 0.00 0.00  0.00\n [8,] 0.20 0.00 0.00 0.20 0.00 0.00 0.00 0.00 0.00  0.20\n [9,] 0.00 0.00 0.00 0.00 0.17 0.17 0.00 0.00 0.00  0.17\n[10,] 0.00 0.00 0.12 0.00 0.12 0.00 0.00 0.12 0.12  0.00\n\n\nNote that the entries are now numbers between zero and one and the matrix is asymmetric that is \\(p_{ij}\\) is not necessarily equal to \\(p_{ji}\\). In fact \\(p_{ij}\\) will only be equal to \\(p_{ji}\\) when \\(k_i = k_j\\) (nodes have the same degree).\nMoreover the rows of \\(\\mathbf{P}\\) sum to one:\n\n   rowSums(P)\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n\nWhich means that the \\(\\mathbf{P}\\) matrix is row stochastic. That is the “outdegree” of each node in the matrix is forced to sum to a fixed number (which means that it is a useless quantity). However, the indegree is not:\n\n   round(colSums(P), 2)\n\n [1] 1.11 1.34 0.63 0.86 1.56 1.06 0.37 0.51 0.61 1.21 2.33 0.92 0.17 0.87 1.08\n[16] 0.53 2.73 0.54 1.21 0.60 0.77\n\n\nWhich means that inequalities in the system will be tied to the indegree of each node in the \\(\\mathbf{P}\\) matrix, which is given by either the column sums of the matrix (as we just saw) or the row sums of the transpose of the same matrix:\n\n   round(rowSums(t(P)), 2)\n\n [1] 1.11 1.34 0.63 0.86 1.56 1.06 0.37 0.51 0.61 1.21 2.33 0.92 0.17 0.87 1.08\n[16] 0.53 2.73 0.54 1.21 0.60 0.77\n\n\nThis will come in handy in a second.\nThe \\(\\mathbf{P}\\) matrix has many interpretations, but here it just quantifies the idea that the amount of centrality each node can distribute is proportional to their degree, and that the larger the degree, the less there is to distribute (the smaller each cell \\(p_{ij}\\) will be). Meanwhile, it is clear that nodes that are pointed to by many other nodes who themselves don’t point to many others have a large indegree in \\(\\mathbf{P}\\).\nNow we can just adapt the the model of status distribution we used for eigenvector centrality but this time using the \\(\\mathbf{P}\\) rather than the \\(\\mathbf{A}\\) matrix. Note that because we are interested in the status that comes into each node we use the transpose of \\(\\mathbf{P}\\) rather than \\(\\mathbf{P}\\). So at each step the status of a node is equivalent to the sum of the status scores of their in-neighbors, with more discerning in-neighbors passing along more status than less discerning ones:\n\n   s2 <- round(status1(t(P)), 3)\n   s2 <- s2/max(s2)\n   round(s2, 3)\n\n [1] 0.500 0.555 0.333 0.388 0.555 0.388 0.167 0.278 0.333 0.445 0.778 0.445\n[13] 0.110 0.333 0.500 0.278 1.000 0.222 0.555 0.278 0.333\n\n\nWhat if I told you that these numbers are the same as the leading eigenvector of \\(\\mathbf{P}^T\\)?\n\n   s2 <- abs(eigen(t(P))$vector[, 1])\n   s2 <- s2/max(s2)\n   round(s2, 3) \n\n [1] 0.500 0.556 0.333 0.389 0.556 0.389 0.167 0.278 0.333 0.444 0.778 0.444\n[13] 0.111 0.333 0.500 0.278 1.000 0.222 0.556 0.278 0.333\n\n\nAnd, of course, the (normalized) scores produced by this approach are identical to those computed by the page_rank function in igraph:\n\n   pr <- page_rank(g, damping = 1, algo = \"arpack\")$vector\n   round(pr/max(pr), 3)\n\n [1] 0.500 0.556 0.333 0.389 0.556 0.389 0.167 0.278 0.333 0.444 0.778 0.444\n[13] 0.111 0.333 0.500 0.278 1.000 0.222 0.556 0.278 0.333\n\n\nSo the distributional model of status is the same one implemented in the PageRank algorithm!\nPageRank of course was designed to deal with directed graphs (like the World Wide Web). So let’s load up the version of the Krackhardt’s Managers data that contains the advice network which is an unambiguously directed relation.\n\n   g <- ht_advice\n\nWe then compute the \\(\\mathbf{P}\\) matrix:\n\n   A <- as.matrix(as_adjacency_matrix(g))\n   P <- A/rowSums(A)\n   round(P[1:10, 1:10], 2)\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n [1,] 0.00 0.17 0.00 0.17 0.00 0.00 0.00 0.17 0.00  0.00\n [2,] 0.00 0.00 0.00 0.00 0.00 0.33 0.33 0.00 0.00  0.00\n [3,] 0.07 0.07 0.00 0.07 0.00 0.07 0.07 0.07 0.07  0.07\n [4,] 0.08 0.08 0.00 0.00 0.00 0.08 0.00 0.08 0.00  0.08\n [5,] 0.07 0.07 0.00 0.00 0.00 0.07 0.07 0.07 0.00  0.07\n [6,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.00\n [7,] 0.00 0.12 0.00 0.00 0.00 0.12 0.00 0.00 0.00  0.00\n [8,] 0.00 0.12 0.00 0.12 0.00 0.12 0.12 0.00 0.00  0.12\n [9,] 0.08 0.08 0.00 0.00 0.00 0.08 0.08 0.08 0.00  0.08\n[10,] 0.07 0.07 0.07 0.07 0.07 0.00 0.00 0.07 0.00  0.00\n\n\nRemember how we said earlier that there are multiple ways of thinking about \\(\\mathbf{P}\\)? Another way of thinking about the \\(\\mathbf{P}\\) matrix is as characterizing the behavior of a random walker in the directed graph. At any time point \\(t\\) the walker (a piece of information, a virus, or status itself) sits on a node and the with probability \\(p_{ij}\\) it jumps to one of that node’s out-neighbors. The probabilities are stored in the matrix \\(\\mathbf{P}\\).\nOne issue that arises is that there could be nodes with no out-neighbors (so-called sink nodes) or like node 6 above, who has just one out-neighbor, in which case the probability is 1.0 that if the random walker is at node 6 it will go to node 21.\nTo avoid this issue the original designers of the PageRank algorithm (Brin and Page 1998) added a “fudge” factor: That is, with probability \\(\\alpha\\) the random walker should hop from node to node following the directed links in the graph. But once in a while with probability \\(1-\\alpha\\) the walker should decide to “teleport” (with uniform probability) to any node in the graph whether it is an out-neighbor of the current node or not.\nHow do we do that? Well we need to “fix” the \\(\\mathbf{P}\\) matrix to allow for such behavior. So instead of \\(\\mathbf{P}\\) we estimate our distributive status model on the matrix \\(\\mathbf{G}\\) (yes, for Google):\n\\[\n   \\mathbf{G} = \\alpha \\mathbf{P} + (1 - \\alpha) \\mathbf{E}\n\\]\nWhere \\(\\mathbf{E}\\) is a matrix of the same dimensions as \\(\\mathbf{P}\\) but containing \\(1/n\\) in every cell indicating that every node has an equal chance of being teleported to.\nSo fixing \\(\\alpha = 0.85\\) our \\(\\mathbf{G}\\) matrix would be:\n\n   n <- vcount(g)\n   E <- matrix(1/n, n, n)\n   G <- (0.85 * P) + (0.15 * E)\n\nAnd then we just play our status distribution game on the transpose of \\(\\mathbf{G}\\):\n\n   s3 <- round(status1(t(G)), 3)\n   round(s3/max(s3), 3)\n\n [1] 0.302 0.573 0.165 0.282 0.100 0.437 0.635 0.255 0.092 0.180 0.237 0.242\n[13] 0.087 0.268 0.097 0.170 0.258 0.440 0.087 0.200 1.000\n\n\nWhich is the same answer you would get from the igraph function page_rank by setting the “damping” parameter to 0.85:\n\n   pr <- page_rank(g, damping = 0.85, algo = \"arpack\")$vector\n   round(pr/max(pr), 3)\n\n [1] 0.302 0.573 0.165 0.281 0.100 0.437 0.635 0.256 0.091 0.180 0.237 0.242\n[13] 0.086 0.269 0.097 0.169 0.258 0.440 0.086 0.200 1.000"
  },
  {
    "objectID": "handout3.html#hubs-and-authorities",
    "href": "handout3.html#hubs-and-authorities",
    "title": "Status and Prestige",
    "section": "Hubs and Authorities",
    "text": "Hubs and Authorities\nRecall from previous discussions that everything doubles (sometimes quadruples like degree correlations) in directed graphs. The same goes for status as reflected in a distributive model through the network.\nConsider two ways of showing your status in a system governed by directed relations (like advice). You can be highly sought after by others (be an “authority”), or you can be discerning in who you seek advice from, preferably seeking out people who are also sought after (e.g., be a “hub” pointing to high-quality others).\nThese two forms of status are mutually defining. The top authorities are those who are sought after by the top hubs, and the top hubs are the ones who seek the top authorities!\nSo this leads to a doubling of Equation 1:\n\\[  \n   x^h_i = \\sum_j a_{ij} x^a_j\n\\]\n\\[\n   x^a_i = \\sum_i a_{ij} x^h_i\n\\]\nWhich says that the hub score \\(x^h\\) of a node is the sum of the authority scores \\(x^a\\) of the nodes they point to (sum over \\(j\\); the outdegree), and the authority score of a node is the sum of the hub scores of the nodes that point to it (sum over \\(i\\); the indegree).\nSo we need to make our status game a bit more complicated (but not too much) to account for this duality:\n\n   status2 <- function(A) {\n     n <- nrow(A)\n     a <- rep(1, n)  #initializing authority scores\n     diff.a.h <- 1 #initializing diff.\n     while (diff.a.h >= 0.0001) {\n         o.a <- a #old authority scores\n         h <- A %*% o.a #new hub scores a function of previous authority scores of their out-neighbors\n         h <- h/norm(h, type = \"E\")\n         a <- t(A) %*% h #new authority scores a function of current hub scores of their in-neighbors\n         a <- a/norm(a, type = \"E\")\n         diff.a.h <- abs(sum(abs(o.a) - abs(a))) #diff. between old and new authority scores\n         }\n   return(list(h = as.vector(a), a = as.vector(h)))\n   }\n\nEverything is like our previous status1 function except now we are keeping track of two mutually defining scores a and h. As you may have guessed this is just an implementation of the “HITS” algorithm developed by Kleinberg (1999).\nThe results for the Krackhardt advice network are:\n\n   round(status2(A)$a/max(status2(A)$a), 3)\n\n [1] 0.370 0.176 0.841 0.709 0.835 0.065 0.492 0.490 0.773 0.672 0.206 0.122\n[13] 0.331 0.279 1.000 0.274 0.313 0.800 0.581 0.687 0.600\n\n   round(status2(A)$h/max(status2(A)$h), 3)\n\n [1] 0.782 1.000 0.356 0.496 0.330 0.644 0.684 0.711 0.290 0.615 0.769 0.498\n[13] 0.323 0.677 0.267 0.570 0.645 0.871 0.323 0.589 0.776\n\n\nWhich are equivalent to using the igraph functions authority_score and hub_score:\n\n   round(authority_score(g, scale = TRUE)$vector, 3)\n\n [1] 0.782 1.000 0.356 0.496 0.330 0.644 0.684 0.711 0.290 0.615 0.769 0.498\n[13] 0.323 0.677 0.267 0.570 0.645 0.871 0.323 0.589 0.776\n\n   round(hub_score(g, scale = TRUE)$vector, 3)\n\n [1] 0.370 0.176 0.841 0.709 0.835 0.065 0.492 0.490 0.773 0.672 0.206 0.122\n[13] 0.331 0.279 1.000 0.274 0.313 0.800 0.581 0.687 0.600\n\n\nRecall that both the eigenvector and PageRank status scores computed via the network status distribution game routine ended up being equivalent to the eigenvectors of a network proximity matrix (the adjacency matrix \\(\\mathbf{A}\\) and the probability matrix \\(\\mathbf{P}\\) respectively). It would be surprising if the same wasn’t true of the hub and authority status scores.\nLet’s find out which ones!\nConsider the matrices:\n\\[\n\\mathbf{M} = \\mathbf{A}\\mathbf{A}^T\n\\]\n\\[\n\\mathbf{N} = \\mathbf{A}^T\\mathbf{A}\n\\]\nLet’s see what they look like in the Krackhardt manager’s network:\n\n   M = A %*% t(A)\n   N = t(A) %*% A\n   M[1:10, 1:10]\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n [1,]    6    1    5    5    5    1    3    4    5     5\n [2,]    1    3    3    2    3    1    2    3    3     0\n [3,]    5    3   15   11   12    1    8    8   12     8\n [4,]    5    2   11   12   11    1    7    6   11     8\n [5,]    5    3   12   11   15    1    7    7   12    10\n [6,]    1    1    1    1    1    1    1    1    1     0\n [7,]    3    2    8    7    7    1    8    5    8     4\n [8,]    4    3    8    6    7    1    5    8    7     4\n [9,]    5    3   12   11   12    1    8    7   13     7\n[10,]    5    0    8    8   10    0    4    4    7    14\n\n   N[1:10, 1:10]\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n [1,]   13   13    4    5    5    6    8    8    4     8\n [2,]   13   18    5    8    5    9   11   10    4     9\n [3,]    4    5    5    4    4    2    4    4    2     3\n [4,]    5    8    4    8    3    4    6    6    3     4\n [5,]    5    5    4    3    5    1    3    3    3     3\n [6,]    6    9    2    4    1   10    7    7    2     6\n [7,]    8   11    4    6    3    7   13    6    3     7\n [8,]    8   10    4    6    3    7    6   10    3     6\n [9,]    4    4    2    3    3    2    3    3    4     3\n[10,]    8    9    3    4    3    6    7    6    3     9\n\n\nWhat’s in these matrices? Well let’s look at \\(\\mathbf{M}\\). The diagonals will look familiar because they happen to be the outdegree of each node:\n\n   degree(g, mode = \"out\")[1:10]\n\n [1]  6  3 15 12 15  1  8  8 13 14\n\n\nYou may have guessed that the diagonals of matrix \\(\\mathbf{N}\\) contain the indegrees:\n\n   degree(g, mode = \"in\")[1:10]\n\n [1] 13 18  5  8  5 10 13 10  4  9\n\n\nWhich means that the off-diagonals cells of each matrix \\(m_{ij}\\) and \\(n_{ij}\\), contain the common out-neighbors and common in-neighbors shared by nodes \\(i\\) and \\(j\\) in the graph, respectively.\nAs you may already be suspecting, the hub and authorities scores are the leading eigenvectors of the \\(\\mathbf{M}\\) and \\(\\mathbf{N}\\) matrices (Kleinberg 1999):\n\n   h <- eigen(M)$vector[,1] * -1\n   a <- eigen(N)$vector[,1] * -1\n   round(h/max(h), 3)\n\n [1] 0.370 0.176 0.841 0.709 0.835 0.065 0.492 0.490 0.773 0.672 0.206 0.122\n[13] 0.331 0.279 1.000 0.274 0.313 0.800 0.581 0.687 0.600\n\n   round(a/max(a), 3)\n\n [1] 0.782 1.000 0.356 0.496 0.330 0.644 0.684 0.711 0.290 0.615 0.769 0.498\n[13] 0.323 0.677 0.267 0.570 0.645 0.871 0.323 0.589 0.776\n\n\nOnce again demonstrating the equivalence between eigenvectors of proximity matrices in networks and our prismatic status distribution game!\nOf course, like before, we can treat the PageRank, Hub, and Authority Scores as “centralities” defined over nodes in the graph. In that case we might be interested in how different nodes stack up according to the different status criteria:\n\n\n\n\nTop Five PageRank Scores.\n \n  \n    Nodes \n    PR.Cent \n    ODeg.Cent \n    IDeg.Cent \n  \n \n\n  \n    21 \n    1.000 \n    11 \n    15 \n  \n  \n    7 \n    0.635 \n    8 \n    13 \n  \n  \n    2 \n    0.573 \n    3 \n    18 \n  \n  \n    18 \n    0.440 \n    17 \n    15 \n  \n  \n    6 \n    0.437 \n    1 \n    10 \n  \n\n\n\n\n\n\n\nTop Five Hub Scores.\n \n  \n    Nodes \n    Hub.Cent \n    ODeg.Cent \n    IDeg.Cent \n  \n \n\n  \n    15 \n    1.000 \n    20 \n    4 \n  \n  \n    3 \n    0.841 \n    15 \n    5 \n  \n  \n    5 \n    0.835 \n    15 \n    5 \n  \n  \n    18 \n    0.800 \n    17 \n    15 \n  \n  \n    9 \n    0.773 \n    13 \n    4 \n  \n\n\n\n\n\n\n\nTop Five Authority Scores.\n \n  \n    Nodes \n    Auth.Cent \n    ODeg.Cent \n    IDeg.Cent \n  \n \n\n  \n    2 \n    1.000 \n    3 \n    18 \n  \n  \n    18 \n    0.871 \n    17 \n    15 \n  \n  \n    1 \n    0.782 \n    6 \n    13 \n  \n  \n    21 \n    0.776 \n    11 \n    15 \n  \n  \n    11 \n    0.769 \n    3 \n    11"
  },
  {
    "objectID": "handout4.html",
    "href": "handout4.html",
    "title": "Role Equivalence and Structural Similarity",
    "section": "",
    "text": "One of the earlier “proofs of concept” of the power of social network analysis came from demonstrating that you could formalize the fuzzy idea of “role” central to functionalist sociology and British social anthropology using the combined tools of graph theoretical and matrix representations of networks (White, Boorman, and Breiger 1976).\nThis and other contemporaneous work (Breiger, Boorman, and Arabie 1975) set off an entire sub-tradition of data analysis of networks focused on the idea that one could partition the set of vertices in a graph into meaningful classes based on some mathematical (e.g., graph theoretic) criterion.\nThese classes would in turn would be isomorphic with the concept of role as social position and the classes thereby derived as indicating the number of such positions in the social structure under investigation as well as which actors belonged to which positions."
  },
  {
    "objectID": "handout4.html#structural-equivalence",
    "href": "handout4.html#structural-equivalence",
    "title": "Role Equivalence and Structural Similarity",
    "section": "Structural Equivalence",
    "text": "Structural Equivalence\nThe earliest work pursued simultaneously by analysts at Harvard (White, Boorman, and Breiger 1976) and Chicago (Burt 1976) relied on the idea of structural equivalence.\nIn a graph \\(G = \\{E, V\\}\\) two nodes \\(v_i, v_j\\) are structurally equivalent if they are connected to the same others in the network; that is, if \\(N(v_i)\\) is the set of nodes adjacent to node \\(v_i\\) and \\(N(v_j)\\) is the set of nodes adjacent to node \\(v_j\\), then:\n\\[\n   v_i \\equiv v_j \\iff N(v_i) = N(v_j)\n\\]\nIn a graph, an equivalence class \\(C\\) is just a set of nodes that are structurally equivalent, such that if \\(v_i \\in C_i\\) and \\(v_j \\in C_i\\) then \\(v_i \\equiv v_j\\) for all pairs \\((v_i, v_j) \\in C_i\\).\nThe partitioning of the vertex set into a set of equivalence classes \\(\\{C_1, C_2 \\ldots C_k\\}\\) as well as the adjacency relations between nodes in the same class and nodes in different classes defines the role structure of the network."
  },
  {
    "objectID": "handout4.html#structural-equivalence-in-an-ideal-world",
    "href": "handout4.html#structural-equivalence-in-an-ideal-world",
    "title": "Role Equivalence and Structural Similarity",
    "section": "Structural Equivalence in an Ideal World",
    "text": "Structural Equivalence in an Ideal World\nLet us illustrate these concepts. Consider the following toy graph:\n\n\n\n\n\nFigure 1: A toy graph demonstrating structural equivalence.\n\n\n\n\nWith associated adjacency matrix:\n\n\n\n\n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n  \n \n\n  \n    A \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    B \n    1 \n    0 \n    0 \n    1 \n    1 \n    1 \n    1 \n    0 \n    0 \n  \n  \n    C \n    1 \n    0 \n    0 \n    1 \n    1 \n    1 \n    1 \n    0 \n    0 \n  \n  \n    D \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n  \n  \n    E \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n  \n  \n    F \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n  \n  \n    G \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    H \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n    0 \n    0 \n    1 \n  \n  \n    I \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n    0 \n    1 \n    0 \n  \n\n\n\n\n\nA simple function to check for structural equivalence in the graph, relying on the native R function setequal would be:\n\n   check.equiv <- function(x) {\n      n <- vcount(x)\n      v <- V(x)$name\n      E <- matrix(0, n, n)\n      for (i in v) {\n         for (j in v) {\n            if (i != j & E[which(v == j), which(v == i)] != 1) {\n               N.i <- neighbors(x, i)\n               N.j <- neighbors(x, j)\n               if (are_adjacent(x, i, j) == TRUE) {\n                  N.i <- c(names(N.i), i)\n                  N.j <- c(names(N.j), j)\n                  } #end sub-if\n               if (setequal(N.i, N.j) == TRUE) {\n                  E[which(v == i), which(v == j)] <- 1\n                  E[which(v == j), which(v == i)] <- 1\n                  } #end sub-if\n               } #end main if\n            } #end j loop\n         } #end i loop\n      rownames(E) <- v\n      colnames(E) <- v\n   return(E)\n   }\n\nThis function creates an empty “equivalence” matrix \\(\\mathbf{E}\\) in line 4, loops through each pair of nodes in the graph in lines 5-20. The main condition restricts the checking to nodes that are not the same or have not yet to be found to be equivalent (line 7). Lines 8-9 extract the node neighborhoods using the igraph function neighbors.\nLines 10-13 check to see if the pair of nodes that are being checked for equivalence are themselves adjacent. If they are indeed adjacent (the conditional in line 10 is TRUE) then we need to use the so-called closed neighborhood of \\(v_i\\) and \\(v_j\\), written \\(N[v_i], N[v_j]\\), to do the equivalence check, or otherwise we get the wrong answer.1\nThe equivalence check is done in line 14 using the native R function setequal. This function takes two inputs (e.g., two vectors) and will return a value of TRUE if the elements in the first vector are the same as the elements in the second vector. In that case we update the matrix \\(\\mathbf{E}\\) accordingly.\nAfter writing our function, we can then type:\n\n   Equiv <- check.equiv(g)\n\nAnd the resulting equivalence matrix \\(\\mathbf{E}\\) corresponding to the graph in Figure 1 is:\n\n\n\n\n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n  \n \n\n  \n    A \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    B \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    C \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    D \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    E \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    F \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    G \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    H \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n  \n  \n    I \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n  \n\n\n\n\n\nIn this matrix, there is a 1 in the corresponding cell if the row node is structurally equivalent to the column node.\nOne thing we can do with this matrix is re-order the rows and columns, so that rows(columns) corresponding to nodes that are “adjacent” in the equivalence relation appear next to one another in the matrix.\nTo do that we can use the corrMatOrder function from the corrplot package, designed to work with correlation matrices, but works with any matrix of values:\n\n   #install.packages(\"corrplot\")\n   library(corrplot)\n   SE.ord <- corrMatOrder(Equiv, order = \"hclust\", hclust.method = \"ward.D2\")\n   SE.ord\n\n[1] 6 4 5 8 9 1 7 2 3\n\n\nThe corrplot function corrMatorder takes a matrix as input and returns a vector of reordered values of the rows(columns) as output. We use a hierarchical clustering algorithm using Ward’s method to do the job.\nWe can see that the new re-ordered vector has the previous row(column) 6 in fist position, 4 at second, five at third, 8 at fourth, and so forth.;\nWe can then re-order rows and columns of the old equivalence matrix using this new ordering by typing:\n\n   Equiv <- Equiv[SE.ord, SE.ord]\n\nThe resulting re-ordered matrix looks like:\n\n\n\n\n \n  \n      \n    F \n    D \n    E \n    H \n    I \n    A \n    G \n    B \n    C \n  \n \n\n  \n    F \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    D \n    1 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    E \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    H \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    I \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    A \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    G \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    B \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n  \n  \n    C \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n  \n\n\n\n\n\nOnce the equivalence matrix is re-ordered we can see that sets of structurally equivalent nodes in Figure 1, appear clustered along the diagonals. This type of re-arranged matrix is said to be in block-diagonal form (e.g., non-zero entries clustered along the diagonals).\nEven more interestingly, we can do the same re-arranging on the original adjacency matrix, to reveal:\n\n\n\n\n \n  \n      \n    F \n    D \n    E \n    H \n    I \n    A \n    G \n    B \n    C \n  \n \n\n  \n    F \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    1 \n    1 \n  \n  \n    D \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    1 \n    1 \n  \n  \n    E \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    1 \n    1 \n  \n  \n    H \n    1 \n    1 \n    1 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    I \n    1 \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    A \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n  \n  \n    G \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    1 \n  \n  \n    B \n    1 \n    1 \n    1 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n  \n  \n    C \n    1 \n    1 \n    1 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n  \n\n\n\n\n\nThis is called a blocked adjacency matrix. As you can see, once the structural equivalence relations in the network are revealed by permuting the rows and columns, the adjacency matrix shows an orderly pattern.\nThe way to interpret the blocked adjacency matrix is as follows:\n\nThe block diagonals of the matrix reveal the intra-block relations between sets of structurally equivalent nodes. If the block diagonal is empty–called a zero block–it means that set of structurally equivalent nodes does not connect with one another directly. If it has ones–called a one block–it it means that members of that set of structurally equivalent nodes are also neighbors.\nThe off diagonal blocks reveals the inter-block adjacency relations between different clusters of structurally equivalent nodes. If an off-diagonal block is a one-block, it means that members of block \\(C_i\\) send ties to members of block \\(C_j\\). If and off diagonal block is a zero-block, it means that members of block \\(C_i\\) avoid associating with members of block \\(C_j\\).\n\nSo if:\n\\[\nC_1 = \\{D, E, F\\}\n\\]\n\\[\nC_2 = \\{H, I\\}\n\\]\n\\[\nC_3 = \\{A, G\\}\n\\]\n\\[\nC_4 = \\{B, C\\}\n\\]\nThen we can see that:\n\nMembers of \\(C_1\\) connect with members of \\(C_2\\) and \\(C_4\\) but not among themselves.\nMembers of \\(C_2\\) connect among themselves and with \\(C_1\\).\nMembers of \\(C_3\\) connect among themselves and with \\(C_4\\).\nMembers of \\(C_4\\) connect with \\(C_1\\) and \\(C_3\\) but avoid associating with their own block.\n\nThese intra and inter-block relations can then be represented in the reduced image matrix:\n\n\n\n\n \n  \n      \n    C_1 \n    C_2 \n    C_3 \n    C_4 \n  \n \n\n  \n    C_1 \n    0 \n    1 \n    0 \n    1 \n  \n  \n    C_2 \n    1 \n    1 \n    0 \n    0 \n  \n  \n    C_3 \n    0 \n    0 \n    1 \n    1 \n  \n  \n    C_4 \n    0 \n    0 \n    1 \n    0 \n  \n\n\n\n\n\nWhich reveals a more economical representation of the system based on structural equivalence."
  },
  {
    "objectID": "handout4.html#structural-equivalence-in-the-real-world",
    "href": "handout4.html#structural-equivalence-in-the-real-world",
    "title": "Role Equivalence and Structural Similarity",
    "section": "Structural Equivalence in the Real World",
    "text": "Structural Equivalence in the Real World\nOf course, in real data, it is very unlikely that two nodes will meet the exact mathematical criterion of structural equivalence. They might have three out of five, or eight out of nine common neighbors but would still get a big zero in the \\(\\mathbf{E}\\) defined using our strict function.\nSo, a lot of role analysis in real networks follows instead the path of search for a near cousin to structural equivalence that follows the path of search for the large class of distance and similarity metrics, and pick one where structural equivalence falls off as a special case.\nFor random reasons, early work in social network analysis focused on distance metrics, while more recent work inspired by network science focuses on similarity metrics. The end goal is the same though.\nLet us, therefore, begin with the distance approach. Here the goal is simply to pick a distance metric \\(d\\) with a well defined minimum \\(d_{min}\\) or maximum value \\(d_{max}\\), such that:\n\\[\n   v_i \\equiv v_j \\iff d(v_i, v_j) = d_{min} \\lor d(v_i, v_j) = d_{max}\n\\]\nWhere whether we pick the maximum or minimum value depends on the particularities of the measure \\(d\\).\nWe then populate the \\(\\mathbf{E}\\) matrix with the values of \\(d\\) for each pair of nodes \\((v_i, v_j)\\), do some kind of clustering on the matrix, and use our clusters assignments to re-arrange the original adjacency matrix to find our blocks, and so forth.\nA very obvious candidate for \\(d\\) is the Euclidean Distance (Burt 1976):\n\\[\n   d_{i,j} = \\sqrt{\\sum_{k \\neq i,j} (a_{ik} - a_{jk})^2}\n\\]\nWhere \\(a_{ik}\\) and \\(a_{jk}\\) are the corresponding entries in the graph’s adjacency matrix \\(\\mathbf{A}\\). The minimum for this measure is \\(d_{min} = 0\\), so this is the value we should find for structurally equivalent nodes.\nA function that does this for any graph is:\n\n   d.euclid <- function(x) {\n      A <- as.matrix(as_adjacency_matrix(x))\n      n <- nrow(A)\n      E <- matrix(0, n, n)\n      for (i in 1:n) {\n         for (j in 1:n) {\n            if (i < j & i != j) {\n               d.ij <- 0\n               for (k in 1:n) {\n                  if (k != i & k != j) {\n                     d.ij <- d.ij + (A[i,k] - A[j,k])^2\n                     }\n                  }\n               E[i,j] <- sqrt(d.ij)\n               E[j,i] <- sqrt(d.ij)\n            }\n         }\n      }\n   rownames(E) <- rownames(A)\n   colnames(E) <- colnames(A)\n   return(E)\n   }\n\nAnd we can try it out with our toy graph:\n\n   E <- d.euclid(g)\n   round(E, 1)\n\n    A   B   C   D   E   F   G   H   I\nA 0.0 2.0 2.0 1.7 1.7 1.7 0.0 2.6 2.6\nB 2.0 0.0 0.0 2.6 2.6 2.6 2.0 1.7 1.7\nC 2.0 0.0 0.0 2.6 2.6 2.6 2.0 1.7 1.7\nD 1.7 2.6 2.6 0.0 0.0 0.0 1.7 2.0 2.0\nE 1.7 2.6 2.6 0.0 0.0 0.0 1.7 2.0 2.0\nF 1.7 2.6 2.6 0.0 0.0 0.0 1.7 2.0 2.0\nG 0.0 2.0 2.0 1.7 1.7 1.7 0.0 2.6 2.6\nH 2.6 1.7 1.7 2.0 2.0 2.0 2.6 0.0 0.0\nI 2.6 1.7 1.7 2.0 2.0 2.0 2.6 0.0 0.0\n\n\nAnd it looks like indeed it detected the structurally equivalent nodes in the graph. We can see it clearly by re-ordering the rows and columns according to our known ordering:\n\n   E <- E[SE.ord, SE.ord]\n   round(E, 1)\n\n    F   D   E   H   I   A   G   B   C\nF 0.0 0.0 0.0 2.0 2.0 1.7 1.7 2.6 2.6\nD 0.0 0.0 0.0 2.0 2.0 1.7 1.7 2.6 2.6\nE 0.0 0.0 0.0 2.0 2.0 1.7 1.7 2.6 2.6\nH 2.0 2.0 2.0 0.0 0.0 2.6 2.6 1.7 1.7\nI 2.0 2.0 2.0 0.0 0.0 2.6 2.6 1.7 1.7\nA 1.7 1.7 1.7 2.6 2.6 0.0 0.0 2.0 2.0\nG 1.7 1.7 1.7 2.6 2.6 0.0 0.0 2.0 2.0\nB 2.6 2.6 2.6 1.7 1.7 2.0 2.0 0.0 0.0\nC 2.6 2.6 2.6 1.7 1.7 2.0 2.0 0.0 0.0\n\n\nHere the block-diagonals of the matrix contain zeroes because the \\(d\\) is a distance function with a minimum of zero. If we wanted it to contain ones instead we would normalize:\n\\[\n   d^* = 1-\\left(\\frac{d}{max(d)}\\right)\n\\]\nWhich would give us:\n\n   E.norm <- 1 - E/max(E)\n   round(E.norm, 1)\n\n    F   D   E   H   I   A   G   B   C\nF 1.0 1.0 1.0 0.2 0.2 0.3 0.3 0.0 0.0\nD 1.0 1.0 1.0 0.2 0.2 0.3 0.3 0.0 0.0\nE 1.0 1.0 1.0 0.2 0.2 0.3 0.3 0.0 0.0\nH 0.2 0.2 0.2 1.0 1.0 0.0 0.0 0.3 0.3\nI 0.2 0.2 0.2 1.0 1.0 0.0 0.0 0.3 0.3\nA 0.3 0.3 0.3 0.0 0.0 1.0 1.0 0.2 0.2\nG 0.3 0.3 0.3 0.0 0.0 1.0 1.0 0.2 0.2\nB 0.0 0.0 0.0 0.3 0.3 0.2 0.2 1.0 1.0\nC 0.0 0.0 0.0 0.3 0.3 0.2 0.2 1.0 1.0\n\n\nAs noted, a distance function defines a clustering on the nodes, and the results of the clustering generate our blocks. In R we can use the functions dist and hclust to do the job:\n\n   E <- dist(E) #transforming E to a dist object\n   h.res <- hclust(E, method = \"ward.D2\")\n   plot(h.res)\n\n\n\n\nWhich are our original clusters of structurally equivalent nodes!\nLet’s see how this would work in real data. Let’s take the Flintstones (film) co-appearance network as an example:\n\n   library(networkdata)\n   g.flint <- movie_267\n   g.flint <- delete_vertices(g.flint, degree(g.flint) <= 3)\n   E <- d.euclid(g.flint)\n   E <- dist(E) #transforming E to a dist object\n   h.res <- hclust(E, method = \"ward.D2\") #hierarchical clustering\n   #install.packages(\"dendextend\")\n   library(dendextend) #nice dendrogram plotting\n   dend <- as.dendrogram(h.res) %>% \n      color_branches(k = 8) %>% \n      color_labels(k = 8) %>% \n      set(\"labels_cex\", 0.65) %>% \n      set(\"branches_lwd\", 2) %>% \n      plot\n\n\n\n\nThe Flintstones is a film based on a family, and families are the prototypes of social roles in networks, so if structural equivalence gets at roles, then it should recover known kin roles here, and indeed it does to some extent. One cluster is the focal parents separated into “moms” and “dads” roles, and another has the kids."
  },
  {
    "objectID": "handout4.html#concor",
    "href": "handout4.html#concor",
    "title": "Role Equivalence and Structural Similarity",
    "section": "CONCOR",
    "text": "CONCOR\nThe other (perhaps less obvious) way of defining a distance between nodes in the network based on their connectivity patterns to other nodes is the correlation distance:\n\\[\n    d_{i,j} =\n    \\frac{\n    \\sum_{i \\neq k}\n    (a_{ki} - \\overline{a}_{i}) \\times\n    \\sum_{j \\neq k}\n    (a_{kj} - \\overline{a}_{j})\n    }\n    {\n    \\sqrt{\n    \\sum_{i \\neq k}\n    (a_{ki} - \\overline{a}_{i})^2 \\times\n    \\sum_{j \\neq k}\n    (a_{kj} - \\overline{a}_{j})^2\n        }\n    }\n\\]\nA more involved but still meaningful formula compared of the Euclidean distance. Here \\(\\overline{a}_{i}\\) is the column mean of the entries of node \\(i\\) in the affiliation matrix.\nSo the correlation distance is the ratio of the covariance of the column vectors corresponding to each node in the adjacency matrix and the product of their standard deviations.\nThe correlation distance between nodes in our toy network is given by simply typing:\n\n   m <- as.matrix(as_adjacency_matrix(g))\n   C <- cor(m)\n   round(C, 2)\n\n      A     B     C     D     E     F     G     H     I\nA  1.00 -0.32 -0.32  0.32  0.32  0.32  0.50 -0.63 -0.63\nB -0.32  1.00  1.00 -1.00 -1.00 -1.00 -0.32  0.35  0.35\nC -0.32  1.00  1.00 -1.00 -1.00 -1.00 -0.32  0.35  0.35\nD  0.32 -1.00 -1.00  1.00  1.00  1.00  0.32 -0.35 -0.35\nE  0.32 -1.00 -1.00  1.00  1.00  1.00  0.32 -0.35 -0.35\nF  0.32 -1.00 -1.00  1.00  1.00  1.00  0.32 -0.35 -0.35\nG  0.50 -0.32 -0.32  0.32  0.32  0.32  1.00 -0.63 -0.63\nH -0.63  0.35  0.35 -0.35 -0.35 -0.35 -0.63  1.00  0.55\nI -0.63  0.35  0.35 -0.35 -0.35 -0.35 -0.63  0.55  1.00\n\n\nWhich gives the Pearson product moment correlation of each pair of columns in the adjacency matrix.\nThe key thing that was noticed by Breiger, Boorman, and Arabie (1975) is that we can iterate this process, and compute correlation distances of correlation distances between nodes in the graph. If we do this for our toy network a few (e.g., three) times we get:\n\n   C1 <- cor(C)\n   C2 <- cor(C1)\n   C3 <- cor(C2)\n   round(C3, 2)\n\n   A  B  C  D  E  F  G  H  I\nA  1 -1 -1  1  1  1  1 -1 -1\nB -1  1  1 -1 -1 -1 -1  1  1\nC -1  1  1 -1 -1 -1 -1  1  1\nD  1 -1 -1  1  1  1  1 -1 -1\nE  1 -1 -1  1  1  1  1 -1 -1\nF  1 -1 -1  1  1  1  1 -1 -1\nG  1 -1 -1  1  1  1  1 -1 -1\nH -1  1  1 -1 -1 -1 -1  1  1\nI -1  1  1 -1 -1 -1 -1  1  1\n\n\nInterestingly the positive correlations converge to 1.0 and the negative correlations converge to -1.0!\nIf we sort the rows and columns of the new matrix according to these values, we get:\n\n   C.ord <- corrMatOrder(C3, order = \"hclust\", hclust.method = \"ward.D2\")\n   C3 <- C3[C.ord, C.ord]\n   round(C3, 2)\n\n   B  C  H  I  F  D  E  A  G\nB  1  1  1  1 -1 -1 -1 -1 -1\nC  1  1  1  1 -1 -1 -1 -1 -1\nH  1  1  1  1 -1 -1 -1 -1 -1\nI  1  1  1  1 -1 -1 -1 -1 -1\nF -1 -1 -1 -1  1  1  1  1  1\nD -1 -1 -1 -1  1  1  1  1  1\nE -1 -1 -1 -1  1  1  1  1  1\nA -1 -1 -1 -1  1  1  1  1  1\nG -1 -1 -1 -1  1  1  1  1  1\n\n\nAha! The iterated correlations seems to have split the matrix into two blocks \\(C_1 = \\{G, F, D, A, G\\}\\) and \\(C_2 = \\{I, H, B, C\\}\\). Each of the blocks is composed of two sub-blocks that we know are structurally equivalent from our previous analysis.\nA function implementing this method of iterated correlation distances until convergence looks like:\n\n   con.cor <- function(x) {\n      C <- x\n      while (mean(abs(C)) != 1) {\n         C <- cor(C)\n         }\n      b1 <- C[, 1] > 0\n      b2 <- !b1\n      return(list(x[, b1, drop = FALSE], x[, b2, drop = FALSE]))\n      }\n\nThis function takes a graph’s adjacency matrix as input, creates a copy of the adjacency matrix in line 2 (to be put through the iterated correlations meat grinder). The three-line (3-5) while loop goes through the iterated correlations (stopping when the matrix is full of ones). Then the resulting two blocks are returned as the columns of a couple of matrices stored in a list in line 8.\nFor instance, to use our example above:\n\n   con.cor(m)\n\n[[1]]\n  A D E F G\nA 0 0 0 0 1\nB 1 1 1 1 1\nC 1 1 1 1 1\nD 0 0 0 0 0\nE 0 0 0 0 0\nF 0 0 0 0 0\nG 1 0 0 0 0\nH 0 1 1 1 0\nI 0 1 1 1 0\n\n[[2]]\n  B C H I\nA 1 1 0 0\nB 0 0 0 0\nC 0 0 0 0\nD 1 1 1 1\nE 1 1 1 1\nF 1 1 1 1\nG 1 1 0 0\nH 0 0 0 1\nI 0 0 1 0\n\n\nThe columns of these two matrices are the two blocks we found before. Of course, to implement this method as a divisive clustering algorithm, what we want is to split these two blocks into two finer grained blocks, by iterative correlations of the columns of these two sub-matrices (to reveal two further sub-matrices each) and thus find our original four structurally equivalent groups.\nThe following function–simplified and adapted from Adam Slez’s work—which includes the con.cor function shown earlier inside of it, will do it:\n\n  blocks <- function(g, s = 2) {\n     A <- as.matrix(as_adjacency_matrix(g))\n     B <- list(A)\n     for(i in 1:s) {\n       B <- unlist(lapply(B, con.cor), recursive = FALSE)\n       }\n     return(lapply(B, colnames))\n   }\n\nThis function takes the graph as input and returns a list of column names containing the structurally equivalent blocks as output:\n\n   blocks(g)\n\n[[1]]\n[1] \"A\" \"G\"\n\n[[2]]\n[1] \"D\" \"E\" \"F\"\n\n[[3]]\n[1] \"B\" \"C\"\n\n[[4]]\n[1] \"H\" \"I\"\n\n\nWhich are the original structurally equivalent classes. The argument s controls the number of splits. When it is equal to one, the function produces two blocks, and when it is equal to two it produces four blocks, when it is equal to three, six blocks, and so on.\nThis is the algorithm called CONCOR (Breiger, Boorman, and Arabie 1975), short for convergence of iterate correlations, and can be used to cluster the rows(columns) of any valued square data matrix.\nFor instance, if we wanted to split the Flintstones network into four blocks we would proceed as follows:\n\n   g.flint <- movie_267\n   g.flint <- delete_vertices(g.flint, degree(g.flint) <= 3)\n   blocks(g.flint)\n\n[[1]]\n[1] \"BAM-BAM\"    \"BETTY\"      \"PEBBLES\"    \"POINDEXTER\" \"WILMA\"     \n\n[[2]]\n[1] \"FELDSPAR\"   \"HERDMASTER\" \"LAVA\"       \"PILTDOWN\"   \"PYRITE\"    \n[6] \"SLATE\"     \n\n[[3]]\n[1] \"BARNEY\"          \"FRED\"            \"LEACH\"           \"PEBBLES BAM-BAM\"\n\n[[4]]\n[1] \"HEADMISTRESS\" \"MORRIS\"       \"MRS SLATE\"   \n\n\nWhich is similar to the results we got from the Euclidean distance method, except that now the children are put in the same blocks as the moms.\nWe could then visualize the results as follows:\n\n   #install.packages(\"ggcorrplot\")\n   library(ggcorrplot)\n   A <- as.matrix(as_adjacency_matrix(g.flint))\n   ord <- unlist(blocks(g.flint, 2))\n   A <- A[ord, ord]\n   p <- ggcorrplot(A, colors = c(\"white\", \"white\", \"black\")) \n   p <- p + theme(legend.position = \"none\", \n                  axis.text.y = element_text(size = 8),\n                  axis.text.x = element_text(size = 8),\n                  )\n   p <- p + geom_hline(yintercept = 5.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_vline(xintercept = 5.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_hline(yintercept = 11.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_vline(xintercept = 11.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_hline(yintercept = 15.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_vline(xintercept = 15.5, linewidth = 2, color = \"blue\")\n   p\n\n\n\n\nWhich is just a tile plot of the original adjacency matrix (adjacent cells in black) with rows and columns re-ordered according to the four-block solution and ggplot vertical and horizontal lines highlighting the boundaries of the blocked matrix.\nAs you can see, characters with similar patterns of scene co-appearances (like Barney and Fred) are drawn next to one another, revealing the larger “roles” in the network."
  },
  {
    "objectID": "handout4.html#structural-equivalence-in-directed-graphs",
    "href": "handout4.html#structural-equivalence-in-directed-graphs",
    "title": "Role Equivalence and Structural Similarity",
    "section": "Structural Equivalence in Directed Graphs",
    "text": "Structural Equivalence in Directed Graphs\nLike before, the main complication introduced by the directed case is the “doubling” of the relations considered.\nIn the Euclidean distance case, we have to decide whether we want to compute two set of distances between nodes, one based on the in-distance vectors and the other on the out-distance vectors, and the two sets of hierarchical clustering partitions.\nAnother approach is simply to combine both according to the formula:\n\\[\n   d_{i,j} = \\sqrt{\n                  \\sum_{k \\neq i,j} (a_{ik} - a_{jk})^2 +\n                  \\sum_{k \\neq i,j} (a_{ki} - a_{kj})^2\n                  }\n\\]\nWhich just computes the Euclidean distances between nodes using both in and out neighbors. Here nodes would be structurally equivalent only if they have the same set of in and out-neighbors. This would mean changing line 11 in the function d.euclid above with the following:\n\n   d.ij <- d.ij + ((A[i,k] - A[j,k])^2 + (A[k,i] - A[k,j])^2)\n\nThis way, distances are computed on both the row and columns of the directed graph’s adjacency matrix.\nIf we are using the correlation distance approach in a directed graph, then the main trick is to stack the original adjacency matrix against its transpose, and then compute the correlation distance on the columns of the stacked matrices, which by definition combines information in incoming and outgoing ties.\nLet’s see a brief example. Let’s load up the Krackhardt’s high-tech managers data on advice relations and look at the adjacency matrix:\n\n   g <- ht_advice\n   A <- as.matrix(as_adjacency_matrix(g))\n   A\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]\n [1,]    0    1    0    1    0    0    0    1    0     0     0     0     0\n [2,]    0    0    0    0    0    1    1    0    0     0     0     0     0\n [3,]    1    1    0    1    0    1    1    1    1     1     1     1     0\n [4,]    1    1    0    0    0    1    0    1    0     1     1     1     0\n [5,]    1    1    0    0    0    1    1    1    0     1     1     0     1\n [6,]    0    0    0    0    0    0    0    0    0     0     0     0     0\n [7,]    0    1    0    0    0    1    0    0    0     0     1     1     0\n [8,]    0    1    0    1    0    1    1    0    0     1     1     0     0\n [9,]    1    1    0    0    0    1    1    1    0     1     1     1     0\n[10,]    1    1    1    1    1    0    0    1    0     0     1     0     1\n[11,]    1    1    0    0    0    0    1    0    0     0     0     0     0\n[12,]    0    0    0    0    0    0    1    0    0     0     0     0     0\n[13,]    1    1    0    0    1    0    0    0    1     0     0     0     0\n[14,]    0    1    0    0    0    0    1    0    0     0     0     0     0\n[15,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[16,]    1    1    0    0    0    0    0    0    0     1     0     0     0\n[17,]    1    1    0    1    0    0    1    0    0     0     0     0     0\n[18,]    1    1    1    1    1    0    1    1    1     1     1     0     1\n[19,]    1    1    1    0    1    0    1    0    0     1     1     0     0\n[20,]    1    1    0    0    0    1    0    1    0     0     1     1     0\n[21,]    0    1    1    1    0    1    1    1    0     0     0     1     0\n      [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21]\n [1,]     0     0     1     0     1     0     0     1\n [2,]     0     0     0     0     0     0     0     1\n [3,]     1     0     0     1     1     0     1     1\n [4,]     0     0     1     1     1     0     1     1\n [5,]     1     0     1     1     1     1     1     1\n [6,]     0     0     0     0     0     0     0     1\n [7,]     1     0     0     1     1     0     0     1\n [8,]     0     0     0     0     1     0     0     1\n [9,]     1     0     1     1     1     0     0     1\n[10,]     0     1     1     1     1     1     1     0\n[11,]     0     0     0     0     0     0     0     0\n[12,]     0     0     0     0     0     0     0     1\n[13,]     1     0     0     0     1     0     0     0\n[14,]     0     0     0     0     1     0     0     1\n[15,]     1     0     1     1     1     1     1     1\n[16,]     0     0     0     0     1     0     0     0\n[17,]     0     0     0     0     0     0     0     1\n[18,]     1     1     1     0     0     1     1     1\n[19,]     1     1     0     0     1     0     1     0\n[20,]     1     1     1     1     1     0     0     1\n[21,]     1     0     0     1     1     0     1     0\n\n\nRecall that in these data a tie goes from an advice seeker to an advisor. So the standard correlation distance on the columns computes the in-correlation, or structural equivalence based on incoming ties (two managers are equivalent if they are nominated as advisors by the same others).\nWe may also be interested in the out-correlation, that is structural equivalence based on out-going ties. Here two managers are structurally equivalent is they seek advice from the same others. This information is contained in the transpose of the original adjacency matrix:\n\n   A.t <- t(A)\n   A.t\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]\n [1,]    0    0    1    1    1    0    0    0    1     1     1     0     1\n [2,]    1    0    1    1    1    0    1    1    1     1     1     0     1\n [3,]    0    0    0    0    0    0    0    0    0     1     0     0     0\n [4,]    1    0    1    0    0    0    0    1    0     1     0     0     0\n [5,]    0    0    0    0    0    0    0    0    0     1     0     0     1\n [6,]    0    1    1    1    1    0    1    1    1     0     0     0     0\n [7,]    0    1    1    0    1    0    0    1    1     0     1     1     0\n [8,]    1    0    1    1    1    0    0    0    1     1     0     0     0\n [9,]    0    0    1    0    0    0    0    0    0     0     0     0     1\n[10,]    0    0    1    1    1    0    0    1    1     0     0     0     0\n[11,]    0    0    1    1    1    0    1    1    1     1     0     0     0\n[12,]    0    0    1    1    0    0    1    0    1     0     0     0     0\n[13,]    0    0    0    0    1    0    0    0    0     1     0     0     0\n[14,]    0    0    1    0    1    0    1    0    1     0     0     0     1\n[15,]    0    0    0    0    0    0    0    0    0     1     0     0     0\n[16,]    1    0    0    1    1    0    0    0    1     1     0     0     0\n[17,]    0    0    1    1    1    0    1    0    1     1     0     0     0\n[18,]    1    0    1    1    1    0    1    1    1     1     0     0     1\n[19,]    0    0    0    0    1    0    0    0    0     1     0     0     0\n[20,]    0    0    1    1    1    0    0    0    0     1     0     0     0\n[21,]    1    1    1    1    1    1    1    1    1     0     0     1     0\n      [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21]\n [1,]     0     1     1     1     1     1     1     0\n [2,]     1     1     1     1     1     1     1     1\n [3,]     0     1     0     0     1     1     0     1\n [4,]     0     1     0     1     1     0     0     1\n [5,]     0     1     0     0     1     1     0     0\n [6,]     0     1     0     0     0     0     1     1\n [7,]     1     1     0     1     1     1     0     1\n [8,]     0     1     0     0     1     0     1     1\n [9,]     0     1     0     0     1     0     0     0\n[10,]     0     1     1     0     1     1     0     0\n[11,]     0     1     0     0     1     1     1     0\n[12,]     0     1     0     0     0     0     1     1\n[13,]     0     1     0     0     1     0     0     0\n[14,]     0     1     0     0     1     1     1     1\n[15,]     0     0     0     0     1     1     1     0\n[16,]     0     1     0     0     1     0     1     0\n[17,]     0     1     0     0     0     0     1     1\n[18,]     1     1     1     0     0     1     1     1\n[19,]     0     1     0     0     1     0     0     0\n[20,]     0     1     0     0     1     1     0     1\n[21,]     1     1     0     1     1     0     1     0\n\n\nCorrelating the columns of this matrix would thus give us the out-correlation distance based on advice seeking relations.\n“Stacking” is a way to combine both in and out-going ties and compute a single distance based on both. It just means that we literally bind the rows of the firs matrix and its transpose:\n\n   A.stack <- rbind(A, A.t)\n   A.stack\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]\n [1,]    0    1    0    1    0    0    0    1    0     0     0     0     0\n [2,]    0    0    0    0    0    1    1    0    0     0     0     0     0\n [3,]    1    1    0    1    0    1    1    1    1     1     1     1     0\n [4,]    1    1    0    0    0    1    0    1    0     1     1     1     0\n [5,]    1    1    0    0    0    1    1    1    0     1     1     0     1\n [6,]    0    0    0    0    0    0    0    0    0     0     0     0     0\n [7,]    0    1    0    0    0    1    0    0    0     0     1     1     0\n [8,]    0    1    0    1    0    1    1    0    0     1     1     0     0\n [9,]    1    1    0    0    0    1    1    1    0     1     1     1     0\n[10,]    1    1    1    1    1    0    0    1    0     0     1     0     1\n[11,]    1    1    0    0    0    0    1    0    0     0     0     0     0\n[12,]    0    0    0    0    0    0    1    0    0     0     0     0     0\n[13,]    1    1    0    0    1    0    0    0    1     0     0     0     0\n[14,]    0    1    0    0    0    0    1    0    0     0     0     0     0\n[15,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[16,]    1    1    0    0    0    0    0    0    0     1     0     0     0\n[17,]    1    1    0    1    0    0    1    0    0     0     0     0     0\n[18,]    1    1    1    1    1    0    1    1    1     1     1     0     1\n[19,]    1    1    1    0    1    0    1    0    0     1     1     0     0\n[20,]    1    1    0    0    0    1    0    1    0     0     1     1     0\n[21,]    0    1    1    1    0    1    1    1    0     0     0     1     0\n[22,]    0    0    1    1    1    0    0    0    1     1     1     0     1\n[23,]    1    0    1    1    1    0    1    1    1     1     1     0     1\n[24,]    0    0    0    0    0    0    0    0    0     1     0     0     0\n[25,]    1    0    1    0    0    0    0    1    0     1     0     0     0\n[26,]    0    0    0    0    0    0    0    0    0     1     0     0     1\n[27,]    0    1    1    1    1    0    1    1    1     0     0     0     0\n[28,]    0    1    1    0    1    0    0    1    1     0     1     1     0\n[29,]    1    0    1    1    1    0    0    0    1     1     0     0     0\n[30,]    0    0    1    0    0    0    0    0    0     0     0     0     1\n[31,]    0    0    1    1    1    0    0    1    1     0     0     0     0\n[32,]    0    0    1    1    1    0    1    1    1     1     0     0     0\n[33,]    0    0    1    1    0    0    1    0    1     0     0     0     0\n[34,]    0    0    0    0    1    0    0    0    0     1     0     0     0\n[35,]    0    0    1    0    1    0    1    0    1     0     0     0     1\n[36,]    0    0    0    0    0    0    0    0    0     1     0     0     0\n[37,]    1    0    0    1    1    0    0    0    1     1     0     0     0\n[38,]    0    0    1    1    1    0    1    0    1     1     0     0     0\n[39,]    1    0    1    1    1    0    1    1    1     1     0     0     1\n[40,]    0    0    0    0    1    0    0    0    0     1     0     0     0\n[41,]    0    0    1    1    1    0    0    0    0     1     0     0     0\n[42,]    1    1    1    1    1    1    1    1    1     0     0     1     0\n      [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21]\n [1,]     0     0     1     0     1     0     0     1\n [2,]     0     0     0     0     0     0     0     1\n [3,]     1     0     0     1     1     0     1     1\n [4,]     0     0     1     1     1     0     1     1\n [5,]     1     0     1     1     1     1     1     1\n [6,]     0     0     0     0     0     0     0     1\n [7,]     1     0     0     1     1     0     0     1\n [8,]     0     0     0     0     1     0     0     1\n [9,]     1     0     1     1     1     0     0     1\n[10,]     0     1     1     1     1     1     1     0\n[11,]     0     0     0     0     0     0     0     0\n[12,]     0     0     0     0     0     0     0     1\n[13,]     1     0     0     0     1     0     0     0\n[14,]     0     0     0     0     1     0     0     1\n[15,]     1     0     1     1     1     1     1     1\n[16,]     0     0     0     0     1     0     0     0\n[17,]     0     0     0     0     0     0     0     1\n[18,]     1     1     1     0     0     1     1     1\n[19,]     1     1     0     0     1     0     1     0\n[20,]     1     1     1     1     1     0     0     1\n[21,]     1     0     0     1     1     0     1     0\n[22,]     0     1     1     1     1     1     1     0\n[23,]     1     1     1     1     1     1     1     1\n[24,]     0     1     0     0     1     1     0     1\n[25,]     0     1     0     1     1     0     0     1\n[26,]     0     1     0     0     1     1     0     0\n[27,]     0     1     0     0     0     0     1     1\n[28,]     1     1     0     1     1     1     0     1\n[29,]     0     1     0     0     1     0     1     1\n[30,]     0     1     0     0     1     0     0     0\n[31,]     0     1     1     0     1     1     0     0\n[32,]     0     1     0     0     1     1     1     0\n[33,]     0     1     0     0     0     0     1     1\n[34,]     0     1     0     0     1     0     0     0\n[35,]     0     1     0     0     1     1     1     1\n[36,]     0     0     0     0     1     1     1     0\n[37,]     0     1     0     0     1     0     1     0\n[38,]     0     1     0     0     0     0     1     1\n[39,]     1     1     1     0     0     1     1     1\n[40,]     0     1     0     0     1     0     0     0\n[41,]     0     1     0     0     1     1     0     1\n[42,]     1     1     0     1     1     0     1     0\n\n\nNote that this matrix has the same number of columns as the original adjacency matrix and double the number of rows. This doesn’t matter since the correlation distance works on the columns, meaning that it will return a matrix of the same dimensions as the original:\n\n   round(cor(A.stack), 2)\n\n       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7] [,8]  [,9] [,10] [,11] [,12]\n [1,]  1.00  0.43  0.00  0.09  0.09  0.22  0.14 0.37  0.13  0.25  0.37  0.22\n [2,]  0.43  1.00 -0.19  0.00 -0.19  0.49  0.24 0.38 -0.15 -0.24  0.51  0.52\n [3,]  0.00 -0.19  1.00  0.52  0.62 -0.24  0.19 0.33  0.57  0.00  0.03 -0.03\n [4,]  0.09  0.00  0.52  1.00  0.43 -0.03  0.29 0.33  0.57  0.10  0.03 -0.03\n [5,]  0.09 -0.19  0.62  0.43  1.00 -0.35  0.00 0.14  0.67  0.20  0.03 -0.15\n [6,]  0.22  0.49 -0.24 -0.03 -0.35  1.00  0.27 0.36 -0.16  0.00  0.50  0.74\n [7,]  0.14  0.24  0.19  0.29  0.00  0.27  1.00 0.19  0.24 -0.05  0.10  0.06\n [8,]  0.37  0.38  0.33  0.33  0.14  0.36  0.19 1.00  0.27  0.01  0.41  0.49\n [9,]  0.13 -0.15  0.57  0.57  0.67 -0.16  0.24 0.27  1.00  0.07  0.03  0.04\n[10,]  0.25 -0.24  0.00  0.10  0.20  0.00 -0.05 0.01  0.07  1.00  0.24 -0.11\n[11,]  0.37  0.51  0.03  0.03  0.03  0.50  0.10 0.41  0.03  0.24  1.00  0.49\n[12,]  0.22  0.52 -0.03 -0.03 -0.15  0.74  0.06 0.49  0.04 -0.11  0.49  1.00\n[13,]  0.17 -0.11  0.36  0.14  0.25 -0.08  0.11 0.19  0.22  0.17  0.32 -0.16\n[14,]  0.47  0.51  0.13  0.03  0.13  0.50  0.30 0.51  0.24  0.03  0.57  0.62\n[15,] -0.08 -0.48  0.63  0.25  0.63 -0.47 -0.19 0.07  0.42  0.18 -0.10 -0.25\n[16,]  0.38  0.21  0.14  0.24  0.14  0.22  0.00 0.62  0.12  0.15  0.56  0.18\n[17,]  0.37  0.40  0.13  0.03 -0.07  0.61  0.00 0.61  0.03  0.03  0.68  0.74\n[18,]  0.06  0.11 -0.03 -0.14  0.09  0.21 -0.45 0.15 -0.11  0.28  0.28  0.29\n[19,] -0.08 -0.25  0.38  0.18  0.38 -0.22 -0.05 0.26  0.30  0.28  0.21 -0.15\n[20,]  0.28  0.00  0.52  0.52  0.43  0.08  0.38 0.33  0.57  0.29  0.24  0.08\n[21,]  0.02  0.10 -0.04  0.06 -0.23  0.24  0.29 0.18  0.05 -0.02  0.24  0.17\n      [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21]\n [1,]  0.17  0.47 -0.08  0.38  0.37  0.06 -0.08  0.28  0.02\n [2,] -0.11  0.51 -0.48  0.21  0.40  0.11 -0.25  0.00  0.10\n [3,]  0.36  0.13  0.63  0.14  0.13 -0.03  0.38  0.52 -0.04\n [4,]  0.14  0.03  0.25  0.24  0.03 -0.14  0.18  0.52  0.06\n [5,]  0.25  0.13  0.63  0.14 -0.07  0.09  0.38  0.43 -0.23\n [6,] -0.08  0.50 -0.47  0.22  0.61  0.21 -0.22  0.08  0.24\n [7,]  0.11  0.30 -0.19  0.00  0.00 -0.45 -0.05  0.38  0.29\n [8,]  0.19  0.51  0.07  0.62  0.61  0.15  0.26  0.33  0.18\n [9,]  0.22  0.24  0.42  0.12  0.03 -0.11  0.30  0.57  0.05\n[10,]  0.17  0.03  0.18  0.15  0.03  0.28  0.28  0.29 -0.02\n[11,]  0.32  0.57 -0.10  0.56  0.68  0.28  0.21  0.24  0.24\n[12,] -0.16  0.62 -0.25  0.18  0.74  0.29 -0.15  0.08  0.17\n[13,]  1.00  0.20  0.26  0.51  0.20  0.05  0.63  0.36 -0.02\n[14,]  0.20  1.00 -0.10  0.34  0.57  0.16  0.11  0.24  0.14\n[15,]  0.26 -0.10  1.00  0.02 -0.10  0.08  0.34  0.25 -0.18\n[16,]  0.51  0.34  0.02  1.00  0.45  0.11  0.41  0.24  0.17\n[17,]  0.20  0.57 -0.10  0.45  1.00  0.40  0.11  0.24  0.14\n[18,]  0.05  0.16  0.08  0.11  0.40  1.00  0.18 -0.03 -0.32\n[19,]  0.63  0.11  0.34  0.41  0.11  0.18  1.00  0.28 -0.03\n[20,]  0.36  0.24  0.25  0.24  0.24 -0.03  0.28  1.00 -0.04\n[21,] -0.02  0.14 -0.18  0.17  0.14 -0.32 -0.03 -0.04  1.00\n\n\nAnd we would then do a blockmodel based on these distances:\n\n   blocks2 <- function(A, s = 2) {\n     colnames(A) <- 1:ncol(A) #use only if the original matrix has no names\n     B <- list(A)\n     for(i in 1:s) {\n       B <- unlist(lapply(B, con.cor), recursive = FALSE)\n       }\n     return(lapply(B, colnames))\n   }\n   blocks2(A.stack)\n\n[[1]]\n [1] \"1\"  \"2\"  \"6\"  \"8\"  \"11\" \"12\" \"14\" \"16\" \"17\" \"18\"\n\n[[2]]\n[1] \"7\"  \"21\"\n\n[[3]]\n[1] \"3\"  \"4\"  \"5\"  \"9\"  \"15\" \"20\"\n\n[[4]]\n[1] \"10\" \"13\" \"19\""
  },
  {
    "objectID": "handout4.html#structuraly-equivalence-in-multigraphs",
    "href": "handout4.html#structuraly-equivalence-in-multigraphs",
    "title": "Role Equivalence and Structural Similarity",
    "section": "Structuraly Equivalence in Multigraphs",
    "text": "Structuraly Equivalence in Multigraphs\nNote that we would apply the same trick if we wanted to do a blockmodel based on multiple relations like friendship and advice. Here’s a blockmodel based on the stacked matrices of incoming ties of both types:\n\n   A.f <- as.matrix(as_adjacency_matrix(ht_friends))\n   A.stack <- rbind(A, A.f)\n   blocks2(A.stack)\n\n[[1]]\n[1] \"1\"  \"3\"  \"5\"  \"14\" \"15\" \"20\"\n\n[[2]]\n[1] \"4\"  \"9\"  \"13\" \"19\"\n\n[[3]]\n[1] \"2\"  \"8\"  \"12\" \"16\" \"17\" \"18\"\n\n[[4]]\n[1] \"6\"  \"7\"  \"10\" \"11\" \"21\"\n\n\nAnd one combining incoming and outgoing friendship and advice ties:\n\n   A.stack <- rbind(A, A.t, A.f, t(A.f))\n   blocks2(A.stack)\n\n[[1]]\n[1] \"1\"  \"2\"  \"8\"  \"12\" \"16\" \"21\"\n\n[[2]]\n[1] \"6\"  \"11\" \"14\" \"17\"\n\n[[3]]\n[1] \"3\"  \"4\"  \"5\"  \"7\"  \"9\"  \"20\"\n\n[[4]]\n[1] \"10\" \"13\" \"15\" \"18\" \"19\"\n\n\nNote that here the stacked matrix has four sub-matrices: (1) Incoming advice, (2) outgoing advice, (3) incoming friendship, and (4) Outgoing friendship."
  },
  {
    "objectID": "handout5.html",
    "href": "handout5.html",
    "title": "Vertex Similarity",
    "section": "",
    "text": "As we noted in the role equivalence handout, for odd reasons, classical approaches to structural similarity in networks used distance based approaches but did not measure similarity directly. More recent approaches from network and information science prefer to define vertex similarity using direct similarity measures based on local structural characteristics, like node neighborhoods.\nMathematically, similarity is a less stringent (but also less well-defined) relation between pairs of nodes in a graph than distance, so it can be easier to work with in most applications.\nFor instance, similarity is required to be symmetric (\\(s_{ij} = s_{ji}\\) for all \\(i\\) and \\(j\\)) and most metrics have reasonable bounds (e.g., 0 for least similar and 1.0 for maximally similar). Given such a bounded similarity we can get to dissimilarity by subtracting one: \\(d_{ij} = 1 - s_{ij}\\)"
  },
  {
    "objectID": "handout5.html#basic-ingredients-of-vertex-similarity-metrics",
    "href": "handout5.html#basic-ingredients-of-vertex-similarity-metrics",
    "title": "Vertex Similarity",
    "section": "Basic Ingredients of Vertex Similarity Metrics",
    "text": "Basic Ingredients of Vertex Similarity Metrics\nConsider two nodes and the set of nodes that are the immediate neighbors to each. In this case, most vertex similarity measures will make use of three pieces of information:\n\nThe number of common neighbors \\(p\\).\nThe number of actors \\(q\\) who are connected to node \\(i\\) but not to node \\(j\\).\nThe number of actors \\(r\\) who are connected to node \\(j\\) but not to node \\(i\\).\n\nIn the simplest case of the binary undirected graph then these are given by:\n\\[\n   p = \\sum_{k = 1}^{n} a_{ik} a_{jk}\n\\]\n\\[\n   q = \\sum_{k = 1}^{n} a_{ik} (1 - a_{jk})\n\\]\n\\[\n   r = \\sum_{k = 1}^{n} (1- a_{ik}) a_{jk}\n\\]\nIn matrix form:\n\\[\n   \\mathbf{A}(p) = \\mathbf{A} \\mathbf{A} = \\mathbf{A}^2\n\\]\n\\[\n   \\mathbf{A}(q) = \\mathbf{A} (1 - \\mathbf{A})\n\\]\n\\[\n   \\mathbf{A}(r) = (1 - \\mathbf{A}) \\mathbf{A}\n\\]\nLet’s look at an example:\n\n   library(networkdata)\n   library(igraph)\n   g.flint <- movie_267\n   g.flint <- delete_vertices(g.flint, degree(g.flint) <= 3)\n   A <- as.matrix(as_adjacency_matrix(g.flint))\n   A.p <- A %*% A #common neighbors matrix\n   A.q <- A %*% (1 - A) #neighbors of i not connected to j\n   A.r <- (1 - A) %*% A #neighbors of j not connected to i\n   A.p[1:10, 1:10]\n\n             BAM-BAM BARNEY BETTY FELDSPAR FRED HEADMISTRESS HERDMASTER LAVA\nBAM-BAM            6      4     5        2    5            2          2    5\nBARNEY             4     13     9        1   11            5          3    7\nBETTY              5      9    13        2    9            3          4    8\nFELDSPAR           2      1     2        2    1            0          2    2\nFRED               5     11     9        1   14            4          3    9\nHEADMISTRESS       2      5     3        0    4            5          0    3\nHERDMASTER         2      3     4        2    3            0          4    4\nLAVA               5      7     8        2    9            3          4   11\nLEACH              2      3     3        1    2            2          1    2\nMORRIS             2      3     2        0    2            3          0    2\n             LEACH MORRIS\nBAM-BAM          2      2\nBARNEY           3      3\nBETTY            3      2\nFELDSPAR         1      0\nFRED             2      2\nHEADMISTRESS     2      3\nHERDMASTER       1      0\nLAVA             2      2\nLEACH            3      1\nMORRIS           1      3\n\n   A.q[1:10, 1:10]\n\n             BAM-BAM BARNEY BETTY FELDSPAR FRED HEADMISTRESS HERDMASTER LAVA\nBAM-BAM            0      2     1        4    1            4          4    1\nBARNEY             9      0     4       12    2            8         10    6\nBETTY              8      4     0       11    4           10          9    5\nFELDSPAR           0      1     0        0    1            2          0    0\nFRED               9      3     5       13    0           10         11    5\nHEADMISTRESS       3      0     2        5    1            0          5    2\nHERDMASTER         2      1     0        2    1            4          0    0\nLAVA               6      4     3        9    2            8          7    0\nLEACH              1      0     0        2    1            1          2    1\nMORRIS             1      0     1        3    1            0          3    1\n             LEACH MORRIS\nBAM-BAM          4      4\nBARNEY          10     10\nBETTY           10     11\nFELDSPAR         1      2\nFRED            12     12\nHEADMISTRESS     3      2\nHERDMASTER       3      4\nLAVA             9      9\nLEACH            0      2\nMORRIS           2      0\n\n   A.r[1:10, 1:10]\n\n             BAM-BAM BARNEY BETTY FELDSPAR FRED HEADMISTRESS HERDMASTER LAVA\nBAM-BAM            0      9     8        0    9            3          2    6\nBARNEY             2      0     4        1    3            0          1    4\nBETTY              1      4     0        0    5            2          0    3\nFELDSPAR           4     12    11        0   13            5          2    9\nFRED               1      2     4        1    0            1          1    2\nHEADMISTRESS       4      8    10        2   10            0          4    8\nHERDMASTER         4     10     9        0   11            5          0    7\nLAVA               1      6     5        0    5            2          0    0\nLEACH              4     10    10        1   12            3          3    9\nMORRIS             4     10    11        2   12            2          4    9\n             LEACH MORRIS\nBAM-BAM          1      1\nBARNEY           0      0\nBETTY            0      1\nFELDSPAR         2      3\nFRED             1      1\nHEADMISTRESS     1      0\nHERDMASTER       2      3\nLAVA             1      1\nLEACH            0      2\nMORRIS           2      0\n\n\nNote that while \\(\\mathbf{A}(p)\\) is necessarily symmetric, neither \\(q\\) nor \\(r\\) have to be. Barney has many more neighbors that Bam-Bam is not connected to than vice versa. Also note that the \\(\\mathbf{A}(r)\\) matrix is just the transpose of the \\(\\mathbf{A}(q)\\) matrix in the undirected case.\nSo the most obvious measure of similarity between two nodes is simply the number of common neighbors (Leicht, Holme, and Newman 2006):\n\\[\n   s_{ij} = p_{ij}\n\\]\nWe have already seen a version of this in the directed case when talking about the HITS algorithm (Kleinberg 1999), which computes a spectral (eigenvector-based) ranking based on the matrices of common in and out-neighbors in a directed graph.\n\\[\n   p^{in}_{ij} = \\sum_{k = 1}^{n} a_{ki} a_{kj}\n\\]\n\\[\n   p^{out}_{ij} = \\sum_{k = 1}^{n} a_{ik} a_{jk}\n\\]\nWhich in matrix form is:\n\\[\n   \\mathbf{A}(p^{out}) = \\mathbf{A}^T \\mathbf{A}\n\\]\n\\[\n   \\mathbf{A}(p^{in}) = \\mathbf{A} \\mathbf{A}^T\n\\]\nIn this case, similarity can be measured either by the number of common in-neighbors or the number of common out-neighbors.\nIf the network under consideration is a (directed) citation network with nodes being papers and links between papers defined as a citation from paper \\(i\\) to paper \\(j\\), then the number of common in-neighbors between two papers is their co-citation similarity (the number of other papers that cite both papers), and the number of common out-neighbors is their bibliographic coupling similarity (the overlap in their list of references).\nOne problem with using unbounded quantities like the sheer number of common (in or out) neighbors to define node similarity is that they are only limited by the number of nodes in the network (Leicht, Holme, and Newman 2006). Thus, an actor with many neighbors will end up having lots of other neighbors in common with lots of other nodes, which will mean we would count them as “similar” to almost everyone."
  },
  {
    "objectID": "handout5.html#normalized-similarity-metrics",
    "href": "handout5.html#normalized-similarity-metrics",
    "title": "Vertex Similarity",
    "section": "Normalized Similarity Metrics",
    "text": "Normalized Similarity Metrics\nNormalized similarity metrics deal with this issue by adjusting the raw similarity based on \\(p\\) using the number of non-shared neighbors \\(q\\) and \\(r\\).\nThe two most popular versions of normalized vertex similarity scores are the Jaccard index and the cosine similarity (sometimes also referred to as the Salton Index).\nThe Jacccard index is given by:\n\\[\n   s_{ij} = \\frac{p}{p + q + r}\n\\]\nWhich is the ratio of the size of the intersection of the neighborhoods of the two nodes (number of common neighbors) divided by the size of the union of the two neighborhoods.\nIn our example, this would be:\n\n   J <- A.p / (A.p + A.q + A.r)\n   round(J[1:10, 1:10], 2)\n\n             BAM-BAM BARNEY BETTY FELDSPAR FRED HEADMISTRESS HERDMASTER LAVA\nBAM-BAM         1.00   0.27  0.36     0.33 0.33         0.22       0.25 0.42\nBARNEY          0.27   1.00  0.53     0.07 0.69         0.38       0.21 0.41\nBETTY           0.36   0.53  1.00     0.15 0.50         0.20       0.31 0.50\nFELDSPAR        0.33   0.07  0.15     1.00 0.07         0.00       0.50 0.18\nFRED            0.33   0.69  0.50     0.07 1.00         0.27       0.20 0.56\nHEADMISTRESS    0.22   0.38  0.20     0.00 0.27         1.00       0.00 0.23\nHERDMASTER      0.25   0.21  0.31     0.50 0.20         0.00       1.00 0.36\nLAVA            0.42   0.41  0.50     0.18 0.56         0.23       0.36 1.00\nLEACH           0.29   0.23  0.23     0.25 0.13         0.33       0.17 0.17\nMORRIS          0.29   0.23  0.14     0.00 0.13         0.60       0.00 0.17\n             LEACH MORRIS\nBAM-BAM       0.29   0.29\nBARNEY        0.23   0.23\nBETTY         0.23   0.14\nFELDSPAR      0.25   0.00\nFRED          0.13   0.13\nHEADMISTRESS  0.33   0.60\nHERDMASTER    0.17   0.00\nLAVA          0.17   0.17\nLEACH         1.00   0.20\nMORRIS        0.20   1.00\n\n\nHere showing that Barney is more similar to Fred and Betty than he is to Bam-Bam.\nThe cosine similarity is given by:\n\\[\n   s_{ij} = \\frac{p}{\\sqrt{p + q} \\sqrt{p + r}}\n\\]\nWhich is the ratio of the number of common neighbors divided by the product of the square root of the degrees of each node (or the square root of the product which is the same thing), since \\(p\\) + \\(q\\) is the degree of node \\(i\\) and \\(p\\) + \\(r\\) is the degree of node \\(j\\).\nIn our example, this would be:\n\n   C <- A.p / (sqrt(A.p + A.q) * sqrt(A.p + A.r))\n   round(C[1:10, 1:10], 2)\n\n             BAM-BAM BARNEY BETTY FELDSPAR FRED HEADMISTRESS HERDMASTER LAVA\nBAM-BAM         1.00   0.45  0.57     0.58 0.55         0.37       0.41 0.62\nBARNEY          0.45   1.00  0.69     0.20 0.82         0.62       0.42 0.59\nBETTY           0.57   0.69  1.00     0.39 0.67         0.37       0.55 0.67\nFELDSPAR        0.58   0.20  0.39     1.00 0.19         0.00       0.71 0.43\nFRED            0.55   0.82  0.67     0.19 1.00         0.48       0.40 0.73\nHEADMISTRESS    0.37   0.62  0.37     0.00 0.48         1.00       0.00 0.40\nHERDMASTER      0.41   0.42  0.55     0.71 0.40         0.00       1.00 0.60\nLAVA            0.62   0.59  0.67     0.43 0.73         0.40       0.60 1.00\nLEACH           0.47   0.48  0.48     0.41 0.31         0.52       0.29 0.35\nMORRIS          0.47   0.48  0.32     0.00 0.31         0.77       0.00 0.35\n             LEACH MORRIS\nBAM-BAM       0.47   0.47\nBARNEY        0.48   0.48\nBETTY         0.48   0.32\nFELDSPAR      0.41   0.00\nFRED          0.31   0.31\nHEADMISTRESS  0.52   0.77\nHERDMASTER    0.29   0.00\nLAVA          0.35   0.35\nLEACH         1.00   0.33\nMORRIS        0.33   1.00\n\n\nShowing results similar (pun intended) to those obtained using the Jaccard index.\nA less commonly used option is the Dice Coefficient (sometimes called the Sorensen Index) given by:\n\\[\n   s_{ij} = \\frac{2p}{2p + q + r}\n\\]\nWhich is given by the ratio of the number of twice the number of common neighbors divided by twice the same quantity plus the sum of the non-shared neighbors (and thus a variation of the Jaccard measure).\nIn our example, this would be:\n\n   D <- (2 * A.p) / ((2 * A.p) + A.q + A.r)\n   round(D[1:10, 1:10], 2)\n\n             BAM-BAM BARNEY BETTY FELDSPAR FRED HEADMISTRESS HERDMASTER LAVA\nBAM-BAM         1.00   0.42  0.53     0.50 0.50         0.36       0.40 0.59\nBARNEY          0.42   1.00  0.69     0.13 0.81         0.56       0.35 0.58\nBETTY           0.53   0.69  1.00     0.27 0.67         0.33       0.47 0.67\nFELDSPAR        0.50   0.13  0.27     1.00 0.12         0.00       0.67 0.31\nFRED            0.50   0.81  0.67     0.12 1.00         0.42       0.33 0.72\nHEADMISTRESS    0.36   0.56  0.33     0.00 0.42         1.00       0.00 0.38\nHERDMASTER      0.40   0.35  0.47     0.67 0.33         0.00       1.00 0.53\nLAVA            0.59   0.58  0.67     0.31 0.72         0.38       0.53 1.00\nLEACH           0.44   0.38  0.38     0.40 0.24         0.50       0.29 0.29\nMORRIS          0.44   0.38  0.25     0.00 0.24         0.75       0.00 0.29\n             LEACH MORRIS\nBAM-BAM       0.44   0.44\nBARNEY        0.38   0.38\nBETTY         0.38   0.25\nFELDSPAR      0.40   0.00\nFRED          0.24   0.24\nHEADMISTRESS  0.50   0.75\nHERDMASTER    0.29   0.00\nLAVA          0.29   0.29\nLEACH         1.00   0.33\nMORRIS        0.33   1.00\n\n\nOnce again, showing results comparable to the previous.\nNote, that all three of these pairwise measures of similarity are bounded between zero and one, with nodes being maximally similar to themselves and with pairs of distinct nodes being maximally similar when they have the same set of neighbors (e.g., they are structurally equivalent).\nLeicht, Holme, and Newman (2006) introduce a variation on the theme of normalized structural similarity scores. Their point is that maybe we should care about nodes that are surprisingly similar given some suitable null model. They propose the configuration model as such a null model. This model takes a graph with the same degree distribution as the original but with connections formed at random as reference.\nThe LHN similarity index (for Leicht, Holme, and Newman) is then given by:\n\\[\ns_{ij} = \\frac{p}{(p + q)(p + r)}\n\\]\nWhich can be seen as a variation of the cosine similarity defined earlier.\nIn our example, this would be:\n\n   D <- A.p / ((A.p + A.q) * (A.p + A.r))\n   round(D[1:10, 1:10], 2)\n\n             BAM-BAM BARNEY BETTY FELDSPAR FRED HEADMISTRESS HERDMASTER LAVA\nBAM-BAM         0.17   0.05  0.06     0.17 0.06         0.07       0.08 0.08\nBARNEY          0.05   0.08  0.05     0.04 0.06         0.08       0.06 0.05\nBETTY           0.06   0.05  0.08     0.08 0.05         0.05       0.08 0.06\nFELDSPAR        0.17   0.04  0.08     0.50 0.04         0.00       0.25 0.09\nFRED            0.06   0.06  0.05     0.04 0.07         0.06       0.05 0.06\nHEADMISTRESS    0.07   0.08  0.05     0.00 0.06         0.20       0.00 0.05\nHERDMASTER      0.08   0.06  0.08     0.25 0.05         0.00       0.25 0.09\nLAVA            0.08   0.05  0.06     0.09 0.06         0.05       0.09 0.09\nLEACH           0.11   0.08  0.08     0.17 0.05         0.13       0.08 0.06\nMORRIS          0.11   0.08  0.05     0.00 0.05         0.20       0.00 0.06\n             LEACH MORRIS\nBAM-BAM       0.11   0.11\nBARNEY        0.08   0.08\nBETTY         0.08   0.05\nFELDSPAR      0.17   0.00\nFRED          0.05   0.05\nHEADMISTRESS  0.13   0.20\nHERDMASTER    0.08   0.00\nLAVA          0.06   0.06\nLEACH         0.33   0.11\nMORRIS        0.11   0.33\n\n\nWhich, once again, produces similar results to what we found before. Note, however, that the LHN is not naturally maximal for self-similar nodes."
  },
  {
    "objectID": "handout5.html#similarity-and-structural-equivalence",
    "href": "handout5.html#similarity-and-structural-equivalence",
    "title": "Vertex Similarity",
    "section": "Similarity and Structural Equivalence",
    "text": "Similarity and Structural Equivalence\nAll normalized similarity measures bounded between zero and one (like Jaccard, Cosine, and Dice) also define a distance on each pair of nodes which is equal to one minus the similarity. So the cosine distance between two nodes is one minus the cosine similarity, and so on for the Jaccard and Dice indexes.\nBecause they define distances, this also means that these approaches can be used to find approximately structurally equivalent classes of nodes in a graph just like we did with the Euclidean and correlation distances.\nFor instance, consider our toy graph from before with four structurally equivalent sets of nodes:\n\n\n\n\n\nA toy graph demonstrating structural equivalence.\n\n\n\n\nThe cosine similarity matrix for this graph is:\n\n   A <- as.matrix(as_adjacency_matrix(g))\n   A.p <- A %*% A \n   A.q <- A %*% (1 - A) \n   A.r <- (1 - A) %*% A \n   C <- A.p / (sqrt(A.p + A.q) * sqrt(A.p + A.r))\n   round(C, 2)\n\n     A    B    C    D    E    F    G    H    I\nA 1.00 0.26 0.26 0.58 0.58 0.58 0.67 0.00 0.00\nB 0.26 1.00 1.00 0.00 0.00 0.00 0.26 0.67 0.67\nC 0.26 1.00 1.00 0.00 0.00 0.00 0.26 0.67 0.67\nD 0.58 0.00 0.00 1.00 1.00 1.00 0.58 0.25 0.25\nE 0.58 0.00 0.00 1.00 1.00 1.00 0.58 0.25 0.25\nF 0.58 0.00 0.00 1.00 1.00 1.00 0.58 0.25 0.25\nG 0.67 0.26 0.26 0.58 0.58 0.58 1.00 0.00 0.00\nH 0.00 0.67 0.67 0.25 0.25 0.25 0.00 1.00 0.75\nI 0.00 0.67 0.67 0.25 0.25 0.25 0.00 0.75 1.00\n\n\nNote that structurally equivalent nodes have similarity scores equal to 1.0. In this case, the distance matrix is given by:\n\n   D <- 1 - C\n\nAnd a hierarchical clustering on this matrix reveals the structurally equivalent classes:\n\n   D <- dist(D) \n   h.res <- hclust(D, method = \"ward.D2\")\n   plot(h.res)\n\n\n\n\nWe can package all that we said before into a handy function that takes a graph as input and returns all four normalized similarity metrics as output:\n\n   vertex.sim <- function(x) {\n      A <- as.matrix(as_adjacency_matrix(x))\n      A.p <- A %*% A \n      A.q <- A %*% (1 - A) \n      A.r <- (1 - A) %*% A \n      J <- A.p / (A.p + A.q + A.r)\n      C <- A.p / (sqrt(A.p + A.q) * sqrt(A.p + A.r))\n      D <- (2 * A.p) / ((2 * A.p) + A.q + A.r)\n      L <- A.p / ((A.p + A.q) * (A.p + A.r))\n      return(list(J = J, C = C, D = D, L = L))\n      }\n\nIn the Flintstones network, we could then find structurally equivalent blocks from the similarity analysis as follows (using Cosine):\n\n   g.flint <- movie_267\n   g.flint <- delete_vertices(g.flint, degree(g.flint) <= 3)\n   D <- dist(1- vertex.sim(g.flint)$C)\n   h.res <- hclust(D, method = \"ward.D2\") #hierarchical clustering\n   plot(h.res)"
  },
  {
    "objectID": "handout5.html#generalized-similarities",
    "href": "handout5.html#generalized-similarities",
    "title": "Vertex Similarity",
    "section": "Generalized Similarities",
    "text": "Generalized Similarities\nSo far, we have defined distances and similarities mainly as a function of the number of shared neighbors between two nodes. Structural equivalence is an idealized version of this, obtaining whenever people share exactly the same set of neighbors.\nYet, similarities based on local neighborhood information only have been criticized (e.g., by Borgatti and Everett (1992)) for not quite capturing the sociological intuition behind the idea of a role which is usually what they are deployed for. That is, two doctors don’t occupy the same role because they treat the same set of patients (in fact, this would be weird); instead, the set of doctors occupy the same role to the extent they treat a set of actors in the same (or similar) role: Namely, patients, regardless of whether those actors are literally the same or not.\nThis worry led social network analysts to try to generalize the concept of structural equivalence using less stringent algebraic definition, resulting in something of a proliferation of different equivalences such as automorphic or regular equivalence (Borgatti and Everett 1992).\nUnfortunately, none of this work led (unlike the work on structural equivalence) to useful or actionable tools for data analysis, with some even concluding that some definitions of equivalence (like regular equivalence) are unlikely to be found in any interesting substantive setting.\nA better approach here is to use the more relaxed notion of similarity like we did above to come up with a more general definition that can capture our role-based intuitions. Note that all the similarity notions studied earlier have structural equivalence as the limiting case of maximum similarity. So they are still based on the idea that nodes are similar to the extent they connect to the same others.\nWe can generalize this idea (to deal with the doctor/patient role problem) in the following way: nodes are similar to the extent they connect to similar others, with the restriction that we can only use endogenous (structural connectivity) information—like with structural equivalence or common-neighbor approaches—to define everyone’s similarity (no exogenous attribute stuff).\nAs Jeh and Widom (2002) note, this approach does lead to a coherent generalization of the idea of similarity for nodes in graphs because we can just define similarity recursively and iterate through the graph until the similarity scores stop changing.1 More specifically, they propose to measure the similarity between two nodes \\(i\\) and \\(j\\) at each time-step in the iteration using the formula:\n\\[\\begin{equation}\n   s_{ij} = \\frac{\\alpha}{d_i d_j} \\sum_{k \\in N(i)} \\sum_{l \\in N(j)} s_{kl}\n\\end{equation}\\]\nSo the similarity between two nodes is just the sum of the pairwise similarities between each of their neighbors (computed in the previous step), weighted by the ratio of a free parameter \\(\\alpha\\) (a number between zero and one) to the product of their degrees (to take a weighted average).\nThis measure nicely captures the idea that nodes are similar to the extent they both connect to people who are themselves similar. Note that it doesn’t matter whether these neighbors are shared between the two nodes (the summation occurs over each pair of nodes in \\(p\\) + \\(q\\) versus \\(p\\) + \\(r\\) as defined earlier) or whether the set of neighbors are themselves neighbors, which deals with the doctor/patient problem.\nA function that implements this idea looks like:\n\n   SimRank.in <- function(A, C = 0.8, iter = 10) {\n      d <- colSums(A)\n      n <- nrow(A)\n      S <- diag(1, n, n)\n      rownames(S) <- rownames(A)\n      colnames(S) <- colnames(A)\n      m <- 1\n      while(m < iter) {\n          for(i in 1:n) {\n               for(j in 1:n) {\n                    if (i < j) {\n                        a <- names(which(A[, i] == 1)) \n                        b <- names(which(A[, j] == 1)) \n                        Sij <- 0\n                        for (k in a) {\n                           for (l in b) {\n                              Sij <- Sij + S[k, l] #i's similarity to j\n                           }\n                        }\n                        S[i, j] <- C/(d[i] * d[j]) * Sij\n                        S[j, i] <- C/(d[i] * d[j]) * Sij\n                    }\n               }\n          }\n         m <- m + 1\n      }\n   return(S)\n}\n\nNote that this function calculates SimRank using each node’s in-neighbors (this doesn’t matter if the graph is undirected).\nLet’s try it out in the Flintstones graph using \\(\\alpha = 0.95\\):\n\n   A <- as.matrix(as_adjacency_matrix(g.flint))\n   S <- SimRank.in(A, 0.95)\n   round(S[, 1:5], 2)\n\n                BAM-BAM BARNEY BETTY FELDSPAR FRED\nBAM-BAM            1.00   0.46  0.47     0.52 0.47\nBARNEY             0.46   1.00  0.47     0.45 0.48\nBETTY              0.47   0.47  1.00     0.48 0.47\nFELDSPAR           0.52   0.45  0.48     1.00 0.46\nFRED               0.47   0.48  0.47     0.46 1.00\nHEADMISTRESS       0.47   0.48  0.46     0.44 0.48\nHERDMASTER         0.48   0.47  0.48     0.57 0.48\nLAVA               0.48   0.47  0.47     0.49 0.48\nLEACH              0.49   0.48  0.48     0.53 0.47\nMORRIS             0.49   0.48  0.47     0.44 0.47\nMRS SLATE          0.47   0.46  0.47     0.48 0.47\nPEBBLES            0.51   0.47  0.48     0.53 0.48\nPEBBLES BAM-BAM    0.49   0.48  0.48     0.48 0.48\nPILTDOWN           0.47   0.48  0.47     0.52 0.48\nPOINDEXTER         0.47   0.47  0.48     0.51 0.47\nPYRITE             0.47   0.48  0.47     0.52 0.48\nSLATE              0.47   0.47  0.48     0.50 0.48\nWILMA              0.47   0.47  0.48     0.48 0.47\n\n\nWe can transform the generalized similarities to distances and plot:\n\n   D <- dist(1- S)\n   h.res <- hclust(D, method = \"ward.D2\") #hierarchical clustering\n   plot(h.res)\n\n\n\n\nWhich returns an intuitive role partition."
  },
  {
    "objectID": "handout5.html#global-similarity-indices",
    "href": "handout5.html#global-similarity-indices",
    "title": "Vertex Similarity",
    "section": "Global Similarity Indices",
    "text": "Global Similarity Indices\nAs we saw earlier, the most important ingredient of structural similarity measures between pairs of nodes is the number of common of neighbors (followed by the degrees of each node), and this quantity is given by the square of the adjacency matrix \\(\\mathbf{A}^2\\). So we can say that this matrix gives us a basic similarity measure between nodes, namely, the common neighbors similarity:\n\\[\n\\mathbf{S} = \\mathbf{A}^2\n\\]\nAnother way of seeing this is that a common neighbor defines a path of length two between a pair of nodes. So the number of common neighbors between two nodes is equivalent to the number of paths of length two between them. We are thus saying that the similarity between two nodes increases as the number of paths of length two between them increases, and that info is also recorded in the \\(\\mathbf{A}^2\\) matrix.\nBut if the similarity between node pairs increases in the number of paths of length two between them, wouldn’t nodes be even more similar if they also have a bunch of paths of length three between them?\nLü, Jin, and Zhou (2009) asked themselves the same question and proposed the following as a similarity metric based on paths:\n\\[\\begin{equation}\n\\mathbf{S} = \\mathbf{A}^2 + \\alpha \\mathbf{A}^3\n\\end{equation}\\]\nThis is the so-called local path similarity index (Lü, Jin, and Zhou 2009). Obviously, structurally equivalent nodes will be counted as similar by this metric (lots of paths of length two between them) but also nodes indirectly connected by many paths of length three, but to a lesser extent given the discount parameter \\(\\alpha\\) (a number between zero and one).\nA function that does this is:\n\n   local.path <- function(A, alpha = 0.5) {\n      A2 <- A %*% A\n      S <- A2 + alpha*(A2 %*% A)\n   return(S)\n   }\n\nHere’s how the local path similarity looks in the Flintstones network:\n\n   A <- as.matrix(as_adjacency_matrix(g.flint))\n   S <- local.path(A)\n   S[1:10, 1:10]\n\n             BAM-BAM BARNEY BETTY FELDSPAR FRED HEADMISTRESS HERDMASTER LAVA\nBAM-BAM         20.0   29.5  30.5      6.5 31.5         12.5        9.5 25.0\nBARNEY          29.5   50.0  51.0     13.0 52.0         21.0       21.0 46.5\nBETTY           30.5   51.0  52.0     11.0 53.0         24.0       18.0 46.0\nFELDSPAR         6.5   13.0  11.0      3.0 13.5          4.5        5.0 10.0\nFRED            31.5   52.0  53.0     13.5 55.0         21.5       21.5 47.5\nHEADMISTRESS    12.5   21.0  24.0      4.5 21.5         13.0        6.5 21.5\nHERDMASTER       9.5   21.0  18.0      5.0 21.5          6.5       10.0 17.0\nLAVA            25.0   46.5  46.0     10.0 47.5         21.5       17.0 42.0\nLEACH            9.5   15.5  16.0      3.5 17.5          7.5        5.5 15.5\nMORRIS           8.5   13.0  15.5      2.5 13.0          8.0        3.5 12.5\n             LEACH MORRIS\nBAM-BAM        9.5    8.5\nBARNEY        15.5   13.0\nBETTY         16.0   15.5\nFELDSPAR       3.5    2.5\nFRED          17.5   13.0\nHEADMISTRESS   7.5    8.0\nHERDMASTER     5.5    3.5\nLAVA          15.5   12.5\nLEACH          6.0    4.0\nMORRIS         4.0    6.0\n\n\nOf course as \\(\\alpha\\) approaches zero, then the local path measure reduces to the number of common neighbors, while numbers closer to one count paths of length three more.\nAnother thing people may wonder if why not keep going and add paths of length four:\n\\[\\begin{equation}\n\\mathbf{S} = \\mathbf{A}^2 + \\alpha \\mathbf{A}^3 + \\alpha^2 \\mathbf{A}^4\n\\end{equation}\\]\nOr paths of length whatever:\n\\[\n\\mathbf{S} = \\mathbf{A}^2 + \\alpha \\mathbf{A}^3 + \\alpha^2 \\mathbf{A}^4 ... + \\alpha^{k-2} \\mathbf{A}^k\n\\]\nWhere \\(k\\) is the length of the maximum path considered. Lü, Jin, and Zhou (2009) argue that these higher order paths don’t matter. Another issue is that there was already an all-paths similarity measure in existence, one developed by the mathematical social scientist Leo Katz (1953) in the 1950s (!).\nThe basic idea was to use linear algebra tricks to solve:\n\\[\n\\mathbf{S} = \\sum_{k=1}^{\\infty} \\alpha^k A^k\n\\]\nWhich would theoretically count the paths of all possible lengths between two nodes while discounting the contribution of the longer paths in proportion to their length (as \\(k\\) gets bigger, with \\(\\alpha\\) a number between zero and one, \\(\\alpha^k\\) gets smaller and smaller).\nLinear algebra hocus-pocus (non-technical explainer here) turns the above infinite sum into the more tractable:\n\\[\n\\mathbf{S} = (\\mathbf{I} - \\alpha A)^{-1}\n\\]\nWhere \\(\\mathbf{I}\\) is the identity matrix (a matrix of all zeros except that it has the number one in each diagonal cell) of the same dimensions as the original. Raising the result of the subtraction in parentheses to minus one indicates the matrix inverse operation (most matrices are invertible, unless they are weird).\nA function to compute the Katz similarity between all node pairs is:\n\n   katz.sim <- function(A, min.alpha = 0.05) {\n      I <- diag(nrow(A)) #creating identity matrix\n      alpha <- runif(1, min = min.alpha, max = 1/eigen(A)$values[1])\n      S <- solve(I - alpha * A) \n      return(S)\n   }\n\nFor technical reasons (e.g., guarantee that the infinite sum converges) we need to choose \\(\\alpha\\) to be a number larger than zero but smaller than the reciprocal of the first eigenvalue of the matrix. Hence we just pick a random value in that interval in line 3 (set the seed to get reproducible answers). Line 4 computes the actual Katz similarity using the native R function solve to find the relevant matrix inverse.2\nIn the Flintstones network the Katz similarity looks like:\n\n   set.seed(456)\n   S <- katz.sim(A)\n   round(S[1:10, 1:10], 2)\n\n             BAM-BAM BARNEY BETTY FELDSPAR FRED HEADMISTRESS HERDMASTER LAVA\nBAM-BAM         1.03   0.08  0.09     0.01 0.09         0.01       0.01 0.03\nBARNEY          0.08   1.07  0.11     0.07 0.12         0.03       0.08 0.10\nBETTY           0.09   0.11  1.07     0.01 0.11         0.08       0.02 0.11\nFELDSPAR        0.01   0.07  0.01     1.01 0.07         0.00       0.01 0.01\nFRED            0.09   0.12  0.11     0.07 1.07         0.02       0.08 0.11\nHEADMISTRESS    0.01   0.03  0.08     0.00 0.02         1.02       0.00 0.08\nHERDMASTER      0.01   0.08  0.02     0.01 0.08         0.00       1.02 0.02\nLAVA            0.03   0.10  0.11     0.01 0.11         0.08       0.02 1.06\nLEACH           0.01   0.02  0.02     0.00 0.07         0.01       0.01 0.07\nMORRIS          0.01   0.02  0.07     0.00 0.01         0.01       0.00 0.01\n             LEACH MORRIS\nBAM-BAM       0.01   0.01\nBARNEY        0.02   0.02\nBETTY         0.02   0.07\nFELDSPAR      0.00   0.00\nFRED          0.07   0.01\nHEADMISTRESS  0.01   0.01\nHERDMASTER    0.01   0.00\nLAVA          0.07   0.01\nLEACH         1.01   0.01\nMORRIS        0.01   1.01\n\n\nLeicht, Holme, and Newman (2006) argue that the Katz approach is fine and dandy as a similarity measure, but note that is an unweighted index (like the raw number of common neighbors). This means that nodes with large degree will end up being “similar” to a buch of other nodes in the graph, just because they have lots of paths of length whatever between them and those nodes.\nLeicht, Holme, and Newman (2006) propose a “fix” for this weakness in the Katz similarity, resulting in the matrix linear algebra equivalent of a degree-normalized similarity measure like the Jaccard or Cosine.\nSo instead of Katz they suggest we compute:\n\\[\n\\mathbf{S} = \\mathbf{D}^{-1} \\left( \\frac{\\alpha A}{\\lambda_1} \\right)^{-1} \\mathbf{D}^{-1}\n\\]\nHere \\(\\mathbf{D}\\) is a matrix containing the degrees of each node along the diagonal. The inverse of this matrix \\(\\mathbf{D}^{-1}\\) will contain the reciprocal of each degree \\(1/k_i\\) along the diagonals. \\(\\lambda_1\\), on the other hand, is just the first eigenvalue of the adjacency matrix.\nSo, the LHN Similarity is just the Katz similarity weighted by the degree of the sender and receiver node along each path, further discounting paths featuring high-degree nodes at either or both ends.\nWhich leads to the function:\n\n   LHN.sim <- function(A, alpha = 0.9) {\n      D <- solve(diag(rowSums(A))) #inverse of degree matrix\n      lambda <- eigen(A)$values[1] #first eigenvalue of adjacency matrix\n      S <- D %*% solve((alpha * A)/lambda) %*% D #LHN index\n      rownames(S) <- rownames(A)\n      colnames(S) <- colnames(A)\n      return(S)\n   }\n\nAnd the Flintstones result:\n\n   S <- LHN.sim(A)\n   round(S[, 1:5], 2)\n\n                BAM-BAM BARNEY BETTY FELDSPAR  FRED\nBAM-BAM           -0.09   0.01  0.01    -0.16 -0.01\nBARNEY             0.01  -0.02  0.00     0.11  0.01\nBETTY              0.01   0.00 -0.04    -0.48  0.00\nFELDSPAR          -0.16   0.11 -0.48    -2.55  0.25\nFRED              -0.01   0.01  0.00     0.25 -0.01\nHEADMISTRESS       0.20   0.05 -0.13    -2.96 -0.05\nHERDMASTER         0.16   0.08  0.10     0.07 -0.07\nLAVA               0.00  -0.02 -0.02    -0.42  0.02\nLEACH             -0.27  -0.10  0.04     2.01  0.09\nMORRIS            -0.25  -0.03  0.12     1.90  0.03\nMRS SLATE         -0.05   0.01  0.04     0.74 -0.01\nPEBBLES            0.19   0.01  0.01    -0.16 -0.01\nPEBBLES BAM-BAM    0.03   0.00  0.03     0.16  0.00\nPILTDOWN           0.00   0.00  0.00    -0.35  0.00\nPOINDEXTER         0.01   0.03  0.04     0.66 -0.02\nPYRITE             0.00   0.00  0.00    -0.35  0.00\nSLATE             -0.08  -0.01  0.04     0.91  0.01\nWILMA              0.01   0.00  0.02     0.09  0.00\n\n\nNote that as Leicht, Holme, and Newman (2006) discuss, the entries in the LHN version of the similarity \\(\\mathbf{S}\\) can be either positive or negative. Negative entries are nodes that are surprisingly dissimilar given their degrees, and positive numbers indicating node pairs that are surprisingly similar. Numbers closer to zero are nodes that are neither similar nor dissimilar given their degrees.\nHere we can see that Barney and Fred are actually not that similar to one another (after we take into account their very high degree) and that Barney is actually most similar to Feldspar (and Fred even more so)."
  },
  {
    "objectID": "handout5.html#appendix",
    "href": "handout5.html#appendix",
    "title": "Vertex Similarity",
    "section": "Appendix",
    "text": "Appendix\nA more general version of the function to compute SimRank looks like this (partly based on Fouss, Saerens, and Shimbo 2016, 84, algorithm 2.4):\n\n   SimRank <- function(A, sigma = 0.0001, alpha = 0.8) {\n      n <- nrow(A)\n      d <- as.numeric(colSums(A) > 0)\n      e <- matrix(1, n, 1)\n      Q <- diag(as.vector(t(e) %*% A), n, n)\n      diag(Q) <- 1/diag(Q)\n      Q <- A %*% Q\n      Q[is.nan(Q)] <- 0\n      diff <- 1\n      k <- 1\n      K <- diag(1, n, n)\n      while(diff > sigma) {\n         K.old <- K\n         K.p <- alpha * t(Q) %*% K %*% Q\n         K <- K.p - diag(as.vector(diag(K.p)), n, n) + diag(d, n, n)\n         diff <- abs(sum(abs(K.old)) - sum(abs(K)))\n         k <- k + 1\n      }\n      rownames(K) <- rownames(A)\n      colnames(K) <- colnames(A)\n      return(K)\n   }"
  },
  {
    "objectID": "handout6.html",
    "href": "handout6.html",
    "title": "Community Structure",
    "section": "",
    "text": "What are communities? In networks, communities are subset of nodes that have more interactions or connectivity within themselves than they do outside of themselves (these are sometimes called “modules” outside of sociology). Communities thus exemplify the sociological concept of a group.\nA network has community structure if it contains many such subsets or groups of nodes that interact preferentially among themselves. Not all networks have to have community structure; a network in which all nodes interact with equal propensity doesn’t have communities.\nSo whether a network has community structure, and whether a given guess as to what these communities are yields actual communities (cluster of nodes that interact more among themselves than they do with outsiders) is an empirical question than needs to be answered with data.\nBut first, we need to develop a criterion for whether a given partition of the network into mutually exclusive node subsets is actually producing communities as we defined them earlier. This criterion should be independent of particular methods and algorithms that claim to find communities, so that way we can compare them with one another and see whether the partitions they recommend yield actual communities.\nMark Newman, who has done the most influential work in this area, proposed such a criterion and called it the modularity of a partition (e.g., the extent to which a partition has identified the “modules” or subsets of the network)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Social Networks Sequence",
    "section": "",
    "text": "Website containing syllabi, reading schedules, and other instructional materials for the courses in the Social Network Analysis computational sequence (208A & 208B) at UCLA Sociology."
  },
  {
    "objectID": "inoutdegdist.html",
    "href": "inoutdegdist.html",
    "title": "Graph Degree Metrics in Directed Networks",
    "section": "",
    "text": "As we saw in the Basic Network Statistics Handout, the degree distribution and the degree correlation are two basic things we want to have a sense of when characterizing a network, and we showed examples for the undirected graph case.\nAs we also saw on the Centrality handout, the number of things you have to compute in terms of degrees “doubles” in the directed graph case; for instance, instead of a single degree set and sequence, now we have two: An out and an indegree set and sequence.\nThe same thing applies to the degree distribution and the degree correlation."
  },
  {
    "objectID": "inoutdegdist.html#in-and-out-degree-distributions",
    "href": "inoutdegdist.html#in-and-out-degree-distributions",
    "title": "Graph Degree Metrics in Directed Networks",
    "section": "In and Out Degree Distributions",
    "text": "In and Out Degree Distributions\nIn the case of the degree distribution, now we have two distributions: An outdegree distribution and an indegree distribution.\nLet’s see an example using the law_advice data.\n\n   library(networkdata)\n   library(igraph)\n   g <- law_advice\n   i.prop <- degree_distribution(g, mode = \"in\")\n   o.prop <- degree_distribution(g, mode = \"out\")\n\nSo the main complication is that now we have to specify a value for the “mode” argument; “in” for indegree and “out” for outdegree.\nThat also means that when plotting, we have to create two data frames and present two plots.\nFirst the data frames:\n\n   i.d <- degree(g, mode = \"in\")\n   o.d <- degree(g, mode = \"out\")\n   i.d.vals <- c(0:max(i.d))\n   o.d.vals <- c(0:max(o.d))\n   i.deg.dist <- data.frame(i.d.vals, i.prop)\n   o.deg.dist <- data.frame(o.d.vals, o.prop)\n   head(i.deg.dist)\n\n  i.d.vals     i.prop\n1        0 0.01408451\n2        1 0.02816901\n3        2 0.07042254\n4        3 0.02816901\n5        4 0.08450704\n6        5 0.02816901\n\n   head(o.deg.dist)\n\n  o.d.vals     o.prop\n1        0 0.01408451\n2        1 0.00000000\n3        2 0.01408451\n4        3 0.05633803\n5        4 0.04225352\n6        5 0.04225352\n\n\nNow, to plotting. To be effective, the resulting plot has to show the outdegree and indegree distribution side by side so as to allow the reader to compare. To do that, we first generate each plot separately:\n\n   library(ggplot2)\n   p <- ggplot(data = o.deg.dist, aes(x = o.d.vals, y = o.prop))\n   p <- p + geom_bar(stat = \"identity\", fill = \"red\", color = \"red\")\n   p <- p + theme_minimal()\n   p <- p + labs(x = \"\", y = \"Proportion\", \n                 title = \"Outdegree Distribution in Law Advice Network\") \n   p <- p + geom_vline(xintercept = mean(o.d), \n                       linetype = 2, linewidth = 0.75, color = \"blue\")\n   p1 <- p + scale_x_continuous(breaks = c(0, 5, 10, 15, 20, 25, 30, 35, 40)) + xlim(0, 40)\n\n   p <- ggplot(data = i.deg.dist, aes(x = i.d.vals, y = i.prop))\n   p <- p + geom_bar(stat = \"identity\", fill = \"red\", color = \"red\")\n   p <- p + theme_minimal()\n   p <- p + labs(x = \"\", y = \"Proportion\", \n                 title = \"Indegree Distribution in Law Advice Network\") \n   p <- p + geom_vline(xintercept = mean(i.d), \n                       linetype = 2, linewidth = 0.75, color = \"blue\")\n   p2 <- p + scale_x_continuous(breaks = c(0, 5, 10, 15, 20, 25, 30, 35, 40)) + xlim(0, 40)\n\nThen we use the magical package patchwork to combine the plots:\n\n   # install.packages(\"patchwork\")\n   library(patchwork)\n   p <- p1 / p2\n   p\n\n\n\n\nThe data clearly shows that while both distributions are skewed, the indegree distribution is more heterogeneous, with a larger proportion of nodes in the high end of receiving advice as compared to giving advice.\nNote also that since the mean degree is the same regardless of whether we use the out or indegree distribution, then the blue line pointing to the mean degree falls in the same spot on the x-axis for both plots."
  },
  {
    "objectID": "inoutdegdist.html#in-and-out-degree-correlations",
    "href": "inoutdegdist.html#in-and-out-degree-correlations",
    "title": "Graph Degree Metrics in Directed Networks",
    "section": "In and Out Degree Correlations",
    "text": "In and Out Degree Correlations\nThe same doubling (really quadrupling) happens to degree correlations in directed graphs. While in an undirected graph, there is a single degree correlation, in the directed case we have four quantities to compute: The out-out degree correlation, the in-in degree correlation, the out-in degree correlation, and the in-out degree correlation (see here, p. 38).\nTo proceed, we need to create an edge list data set with six columns: The node id of the “from” node, the node id of the “to” node, the indegree of the “from” node, the outdegree of the “from” node, the indegree of the “to” node, and the outdegree of the “to” node.\nWe can adapt the code we used for the undirected case for this purpose. First, we create an edge list data frame using the igraph function as_data_frame:\n\n   library(dplyr)\n   g.el <- igraph::as_data_frame(g) %>% \n      rename(fr = from)\n   head(g.el)\n\n  fr to\n1  1  2\n2  1 17\n3  1 20\n4  2  1\n5  2  6\n6  2 17\n\n\nNote that we have to specify that this is in an igraph function by typing igraph:: in front of the as_data_frame because there is an (older) dplyr function with the same name that was used for data wrangling.\nSecond, we create data frames containing the in and outdegrees of each node in the network:\n\n    deg.dat.fr <- data.frame(fr = 1:vcount(g), o.d, i.d)\n    deg.dat.to <- data.frame(to = 1:vcount(g), o.d, i.d)\n\nThird, we merge this info into the edge list data frame to get the in and outdegrees of the from and to nodes in the directed edge:\n\n   d.el <- g.el %>% \n      left_join(deg.dat.fr) %>% \n      rename(o.d.fr = o.d, i.d.fr = i.d) %>% \n      left_join(deg.dat.to, by = \"to\") %>% \n      rename(o.d.to = o.d, i.d.to = i.d) \n   head(d.el)\n\n  fr to o.d.fr i.d.fr o.d.to i.d.to\n1  1  2      3     22      7     23\n2  1 17      3     22     21     26\n3  1 20      3     22     11     22\n4  2  1      7     23      3     22\n5  2  6      7     23      0     21\n6  2 17      7     23     21     26\n\n\nNow we can compute the four different flavors of the degree correlation for directed graphs:\n\n   round(cor(d.el$o.d.fr, d.el$o.d.to), 4) #out-out correlation\n\n[1] -0.0054\n\n   round(cor(d.el$i.d.fr, d.el$i.d.to), 4) #in-in correlation\n\n[1] 0.187\n\n   round(cor(d.el$i.d.fr, d.el$o.d.to), 4) #in-out correlation\n\n[1] -0.0283\n\n   round(cor(d.el$o.d.fr, d.el$i.d.to), 4) #out-in correlation\n\n[1] -0.0843\n\n\nThese results tell us that there is not much degree assortativity going on in the law advice network, except for a slight tendency of people who receive advice from lots of others to give advice to people who also receive advice from a lot of other people (the “in-in” correlation)\nNote that by default, the assortativity_degree function in igraph only returns the out-in correlation for directed graphs:\n\n   round(assortativity_degree(g, directed = TRUE), 4)\n\n[1] -0.0843\n\n\nThat is, assortativity_degree checks if more active senders are more likely to send ties to people who are popular receivers of ties."
  },
  {
    "objectID": "schedule-208A-F24.html",
    "href": "schedule-208A-F24.html",
    "title": "SOCIOL 208A Reading Schedule (Fall 2024)",
    "section": "",
    "text": "Prell, C. & Schaefer, D. R. (2023). Introducing Social Network Analysis. In J. McLevey, J. Scott, P. J. Carrington (Eds.) The SAGE Handbook of Social Network Analysis. Sage Publications. link\nLight, R. & Moody, J. (2021). Network Basics: Points, Lines, and Positions. In R. Light, and J. Moody (Eds.) The Oxford Handbook of Social Networks Oxford University Press. link\nHarary, F. & Norman., R. Z. (1953). Graph Theory as a Mathematical Model in Social Science. Research Center for Group Dynamics, University of Michigan. link\n\n\n\n\n\nBasic Introduction to R\nThe Basics of the R Programming Language Short Intergraph Tutorial\n\nPackage networkdata"
  },
  {
    "objectID": "schedule-208A-F24.html#week-2-october-9-centrality",
    "href": "schedule-208A-F24.html#week-2-october-9-centrality",
    "title": "SOCIOL 208A Reading Schedule (Fall 2024)",
    "section": "Week 2, October 9: Centrality",
    "text": "Week 2, October 9: Centrality\n\nReadings\n\nMartin G. Everett & Steve P. Borgatti (2023). “Centrality.” In J. McLevey, J. Scott, P. J. Carrington (Eds.) The SAGE Handbook of Social Network Analysis. Sage Publications. link\nFreeman, L. C. (1978). Centrality in Social Networks Conceptual Clarification. Social Networks, 1(3), 215-239. pdf\nKoschützki, D., Lehmann, K.A., Peeters, L., Richter, S., Tenfelde-Podehl, D., Zlotowski, O. (2005). Centrality Indices. In: Brandes, U., Erlebach, T. (eds) Network Analysis. Lecture Notes in Computer Science, vol 3418. Springer, Berlin, Heidelberg (secs. 3.2, 3.3, and 3.4). link\nNeal, Z. P. (2014). A network perspective on the processes of empowered organizations. American Journal of Community Psychology, 53, 407-418. https://doi.org/10.1007/s10464-013-9623-1\nAgneessens, F., Borgatti, S. P., & Everett, M. G. (2017). Geodesic based centrality: Unifying the local and the global. Social Networks, 49, 12-26. link\n\n\n\nExplainers\n\nLizardo, O. (n.d). Centralities based on Degree. link\nLizardo, O. (n.d). Centralities based on the Geodesic Distance. link\nLizardo, O. (n.d). Centralities based on Shortest Paths. link\n\n\n\nOther Material & Further Reading\n\nBorgatti, S. P., & Everett, M. G. (2006). A graph-theoretic perspective on centrality. Social Networks, 28(4), 466-484. link\nBrandes, U., Borgatti, S. P., & Freeman, L. C. (2016). Maintaining the duality of closeness and betweenness centrality. Social networks, 44, 153-159. link\nRossman, G., Esparza, N., & Bonacich, P. (2010). I’d Like To Thank The Academy, Team Spillovers, and Network Centrality. American Sociological Review, 75(1), 31-51. link\nKoschützki, D., Lehmann, K.A., Tenfelde-Podehl, D., Zlotowski, O. (2005). Advanced Centrality Concepts. In: Brandes, U., Erlebach, T. (eds) Network Analysis. Lecture Notes in Computer Science, vol 3418. Springer, Berlin, Heidelberg. link\nComprehensive list of centrality measures with formulas and software"
  },
  {
    "objectID": "schedule-208A-F24.html#week-3-october-16-status-and-prestige",
    "href": "schedule-208A-F24.html#week-3-october-16-status-and-prestige",
    "title": "SOCIOL 208A Reading Schedule (Fall 2024)",
    "section": "Week 3, October 16: Status and Prestige",
    "text": "Week 3, October 16: Status and Prestige\n\nLizardo, O. (n.d.) Status link\nFranceschet, M. (2011). PageRank: standing on the shoulders of giants. Communications of the ACM, 54(6), 92-101. link\nMartin, J. L. & Murphy, J. P. (2021). Networks, Status, and Inequality. In R. Light, and J. Moody (Eds.) The Oxford Handbook of Social Networks Oxford University Press. link\nKoschützki, D., Lehmann, K.A., Peeters, L., Richter, S., Tenfelde-Podehl, D., Zlotowski, O. (2005). Centrality Indices. In: Brandes, U., Erlebach, T. (eds) Network Analysis. Lecture Notes in Computer Science, vol 3418. Springer, Berlin, Heidelberg (sec. 3.9). link\n\n\nFurther (Mathy) Reading\n\nVigna, S. (2016). Spectral ranking. Network Science, 4(4), 433-445. pdf\nBaltz, A., Kliemann, L. (2005). Spectral Analysis. In: Brandes, U., Erlebach, T. (eds) Network Analysis. Lecture Notes in Computer Science, vol 3418. Springer, Berlin, Heidelberg. link\nBonacich, P. (1972). Factoring and Weighting Approaches to Status Scores and Clique Identification. Journal of Mathematical Sociology, 2(1), 113-120. pdf\nKatz, L. (1953). A New Status Index Derived from Sociometric Analysis. Psychometrika, 18(1), 39-43. pdf"
  },
  {
    "objectID": "schedule-208A-F24.html#week-4-october-23",
    "href": "schedule-208A-F24.html#week-4-october-23",
    "title": "SOCIOL 208A Reading Schedule (Fall 2024)",
    "section": "Week 4, October 23:",
    "text": "Week 4, October 23:\n\nNo Class (Traveling)"
  },
  {
    "objectID": "schedule-208A-F24.html#week-5-october-30-similarity-roles-and-positions",
    "href": "schedule-208A-F24.html#week-5-october-30-similarity-roles-and-positions",
    "title": "SOCIOL 208A Reading Schedule (Fall 2024)",
    "section": "Week 5, October 30: Similarity, Roles, and Positions",
    "text": "Week 5, October 30: Similarity, Roles, and Positions\n\nReadings\n\nBurt, R. S. (1976). Positions in networks. Social Forces, 55(1), 93-122. link\nBreiger, R. L., Boorman, S. A., & Arabie, P. (1975). An algorithm for clustering relational data with applications to social network analysis and comparison with multidimensional scaling. Journal of Mathematical Psychology, 12(3), 328-383. link\nLü, L., Jin, C. H., & Zhou, T. (2009). Similarity index based on local paths for link prediction of complex networks. Physical Review E—Statistical, Nonlinear, and Soft Matter Physics, 80(4), 046122. link\nJeh, G., & Widom, J. (2002). Simrank: a measure of structural-context similarity. In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 538-543). link\nLeicht, E. A., Holme, P., & Newman, M. E. (2006). Vertex similarity in networks. Physical Review E—Statistical, Nonlinear, and Soft Matter Physics, 73(2), 026120. link\n\n\n\nFurther Reading\n\nFouss, F., Pirotte, A., Renders, J. M., & Saerens, M. (2007). Random-walk computation of similarities between nodes of a graph with application to collaborative recommendation. IEEE Transactions on knowledge and data engineering, 19(3), 355-369. link\nKovács, B. (2010). A generalized model of relational similarity. Social Networks, 32(3), 197-211. link\nLiben-Nowell, D., & Kleinberg, J. (2003). The link prediction problem for social networks. In Proceedings of the Twelfth Annual ACM International Conference on Information and Knowledge Management (CIKM’03) (pp. 556-559). link to longer paper\nLü, L., & Zhou, T. (2011). Link prediction in complex networks: A survey. Physica A: statistical mechanics and its applications, 390(6), 1150-1170. link\n\n\n\nCheat Sheet:\n\nChroł, B & Bojanowski, M. (2018). Proximity-based Methods for Link Prediction. https://cran.r-project.org/web/packages/linkprediction/vignettes/proxfun.html"
  },
  {
    "objectID": "schedule-208A-F24.html#week-6-november-6-subgroups-and-communities",
    "href": "schedule-208A-F24.html#week-6-november-6-subgroups-and-communities",
    "title": "SOCIOL 208A Reading Schedule (Fall 2024)",
    "section": "Week 6, November 6: Subgroups and Communities",
    "text": "Week 6, November 6: Subgroups and Communities\n\nReadings\n\nMoody, J., & Mucha, P. J. (2023). Structural Cohesion and Cohesive Groups. In J. McLevey, J. Scott, P. J. Carrington (Eds.) The SAGE Handbook of Social Network Analysis. Sage Publications. link\nShai, S., Stanley, N., Granell, C., Taylor, D. & Mucha, P. J. (2021). Case Studies in Network Community Detection. In R. Light, and J. Moody (Eds.) The Oxford Handbook of Social Networks Oxford University Press. link\nGirvan, M., & Newman, M. E. (2002). Community Structure in Social and Biological Networks. Proceedings of the National academy of Sciences, 99(12), 7821-7826. link\nNewman, M. E., & Girvan, M. (2004). Finding and evaluating community structure in networks. Physical review E, 69(2), 026113. link\nLeicht, E. A., and Newman, M. E. J. (2008). Community Structure in Directed Networks. Physical Review Letters 100, 118703. link\nNewman, M. E. “Mixing patterns in networks.” Physical review E 67, no. 2 (2003): 026126. link\n\n\n\nFurther Reading\n\nClauset, A., Newman, M. E., & Moore, C. (2004). Finding community structure in very large networks. Physical Review E—Statistical, Nonlinear, and Soft Matter Physics, 70(6), 066111. link\nFortunato, S., & Castellano, C. (2007). Community structure in graphs. arXiv preprint arXiv:0712.2716. link\nNewman, M. E., & Girvan, M. (2003). Mixing patterns and community structure in networks. In Statistical mechanics of complex networks (pp. 66-87). Berlin, Heidelberg: Springer Berlin Heidelberg.\nNewman, M. E. (2006). Modularity and Community Structure in Networks. Proceedings of the National Academy of Sciences, 103(23), 8577-8582. link"
  },
  {
    "objectID": "schedule-208A-F24.html#week-7-november-13-analyzing-two-mode-networks",
    "href": "schedule-208A-F24.html#week-7-november-13-analyzing-two-mode-networks",
    "title": "SOCIOL 208A Reading Schedule (Fall 2024)",
    "section": "Week 7, November 13: Analyzing Two-Mode Networks",
    "text": "Week 7, November 13: Analyzing Two-Mode Networks\n\nReadings\n\nBreiger, R. L. (1974). The Duality of Persons and Groups. Social Forces, 53(2), 181–190. link\nBorgatti, S. P., & Everett, M. G. (1997). Network Analysis of 2-Mode Data. Social Networks, 19(3), 243-269. pdf\nEverett, M. G., & Borgatti, S. P. (2013). The Dual-Projection Approach for Two-Mode Networks. Social Networks, 35(2), 204-210. link\nNeal, Z. (2014). The backbone of bipartite projections: Inferring relationships from co-authorship, co-sponsorship, co-attendance and other co-behaviors. Social Networks, 39, 84-97. link\n\n\n\nOther Material\n\nDomagalski, R., Neal, Z. P., & Sagan, B. (2021). Backbone: An R package for extracting the backbone of bipartite projections. Plos one, 16(1), e0244363. link\nNeal, Z. P. (2022). backbone: An R package to extract network backbones. PloS one, 17(5), e0269137. link"
  },
  {
    "objectID": "schedule-208A-F24.html#week-8-november-20-ego-networks",
    "href": "schedule-208A-F24.html#week-8-november-20-ego-networks",
    "title": "SOCIOL 208A Reading Schedule (Fall 2024)",
    "section": "Week 8, November 20: Ego Networks",
    "text": "Week 8, November 20: Ego Networks\n\nReadings\n\nSmith, J. A. (2021). The Continued Relevance of Ego Network Data. In R. Light, and J. Moody (Eds.) The Oxford Handbook of Social Networks Oxford University Press. link"
  },
  {
    "objectID": "schedule-208A-F24.html#week-9-november-27-statistical-models-of-network-structure",
    "href": "schedule-208A-F24.html#week-9-november-27-statistical-models-of-network-structure",
    "title": "SOCIOL 208A Reading Schedule (Fall 2024)",
    "section": "Week 9, November 27: Statistical Models of Network Structure",
    "text": "Week 9, November 27: Statistical Models of Network Structure\n\nReadings\n\nLusher D., Wang, P., Brennecke, J., Brailly J., Faye, M., Gallagher, C. (2021). Advances in Exponential Random Graph Models. In R. Light, and J. Moody (Eds.) The Oxford Handbook of Social Networks Oxford University Press. link"
  },
  {
    "objectID": "schedule-208A-F24.html#week-10-december-4-dynamic-networks-and-relational-events",
    "href": "schedule-208A-F24.html#week-10-december-4-dynamic-networks-and-relational-events",
    "title": "SOCIOL 208A Reading Schedule (Fall 2024)",
    "section": "Week 10, December 4: Dynamic Networks and Relational Events",
    "text": "Week 10, December 4: Dynamic Networks and Relational Events\n\nTBA"
  },
  {
    "objectID": "schedule-208B-S22.html",
    "href": "schedule-208B-S22.html",
    "title": "SOCIOL 208B Reading Schedule (Spring 2022)",
    "section": "",
    "text": "Marsden, P. V., & Laumann, E. O. (1984). Mathematical Ideas In Social Structural Analysis. Journal of Mathematical Sociology, 10(3-4), 271-294. [pdf]\nWellman, B. (1988). Structural Analysis: From Method and Metaphor To Theory and Substance. In B. Wellman & S. D. Berkowitz (Eds.), Social Structures: A Network Approach. (Pp. 19–61). Cambridge University Press. [pdf]\nEmirbayer, M. (1997). Manifesto For A Relational Sociology. American Journal of Sociology, 103(2), 281–317. [pdf]\n\n\n\n\n\nErikson, E. (2013). Formalist and Relationalist Theory In Social Network Analysis. Sociological Theory, 31(3), 219–242. [pdf]\nEmirbayer, M., & Goodwin, J. (1994). Network Analysis, Culture, and The Problem of Agency. American Journal of Sociology, 99(6), 1411–1454. [pdf]\nMische, A. (2011). Relational Sociology, Culture, and Agency. In J. Scott & P. J. Carrington (Eds.), The Sage Handbook of Social Network Analysis (Pp. 80–97). Sage. [pdf]"
  },
  {
    "objectID": "schedule-208B-S22.html#week-2-the-greatest-hits",
    "href": "schedule-208B-S22.html#week-2-the-greatest-hits",
    "title": "SOCIOL 208B Reading Schedule (Spring 2022)",
    "section": "Week 2: The Greatest Hits",
    "text": "Week 2: The Greatest Hits\n\nTue., Apr. 5\n\nWhite, H. C., Boorman, S. A., & Breiger, R. L. (1976). Social Structure From Multiple Networks. I. Blockmodels of Roles and Positions. American Journal of Sociology, 81(4), 730–780. [pdf]\nBreiger, R. L. (1974). The Duality of Persons and Groups. Social Forces, 53(2), 181–190. [pdf]\nFreeman, L. C. (1978). Centrality in social networks conceptual clarification. Social networks, 1(3), 215-239. [pdf]\n\n\n\nThu., Apr. 7\n\nGranovetter, M. S. (1973). The Strength of Weak Ties. American Journal of Sociology, 78(3), 1360–1380. [pdf]\n\nRejection letter from American Sociological Review of the first (1969) version of the paper [pdf]\nGranovetter, M. S. (1969) ``Alienation Reconsidered: The Strength of Weak Ties.’’ Reprinted in Connections 5(2): 4-16. [pdf]\n\nCentola, D., & Macy, M. (2007). Complex Contagions and the Weakness of Long Ties. American Journal of Sociology, 113(3), 702-734. [pdf]\nMcPherson, M., Smith-Lovin, L., & Cook, J. M. (2001). Birds of a feather: Homophily in social networks. Annual Review of Sociology, 27(1), 415-444. [pdf]"
  },
  {
    "objectID": "schedule-208B-S22.html#week-3-brokerage-and-intermediation",
    "href": "schedule-208B-S22.html#week-3-brokerage-and-intermediation",
    "title": "SOCIOL 208B Reading Schedule (Spring 2022)",
    "section": "Week 3: Brokerage and Intermediation",
    "text": "Week 3: Brokerage and Intermediation\n\nTue., Apr. 12\n\nGould, R. V., & Fernandez, R. M. (1989). Structures of Mediation: A Formal Approach To Brokerage In Transaction Networks. Sociological Methodology, 89-126. [pdf]\nObstfeld, D., Borgatti, S.P. and Davis, J. (2014). Brokerage As a Process: Decoupling Third Party Action From Social Network Structure. Research In The Sociology of Organizations, 40), 135-159. [pdf]\nStovel, K., & Shaw, L. (2012). Brokerage. Annual Review of Sociology, 38, 139-158. [pdf]\n\n\n\nThu., Apr. 14\n\nBurt, R. S. (2004). Structural Holes and Good Ideas. American Journal of Sociology, 110(2), 349-399.[pdf]\nGoldberg, A., Srivastava, S. B., Manian, V. G., Monroe, W., & Potts, C. (2016). Fitting In Or Standing Out? The Tradeoffs of Structural and Cultural Embeddedness. American Sociological Review, 81(6), 1190-1222. [pdf]\nAral, S., & Van Alstyne, M. (2011). The diversity-bandwidth trade-off. American Journal of Sociology, 117(1), 90-171. [pdf]"
  },
  {
    "objectID": "schedule-208B-S22.html#week-4-networks-in-science",
    "href": "schedule-208B-S22.html#week-4-networks-in-science",
    "title": "SOCIOL 208B Reading Schedule (Spring 2022)",
    "section": "Week 4: Networks In Science",
    "text": "Week 4: Networks In Science\n\nTue., Apr. 19\n\nMoody, J. (2004). The Structure of A Social Scientific Collaboration Network. American Sociological Review, 69, 213-238. [pdf]\nShwed, U., & Bearman, P. S. (2010). The Temporal Structure of Scientific Consensus Formation. American Sociological Review, 75(6), 817-840. [pdf]\nMoody, J., & Light, R. (2006). A view from above: The evolving sociological landscape. The American Sociologist, 37(2), 67-86. [pdf]\n\n\n\nThu., Apr. 21\n\nFoster, J. G., Rzhetsky, A., & Evans, J. A. (2015). Tradition and Innovation In Scientists’ Research Strategies. American Sociological Review, 80(5), 875-908. [pdf]\nMcmahan, P., & Mcfarland, D. A. (2021). Creative Destruction: The Structural Consequences of Scientific Curation. American Sociological Review, 86(2), 341-376. [pdf]"
  },
  {
    "objectID": "schedule-208B-S22.html#week-5-collaboration-creativity-and-field-dynamics",
    "href": "schedule-208B-S22.html#week-5-collaboration-creativity-and-field-dynamics",
    "title": "SOCIOL 208B Reading Schedule (Spring 2022)",
    "section": "Week 5: Collaboration, Creativity, and Field Dynamics",
    "text": "Week 5: Collaboration, Creativity, and Field Dynamics\n\nTue., Apr. 26\n\nVedres, B., & Stark, D. (2010). Structural Folds: Generative Disruption In Overlapping Groups. American Journal of Sociology, 115(4), 1150-1190. [pdf]\nUzzi, B, & Spiro, J. (2005). Collaboration and Creativity: The Small World Problem. American Journal of Sociology, 111, 447-504. [pdf]\n\n\n\nThu., Apr. 28\n\nPowell, W. W., White, D. R., Koput, K. W., & Owen-Smith, J. (2005). Network Dynamics and Field Evolution: The Growth of Interorganizational Collaboration In The Life Sciences. American Journal of Sociology, 110(4), 1132-1205. [pdf]\nRossman, G., Esparza, N., & Bonacich, P. (2010). I’d Like To Thank The Academy, Team Spillovers, and Network Centrality. American Sociological Review, 75(1), 31-51. [pdf]"
  },
  {
    "objectID": "schedule-208B-S22.html#week-6-networks-and-culture-and-culture-in-networks",
    "href": "schedule-208B-S22.html#week-6-networks-and-culture-and-culture-in-networks",
    "title": "SOCIOL 208B Reading Schedule (Spring 2022)",
    "section": "Week 6: Networks and Culture and Culture in Networks",
    "text": "Week 6: Networks and Culture and Culture in Networks\n\nTue., May. 3\n\nBreiger, R. L. (2010). Dualities of culture and structure: Seeing through cultural holes. Pp. 37-47 in Relationale soziologie. VS Verlag für Sozialwissenschaften. [pdf]\nLizardo, 0. (2023). Culture and Networks. Pp. 188-201 in The SAGE Handbook of Social Network Analysis. SAGE Publications Ltd. [pdf]\nLewis, K., & Kaufman, J. (2018). The Conversion of Cultural Tastes Into Social Network Ties. American Journal of Sociology, 123(6), 1684-1742. [pdf]\n\n\n\nThu., May. 5\n\nFuhse, J. A. (2009). The Meaning Structure of Social Networks. Sociological Theory, 27(1), 51-73. [pdf]\nIkegami, E. (2000). A Sociological Theory of Publics: Identity and Culture As Emergent Properties In Networks. Social Research, 989-1029. [pdf]\nMützel, S., & Breiger, R. (2021). Duality Beyond Persons and Groups. In The Oxford Handbook of Social Networks. Oxford University Press. [pdf]"
  },
  {
    "objectID": "schedule-208B-S22.html#week-7-difussion-in-networks",
    "href": "schedule-208B-S22.html#week-7-difussion-in-networks",
    "title": "SOCIOL 208B Reading Schedule (Spring 2022)",
    "section": "Week 7: Difussion In Networks",
    "text": "Week 7: Difussion In Networks\n\nTue., May. 10\n\nDellaposta, D., Shi, Y., & Macy, M. (2015) Why Do Liberals Drink Lattes?. American Journal of Sociology, 120(5), 1473-1511. [pdf]\nCentola, D. (2015). The social origins of networks and diffusion. American Journal of Sociology, 120(5), 1295-1338. [pdf]\nBecker, S. O., Hsiao, Y., Pfaff, S., & Rubin, J. (2020). Multiplex Network Ties and the Spatial Diffusion of Radical Innovations: Martin Luther’s Leadership In The Early Reformation. American Sociological Review, 85(5), 857-894. [pdf]\n\n\n\nThu., May. 12\n\nGoldberg, A., & Stein, S. K. (2018). Beyond Social Contagion: Associative Diffusion and The Emergence of Cultural Variation. American Sociological Review, 83(5), 897-932. [pdf]\nBail, C. A., Brown, T. W., & Mann, M. (2017). Channeling hearts and minds: Advocacy organizations, cognitive-emotional currents, and public conversation. American Sociological Review, 82(6), 1188-1213. [pdf]"
  },
  {
    "objectID": "schedule-208B-S22.html#week-8-networks-in-history",
    "href": "schedule-208B-S22.html#week-8-networks-in-history",
    "title": "SOCIOL 208B Reading Schedule (Spring 2022)",
    "section": "Week 8: Networks In History",
    "text": "Week 8: Networks In History\n\nTue., May. 17\n\nPadgett, J. F., & Ansell, C. K. (1993). Robust Action and the Rise of the Medici, 1400-1434. American Journal of Sociology, 98(6), 1259–1319. [pdf]\nGould, R. V. (1991). Multiple Networks and Mobilization In The Paris Commune, 1871. American Sociological Review, 56(6), 716-729. [pdf]\nErikson, E., & Feltham, E. (2021). Historical Network Research. In The Oxford Handbook of Social Networks. Oxford University Press. [pdf]\n\n\n\nThu., May. 19\n\nErikson, E., & Bearman, P. (2006). Malfeasance and the Foundations For Global Trade: The Structure of English Trade In The East Indies, 1601–1833. American Journal of Sociology, 112(1), 195-230. [pdf]\nBearman, P., Faris, R., & Moody, J. (1999). Blocking The Future: New Solutions For Old Problems In Historical Social Science. Social Science History, 23(4), 501-533. [pdf]"
  },
  {
    "objectID": "schedule-208B-S22.html#week-9-networks-and-inequality",
    "href": "schedule-208B-S22.html#week-9-networks-and-inequality",
    "title": "SOCIOL 208B Reading Schedule (Spring 2022)",
    "section": "Week 9: Networks and Inequality",
    "text": "Week 9: Networks and Inequality\n\nTue., May. 24\n\nGould, R. V. (2002). The Origins of Status Hierarchies: A Formal Theory and Empirical Test. American Journal of Sociology, 107(5), 1143-1178. [pdf]\nGondal, N. (2015). Inequality preservation through uneven diffusion of Cultural materials across stratified groups. Social Forces, 93(3), 1109-1137. [pdf]\nThomas, R. J., & Mark, N. P. (2013). Population size, network density, and the emergence of inherited inequality. Social Forces, 92(2), 521-544. [pdf]\n\n\n\nThu., May. 26\n\nBurris, V. (2004). The Academic Caste System: Prestige Hierarchies In PhD Exchange Networks. American Sociological Review, 69, 239-264. [pdf]\nFowler, J. H., Grofman, B., & Masuoka, N. (2007). Social networks in political science: Hiring and placement of Ph.Ds, 1960–2002. PS: Political Science & Politics, 40(4), 729-739. [pdf]\nClauset, A., Arbesman, S., & Larremore, D. B. (2015). Systematic inequality and hierarchy in faculty hiring networks. Science Advances, 1(1), e1400005. [pdf]\nGondal, N. (2018). Duality of departmental specializations and PhD exchange: A Weberian analysis of status in interaction using multilevel exponential random graph models (mERGM). Social Networks, 55, 202-212. [pdf]"
  },
  {
    "objectID": "schedule-208B-S22.html#week-10-networks-and-the-micromacro-link",
    "href": "schedule-208B-S22.html#week-10-networks-and-the-micromacro-link",
    "title": "SOCIOL 208B Reading Schedule (Spring 2022)",
    "section": "Week 10: Networks and The Micro/Macro Link",
    "text": "Week 10: Networks and The Micro/Macro Link\n\nTue., May. 31\n\nMcfarland, D. A., Moody, J., Diehl, D., Smith, J. A., & Thomas, R. J. (2014). Network Ecology and Adolescent Social Structure. American Sociological Review, 79(6), 1088-1121. [pdf]\nBearman, P. S., Moody, J., & Stovel, K. (2004). Chains of Affection: The Structure of Adolescent Romantic and Sexual Networks. American Journal of Sociology, 110(1), 44-91. [pdf]\n\n\n\nThu., Jun. 2\n\nLevy, B. L., Phillips, N. E., & Sampson, R. J. (2020). Triple Disadvantage: Neighborhood Networks of Everyday Urban Mobility and Violence In US Cities. American Sociological Review, 85(6), 925-956. [pdf]\nPapachristos, A. V. (2009). Murder By Structure: Dominance Relations and the Social Structure of Gang Homicide. American Journal of Sociology, 115(1), 74-128. [pdf]"
  },
  {
    "objectID": "schedule-208B-S24.html",
    "href": "schedule-208B-S24.html",
    "title": "SOCIOL 208B Reading Schedule (Spring 2024)",
    "section": "",
    "text": "Wellman, B. (1988). Structural Analysis: From Method and Metaphor To Theory and Substance. In B. Wellman & S. D. Berkowitz (Eds.), Social Structures: A Network Approach. (Pp. 19–61). Cambridge University Press.\nFreeman, L. (2004). The Development of Social Network Analysis. Vancouver, BC: Empirical Press (Introduction, and Chap. 9)\n\n\n\n\n\nEmirbayer, M. (1997). Manifesto For A Relational Sociology. American Journal of Sociology, 103(2), 281–317.\nErikson, E. (2013). Formalist and Relationalist Theory In Social Network Analysis. Sociological Theory, 31(3), 219–242.\nMische, A. (2011). Relational Sociology, Culture, and Agency. In J. Scott & P. J. Carrington (Eds.), The Sage Handbook of Social Network Analysis (Pp. 80–97). Sage."
  },
  {
    "objectID": "schedule-208B-S24.html#week-2-the-greatest-hits",
    "href": "schedule-208B-S24.html#week-2-the-greatest-hits",
    "title": "SOCIOL 208B Reading Schedule (Spring 2024)",
    "section": "Week 2: The Greatest Hits",
    "text": "Week 2: The Greatest Hits\n\nTuesday, April 9\n\nWhite, H. C., Boorman, S. A., & Breiger, R. L. (1976). Social Structure From Multiple Networks. I. Blockmodels of Roles and Positions. American Journal of Sociology, 81(4), 730–780.\nBreiger, R. L. (1974). The Duality of Persons and Groups. Social Forces, 53(2), 181–190.\nFreeman, L. C. (1978). Centrality in social networks conceptual clarification. Social networks, 1(3), 215-239.\n\n\n\nThursday, April 11\n\nGranovetter, M. S. (1973). The Strength of Weak Ties. American Journal of Sociology, 78(3), 1360–1380.\n\nRejection letter from American Sociological Review of the first (1969) version of the paper\nGranovetter, M. S. (1969) “Alienation Reconsidered: The Strength of Weak Ties.” Reprinted in Connections 5(2): 4-16.\n\nWang, D., & Uzzi, B. (2022). Weak ties, failed tries, and success. Science, 377(6612), 1256-1258.\nMcPherson, M., Smith-Lovin, L., & Cook, J. M. (2001). Birds of a feather: Homophily in social networks. Annual Review of Sociology, 27(1), 415-444."
  },
  {
    "objectID": "schedule-208B-S24.html#week-3-brokerage",
    "href": "schedule-208B-S24.html#week-3-brokerage",
    "title": "SOCIOL 208B Reading Schedule (Spring 2024)",
    "section": "Week 3: Brokerage",
    "text": "Week 3: Brokerage\n\nTuesday, April 16 (Virtual Meeting)\n\nGould, R. V., & Fernandez, R. M. (1989). Structures of Mediation: A Formal Approach To Brokerage In Transaction Networks. Sociological Methodology, 89-126.\nObstfeld, D., Borgatti, S.P. and Davis, J. (2014). Brokerage As a Process: Decoupling Third Party Action From Social Network Structure. Research In The Sociology of Organizations, 40), 135-159.\nStovel, K., & Shaw, L. (2012). Brokerage. Annual Review of Sociology, 38, 139-158.\n\n\n\nThursday, April 18 (Virtual Meeting)\n\nBurt, R. S. (2004). Structural Holes and Good Ideas. American Journal of Sociology, 110(2), 349-399.\nGoldberg, A., Srivastava, S. B., Manian, V. G., Monroe, W., & Potts, C. (2016). Fitting In Or Standing Out? The Tradeoffs of Structural and Cultural Embeddedness. American Sociological Review, 81(6), 1190-1222.\nAral, S. (2016). The future of weak ties. American Journal of Sociology, 121(6), 1931-1939."
  },
  {
    "objectID": "schedule-208B-S24.html#week-4-science",
    "href": "schedule-208B-S24.html#week-4-science",
    "title": "SOCIOL 208B Reading Schedule (Spring 2024)",
    "section": "Week 4: Science",
    "text": "Week 4: Science\n\nTuesday, April 23\n\nMoody, J. (2004). The Structure of A Social Scientific Collaboration Network. American Sociological Review, 69, 213-238.\nClauset, A., Arbesman, S., & Larremore, D. B. (2015). Systematic inequality and hierarchy in faculty hiring networks. Science Advances, 1(1), e1400005.\n\n\n\nThursday, April 25\n\nGondal, N. (2018). Duality of departmental specializations and PhD exchange: A Weberian analysis of status in interaction using multilevel exponential random graph models (mERGM). Social Networks, 55, 202-212.\nMcmahan, P., & Mcfarland, D. A. (2021). Creative Destruction: The Structural Consequences of Scientific Curation. American Sociological Review, 86(2), 341-376.\nShwed, U., & Bearman, P. S. (2010). The Temporal Structure of Scientific Consensus Formation. American Sociological Review, 75(6), 817-840."
  },
  {
    "objectID": "schedule-208B-S24.html#week-5-collaboration-creativity",
    "href": "schedule-208B-S24.html#week-5-collaboration-creativity",
    "title": "SOCIOL 208B Reading Schedule (Spring 2024)",
    "section": "Week 5: Collaboration & Creativity",
    "text": "Week 5: Collaboration & Creativity\n\nTuesday, April 30\n\nUzzi, B, & Spiro, J. (2005). Collaboration and Creativity: The Small World Problem. American Journal of Sociology, 111, 447-504.\nDe Vaan, M., Stark, D., & Vedres, B. (2015). Game changer: The topology of creativity. American Journal of Sociology, 120(4), 1144-1194.\n\n\n\nThursday, May 2\n\nRossman, G., Esparza, N., & Bonacich, P. (2010). I’d Like To Thank The Academy, Team Spillovers, and Network Centrality. American Sociological Review, 75(1), 31-51.\nChoi, Y., Ingram, P., & Han, S. W. (2023). Cultural breadth and embeddedness: The individual adoption of organizational culture as a determinant of creativity. Administrative Science Quarterly, 68(2), 429-464.\nSilver, D., Childress, C., Lee, M., Slez, A., & Dias, F. (2022). Balancing categorical conventionality in music. American Journal of Sociology, 128(1), 224-286."
  },
  {
    "objectID": "schedule-208B-S24.html#week-6-difussion",
    "href": "schedule-208B-S24.html#week-6-difussion",
    "title": "SOCIOL 208B Reading Schedule (Spring 2024)",
    "section": "Week 6: Difussion",
    "text": "Week 6: Difussion\n\nTuesday, May 7\n\nDellaPosta, D., Shi, Y., & Macy, M. (2015). Why do liberals drink lattes?. American Journal of Sociology, 120(5), 1473-1511.\nDellaPosta, D. (2020). Pluralistic collapse: The “oil spill” model of mass opinion polarization. American Sociological Review, 85(3), 507-536.\nGoldberg, A., & Stein, S. K. (2018). Beyond Social Contagion: Associative Diffusion and The Emergence of Cultural Variation. American Sociological Review, 83(5), 897-932.\n\n\n\nThursday, May 9\n\nCentola, D., & Macy, M. (2007). Complex Contagions and the Weakness of Long Ties. American Journal of Sociology, 113(3), 702-734.\nGondal, N. (2023). Diffusion of innovations through social networks: Determinants and implications. Sociology Compass, 17(5), e13084.\nBail, C. A., Brown, T. W., & Wimmer, A. (2019). Prestige, proximity, and prejudice: how Google search terms diffuse across the world. American Journal of Sociology, 124(5), 1496-1548."
  },
  {
    "objectID": "schedule-208B-S24.html#week-7-culture",
    "href": "schedule-208B-S24.html#week-7-culture",
    "title": "SOCIOL 208B Reading Schedule (Spring 2024)",
    "section": "Week 7: Culture",
    "text": "Week 7: Culture\n\nTuesday, May 14\n\nLizardo, O. (2023). Culture and Networks. Pp. 188-201 in The SAGE Handbook of Social Network Analysis. SAGE Publications Ltd.\nLewis, K., & Kaufman, J. (2018). The Conversion of Cultural Tastes Into Social Network Ties. American Journal of Sociology, 123(6), 1684-1742.\nRawlings, C. M., & Childress, C. (2023). The Polarization of Popular Culture: Tracing the Size, Shape, and Depth of the “Oil Spill”. Social Forces, soad150.\n\n\n\nThursday, May 16 (No Meeting)"
  },
  {
    "objectID": "schedule-208B-S24.html#week-8-history",
    "href": "schedule-208B-S24.html#week-8-history",
    "title": "SOCIOL 208B Reading Schedule (Spring 2024)",
    "section": "Week 8: History",
    "text": "Week 8: History\n\nTuesday, May 21 (Virtual Meeting)\n\nPadgett, J. F., & Ansell, C. K. (1993). Robust Action and the Rise of the Medici, 1400-1434. American Journal of Sociology, 98(6), 1259–1319.\nGould, R. V. (1991). Multiple Networks and Mobilization In The Paris Commune, 1871. American Sociological Review, 56(6), 716-729.\nBearman, P., Moody, J., & Faris, R. (2002). Networks and History. Complexity, 8(1), 61-71.\n\n\n\nThursday, May 23\n\nErikson, E., & Feltham E., (2021) Historical Network Research, Pp. 432–442 in R Light, and J Moody (eds), The Oxford Handbook of Social Networks. Oxford University Press.\nErikson, E., & Bearman, P. (2006). Malfeasance and the Foundations For Global Trade: The Structure of English Trade In The East Indies, 1601–1833. American Journal of Sociology, 112(1), 195-230.\nBecker, S. O., Hsiao, Y., Pfaff, S., & Rubin, J. (2020). Multiplex Network Ties and the Spatial Diffusion of Radical Innovations: Martin Luther’s Leadership In The Early Reformation. American Sociological Review, 85(5), 857-894."
  },
  {
    "objectID": "schedule-208B-S24.html#week-9-inequality",
    "href": "schedule-208B-S24.html#week-9-inequality",
    "title": "SOCIOL 208B Reading Schedule (Spring 2024)",
    "section": "Week 9: Inequality",
    "text": "Week 9: Inequality\n\nTuesday, May 28\n\nDiMaggio, P., & Garip, F. (2012). Network effects and social inequality. Annual Review of Sociology, 38, 93-118.\nPedulla, D. S., & Pager, D. (2019). Race and networks in the job search process. American Sociological Review, 84(6), 983-1012.\n\n\n\nThursday, May 30\n\nErikson, E., & Occhiuto, N. (2017). Social networks and macrosocial change. Annual Review of Sociology, 43, 229-248.\nZhang, J., & Centola, D. (2019). Social networks and health: New developments in diffusion, online and offline. Annual Review of Sociology, 45, 91-109.\nHofstra, B., Corten, R., Van Tubergen, F., & Ellison, N. B. (2017). Sources of segregation in social networks: A novel approach using Facebook. American Sociological Review, 82(3), 625-656."
  },
  {
    "objectID": "schedule-208B-S24.html#week-10-linking-micro-and-macro",
    "href": "schedule-208B-S24.html#week-10-linking-micro-and-macro",
    "title": "SOCIOL 208B Reading Schedule (Spring 2024)",
    "section": "Week 10: Linking Micro and Macro",
    "text": "Week 10: Linking Micro and Macro\n\nTuesday, June 4\n\nMcfarland, D. A., Moody, J., Diehl, D., Smith, J. A., & Thomas, R. J. (2014). Network Ecology and Adolescent Social Structure. American Sociological Review, 79(6), 1088-1121.\nBearman, P. S., Moody, J., & Stovel, K. (2004). Chains of Affection: The Structure of Adolescent Romantic and Sexual Networks. American Journal of Sociology, 110(1), 44-91.\n\n\n\nThursday, June 6\n\nLevy, B. L., Phillips, N. E., & Sampson, R. J. (2020). Triple Disadvantage: Neighborhood Networks of Everyday Urban Mobility and Violence In US Cities. American Sociological Review, 85(6), 925-956.\nPapachristos, A. V., & Bastomski, S. (2018). Connected in crime: the enduring effect of neighborhood networks on the spatial patterning of violence. American Journal of Sociology, 124(2), 517-568."
  },
  {
    "objectID": "syllabus-208A.html",
    "href": "syllabus-208A.html",
    "title": "208A Syllabus",
    "section": "",
    "text": "THIS IS A LIVE DOCUMENT CHECK REGULARLY FOR CHANGES"
  },
  {
    "objectID": "syllabus-208A.html#class-description",
    "href": "syllabus-208A.html#class-description",
    "title": "208A Syllabus",
    "section": "Class Description",
    "text": "Class Description\nThis class is an introductory graduate-level seminar focused on techniques in Social Network Analysis (SNA). The seminar covers the most common data analytic tasks that people engage in when analyzing “network data.” What is network data? What counts as network data is itself a point of contention—as we will see, for some people all data is network data—but let us say for the sake of this class that network data is data in which the unit of analysis is the relation or the interaction between at least two actors or objects, and the data come typically arranged in this “dyadic” form. At the end of the course, you will be familiar with (and will have acquired some practice) the basic techniques used to analyze social network data."
  },
  {
    "objectID": "syllabus-208A.html#course-content",
    "href": "syllabus-208A.html#course-content",
    "title": "208A Syllabus",
    "section": "Course Content",
    "text": "Course Content\n\nBasic SNA\nSo, what are the things that people usually do when they have network data? Well, they typically want to figure out basic statistics about the interaction system formed by the set of dyads in the data, where a dyad is any two pairs of actors (whether they are connected or not). This task requires computing basic network quantities like the number of nodes and the number of links between entities as well as more advanced statistics based on representing the network as a graph (like the average path length, number of components, etc., all notions we will cover in the first week of class).\n\n\nCentrality and Prestige\nThen come the various things that almost everyone is interested in computing when using network data to answer social science questions. Primarily, this includes measures and indices of a node’s position in the network (e.g., differentiating between more or less central or more or less prestigious nodes), which we will cover in weeks 2 and 3.\n\n\nClasses and Communities\nAfter taking a break in Week 4, we move to the common case of people wanting to see if the nodes in the network fall into definable clusters or classes, where the criterion for being in the same cluster is based on how they connect to other nodes. Here, we want to find clusters of nodes that are similar to one another by some graph theoretic criterion and partition the graph into clusters based on that criterion.\nWeek 6 is dedicated to the next thing we may want to do, and that is to see if we can uncover clumps of densely connected nodes in the network indicating some natural partition into subgroups or communities, defined as nodes that interact more among themselves than they do among those outside the group, leading to the myriad of group and community detection techniques designed to partition a graph into clusters based on the underlying connectivity structure.\n\n\nTwo-Mode and Ego Networks\nThe next two weeks are dedicated to the analysis of some pretty common “non-standard” types of network data (e.g., data that doesn’t use the dyadic relation between objects of the same type as the analytic unit). The first is ego networks, where we first sample a set of units (egos), and then within each ego, we sample a subset of their contacts (e.g., by asking the people who are their most important friends or figuring out the most frequent interaction partners). These data come closest to the traditional data in social science (a rectangular matrix of cases by variables), so various standard techniques—like regression—apply (with some twists).\nThe second type of non-standard network data comes in a two-mode network form, in which some sets of objects are linked to objects of a different set, but there is no data on the links between objects of the same set. Standard cases by variables data in surveys are two-mode data (people connect to variables), as is any web or archival data collecting memberships or interactions between persons and objects (like attendance at events or people buying books on Amazon). We will see that due to a neat mathematical trick, we can transform two-mode into standard dyadic network data and thus deploy the whole panoply of techniques we learned in weeks 1-6 (which means that we can do SNA on all types of data, not just network data, and therefore all data is network data).\n\n\nProbabilistic Models of Networks\nThe bulk of SNA assumes that the ties exist as recorded in the data. Recently (e.g., over the last two decades or so) developed approaches to social network analysis make the ties the dependent variable and thus see the observed network data as a realization of some stochastic process governing the probability that two objects will be linked and thus one that can be modeled statistically. We analyze the theory and methods behind this approach to thinking of network structure from the bottom up and also cover some models designed to treat networks as composed of “relational events” and thus model how events that link entities in networks evolve."
  },
  {
    "objectID": "syllabus-208A.html#requirements",
    "href": "syllabus-208A.html#requirements",
    "title": "208A Syllabus",
    "section": "Requirements",
    "text": "Requirements\nThere are three main requirements in the class. Participation (mainly attendance and contributions made during our seminar meetings), a short weekly data exercise, and a longer data analysis paper due at the end of the quarter.\n\nClass Attendance and Class Discussion (25% of grade)\nAttendance is required not optional. If you need to miss a class meeting please let me know beforehand. It is part of your professional socialization to commit to attending class meetings and to inform me when that’s not possible (if only as a point of courtesy). The informal part of participation will be gauged by your contribution to our class discussion in the form of questions, comments, suggestions, wonderings, problems.\n\n\nWeekly Data Analysis Exercises (25% of grade)\nThese will be short weekly assignments in which I will ask you to take a (small) social network data set of your choice and compute some of the basic statistics or implement some of the techniques that we covered the week before. They will be due on Sunday at the end of each week. What you submit will take the form of a file containing the code and results from your analysis (typically an R Markdown file). These will not be graded, but will just be counted as submitted or not submitted.\n\n\nFinal Data Analysis Paper (50% of grade)\nThe basic goal here is for you to end up with something that could be useful to you at the end of the day. Hopefully a basic data exercise that can be the basis of a longer substantive paper or as a standalone research note.\nThis will be a 2500 to 5000 word (single-spaced, Times New Roman Font, 12pt, 1in margins) write-up of a data analysis, including some type of network data and/or some kind of network analytic technique (to be discussed and cleared by me). The data source can be obtained from a public repository of network data, or it could be network data that you already have access to or collected yourself.\nIn the paper, you will describe the data, provide key network metrics, describe the data-analytic approach that you will use, and provide a summary of the key empirical patterns that you found, along with a brief conclusion. The paper should be written in the style of a “research note” focused on key empirical findings (not long theory windup or literature review).\nYou will submit an extended abstract of your final project, outlining your main research idea (e.g., data source and type of analysis) due on Sunday of week 6. This will be a one-page, single-spaced document with 12pt Times New Roman Font and 1in margins."
  },
  {
    "objectID": "syllabus-208B.html",
    "href": "syllabus-208B.html",
    "title": "208B Syllabus",
    "section": "",
    "text": "THIS IS A LIVE DOCUMENT CHECK REGULARLY FOR CHANGES"
  },
  {
    "objectID": "syllabus-208B.html#participation-50-of-grade",
    "href": "syllabus-208B.html#participation-50-of-grade",
    "title": "208B Syllabus",
    "section": "Participation (50% of grade)",
    "text": "Participation (50% of grade)\n\nWeekly Analytic Memo\nThe formal side of participation will come in the form of you submitting a short memo (500 to 1000 words) where you try to put two or more of the readings for that week in conversation with one another. The first memo will be due starting on week 2.\nMemo specifications:\n\nYou should pick at least one reading from the Tuesday meeting and at least one reading from the Thursday meeting as the focus of your memo.\nIn your memo you should feel free to raise questions or issues the readings brought up for you as well as any questions, problems, or weaknesses you identify in the argument or the analyses.\nYou may also feel free to connect the readings to other work you are familiar with, pointing to key points of commonality and difference. The main point of the memo is for me to see evidence of you thinking thought the material, as well as providing fodder for discussion during our meeting.\nThe memo will be due by 5p the day before our first class meeting of the week (that’s Monday) so that I have a chance to read it and comment on it. You will submit it via the assignments tool on Canvas.\n\n\n\nClass Attendance\nAttendance is required not optional. If you need to miss a class meeting please let me know before hand. It is part of your professional socialization to commit to attending class meetings and to inform me when that’s not possible (if only as a point of courtesy).\n\n\nClass Discussion\nThe informal part of participation will be gauged by your contribution to our class discussion. You can use the thoughts developed in your analytic memo as a take-off point for framing your contribution."
  },
  {
    "objectID": "syllabus-208B.html#paper-50-of-grade",
    "href": "syllabus-208B.html#paper-50-of-grade",
    "title": "208B Syllabus",
    "section": "Paper (50% of grade)",
    "text": "Paper (50% of grade)\nThe basic goal here is for you to end up with something that could be useful to you at the end of the day. As such, I’ll give you a set of options here, but if you none of these work, we can talk about something that can be customized for your needs and goals. So by the end of the course you will submit one of the following:\n\nDraft of a research paper.- This will be a 3500 to 9000 word (double-spaced, Times New Roman Font, 12pt, 1in margins) draft of a research paper. This paper will contain some kind of data analysis, involving networks broadly defined. It will include an introduction reviewing literature and setting up a research problem or question. It will then move on to a methods section describing your data and analytic approach, and will close with a discussion section summarizing key findings, outlining implications for substantive research and theory, and describing potential future work and extensions. The goal here is to come up with a draft of a paper that could one day be submitted to the various social science and sociology-oriented journal focused on substantive research, whether “generalist” (e.g., American Sociological Review) or “specialist” (e.g., Social Networks).\nDraft of a Conceptual Paper.- This will be a 5000 to 10000 word page (double-spaced, Times New Roman Font, 12pt, 1in margins) draft of a research paper. The paper will focus on a set of concepts, theoretical ideas, or overall perspectives for approaching the study of social life that are based, inspired, extend, or incorporate network ideas, network thinking, or network concepts and techniques (broadly defined). The goal here is to come up with a draft of a paper that could one day be submitted to the various social science and sociology-oriented journal focused on “theory.”\nExtended Literature Review Draft.- This will be a 10-15 page (double-spaced, Times New Roman Font, 12pt, 1in Margins) draft of a literature review of work done from a social network perspective on a topic of your interest. The paper will cover what has been done in the field so far, what the strengths and limitations of previous is, and will note gaps or opportunities for future work addressing those limitations or extending the literature to new substantive domains, perhaps linking previous work to the some of the stuff we will be reading in class.\nDraft of a Research Proposal.- This will be a 10 page (single-spaced, Times New Roman Font, 12pt, 1in margins, including references) draft of a research proposal for a project incorporating either network thinking, theories, or techniques that you plan to start in the near future. The proposal will include a background section reviewing previous work, noting their strengths and limitations, and pointing to gaps in the literature. It will also include an “approach” section describing your research project, your main research questions, the data gathering procedures you will use, and the data-analytic techniques you plan to implement once your data is collected. It will close with an implication sections describing what the contributions of your project will be and why it is relevant and important.\nData Analysis Exercise.- This will be a 2500 to 5000 word (single-spaced, Times New Roman Font, 12pt, 1in margins) write-up of a data analysis, including some type of network data and/or some kind of network analytic technique (to be discussed and cleared by me). The data source can be obtained from a public repository of network data, or it could be network data that you already have access to or collected yourself. In the paper, you will describe the data, provide key network metrics, describe the data-analytic approach that you will use, and provide a summary of the key empirical patterns that you found, along with a brief conclusion. The paper should be written in the style of a “research note” focused on key empirical findings (not long theory windup or literature review).\n\nWhatever you decide, you will submit an extended abstract of your final project, due on Friday of week 6. This will be a one-page, single-spaced document with 12pt Times New Roman Font and 1in margins."
  },
  {
    "objectID": "handout6.html#a-bag-of-links",
    "href": "handout6.html#a-bag-of-links",
    "title": "Community Structure",
    "section": "A bag of links",
    "text": "A bag of links\nAn intuitive way to understand the modularity is as follows. Imagine we have an idea of what the communities in a network are. This could be given by some special community partition method, our intuition, or some exogenous coloring on the nodes (e.g., as given by a node attribute like race, gender, position in the organization, etc.). The membership of each node in each community is thus stored in a vector, \\(C_i = k\\) if node \\(i\\) belongs to community \\(k\\).\nOur job is to decide whether that community partition is a good one. One way to proceed is to imagine that we take the network in question, and we throw each observed link into a bag. Then our modularity measure should give us an idea of the probability that if we drew a link at random, the nodes at the end of each link belong to same community (or have the same color if the communities are defined by exogenous attributes). If the probability is high, then the network has community structure. If the probability is no better than we would expect given a null model that says there is nothing going on in the network connectivity-wise except random chance, then the modularity should be low (and we decide that the partition we chose actually does not divide the network into meaningful communities).\nLet’s assume our bag of links is full of directed links and we draw a bunch of them at random. Let’s call the node at the starting end of the link \\(s\\) and the node at the destination end of the link \\(d\\). Then we can measure the modularity—let’s call it \\(Q\\)—of a given partition of the network into \\(m\\) clusters as follows:\n\\[\nQ <- \\sum_{k = 1}^m \\left[P(s \\in k \\land d \\in k) - P(s \\in k) \\times P(d \\in k) \\right]\n\\]\nIn this equation, \\(P(s \\in k \\land d \\in k)\\) is the probability that a link drawn at random has a source and a destination node that belong to the same community \\(k\\); \\(P(s \\in k)\\) is just the probability of drawing a link that has a starting node in community \\(k\\) (regardless of the membership of the destination node) and \\(P(d \\in k)\\) is the probability of drawing a link that has a destination node that belongs to community \\(k\\) (regardless of the membership fo the source node).\nIf you remember your elementary probability theory, you know that the joint probability of two events that are assumed to be independent of one another is just the product of their individual probabilities. So that means that \\(P(s \\in k) \\times P(d \\in k)\\) is the expected probability of finding that both the source and destination node are in the same community \\(k\\) in a network in which communities don’t matter for link formation (because the two probabilities are assumed to be independent).\nSo the formula for the modularity subtracts the observed probability of finding links with two nodes in the same community from what we would expect if communities didn’t matter. So it measures the extent to which we find within-community links in the network beyond what we would expect by chance. Obviously, the higher this number, the higher the deviation from chance is, and the more community structure there is in the network.\nLet’s move to a real example. Below are two plots of the advice network from the law_friends data from the networkdata package. The first shows the nodes colored by status in the firm (partners versus associates) and the other shows the nodes colored by gender. It is pretty evident that there is more community by status than by gender; that is, friendship nominations tend to go from people of a given rank to people of the same rank, which makes sense. So a measure of modularity should be higher when using status as our partition than when using gender.\n\n\n\n\n\nLaw Firm Friendship Network by Status\n\n\n\n\n\n\n\nLaw Firm Friendship Network by Gender\n\n\n\n\nLet’s see an example of the modularity computed using our “bag of links” framework. Below is a quick function that uses dplyr code to take a graph as input and return and edge list data frame containing the “community membership” of each node by some characteristic given by the second input into the function.\n\n   link.bag <- function(x, c) {\n      library(dplyr)\n      g.el <- data.frame(as_edgelist(x))\n      names(g.el) <- c(\"vi\", \"vj\")\n      comm.dat <- data.frame(vi = as.vector(V(x)), \n                             vj = as.vector(V(x)), \n                             c = as.vector(c))\n      el.temp <- data.frame(vj = g.el[, 2]) %>% \n         left_join(comm.dat, by = \"vj\") %>% \n         dplyr::select(c(\"vj\", \"c\")) %>% \n         rename(c2 = c) \n      d.el <- data.frame(vi = g.el[, 1]) %>% \n         left_join(comm.dat, by = \"vi\") %>% \n         dplyr::select(c(\"vi\", \"c\")) %>% \n         rename(c1 = c) %>% \n         cbind(el.temp)\n   return(d.el)\n   }\n\nSo if we wanted an edge list data frame containing each node’s status membership in the law_advice network, we would just type:\n\n   link.dat <- link.bag(g, V(g)$status)\n   head(link.dat)\n\n  vi c1 vj c2\n1  1  1  2  1\n2  1  1  4  1\n3  1  1  8  1\n4  1  1 17  1\n5  2  1 16  1\n6  2  1 17  1\n\n\nNote than an edge list is already a “bag of links” so we can compute the three probabilities we need to compute the modularity of a partition directly from the edge list data frame. Here’s a function that does this:\n\n   mod.Q1 <- function(x, c1 = 2, c2 = 4) {\n      Q <- 0\n      comms <- unique(x[, c1])\n      for (k in comms) {\n         e.same <- x[x[, c1] == k & x[, c2] == k, ]\n         e.sour <- x[x[, c1] == k, ]\n         e.dest <- x[x[, c2] == k, ]\n         e.total <- nrow(x)\n         p.same <- nrow(e.same)/e.total\n         p.sour <- nrow(e.sour)/e.total\n         p.dest <- nrow(e.dest)/e.total\n         Q <- Q + (p.same - (p.sour * p.dest))\n      }\n   return(Q)\n   }\n\nThis function takes the edge list data frame as input. Optionally, you can specify the two columns containing community membership info for the source and destination node in each link (in this case this happens to be the second and fourth columns). The probability of a link drawn randomly from the bag is just the number of links where the source and destination links belong to the same community (e.same, computed in line 5) divided by the total number of links (which is the number of rows in the edge list data frame), this ratio is computed in line 9 (p.same). The overall probability of a link containing a source node in community \\(k\\) is computed in line 10 (p.sour), and the overall probability of a link containing a destination node in community \\(k\\) is computed in line 11 (p.dest). The actual modularity is computed step by step by summation across levels of the community indicator variable in line 12 which is the sum of the difference between p.same and the product of p.same times p.same across all communities \\(m\\) (in this case \\(m = 2\\) as there are only two levels of status).\nSo if wanted to check if status was more powerful in structuring the community organization of the law_advice network than gender, we would just type:\n\n   mod.Q1(link.bag(g, V(g)$status))\n\n[1] 0.2715221\n\n   mod.Q1( link.bag(g, V(g)$gender))\n\n[1] 0.07495538\n\n\nWhich indeed confirms that status is a more powerful organizing community principle than gender in this network."
  },
  {
    "objectID": "handout6.html#modularity-from-the-adjancency-matrix",
    "href": "handout6.html#modularity-from-the-adjancency-matrix",
    "title": "Community Structure",
    "section": "Modularity from the Adjancency Matrix",
    "text": "Modularity from the Adjancency Matrix\nNote that while the “bag of links” idea is good to show the basic probabilistic principle behind the modularity in a network we can compute directly from the adjacency matrix without going through the edge list data frame construction step.\nA function that does this looks like:\n\nmod.Q2 <- function(x, c) {\n   A <- as.matrix(as_adjacency_matrix(x))\n   vol <- sum(A)\n   Q <- 0\n   for (k in unique(c)) {\n      A.sub <- A[which(c == k), which(c == k)]\n      vol.k <- sum(A.sub)\n      A.i <- A[which(c == k), ]\n      A.j <- A[, which(c == k)]\n      Q <- Q + ((vol.k/vol) - ((sum(A.i) * sum(A.j))/vol^2))\n      }\n   return(Q)\n   }\n\nThis function just takes a graph and a vector indicating the community membership of each node and returns the modularity as output. Like before the modularity is the difference in the probability of observing a within-community link between two nodes (given by the ratio of the number of links in the sub-adjacency matrix containing only within community-nodes—obtained in line 7—and the overall number of links in the adjacency matrix, computed in line 3) and the expected probability of a link between a source node with community membership \\(k\\) and a destination node with the same community membership.\nThis last quantity is given by the product of the sum links in a the sub-adjacency matrix with row nodes that belong to community \\(k\\) (the source nodes) and the number of links in the sub adjacency matrix with the column nodes belonging to community \\(k\\) (the destination nodes) divided by the square of the total number of links observed the network.\nIn formulese:\n\\[\\begin{equation}\nQ = \\sum_{k=1}^m \\left[\\frac{\\sum_{i \\in k} \\sum_{j \\in k} a_{ij}}{\\sum_i \\sum_j a_{ij}}  - \\frac{(\\sum_{i \\in k} \\sum_j a_{ij})(\\sum_i \\sum_{j \\in k} a_{ij})}{(\\sum_i \\sum_j a_{ij})^2} \\right]\n\\end{equation}\\]\nWhere \\(\\sum_i \\sum_ja_{ij}\\) is the sum of all the entries in the network adjacency matrix, \\(\\sum_{i \\in k} \\sum_{j \\in k} a_{ij}\\) is the sum of all the entries of the sub-adjacency matrix where both the row and column nodes come from community \\(k\\), \\(\\sum_{i \\in k} \\sum_j a_{ij}\\) is the sum of all the entries in the sub-adjacency matrix where only the row nodes come from community \\(k\\), and \\(\\sum_i \\sum_{j \\in k} a_{ij}\\) is the sum of all the entries in the sub-adjacency matrix where only the column nodes come from community \\(k\\).\nWe can easily see that this approach gives us the same answer as the bag of links version:\n\n   mod.Q2(g, V(g)$status)\n\n[1] 0.2715221\n\n   mod.Q2(g, V(g)$gender)\n\n[1] 0.07495538\n\n\nOf course, you don’t even have to use a custom function like the above, because igraph has one, called (you guessed it) modularity:\n\n   modularity(g, V(g)$status)\n\n[1] 0.2715221\n\n   modularity(g, V(g)$gender)\n\n[1] 0.07495538\n\n\nBut at least now you know what’s going on inside of it!"
  },
  {
    "objectID": "handout6.html#the-modularity-matrix",
    "href": "handout6.html#the-modularity-matrix",
    "title": "Community Structure",
    "section": "The Modularity Matrix",
    "text": "The Modularity Matrix\nObviously, the modularity wouldn’t be a famous method if it was just a way of measuring the goodness of a community partition produced by other methods. It itself can be used to find community partitions by using a method that somehow produces node partition that find the largest values that it can take in a graph.\nA useful tool in this quest is what is called the modularity matrix (\\(\\mathbf{B}\\)), which is defined as a variation on the adjacency matrix (\\(\\mathbf{A}\\)), with each cell \\(b_{ij}\\) taking having the value:\n\\[\nb_{ij} = a_{ij} - \\frac{k^{out}_ik^{in}_j}{\\sum_i\\sum_j a_{ij}}\n\\]\nWhere \\(k^{out}_i\\) is node \\(i\\)’s outdegree and \\(k^{in}_j\\) is node j’s indegree. Note that the modularity matrix has the same “observed minus expected” structure as the formulas for the modularity. In this case we compare whether we see a link from \\(i\\) to \\(j\\) as given by \\(a_{ij}\\) against the probability of observing a link in a graph in which nodes connect at random with probability proportional to their degrees (as given by the right-hand side fraction).\nA function to compute the modularity matrix by looping through every element of the adjacency matrix looks like:\n\n   mod.mat <- function(x){\n      A <- as.matrix(as_adjacency_matrix(x))\n      od <- rowSums(A) #outdegrees\n      id <- colSums(A) #indegrees\n      vol <- sum(A)\n      n <- nrow(A)\n      B <- matrix(0, n, n)\n      for (i in 1:n){\n         for (j in 1:n) {\n            B[i,j] <- A[i,j] - ((od[i]*id[j])/vol)\n         }\n      }\n   return(B)\n   }\n\nPeeking inside the resulting matrix:\n\n   round(mod.mat(g)[1:10, 1:10], 3)\n\n        [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]   [,8]   [,9]  [,10]\n [1,] -0.035  0.930 -0.028  0.902 -0.035 -0.014 -0.014  0.951 -0.098 -0.028\n [2,] -0.035 -0.070 -0.028 -0.098 -0.035 -0.014 -0.014 -0.049 -0.098 -0.028\n [3,]  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n [4,] -0.131  0.739  0.895 -0.366 -0.131 -0.052 -0.052 -0.183  0.634 -0.105\n [5,] -0.026 -0.052 -0.021 -0.073 -0.026 -0.010  0.990 -0.037 -0.073 -0.021\n [6,]  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n [7,] -0.017 -0.035  0.986 -0.049  0.983 -0.007 -0.007 -0.024 -0.049 -0.014\n [8,] -0.009 -0.017 -0.007 -0.024 -0.009 -0.003 -0.003 -0.012 -0.024 -0.007\n [9,] -0.052 -0.105 -0.042  0.854 -0.052 -0.021 -0.021 -0.073 -0.146 -0.042\n[10,] -0.122  0.756 -0.098 -0.341 -0.122 -0.049 -0.049  0.829  0.659 -0.098\n\n\nWhich gives us the same numbers we would have obtained using the igraph function modularity_matrix:\n\n   B <- modularity_matrix(g)\n   round(B[1:10, 1:10], 3)\n\n        [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]   [,8]   [,9]  [,10]\n [1,] -0.035  0.930 -0.028  0.902 -0.035 -0.014 -0.014  0.951 -0.098 -0.028\n [2,] -0.035 -0.070 -0.028 -0.098 -0.035 -0.014 -0.014 -0.049 -0.098 -0.028\n [3,]  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n [4,] -0.131  0.739  0.895 -0.366 -0.131 -0.052 -0.052 -0.183  0.634 -0.105\n [5,] -0.026 -0.052 -0.021 -0.073 -0.026 -0.010  0.990 -0.037 -0.073 -0.021\n [6,]  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n [7,] -0.017 -0.035  0.986 -0.049  0.983 -0.007 -0.007 -0.024 -0.049 -0.014\n [8,] -0.009 -0.017 -0.007 -0.024 -0.009 -0.003 -0.003 -0.012 -0.024 -0.007\n [9,] -0.052 -0.105 -0.042  0.854 -0.052 -0.021 -0.021 -0.073 -0.146 -0.042\n[10,] -0.122  0.756 -0.098 -0.341 -0.122 -0.049 -0.049  0.829  0.659 -0.098\n\n\nThe modularity matrix has some interesting properties. For instance, both its rows and columns sum to zero:\n\n   round(rowSums(B), 2)\n\n [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[39] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n\n   round(colSums(B), 2)\n\n [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[39] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n\n\nWhich makes \\(\\mathbf{B}\\) doubly centered a neat but so far useless property.\nHere’s a more useful property. Remember the network distribution game we played to defined our various reflective status measure in Handout 3? What if we played it with the modularity matrix?\n\n   status1 <- function(A) {\n      n <- nrow(A) #number of actors\n      x <- rep(1, n) #initial status vector set to all ones\n      w <- 1 \n      k <- 0 #initializing counter\n      while (w > 0.0001) {\n          o.x <- x #old status scores\n          x <- A %*% x #new scores a function of old scores and adjacency matrix\n          x <- x/norm(x, type = \"E\") #normalizing new status scores\n          w <- abs(sum(abs(x) - abs(o.x))) #diff. between new and old scores\n          k <- k + 1 #incrementing while counter\n      }\n   return(as.vector(x))\n   }\n\n\n   C <- status1(B)\n   round(C,2)\n\n [1]  0.06  0.07  0.00  0.18  0.00  0.00 -0.01  0.01  0.09  0.22  0.08  0.28\n[13]  0.15  0.05  0.06  0.08  0.28 -0.11 -0.02  0.15  0.13  0.13  0.01  0.23\n[25]  0.18  0.15  0.19 -0.05  0.17  0.07 -0.07 -0.03 -0.05  0.04 -0.08  0.04\n[37]  0.04  0.02  0.05 -0.06 -0.19  0.06 -0.04 -0.03 -0.02 -0.08 -0.10 -0.09\n[49] -0.12 -0.02 -0.17  0.00 -0.17 -0.12 -0.23 -0.08 -0.07 -0.07 -0.08 -0.02\n[61] -0.30 -0.15 -0.20 -0.06 -0.11 -0.09 -0.13 -0.04\n\n\nThis seems more interesting. The resulting “status” vector has both negative and positive entries. What if I told you that this vector is actual a partition of the original graph into two communities? Not only that, I have even better news. This is the partition that maximizes that modularity, the best two-group partition in the graph that has the most edges within groups and the least edges going across groups.\nLet’s see some visual evidence.\nFirst, let’s turn the “status” vector into a community indicator vector. We assign all nodes with scores zero and larger to one community and nodes with scores less than zero to the other one:\n\n   C[C>0] <- 1\n   C[C<=0] <-2\n   C\n\n [1] 1 1 2 1 1 2 2 1 1 1 1 1 1 1 1 1 1 2 2 1 1 1 1 1 1 1 1 2 1 1 2 2 2 1 2 1 1 1\n[39] 1 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n\n\nAnd for the big check:\n\n   modularity(g, C)\n\n[1] 0.3240418\n\n\nWhich is a pretty big score, at least larger than we obtained using status in the law firm as a binary partition. Here’s some visual evidence:\n\n   V(g)$color <- C\n   plot(g, \n     vertex.size=6, vertex.frame.color=\"lightgray\", \n     vertex.label = NA, edge.arrow.size = 0.25,\n     vertex.label.dist=1, edge.curved=0.2)\n\n\n\n\nLaw Firm Friendship Network by The Best Binary Partition According to the Modularity\n\n\n\n\nWhich definitely looks like an optimal clustering into two groups based on friendship!"
  }
]