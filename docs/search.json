[
  {
    "objectID": "ahn.html",
    "href": "ahn.html",
    "title": "Assigning Nodes to Multiple Communities",
    "section": "",
    "text": "In the previous handout, we examined various approaches to community and dense subgraph detection. What they all have in common is they assign nodes into non-overlapping groups. That is, nodes are either in one group or another but cannot belong to multiple groups at once. While this may make sense for a lot of substantive settings, it might not make sense for other ones, where multiple group memberships are normal (e.g., think of high school)."
  },
  {
    "objectID": "ahn.html#detecting-overlapping-communities",
    "href": "ahn.html#detecting-overlapping-communities",
    "title": "Assigning Nodes to Multiple Communities",
    "section": "Detecting Overlapping Communities",
    "text": "Detecting Overlapping Communities\nMethodologically, overlapping community detection methods are not as well-developed as classical community detection methods. Here, we review one simple an intuitive approach that combines the idea of clustering nodes by computing a quantity on the links but instead of computing a rank order (like Newman and Girvan’s edge betweenness), we compute pairwise similarities between links like we did in handout 5. We then cluster the links using standard hierarchical clustering methods, which because nodes are incident to many links results in a multigroup clustering of the nodes for free. This approach is called link clustering (Ahn, Bagrow, and Lehmann 2010).\nLet’s see how it works.\nFirst we load data from an undirected graph:\n\n   library(igraph)\n   library(networkdata)\n   g <- movie_559 #Pulp Fiction\n\nAnd we plot:\n\n\n\n\n\n\nOriginal Graph.\n\n\n\n\n\n\n\nNodes Clusterered Into Communities According to the Modularity.\n\n\n\n\n\n\nThe Pulp Fiction Movie Network.\n\n\n\nThe key idea behind link clustering is that similar links should be assigned to the same clusters. How do we compute the similarity between links?"
  },
  {
    "objectID": "ahn.html#measuring-edge-similarity",
    "href": "ahn.html#measuring-edge-similarity",
    "title": "Assigning Nodes to Multiple Communities",
    "section": "Measuring Edge Similarity",
    "text": "Measuring Edge Similarity\nAccording to Ahn, Bagrow, and Lehmann (2010) two links \\(e_{ik}\\) and \\(e_{jk}\\) are similar if they share a node \\(v_k\\) and the other two nodes incident to each link (\\(v_i, v_k\\)) are themselves similar. To measure the similarity between these two nodes, we can use any of the off-the-shelf vertex similarity measures that we have seen in action, like Jaccard, cosine, or Dice.\nThe first step is thus to build a link by link similarity matrix based on this idea. The following function loops through each pair of links in the graph and computes the similarity between two links featuring a common node \\(v_k\\) based on the Jaccard vertex similarity of the two other nodes:\n\n    edge.sim <- function(x) {\n      el <- as_edgelist(x)\n      A <- as.matrix(as_adjacency_matrix(x))\n      S <- A %*% A #shared neighbors\n      d <- degree(x)\n      E <- nrow(el)\n      E.sim <- matrix(0, E, E)\n      for (e1 in 1:E) {\n        for (e2 in 1:E) {\n          if (e1 < e2 & sum(as.numeric(intersect(el[e1,], el[e2,])!=\"0\"))==1) {\n              v <- setdiff(union(el[e1,], el[e2,]), intersect(el[e1,], el[e2,]))\n              E.sim[e1, e2] <- S[v[1], v[2]]/(S[v[1], v[2]] + d[v[1]] + d[v[2]])\n              E.sim[e2, e1] <- S[v[1], v[2]]/(S[v[1], v[2]] + d[v[1]] + d[v[2]])\n            }\n          }\n        }\n    return(round(E.sim, 3))\n    }\n\nThe function takes the graph as input and returns an inter-link similarity matrix of dimensions \\(E \\times E\\) where \\(E\\) is the number of edges in the graph:\n\n    E.sim <- edge.sim(g)\n    E.sim[1:10, 1:10]\n\n       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10]\n [1,] 0.000 0.238 0.238 0.167 0.000 0.000 0.250 0.000 0.000 0.000\n [2,] 0.238 0.000 0.294 0.139 0.000 0.000 0.179 0.000 0.000 0.000\n [3,] 0.238 0.294 0.000 0.139 0.000 0.000 0.179 0.000 0.000 0.000\n [4,] 0.167 0.139 0.139 0.000 0.000 0.100 0.192 0.000 0.000 0.000\n [5,] 0.000 0.000 0.000 0.000 0.000 0.217 0.000 0.000 0.000 0.000\n [6,] 0.000 0.000 0.000 0.100 0.217 0.000 0.000 0.000 0.000 0.000\n [7,] 0.250 0.179 0.179 0.192 0.000 0.000 0.000 0.143 0.111 0.167\n [8,] 0.000 0.000 0.000 0.000 0.000 0.000 0.143 0.000 0.143 0.111\n [9,] 0.000 0.000 0.000 0.000 0.000 0.000 0.111 0.143 0.000 0.200\n[10,] 0.000 0.000 0.000 0.000 0.000 0.000 0.167 0.111 0.200 0.000\n\n\nWe can then transform the link similarities into distances, and cluster them:\n\n    D <- 1 - E.sim\n    D <- as.dist(D)\n    hc.res <- hclust(D, method = \"ward.D2\")\n\nThe resulting dendrogram looks like this:\n\n    par(cex = 0.5)\n    plot(hc.res)\n\n\n\n\nThe leaves of the dendrogram (bottom-most objects) represent each link in the graph (\\(E = 102\\) in this case), and the clusters are “link communities” (Ahn, Bagrow, and Lehmann 2010)."
  },
  {
    "objectID": "ahn.html#clustering-nodes",
    "href": "ahn.html#clustering-nodes",
    "title": "Assigning Nodes to Multiple Communities",
    "section": "Clustering Nodes",
    "text": "Clustering Nodes\nAs we noted, because it is the links that got clustered, the nodes incident to each link can go to more than one cluster (because nodes with degree \\(k>1\\) will be incident to multiple links).\nThe following function uses the dendrogram information to return a list of node assignments to multiple communities, controlled by the parameter k:\n\n    create.clus <- function(x, k) {\n      library(dendextend)\n      link.clus <- cutree(x, k = k)\n      link.dat <- data.frame(as_edgelist(g), link.clus)\n      clus.list <- list()\n      for (i in 1:max(link.clus)) {\n        sub.dat <- link.dat[link.dat$link.clus == i, ]\n        clus.list[[i]] <- unique(c(sub.dat[, 1], sub.dat[, 2]))\n        }\n    return(clus.list)\n    }\n\nLet’s see the list with twelve overlapping communities:\n\n    C <- create.clus(hc.res, k = 12)\n    C\n\n[[1]]\n[1] \"BRETT\"      \"GAWKER #2\"  \"JULES\"      \"MARSELLUS\"  \"MARVIN\"    \n[6] \"PEDESTRIAN\" \"ROGER\"      \"VINCENT\"   \n\n[[2]]\n [1] \"BRETT\"           \"FABIENNE\"        \"MARVIN\"          \"SPORTSCASTER #1\"\n [5] \"JIMMIE\"          \"RAQUEL\"          \"ROGER\"           \"JULES\"          \n [9] \"SPORTSCASTER #2\" \"THE WOLF\"        \"WINSTON\"        \n\n[[3]]\n [1] \"BRETT\"       \"FOURTH MAN\"  \"RAQUEL\"      \"THE WOLF\"    \"HONEY BUNNY\"\n [6] \"JIMMIE\"      \"JULES\"       \"MANAGER\"     \"MARVIN\"      \"PATRON\"     \n[11] \"PUMPKIN\"     \"ROGER\"       \"VINCENT\"     \"WINSTON\"    \n\n[[4]]\n [1] \"BUDDY\"        \"JODY\"         \"LANCE\"        \"BUTCH\"        \"CAPT KOONS\"  \n [6] \"ED SULLIVAN\"  \"ENGLISH DAVE\" \"MARSELLUS\"    \"MIA\"          \"MOTHER\"      \n[11] \"WOMAN\"        \"VINCENT\"     \n\n[[5]]\n [1] \"BUDDY\"        \"LANCE\"        \"PREACHER\"     \"CAPT KOONS\"   \"ED SULLIVAN\" \n [6] \"ENGLISH DAVE\" \"JODY\"         \"MOTHER\"       \"VINCENT\"      \"WOMAN\"       \n\n[[6]]\n [1] \"BRETT\"           \"BUTCH\"           \"ENGLISH DAVE\"    \"CAPT KOONS\"     \n [5] \"ESMARELDA\"       \"GAWKER #2\"       \"JULES\"           \"MARSELLUS\"      \n [9] \"PEDESTRIAN\"      \"SPORTSCASTER #1\" \"FABIENNE\"        \"MARVIN\"         \n[13] \"MAYNARD\"         \"MOTHER\"          \"ROGER\"           \"VINCENT\"        \n[17] \"WOMAN\"          \n\n[[7]]\n [1] \"FOURTH MAN\"  \"JIMMIE\"      \"BRETT\"       \"HONEY BUNNY\" \"JULES\"      \n [6] \"MANAGER\"     \"MARVIN\"      \"PATRON\"      \"PUMPKIN\"     \"RAQUEL\"     \n[11] \"ROGER\"       \"WINSTON\"     \"THE WOLF\"   \n\n[[8]]\n[1] \"HONEY BUNNY\" \"MANAGER\"     \"PATRON\"      \"PUMPKIN\"    \n\n[[9]]\n[1] \"JODY\"     \"LANCE\"    \"PREACHER\"\n\n[[10]]\n[1] \"MAYNARD\"  \"THE GIMP\" \"ZED\"     \n\n[[11]]\n[1] \"CAPT KOONS\" \"MOTHER\"     \"WOMAN\"     \n\n[[12]]\n[1] \"HONEY BUNNY\" \"PUMPKIN\"     \"WAITRESS\"    \"YOUNG MAN\"   \"YOUNG WOMAN\"\n\n\nBecause there are nodes that belong to multiple communities, the resulting actor by community ties form a two-mode network.\nWe can see re-construct this network from the list of community memberships for each node using this function:\n\n    create.two <- function(x) {\n      v <- unique(unlist(x))\n      B <- as.numeric(v %in% x[[1]])\n      for(j in 2:length(x)) {\n        B <- cbind(B, as.numeric(v %in% x[[j]]))\n      }\n      rownames(B) <- v\n      colnames(B) <- paste(\"c\", 1:12, sep = \"\")\n      return(B)\n      }\n\nHere’s the two mode matrix of characters by communities:\n\n    B <- create.two(C)\n    B\n\n                c1 c2 c3 c4 c5 c6 c7 c8 c9 c10 c11 c12\nBRETT            1  1  1  0  0  1  1  0  0   0   0   0\nGAWKER #2        1  0  0  0  0  1  0  0  0   0   0   0\nJULES            1  1  1  0  0  1  1  0  0   0   0   0\nMARSELLUS        1  0  0  1  0  1  0  0  0   0   0   0\nMARVIN           1  1  1  0  0  1  1  0  0   0   0   0\nPEDESTRIAN       1  0  0  0  0  1  0  0  0   0   0   0\nROGER            1  1  1  0  0  1  1  0  0   0   0   0\nVINCENT          1  0  1  1  1  1  0  0  0   0   0   0\nFABIENNE         0  1  0  0  0  1  0  0  0   0   0   0\nSPORTSCASTER #1  0  1  0  0  0  1  0  0  0   0   0   0\nJIMMIE           0  1  1  0  0  0  1  0  0   0   0   0\nRAQUEL           0  1  1  0  0  0  1  0  0   0   0   0\nSPORTSCASTER #2  0  1  0  0  0  0  0  0  0   0   0   0\nTHE WOLF         0  1  1  0  0  0  1  0  0   0   0   0\nWINSTON          0  1  1  0  0  0  1  0  0   0   0   0\nFOURTH MAN       0  0  1  0  0  0  1  0  0   0   0   0\nHONEY BUNNY      0  0  1  0  0  0  1  1  0   0   0   1\nMANAGER          0  0  1  0  0  0  1  1  0   0   0   0\nPATRON           0  0  1  0  0  0  1  1  0   0   0   0\nPUMPKIN          0  0  1  0  0  0  1  1  0   0   0   1\nBUDDY            0  0  0  1  1  0  0  0  0   0   0   0\nJODY             0  0  0  1  1  0  0  0  1   0   0   0\nLANCE            0  0  0  1  1  0  0  0  1   0   0   0\nBUTCH            0  0  0  1  0  1  0  0  0   0   0   0\nCAPT KOONS       0  0  0  1  1  1  0  0  0   0   1   0\nED SULLIVAN      0  0  0  1  1  0  0  0  0   0   0   0\nENGLISH DAVE     0  0  0  1  1  1  0  0  0   0   0   0\nMIA              0  0  0  1  0  0  0  0  0   0   0   0\nMOTHER           0  0  0  1  1  1  0  0  0   0   1   0\nWOMAN            0  0  0  1  1  1  0  0  0   0   1   0\nPREACHER         0  0  0  0  1  0  0  0  1   0   0   0\nESMARELDA        0  0  0  0  0  1  0  0  0   0   0   0\nMAYNARD          0  0  0  0  0  1  0  0  0   1   0   0\nTHE GIMP         0  0  0  0  0  0  0  0  0   1   0   0\nZED              0  0  0  0  0  0  0  0  0   1   0   0\nWAITRESS         0  0  0  0  0  0  0  0  0   0   0   1\nYOUNG MAN        0  0  0  0  0  0  0  0  0   0   0   1\nYOUNG WOMAN      0  0  0  0  0  0  0  0  0   0   0   1\n\n\nAnd the matrix of inter-community ties based on shared characters:\n\n    M <- t(B) %*% B\n    M\n\n    c1 c2 c3 c4 c5 c6 c7 c8 c9 c10 c11 c12\nc1   8  4  5  2  1  8  4  0  0   0   0   0\nc2   4 11  8  0  0  6  8  0  0   0   0   0\nc3   5  8 14  1  1  5 13  4  0   0   0   2\nc4   2  0  1 12  9  7  0  0  2   0   3   0\nc5   1  0  1  9 10  5  0  0  3   0   3   0\nc6   8  6  5  7  5 17  4  0  0   1   3   0\nc7   4  8 13  0  0  4 13  4  0   0   0   2\nc8   0  0  4  0  0  0  4  4  0   0   0   2\nc9   0  0  0  2  3  0  0  0  3   0   0   0\nc10  0  0  0  0  0  1  0  0  0   3   0   0\nc11  0  0  0  3  3  3  0  0  0   0   3   0\nc12  0  0  2  0  0  0  2  2  0   0   0   5\n\n\nAnd we can visualize the nodes connected to multiple communities as follows:\n\n    library(RColorBrewer)\n    set.seed(45)\n    g <- graph_from_biadjacency_matrix(B)\n    V(g)$type <- bipartite_mapping(g)$type\n    V(g)$shape <- ifelse(V(g)$type, \"square\", \"circle\")\n    V(g)$color <- c(rep(\"orange\", 38), \n                    c(brewer.pal(8, \"Paired\"), brewer.pal(4, \"Dark2\"))) \n    E(g)$color <- \"lightgray\"\n    plot(g, \n    vertex.size=5, vertex.frame.color=\"lightgray\", \n    vertex.label = V(g)$name,\n    vertex.label.dist=1, vertex.label.cex = 1)"
  },
  {
    "objectID": "backbone.html",
    "href": "backbone.html",
    "title": "Extracting the Backbone of Bipartite Projections Using Two-Mode Graph Ensembles",
    "section": "",
    "text": "Recall from our lecture on the duality of persons and groups that a common approach to analyzing two-mode networks is via what Everett and Borgatti (2013) call the “dual projection” approach. This involves computing the row and column object projections from the original affiliation matrix (Breiger 1974). Nevertheless, one issue with this approach is that the result is a weighted network, which can be hard to analyze using our usual tools.\nMoreover, because just sharing a single membership (member) will create a connection between any two persons (groups) in the row (column) projection, the resulting weighted graphs are dense and therefore lack meaningful or interesting structure. Thus, a common task is to try to prune and binarize bipartite projections. Zak Neal (2014) refers to this problem as extracting the backbone of the bipartite projection.\nSo let’s load up our trusty Southern Women data set and compute the bipartite projections:\n\n   library(igraph)\n   library(networkdata)\n   g <- southern_women\n   Proj <- bipartite_projection(g)\n   G.p <- Proj[[1]]\n   G.g <- Proj[[2]]\n   A <- as.matrix(as_biadjacency_matrix(g))\n   P <- as.matrix(as_adjacency_matrix(G.p, attr = \"weight\"))\n   G <- as.matrix(as_adjacency_matrix(G.g, attr = \"weight\"))\n\nRecall that \\(\\mathbf{A}\\) is the rectangular affiliation matrix recording adjacency relations between persons and groups, \\(\\mathbf{P}\\) is the weighted row projection between people, where the weight between pairs of people is the number of common memberships they share, and \\(\\mathbf{G}\\) is the weighted column projection between groups, where the weight between pairs of groups is the number of common members they share.\nThe \\(\\mathbf{P}\\) matrix looks like this:\n\n   P\n\n          EVELYN LAURA THERESA BRENDA CHARLOTTE FRANCES ELEANOR PEARL RUTH\nEVELYN         0     6       7      6         3       4       3     3    3\nLAURA          6     0       6      6         3       4       4     2    3\nTHERESA        7     6       0      6         4       4       4     3    4\nBRENDA         6     6       6      0         4       4       4     2    3\nCHARLOTTE      3     3       4      4         0       2       2     0    2\nFRANCES        4     4       4      4         2       0       3     2    2\nELEANOR        3     4       4      4         2       3       0     2    3\nPEARL          3     2       3      2         0       2       2     0    2\nRUTH           3     3       4      3         2       2       3     2    0\nVERNE          2     2       3      2         1       1       2     2    3\nMYRNA          2     1       2      1         0       1       1     2    2\nKATHERINE      2     1       2      1         0       1       1     2    2\nSYLVIA         2     2       3      2         1       1       2     2    3\nNORA           2     2       3      2         1       1       2     2    2\nHELEN          1     2       2      2         1       1       2     1    2\nDOROTHY        2     1       2      1         0       1       1     2    2\nOLIVIA         1     0       1      0         0       0       0     1    1\nFLORA          1     0       1      0         0       0       0     1    1\n          VERNE MYRNA KATHERINE SYLVIA NORA HELEN DOROTHY OLIVIA FLORA\nEVELYN        2     2         2      2    2     1       2      1     1\nLAURA         2     1         1      2    2     2       1      0     0\nTHERESA       3     2         2      3    3     2       2      1     1\nBRENDA        2     1         1      2    2     2       1      0     0\nCHARLOTTE     1     0         0      1    1     1       0      0     0\nFRANCES       1     1         1      1    1     1       1      0     0\nELEANOR       2     1         1      2    2     2       1      0     0\nPEARL         2     2         2      2    2     1       2      1     1\nRUTH          3     2         2      3    2     2       2      1     1\nVERNE         0     3         3      4    3     3       2      1     1\nMYRNA         3     0         4      4    3     3       2      1     1\nKATHERINE     3     4         0      6    5     3       2      1     1\nSYLVIA        4     4         6      0    6     4       2      1     1\nNORA          3     3         5      6    0     4       1      2     2\nHELEN         3     3         3      4    4     0       1      1     1\nDOROTHY       2     2         2      2    1     1       0      1     1\nOLIVIA        1     1         1      1    2     1       1      0     2\nFLORA         1     1         1      1    2     1       1      2     0\n\n\nAnd the \\(\\mathbf{G}\\) matrix looks like this:\n\n   G\n\n      6/27 3/2 4/12 9/26 2/25 5/19 3/15 9/16 4/8 6/10 2/23 4/7 11/21 8/3\n6/27     0   2    3    2    3    3    2    3   1    0    0   0     0   0\n3/2      2   0    3    2    3    3    2    3   2    0    0   0     0   0\n4/12     3   3    0    4    6    5    4    5   2    0    0   0     0   0\n9/26     2   2    4    0    4    3    3    3   2    0    0   0     0   0\n2/25     3   3    6    4    0    6    6    7   3    0    0   0     0   0\n5/19     3   3    5    3    6    0    5    7   4    1    1   1     1   1\n3/15     2   2    4    3    6    5    0    8   5    3    2   4     2   2\n9/16     3   3    5    3    7    7    8    0   9    4    1   5     2   2\n4/8      1   2    2    2    3    4    5    9   0    4    3   5     3   3\n6/10     0   0    0    0    0    1    3    4   4    0    2   5     3   3\n2/23     0   0    0    0    0    1    2    1   3    2    0   2     1   1\n4/7      0   0    0    0    0    1    4    5   5    5    2   0     3   3\n11/21    0   0    0    0    0    1    2    2   3    3    1   3     0   3\n8/3      0   0    0    0    0    1    2    2   3    3    1   3     3   0\n\n\nNow, our job is to binarize these matrices by retaining adjacency relations between nodes whose weights is significantly larger than we would expect given a suitable null model. This is a situation perfectly tailored for two-mode graph ensembles!"
  },
  {
    "objectID": "backbone.html#the-stochastic-degree-sequence-model-logit-version",
    "href": "backbone.html#the-stochastic-degree-sequence-model-logit-version",
    "title": "Extracting the Backbone of Bipartite Projections Using Two-Mode Graph Ensembles",
    "section": "The Stochastic Degree Sequence Model (Logit version)",
    "text": "The Stochastic Degree Sequence Model (Logit version)\nNeal (2014) proposes one such approach called the Stochastic Degree Sequence Model (SDSM). This approach compares the observed weights in each bipartite projection against those from a bipartite graph ensemble where the graphs in the ensemble are generated from a model that preserves the expected (average) degrees of the nodes in the two-mode network.\nWe proceed as follows. To begin, we estimate a generalized linear model for binary outcomes (e.g., logit or probit, or your favorite other), where we predict the probability of observing an edge in the bipartite graph from the degrees of each node incident to each edge (and their statistical interaction). In R we can do this in three steps:\nFirst, we create a dataset with the affiliation matrix as a response variable and the degrees of each node as covariates:\n\n   y <- as.numeric(A) #vectorized affiliation matrix\n   d1 <- c(rep(rowSums(A), ncol(A))) #person degree vector\n   d2 <- c(rep(colSums(A), each = nrow(A))) #group degree vector\n   dat <- data.frame(y, d1, d2) #data frame\n   dat[1:25, ] #first 25 rows of data frame\n\n   y d1 d2\n1  1  8  3\n2  1  7  3\n3  0  8  3\n4  1  7  3\n5  0  4  3\n6  0  4  3\n7  0  4  3\n8  0  3  3\n9  0  4  3\n10 0  4  3\n11 0  4  3\n12 0  6  3\n13 0  7  3\n14 0  8  3\n15 0  5  3\n16 0  2  3\n17 0  2  3\n18 0  2  3\n19 1  8  3\n20 1  7  3\n21 1  8  3\n22 0  7  3\n23 0  4  3\n24 0  4  3\n25 0  4  3\n\n\nSecond, we estimate a logit regression with the degrees as predictors:\n\n   mylogit <- glm(y ~ d1 + d2, data = dat, family = \"binomial\")\n   summary(mylogit)\n\n\nCall:\nglm(formula = y ~ d1 + d2, family = \"binomial\", data = dat)\n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -4.64673    0.62672  -7.414 1.22e-13 ***\nd1           0.40645    0.07980   5.093 3.52e-07 ***\nd2           0.29307    0.04788   6.121 9.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 327.29  on 251  degrees of freedom\nResidual deviance: 256.60  on 249  degrees of freedom\nAIC: 262.6\n\nNumber of Fisher Scoring iterations: 4\n\n\nThird, we compute the predicted probabilities for each case:\n\n   mypreds <- predict(mylogit, type = \"response\")\n   round(mypreds[1:25], 2) #first 25 predictions\n\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n0.37 0.28 0.37 0.28 0.11 0.11 0.11 0.07 0.11 0.11 0.11 0.21 0.28 0.37 0.15 0.05 \n  17   18   19   20   21   22   23   24   25 \n0.05 0.05 0.37 0.28 0.37 0.28 0.11 0.11 0.11 \n\n\nNow, we use these predicted probabilities to generate an ensemble of graphs, where each edge in the graph is a Bernoulli draw from the predicted probability response vector. We can do this using the following function:\n\n   bipart.ensemble <- function(x, r, c) {\n      A <- as.numeric(x > runif(length(x)))\n      A <- matrix(A, nrow = r, ncol = c)\n      return(A)\n      }\n\nThe above function uses the runif function in R to compare each predicted probability to a random number between zero and one, and then creates a new affiliation matrix that has a one in each cell if the predicted probability is larger than the corresponding random number.\nLet’s see how this works:\n\n   set.seed(123)\n   A1 <- bipart.ensemble(mypreds, r = nrow(A), c = ncol(A))\n   rownames(A1) <- rownames(A)\n   colnames(A1) <- colnames(A)\n   A1\n\n          6/27 3/2 4/12 9/26 2/25 5/19 3/15 9/16 4/8 6/10 2/23 4/7 11/21 8/3\nEVELYN       1   1    0    0    1    1    1    1   1    0    0   1     1   1\nLAURA        0   0    1    1    1    0    1    1   1    1    1   0     0   0\nTHERESA      0   0    1    1    1    1    0    1   1    1    0   1     0   1\nBRENDA       0   0    1    0    1    0    1    1   1    0    1   0     0   0\nCHARLOTTE    0   0    1    0    0    1    1    1   1    0    0   0     0   0\nFRANCES      1   0    0    0    0    1    0    0   0    0    0   0     0   0\nELEANOR      0   0    0    0    0    0    0    1   0    0    0   0     0   0\nPEARL        0   0    0    1    1    1    1    0   1    0    0   0     0   0\nRUTH         0   0    1    0    1    0    0    1   1    0    0   0     1   0\nVERNE        0   0    1    0    0    0    0    1   1    0    0   0     0   0\nMYRNA        0   0    0    0    0    0    0    0   1    0    0   1     0   0\nKATHERINE    0   1    0    0    0    1    1    1   1    0    0   1     0   0\nSYLVIA       0   0    1    0    1    1    1    0   1    0    0   1     0   0\nNORA         0   0    0    0    1    0    1    1   1    0    0   1     0   0\nHELEN        1   0    1    0    0    0    1    1   1    0    0   1     0   0\nDOROTHY      0   0    0    0    0    0    1    1   1    0    0   0     0   0\nOLIVIA       0   1    0    0    0    0    0    1   0    0    0   1     0   0\nFLORA        1   0    0    0    1    0    0    1   1    0    0   0     0   0\n\n\nWhich generates an affiliation matrix realization from the predicted probabilities we calculated before, with the probability of each edge being a function of the degrees of each node in the original affiliation matrix. Now all we need to do is produce a bunch of these using the replicate function in R:\n\n   set.seed(456)\n   B <- replicate(100, bipart.ensemble(mypreds, r = nrow(A), c = ncol(A)), simplify = FALSE)\n\nWe can then create a bunch of person projections from these matrices using lapply:\n\n   P.list <- lapply(B, function(x) {x %*% t(x)}) #ensemble of row projections\n\nAnd a bunch of group projections:\n\n   G.list <- lapply(B, function(x) {t(x) %*% x}) #ensemble of column projections\n\nNow we generate a list of binary projections where two nodes are tied when their observed weight in the row or column projections is larger than the corresponding weight in the generated projections:\n\n   P.bin <- lapply(P.list, function(x) {(P > x) * 1})\n   G.bin <- lapply(G.list, function(x) {(G > x) * 1})\n\nWe now add all of the matrices in each list to count the number of times the observed value is larger than the expected value. We use the trusty R function Reduce with the \"+\" operator to do this:\n\n   P.exp <- Reduce(\"+\", P.bin)\n   G.exp <- Reduce(\"+\", G.bin)\n\nThe “p-value” for each edge weight in the row and column projections is just one minus the cells of this matrix divided by the number of graphs in the ensemble:\n\n   P.sig <- 1 - P.exp/100\n   G.sig <- 1 - G.exp/100\n   P.sig\n\n          EVELYN LAURA THERESA BRENDA CHARLOTTE FRANCES ELEANOR PEARL RUTH\nEVELYN      1.00  0.27    0.21   0.28      0.50    0.22    0.53  0.33 0.54\nLAURA       0.27  1.00    0.28   0.21      0.40    0.18    0.21  0.62 0.50\nTHERESA     0.21  0.28    1.00   0.30      0.21    0.22    0.24  0.39 0.22\nBRENDA      0.28  0.21    0.30   1.00      0.21    0.13    0.15  0.66 0.51\nCHARLOTTE   0.50  0.40    0.21   0.21      1.00    0.46    0.46  1.00 0.50\nFRANCES     0.22  0.18    0.22   0.13      0.46    1.00    0.16  0.35 0.42\nELEANOR     0.53  0.21    0.24   0.15      0.46    0.16    1.00  0.39 0.19\nPEARL       0.33  0.62    0.39   0.66      1.00    0.35    0.39  1.00 0.41\nRUTH        0.54  0.50    0.22   0.51      0.50    0.42    0.19  0.41 1.00\nVERNE       0.77  0.75    0.59   0.77      0.86    0.80    0.45  0.41 0.15\nMYRNA       0.85  0.99    0.84   0.97      1.00    0.87    0.88  0.39 0.47\nKATHERINE   0.96  0.98    0.98   1.00      1.00    0.92    0.95  0.54 0.67\nSYLVIA      0.98  0.95    0.91   0.98      0.94    0.95    0.76  0.65 0.49\nNORA        1.00  0.99    0.99   1.00      0.97    0.95    0.84  0.78 0.77\nHELEN       1.00  0.92    0.95   0.90      0.90    0.94    0.61  0.87 0.61\nDOROTHY     0.57  0.86    0.62   0.88      1.00    0.71    0.72  0.28 0.32\nOLIVIA      0.88  1.00    0.92   1.00      1.00    1.00    1.00  0.61 0.70\nFLORA       0.93  1.00    0.94   1.00      1.00    1.00    1.00  0.70 0.72\n          VERNE MYRNA KATHERINE SYLVIA NORA HELEN DOROTHY OLIVIA FLORA\nEVELYN     0.77  0.85      0.96   0.98 1.00  1.00    0.57   0.88  0.93\nLAURA      0.75  0.99      0.98   0.95 0.99  0.92    0.86   1.00  1.00\nTHERESA    0.59  0.84      0.98   0.91 0.99  0.95    0.62   0.92  0.94\nBRENDA     0.77  0.97      1.00   0.98 1.00  0.90    0.88   1.00  1.00\nCHARLOTTE  0.86  1.00      1.00   0.94 0.97  0.90    1.00   1.00  1.00\nFRANCES    0.80  0.87      0.92   0.95 0.95  0.94    0.71   1.00  1.00\nELEANOR    0.45  0.88      0.95   0.76 0.84  0.61    0.72   1.00  1.00\nPEARL      0.41  0.39      0.54   0.65 0.78  0.87    0.28   0.61  0.70\nRUTH       0.15  0.47      0.67   0.49 0.77  0.61    0.32   0.70  0.72\nVERNE      1.00  0.16      0.37   0.17 0.54  0.34    0.37   0.71  0.77\nMYRNA      0.16  1.00      0.11   0.17 0.56  0.29    0.33   0.71  0.77\nKATHERINE  0.37  0.11      1.00   0.07 0.32  0.55    0.43   0.80  0.85\nSYLVIA     0.17  0.17      0.07   1.00 0.24  0.35    0.51   0.86  0.93\nNORA       0.54  0.56      0.32   0.24 1.00  0.47    0.88   0.52  0.58\nHELEN      0.34  0.29      0.55   0.35 0.47  1.00    0.75   0.81  0.85\nDOROTHY    0.37  0.33      0.43   0.51 0.88  0.75    1.00   0.56  0.66\nOLIVIA     0.71  0.71      0.80   0.86 0.52  0.81    0.56   1.00  0.15\nFLORA      0.77  0.77      0.85   0.93 0.58  0.85    0.66   0.15  1.00\n\n   G.sig\n\n      6/27  3/2 4/12 9/26 2/25 5/19 3/15 9/16  4/8 6/10 2/23  4/7 11/21  8/3\n6/27  1.00 0.23 0.15 0.27 0.29 0.22 0.69 0.63 0.94 1.00 1.00 1.00  1.00 1.00\n3/2   0.23 1.00 0.12 0.31 0.33 0.27 0.67 0.62 0.73 1.00 1.00 1.00  1.00 1.00\n4/12  0.15 0.12 1.00 0.11 0.04 0.15 0.52 0.52 0.98 1.00 1.00 1.00  1.00 1.00\n9/26  0.27 0.31 0.11 1.00 0.20 0.44 0.53 0.71 0.88 1.00 1.00 1.00  1.00 1.00\n2/25  0.29 0.33 0.04 0.20 1.00 0.12 0.42 0.53 0.96 1.00 1.00 1.00  1.00 1.00\n5/19  0.22 0.27 0.15 0.44 0.12 1.00 0.59 0.45 0.87 0.93 0.95 0.96  0.90 0.84\n3/15  0.69 0.67 0.52 0.53 0.42 0.59 1.00 0.63 0.92 0.63 0.80 0.52  0.68 0.67\n9/16  0.63 0.62 0.52 0.71 0.53 0.45 0.63 1.00 0.78 0.56 0.99 0.48  0.82 0.79\n4/8   0.94 0.73 0.98 0.88 0.96 0.87 0.92 0.78 1.00 0.56 0.63 0.39  0.49 0.43\n6/10  1.00 1.00 1.00 1.00 1.00 0.93 0.63 0.56 0.56 1.00 0.43 0.01  0.14 0.16\n2/23  1.00 1.00 1.00 1.00 1.00 0.95 0.80 0.99 0.63 0.43 1.00 0.55  0.61 0.69\n4/7   1.00 1.00 1.00 1.00 1.00 0.96 0.52 0.48 0.39 0.01 0.55 1.00  0.17 0.14\n11/21 1.00 1.00 1.00 1.00 1.00 0.90 0.68 0.82 0.49 0.14 0.61 0.17  1.00 0.01\n8/3   1.00 1.00 1.00 1.00 1.00 0.84 0.67 0.79 0.43 0.16 0.69 0.14  0.01 1.00\n\n\nWe then connect two nodes in the backbone, when this value is below some standard threshold (e.g., \\(p < 0.25\\)):\n\n   P.back <- (P.sig < 0.25) * 1\n   G.back <- (G.sig < 0.25) * 1\n   P.back\n\n          EVELYN LAURA THERESA BRENDA CHARLOTTE FRANCES ELEANOR PEARL RUTH\nEVELYN         0     0       1      0         0       1       0     0    0\nLAURA          0     0       0      1         0       1       1     0    0\nTHERESA        1     0       0      0         1       1       1     0    1\nBRENDA         0     1       0      0         1       1       1     0    0\nCHARLOTTE      0     0       1      1         0       0       0     0    0\nFRANCES        1     1       1      1         0       0       1     0    0\nELEANOR        0     1       1      1         0       1       0     0    1\nPEARL          0     0       0      0         0       0       0     0    0\nRUTH           0     0       1      0         0       0       1     0    0\nVERNE          0     0       0      0         0       0       0     0    1\nMYRNA          0     0       0      0         0       0       0     0    0\nKATHERINE      0     0       0      0         0       0       0     0    0\nSYLVIA         0     0       0      0         0       0       0     0    0\nNORA           0     0       0      0         0       0       0     0    0\nHELEN          0     0       0      0         0       0       0     0    0\nDOROTHY        0     0       0      0         0       0       0     0    0\nOLIVIA         0     0       0      0         0       0       0     0    0\nFLORA          0     0       0      0         0       0       0     0    0\n          VERNE MYRNA KATHERINE SYLVIA NORA HELEN DOROTHY OLIVIA FLORA\nEVELYN        0     0         0      0    0     0       0      0     0\nLAURA         0     0         0      0    0     0       0      0     0\nTHERESA       0     0         0      0    0     0       0      0     0\nBRENDA        0     0         0      0    0     0       0      0     0\nCHARLOTTE     0     0         0      0    0     0       0      0     0\nFRANCES       0     0         0      0    0     0       0      0     0\nELEANOR       0     0         0      0    0     0       0      0     0\nPEARL         0     0         0      0    0     0       0      0     0\nRUTH          1     0         0      0    0     0       0      0     0\nVERNE         0     1         0      1    0     0       0      0     0\nMYRNA         1     0         1      1    0     0       0      0     0\nKATHERINE     0     1         0      1    0     0       0      0     0\nSYLVIA        1     1         1      0    1     0       0      0     0\nNORA          0     0         0      1    0     0       0      0     0\nHELEN         0     0         0      0    0     0       0      0     0\nDOROTHY       0     0         0      0    0     0       0      0     0\nOLIVIA        0     0         0      0    0     0       0      0     1\nFLORA         0     0         0      0    0     0       0      1     0\n\n   G.back\n\n      6/27 3/2 4/12 9/26 2/25 5/19 3/15 9/16 4/8 6/10 2/23 4/7 11/21 8/3\n6/27     0   1    1    0    0    1    0    0   0    0    0   0     0   0\n3/2      1   0    1    0    0    0    0    0   0    0    0   0     0   0\n4/12     1   1    0    1    1    1    0    0   0    0    0   0     0   0\n9/26     0   0    1    0    1    0    0    0   0    0    0   0     0   0\n2/25     0   0    1    1    0    1    0    0   0    0    0   0     0   0\n5/19     1   0    1    0    1    0    0    0   0    0    0   0     0   0\n3/15     0   0    0    0    0    0    0    0   0    0    0   0     0   0\n9/16     0   0    0    0    0    0    0    0   0    0    0   0     0   0\n4/8      0   0    0    0    0    0    0    0   0    0    0   0     0   0\n6/10     0   0    0    0    0    0    0    0   0    0    0   1     1   1\n2/23     0   0    0    0    0    0    0    0   0    0    0   0     0   0\n4/7      0   0    0    0    0    0    0    0   0    1    0   0     1   1\n11/21    0   0    0    0    0    0    0    0   0    1    0   1     0   1\n8/3      0   0    0    0    0    0    0    0   0    1    0   1     1   0\n\n\nAnd we can now plot the backbones:\n\n\n\n\n\n\n\n(a) Persons\n\n\n\n\n\n\n\n(b) Groups\n\n\n\n\nFigure 1: Backbones of Southern Women Data Using Logit SDSM\n\n\nWe can put together all of the above steps into a handy dandy function called sdsm.logit which takes the affiliation matrix \\(\\mathbf{A}\\) as input and returns the two projection backbones as output:\n\n   sdsm.logit <- function(A, n = 100, p.val = 0.25, seed = 123) {\n      set.seed(seed)\n      r <- nrow(A)\n      c <- ncol(A)\n      P <- A %*% t(A) #row projection\n      G <- t(A) %*% A #column projection\n      y <- as.numeric(A) #vectorized affiliation matrix\n      d1 <- c(rep(rowSums(A), ncol(A))) #person degree vector\n      d2 <- c(rep(colSums(A), each = nrow(A))) #group degree vector\n      pred <- predict(glm(y ~ d1 + d2, data = data.frame(y, d1, d2), \n                 family = \"binomial\")) #getting predictions from glm\n      gen.aff <- function(w) {\n         return(matrix(as.numeric(w > runif(length(w))), nrow = r, ncol = c))    \n         }\n      B <- replicate(n, gen.aff(pred), simplify = FALSE)\n      P.bin <- lapply(B, function(x) {(P > x %*% t(x)) * 1})\n      G.bin <- lapply(B, function(x) {(G > t(x) %*% x) * 1})\n      P.back <- ((1 - Reduce(\"+\", P.bin)/n) < p.val) * 1\n      G.back <- ((1 - Reduce(\"+\", G.bin)/n) < p.val) * 1\n   return(list(P.back = P.back, G.back = G.back))\n   }\n\nAnd voila:\n\n   sdsm.logit(A)\n\n$P.back\n          EVELYN LAURA THERESA BRENDA CHARLOTTE FRANCES ELEANOR PEARL RUTH\nEVELYN         1     1       1      1         1       1       1     1    1\nLAURA          1     1       1      1         1       1       1     1    1\nTHERESA        1     1       1      1         1       1       1     1    1\nBRENDA         1     1       1      1         1       1       1     1    1\nCHARLOTTE      1     1       1      1         1       0       1     0    1\nFRANCES        1     1       1      1         0       1       1     1    0\nELEANOR        1     1       1      1         1       1       1     1    1\nPEARL          1     1       1      1         0       1       1     1    1\nRUTH           1     1       1      1         1       0       1     1    1\nVERNE          0     0       1      0         0       0       0     1    1\nMYRNA          0     0       0      0         0       0       0     1    1\nKATHERINE      0     0       0      0         0       0       0     1    0\nSYLVIA         0     0       0      0         0       0       0     1    1\nNORA           0     0       0      0         0       0       0     1    0\nHELEN          0     0       0      0         0       0       0     0    0\nDOROTHY        1     0       1      0         0       0       0     1    1\nOLIVIA         0     0       0      0         0       0       0     1    0\nFLORA          0     0       0      0         0       0       0     1    0\n          VERNE MYRNA KATHERINE SYLVIA NORA HELEN DOROTHY OLIVIA FLORA\nEVELYN        0     0         0      0    0     0       1      0     0\nLAURA         0     0         0      0    0     0       0      0     0\nTHERESA       1     0         0      0    0     0       1      0     0\nBRENDA        0     0         0      0    0     0       0      0     0\nCHARLOTTE     0     0         0      0    0     0       0      0     0\nFRANCES       0     0         0      0    0     0       0      0     0\nELEANOR       0     0         0      0    0     0       0      0     0\nPEARL         1     1         1      1    1     0       1      1     1\nRUTH          1     1         0      1    0     0       1      0     0\nVERNE         1     1         1      1    1     1       1      0     0\nMYRNA         1     1         1      1    1     1       1      0     0\nKATHERINE     1     1         1      1    1     1       1      0     0\nSYLVIA        1     1         1      1    1     1       1      0     0\nNORA          1     1         1      1    1     1       0      1     1\nHELEN         1     1         1      1    1     1       0      0     0\nDOROTHY       1     1         1      1    0     0       1      1     1\nOLIVIA        0     0         0      0    1     0       1      1     1\nFLORA         0     0         0      0    1     0       1      1     1\n\n$G.back\n      6/27 3/2 4/12 9/26 2/25 5/19 3/15 9/16 4/8 6/10 2/23 4/7 11/21 8/3\n6/27     1   1    1    1    1    1    1    1   1    0    0   0     0   0\n3/2      1   1    1    1    1    1    1    1   1    0    0   0     0   0\n4/12     1   1    1    1    1    1    1    1   0    0    0   0     0   0\n9/26     1   1    1    1    1    1    1    1   1    0    0   0     0   0\n2/25     1   1    1    1    1    1    1    1   0    0    0   0     0   0\n5/19     1   1    1    1    1    1    0    1   0    1    1   0     1   1\n3/15     1   1    1    1    1    0    1    1   0    1    1   1     1   1\n9/16     1   1    1    1    1    1    1    0   0    1    1   1     1   1\n4/8      1   1    0    1    0    0    0    0   0    1    1   1     1   1\n6/10     0   0    0    0    0    1    1    1   1    1    1   1     1   1\n2/23     0   0    0    0    0    1    1    1   1    1    1   1     1   1\n4/7      0   0    0    0    0    0    1    1   1    1    1   1     1   1\n11/21    0   0    0    0    0    1    1    1   1    1    1   1     1   1\n8/3      0   0    0    0    0    1    1    1   1    1    1   1     1   1"
  },
  {
    "objectID": "basic.html",
    "href": "basic.html",
    "title": "Basic Network Statistics",
    "section": "",
    "text": "Here we will analyze a small network and computer some basic statistics of interest. The first thing we need to do is get some data! For this purpose, we will use the package networkdata (available here). To install the package, use the following code:\n\n   #install.packages(\"remotes\") \n   remotes::install_github(\"schochastics/networkdata\")\n\nTo load the network datasets in the networkdata just type:\n\n   library(networkdata)\n\nThe package contains a bunch of human and animal social networks to browse through them, type:\n\n   data(package = \"networkdata\")\n\nWe will pick one of the movies for this analysis, namely, Pulp Fiction. This is movie_559. In the movie network two characters are linked by an edge if they appear in a scene together. The networkdata data sets come in igraph format, so we need to load that package (or install it using install.packages if you haven’t done that yet).\n\n   #install.packages(\"igraph\") \n   library(igraph)\n   g <- movie_559"
  },
  {
    "objectID": "basic.html#number-of-nodes-and-edges",
    "href": "basic.html#number-of-nodes-and-edges",
    "title": "Basic Network Statistics",
    "section": "Number of Nodes and Edges",
    "text": "Number of Nodes and Edges\nNow we are ready to compute some basic network statistics. As with any network, we want to know what the number of nodes and the number of edges (links) are. Since this is a relatively small network, we can begin by listing the actors.\n\n   V(g)\n\n+ 38/38 vertices, named, from 9e7cc7a:\n [1] BRETT           BUDDY           BUTCH           CAPT KOONS     \n [5] ED SULLIVAN     ENGLISH DAVE    ESMARELDA       FABIENNE       \n [9] FOURTH MAN      GAWKER #2       HONEY BUNNY     JIMMIE         \n[13] JODY            JULES           LANCE           MANAGER        \n[17] MARSELLUS       MARVIN          MAYNARD         MIA            \n[21] MOTHER          PATRON          PEDESTRIAN      PREACHER       \n[25] PUMPKIN         RAQUEL          ROGER           SPORTSCASTER #1\n[29] SPORTSCASTER #2 THE GIMP        THE WOLF        VINCENT        \n[33] WAITRESS        WINSTON         WOMAN           YOUNG MAN      \n[37] YOUNG WOMAN     ZED            \n\n\nThe function V takes the igraph network object as input and returns an igraph.vs object as output (short for “igraph vertex sequence”), listing the names (if given as a graph attribute) of each node. The first line also tells us that there are 38 nodes in this network.\nThe igraph.vs object operates much like an R character vector, so we can query its length to figure out the number of nodes:\n\n   length(V(g))\n\n[1] 38\n\n\nThe analogue function for edges in igraph is E which also takes the network object as input and returns an object of class igraph.es (“igraph edge sequence”) as output:\n\n   E(g)\n\n+ 102/102 edges from 9e7cc7a (vertex names):\n [1] BRETT      --MARSELLUS       BRETT      --MARVIN         \n [3] BRETT      --ROGER           BRETT      --VINCENT        \n [5] BUDDY      --MIA             BUDDY      --VINCENT        \n [7] BRETT      --BUTCH           BUTCH      --CAPT KOONS     \n [9] BUTCH      --ESMARELDA       BUTCH      --GAWKER #2      \n[11] BUTCH      --JULES           BUTCH      --MARSELLUS      \n[13] BUTCH      --PEDESTRIAN      BUTCH      --SPORTSCASTER #1\n[15] BUTCH      --ENGLISH DAVE    BRETT      --FABIENNE       \n[17] BUTCH      --FABIENNE        FABIENNE   --JULES          \n[19] FOURTH MAN --JULES           FOURTH MAN --VINCENT        \n+ ... omitted several edges\n\n\nThis tells us that there are 102 edges (connected dyads) in the network. Some of these include Brett and Marsellus and Fabienne and Jules, but not all can be listed for reasons of space.\nigraph also has two dedicated functions that return the number of nodes and edges in the graph in one fell swoop. They are called vcount and ecount and take the graph object as input:\n\n   vcount(g)\n\n[1] 38\n\n   ecount(g)\n\n[1] 102"
  },
  {
    "objectID": "basic.html#graph-density",
    "href": "basic.html#graph-density",
    "title": "Basic Network Statistics",
    "section": "Graph Density",
    "text": "Graph Density\nOnce we have the number of edges and nodes, we can calculate the most basic derived statistic in a network, which is the density. Since the movie network is an undirected graph, the density is given by:\n\\[\n   \\frac{2m}{n(n-1)}\n\\]\nWhere \\(m\\) is the number of edges and \\(n\\) is the number of nodes, or in our case:\n\n   (2 * 102) / (38 * (38 - 1))\n\n[1] 0.1450925\n\n\nOf course, igraph has a dedicated function called edge_density to compute the density too, which takes the igraph object as input:\n\n   edge_density(g)\n\n[1] 0.1450925"
  },
  {
    "objectID": "basic.html#degree",
    "href": "basic.html#degree",
    "title": "Basic Network Statistics",
    "section": "Degree",
    "text": "Degree\nThe next set of graph metrics are based on the degree of the graph. We can list the graph’s degree set using the igraph function degree:\n\n   degree(g)\n\n          BRETT           BUDDY           BUTCH      CAPT KOONS     ED SULLIVAN \n              7               2              17               5               2 \n   ENGLISH DAVE       ESMARELDA        FABIENNE      FOURTH MAN       GAWKER #2 \n              4               1               3               2               3 \n    HONEY BUNNY          JIMMIE            JODY           JULES           LANCE \n              8               3               4              16               4 \n        MANAGER       MARSELLUS          MARVIN         MAYNARD             MIA \n              5              10               6               3              11 \n         MOTHER          PATRON      PEDESTRIAN        PREACHER         PUMPKIN \n              5               5               3               3               8 \n         RAQUEL           ROGER SPORTSCASTER #1 SPORTSCASTER #2        THE GIMP \n              3               6               2               1               2 \n       THE WOLF         VINCENT        WAITRESS         WINSTON           WOMAN \n              3              25               4               3               5 \n      YOUNG MAN     YOUNG WOMAN             ZED \n              4               4               2 \n\n\nThe degree function takes the igraph network object as input and returns a plain old R named vector as output with the names being the names attribute of vertices in the network object.\nUsually we are interested in who are the “top nodes” in the network by degree (a kind of centrality). To figure that out, all we need to do is sort the degree set (to generate the graph’s degree sequence) and list the top entries:\n\n   d <- degree(g)\n   d.sort <- sort(d, decreasing = TRUE)\n   d.sort[1:8]\n\n    VINCENT       BUTCH       JULES         MIA   MARSELLUS HONEY BUNNY \n         25          17          16          11          10           8 \n    PUMPKIN       BRETT \n          8           7 \n\n\nLine 1 stores the degrees in an object “d”, line 2 creates a “sorted” version of the same object (from bigger to smaller) and line 3 shows the first eight entries of the sorted degree sequence.\nBecause the degree vector “d” is just a regular old vector we can use native R mathematical operations to figure out things like the sum, maximum, minimum, and average degree of the graph:\n\n   sum(d)\n\n[1] 204\n\n   max(d)\n\n[1] 25\n\n   min(d)\n\n[1] 1\n\n   mean(d)\n\n[1] 5.368421\n\n\nSo the sum of degrees is 204, the maximum degree is 25 (belonging to Vincent), the minimum is one, and the average is about 5.4.\nNote that these numbers recreate some well-known equalities in graph theory:\n\nThe sum of degrees is twice the number of edges (the first theorem of graph theory):\n\n\n   2 * ecount(g)\n\n[1] 204\n\n\n\nThe average degree is just the sum of degrees divided by the number of nodes:\n\n\n   sum(d)/vcount(g)\n\n[1] 5.368421\n\n\n\nThe density is just the average degree divided by the number of nodes minus one, as explained here:\n\n\n   mean(d)/(vcount(g) - 1)\n\n[1] 0.1450925\n\n\nSome people also consider the degree variance of the graph as a measure of inequality of connectivity in the system. It is equal to the average sum of square deviations of each node’s degree from the average:\n\\[\n  \\mathcal{v}(G) = \\frac{\\sum_i (k_i - \\bar{k})^2}{n}\n\\]\n\n   sum((d - mean(d))^2)/vcount(g)\n\n[1] 22.96953\n\n\nThis tells us that there is a lot of inequality in the distribution of degrees in the graph (a graph with all nodes equal degree would have variance zero)."
  },
  {
    "objectID": "basic.html#the-degree-distribution-in-undirected-graphs",
    "href": "basic.html#the-degree-distribution-in-undirected-graphs",
    "title": "Basic Network Statistics",
    "section": "The Degree Distribution in Undirected Graphs",
    "text": "The Degree Distribution in Undirected Graphs\nAnother way of looking at inequalities of degrees in a graph is to examine its degree distribution. This gives us the probability of observing a node with a given degree k in the graph.\n\n   deg.dist <- degree_distribution(g)\n   deg.dist <- round(deg.dist, 3)\n   deg.dist\n\n [1] 0.000 0.053 0.158 0.237 0.158 0.132 0.053 0.026 0.053 0.000 0.026 0.026\n[13] 0.000 0.000 0.000 0.000 0.026 0.026 0.000 0.000 0.000 0.000 0.000 0.000\n[25] 0.000 0.026\n\n\nThe igraph function degree_distribution just returns a numeric vector of the same length as the maximum degree of the graph plus one. In this case that’s a vector of length 25 + 1 = 26. The first entry gives us the proportion of nodes with degree zero (isolates), the second the proportion of nodes of degree one, and so on up to the graph’s maximum degree.\nSince there are no isolates in the network, we can ignore the first element of this vector, to get the proportion of nodes of each degree in the Pulp Fiction network. To that, we fist create a two-column data.frame with the degrees in the first column and the proportions in the second:\n\n   degree <- c(1:max(d))\n   prop <- deg.dist\n   prop <- prop[-1]\n   deg.dist <- data.frame(degree, prop)\n   deg.dist\n\n   degree  prop\n1       1 0.053\n2       2 0.158\n3       3 0.237\n4       4 0.158\n5       5 0.132\n6       6 0.053\n7       7 0.026\n8       8 0.053\n9       9 0.000\n10     10 0.026\n11     11 0.026\n12     12 0.000\n13     13 0.000\n14     14 0.000\n15     15 0.000\n16     16 0.026\n17     17 0.026\n18     18 0.000\n19     19 0.000\n20     20 0.000\n21     21 0.000\n22     22 0.000\n23     23 0.000\n24     24 0.000\n25     25 0.026\n\n\nOf course, a better way to display the degree distribution of a graph is via some kind of data visualization, particularly for large networks where a long table of numbers is just not feasible. To do that, we can call on our good friend ggplot:\n\n   # install.packages(ggplot2)\n   library(ggplot2)\n   p <- ggplot(data = deg.dist, aes(x = degree, y = prop))\n   p <- p + geom_bar(stat = \"identity\", fill = \"red\", color = \"red\")\n   p <- p + theme_minimal()\n   p <- p + labs(x = \"\", y = \"Proportion\", \n                 title = \"Degree Distribution in Pulp Fiction Network\") \n   p <- p + geom_vline(xintercept = mean(d), \n                       linetype = 2, linewidth = 0.5, color = \"blue\")\n   p <- p + scale_x_continuous(breaks = c(1, 5, 10, 15, 20, 25))\n   p\n\n\n\n\nThe plot clearly shows that the Pulp Fiction network degree distribution is skewed with a small number of characters having a large degree \\(k \\geq 15\\) while most other characters in the movie have a small degree \\(k \\leq 5\\) indicating inequality of connectivity in the system."
  },
  {
    "objectID": "basic.html#the-degree-correlation",
    "href": "basic.html#the-degree-correlation",
    "title": "Basic Network Statistics",
    "section": "The Degree Correlation",
    "text": "The Degree Correlation\nAnother overall network statistic we may want to know is the degree correlation (Newman 2002). How do we compute it? Imagine taking each edge in the network and creating two degree vectors, one based on the degree of the node in one end and the degre of the node in another. Then the degree assortativity coefficient is just the Pearson product moment correlation between these two vectors.\nLet’s see how this would work for the Pulp Fiction network. First we need to extract an edge list from the graph:\n\n   g.el <- as_edgelist(g) #transforming graph to edgelist\n   head(g.el)\n\n     [,1]    [,2]       \n[1,] \"BRETT\" \"MARSELLUS\"\n[2,] \"BRETT\" \"MARVIN\"   \n[3,] \"BRETT\" \"ROGER\"    \n[4,] \"BRETT\" \"VINCENT\"  \n[5,] \"BUDDY\" \"MIA\"      \n[6,] \"BUDDY\" \"VINCENT\"  \n\n\nWe can see that the as_edgelist function takes the igraph network object as input and returns an \\(E \\times 2\\) matrix, with \\(E = 102\\) being the number of rows. Each column of the matrix records the name of the node on each end of the edge. So the first row of the edge list with entries “BRETT” and “MARSELLUS” tells us that there is an edge linking Brett and Marsellus, and so forth for each row.\nTo compute the correlation between the degrees of each node, all we need to do is attach the corresponding degrees to each name for each of the columns of the edge list, which can be done via data wrangling magic from the dplyr package (part of the tidyverse):\n\n   # install.packages(dplyr)\n   library(dplyr)\n   deg.dat <- data.frame(name1 = names(d), name2 = names(d), d)\n   el.temp <- data.frame(name2 = g.el[, 2]) %>% \n      left_join(deg.dat, by = \"name2\") %>% \n      dplyr::select(c(\"name2\", \"d\")) %>% \n      rename(d2 = d) \n   d.el <- data.frame(name1 = g.el[, 1]) %>% \n      left_join(deg.dat, by = \"name1\") %>% \n      dplyr::select(c(\"name1\", \"d\")) %>% \n      rename(d1 = d) %>% \n      cbind(el.temp)\nhead(d.el)\n\n  name1 d1     name2 d2\n1 BRETT  7 MARSELLUS 10\n2 BRETT  7    MARVIN  6\n3 BRETT  7     ROGER  6\n4 BRETT  7   VINCENT 25\n5 BUDDY  2       MIA 11\n6 BUDDY  2   VINCENT 25\n\n\nLine 3 creates a two-column data frame called “deg.dat” with as many rows as there are nodes in the network. The first two columns contain the names of each node (identically listed with different names) and the third columns contains the corresponding node’s degree.\nLines 4-7 use dplyr functions to create a new object “el.temp” joining the degree information to each of the node names listed in the second position in the original edge list “g.el,” and rename the imported column of degrees “d2.”\nLines 8-12 do the same for the nodes listed in the first position in the original edge list, renames the imported columns of degrees “d1,” and the binds the columns of the “el.temp” object to the new object “d.el.” The resulting object has four columns: Two for the names of the nodes incident to each edge on the edge list (columns 1 and 3), and two other ones corresponding to the degrees of the corresponding nodes (columns 2 and 4).\nWe can see from the output of the first few rows of the “d.el” object that indeed “BRETT” is assigned a degree of 7 in each row of the edge list, “BUDDY” a degree of 2, “MARSELLUS” a degree of 10, “VINCENT” a degree of 25 and so forth.\nNow to compute the degree correlation in the network all we need to do is call the native R function cor on the two columns from “d.el” that containing the degree information. Note that because each degree appears twice at the end of each edge in an undirected graph (as both “sender” and “receiver”), we need to double each column by appending the other degree column at the end. So the first degree column is the vector:\n\n   d1 <- c(d.el$d1, d.el$d2)\n\nAnd the second degree column is the vector:\n\n   d2 <- c(d.el$d2, d.el$d1)\n\nAnd the graph’s degree correlation (Newman 2003) is just the Pearson correlation between these two degree vectors:\n\n   cor(d1, d2)\n\n[1] -0.2896427\n\n\nThe result \\(r_{deg} = -0.29\\) tells us that there is anti-correlation by degree in the Pulp Fiction network. That is high-degree characters tend to appear with low degree characters, or conversely, high-degree characters (like Marsellus and Jules) don’t appear together very often.\nIf you hate dplyr (and some people do with a passion) here’s a relatively quick way to do the same thing without it:\n\n   A <- as.matrix(as_adjacency_matrix(g)) #vectorized version of adjacency matrix\n   dat <- data.frame(e = as.vector(A), \n                     rd = rep(rowSums(A), ncol(A)), \n                     cd = rep(colSums(A), each = nrow(A)),\n                     rn = rep(rownames(A), ncol(A)),\n                     cn = rep(colnames(A), each = nrow(A))\n                     )\n   cor(dat[dat$e == 1, ]$rd, dat[dat$e == 1, ]$cd)\n\n[1] -0.2896427\n\n\nOf course, igraph has a function called assortativity_degree that does all the work for us:\n\n   assortativity_degree(g)\n\n[1] -0.2896427"
  },
  {
    "objectID": "basic.html#the-average-shortest-path-length",
    "href": "basic.html#the-average-shortest-path-length",
    "title": "Basic Network Statistics",
    "section": "The Average Shortest Path Length",
    "text": "The Average Shortest Path Length\nThe final statistic people use to characterize networks is the average shortest path length. In a network, even non-adjacent nodes, could be indirectly connected to other nodes via a path of some length (\\(l > 1\\)) So it is useful to know what the average of this quantity is across all dyads in the network.\nTo do that, we first need to compute the length of the shortest path \\(l\\) for each pair of nodes in the network (also known as the geodesic distance). Adjacent nodes get an automatic score of \\(l = 1\\). In igraph this is done as follows:\n\n   S <- distances(g)\n   S[1:7, 1:7]\n\n             BRETT BUDDY BUTCH CAPT KOONS ED SULLIVAN ENGLISH DAVE ESMARELDA\nBRETT            0     2     1          2           2            2         3\nBUDDY            2     0     2          2           2            2         4\nBUTCH            1     2     0          1           2            1         2\nCAPT KOONS       2     2     1          0           2            2         3\nED SULLIVAN      2     2     2          2           0            2         4\nENGLISH DAVE     2     2     1          2           2            0         3\nESMARELDA        3     4     2          3           4            3         0\n\n\nThe igraph function distances takes the network object as input and returns the desired shortest path matrix. So for instance, Brett is directly connected to Butch (they appear in a scene together) but indirectly connected to Buddy via a path of length two (they both appear in scenes with common neighbors even if they don’t appear together).\nThe maximum distance between two nodes in the graph (the longest shortest path to put it confusingly) is called the graph diameter. We can find this out simply by using the native R function for the maximum on the shortest paths matrix:\n\n   max(S)\n\n[1] 8\n\n\nThis means that in the Pulp Fiction network the maximum degree of separation between two characters is a path of length 8.\nOf course, we cann also call the igraph function diameter:\n\n   diameter(g)\n\n[1] 8\n\n\nOnce we have the geodesic distance matrix, it is easy to calculate the average path length of the graph:\n\n   rs.S <- rowSums(S)\n   rm.S <- rs.S/(vcount(g) - 1)\n   mean(rm.S)\n\n[1] 2.769559\n\n\n\nFirst (line 1) we sum all the rows (or columns) of the geodesic distance matrix. This vector (of the same length as the number of nodes) gives us the sum of the geodesic distance of each node to each of the nodes (we will use this to compute closeness centrality later).\nThen (line 2) we divide this vector by the number of nodes minus one (to exclude the focal node) to create a vector of the average distance of each node to each of the other nodes.\nFinally (line 3) we take the average across all nodes of this average distance vector to get the graph’s average shortest path length, which in this case equals L = 2.8.\n\nThis means that, on average, each character in Pulp Fiction is separated by little less than three contacts in the co-appearance network (a fairly small world).\nOf course this can also be done in just one step on igraph:\n\n   mean_distance(g)\n\n[1] 2.769559"
  },
  {
    "objectID": "basic.html#putting-it-all-together",
    "href": "basic.html#putting-it-all-together",
    "title": "Basic Network Statistics",
    "section": "Putting it all Together",
    "text": "Putting it all Together\nNow we can put together all the basic network statistics that we have computed into some sort of summary table, like the ones here. We first create a vector with the names of each statistic:\n\n   Stat <- c(\"Nodes\", \"Edges\", \"Min. Degree\", \"Max. Degree\", \"Avg. Degree\", \"Degree Corr.\", \"Diameter\", \"Avg. Shortest Path Length\")\n\nThen we create a vector with the values:\n\n   Value <- c(vcount(g), ecount(g), min(d), max(d), round(mean(d), 2), round(assortativity_degree(g), 2), max(S), round(mean_distance(g), 2))\n\nWe can then put these two vector together into a data frame:\n\n   net.stats <- data.frame(Stat, Value)\n\nWe can then use the package kableExtra (a nice table maker) to create a nice html table:\n\n   # intall.packages(kableExtra)\n   library(kableExtra)\n   kbl(net.stats, format = \"pipe\", align = c(\"l\", \"c\"),\n       caption = \"Key Statistics for Pulp Fiction Network.\") %>% \n   kable_styling(bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\nKey Statistics for Pulp Fiction Network.\n \n  \n    Stat \n    Value \n  \n \n\n  \n    Nodes \n    38.00 \n  \n  \n    Edges \n    102.00 \n  \n  \n    Min. Degree \n    1.00 \n  \n  \n    Max. Degree \n    25.00 \n  \n  \n    Avg. Degree \n    5.37 \n  \n  \n    Degree Corr. \n    -0.29 \n  \n  \n    Diameter \n    8.00 \n  \n  \n    Avg. Shortest Path Length \n    2.77"
  },
  {
    "objectID": "basic.html#graph-degree-metrics-in-directed-graphs",
    "href": "basic.html#graph-degree-metrics-in-directed-graphs",
    "title": "Basic Network Statistics",
    "section": "Graph Degree Metrics in Directed Graphs",
    "text": "Graph Degree Metrics in Directed Graphs\nAs we saw earlier, the degree distribution and the degree correlation are two basic things we want to have a sense of when characterizing a network, and we showed examples for the undirected graph case. However, when you have a network measuring a tie with some meaningful “from” “to” directionality (represented as a directed graph) the number of things you have to compute in terms of degrees “doubles.”\nFor instance, instead of a single degree set and sequence, now we have two: An outdegree and an indegree set and sequence. The same thing applies to the degree distribution and the degree correlation.\n\nThe Indegree and Outdegree Distributions\nIn the case of the degree distribution, now we have two distributions: An outdegree distribution and an indegree distribution.\nLet’s see an example using the law_advice data.\n\n   library(networkdata)\n   library(igraph)\n   g <- law_advice\n   i.prop <- degree_distribution(g, mode = \"in\")\n   o.prop <- degree_distribution(g, mode = \"out\")\n\nSo the main complication is that now we have to specify a value for the “mode” argument; “in” for indegree and “out” for outdegree.\nThat also means that when plotting, we have to create two data frames and present two plots.\nFirst the data frames:\n\n   i.d <- degree(g, mode = \"in\")\n   o.d <- degree(g, mode = \"out\")\n   i.d.vals <- c(0:max(i.d))\n   o.d.vals <- c(0:max(o.d))\n   i.deg.dist <- data.frame(i.d.vals, i.prop)\n   o.deg.dist <- data.frame(o.d.vals, o.prop)\n   head(i.deg.dist)\n\n  i.d.vals     i.prop\n1        0 0.01408451\n2        1 0.02816901\n3        2 0.07042254\n4        3 0.02816901\n5        4 0.08450704\n6        5 0.02816901\n\n   head(o.deg.dist)\n\n  o.d.vals     o.prop\n1        0 0.01408451\n2        1 0.00000000\n3        2 0.01408451\n4        3 0.05633803\n5        4 0.04225352\n6        5 0.04225352\n\n\nNow, to plotting. To be effective, the resulting plot has to show the outdegree and indegree distribution side by side so as to allow the reader to compare. To do that, we first generate each plot separately:\n\n   library(ggplot2)\n   p <- ggplot(data = o.deg.dist, aes(x = o.d.vals, y = o.prop))\n   p <- p + geom_bar(stat = \"identity\", fill = \"red\", color = \"red\")\n   p <- p + theme_minimal()\n   p <- p + labs(x = \"\", y = \"Proportion\", \n                 title = \"Outdegree Distribution in Law Advice Network\") \n   p <- p + geom_vline(xintercept = mean(o.d), \n                       linetype = 2, linewidth = 0.75, color = \"blue\")\n   p1 <- p + scale_x_continuous(breaks = c(0, 5, 10, 15, 20, 25, 30, 35, 40)) + xlim(0, 40)\n\n   p <- ggplot(data = i.deg.dist, aes(x = i.d.vals, y = i.prop))\n   p <- p + geom_bar(stat = \"identity\", fill = \"red\", color = \"red\")\n   p <- p + theme_minimal()\n   p <- p + labs(x = \"\", y = \"Proportion\", \n                 title = \"Indegree Distribution in Law Advice Network\") \n   p <- p + geom_vline(xintercept = mean(i.d), \n                       linetype = 2, linewidth = 0.75, color = \"blue\")\n   p2 <- p + scale_x_continuous(breaks = c(0, 5, 10, 15, 20, 25, 30, 35, 40)) + xlim(0, 40)\n\nThen we use the magical package patchwork to combine the plots:\n\n   # install.packages(\"patchwork\")\n   library(patchwork)\n   p <- p1 / p2\n   p\n\n\n\n\nThe data clearly shows that while both distributions are skewed, the indegree distribution is more heterogeneous, with a larger proportion of nodes in the high end of receiving advice as compared to giving advice.\nNote also that since the mean degree is the same regardless of whether we use the out or indegree distribution, then the blue line pointing to the mean degree falls in the same spot on the x-axis for both plots.\n\n\nThe Four Different Flavors of Degree Correlations in the Directed Case\nThe same doubling (really quadrupling) happens to degree correlations in directed graphs. While in an undirected graph, there is a single degree correlation, in the directed case we have four quantities to compute: The out-out degree correlation, the in-in degree correlation, the out-in degree correlation, and the in-out degree correlation (see here, p. 38).\nTo proceed, we need to create an edge list data set with six columns: The node id of the “from” node, the node id of the “to” node, the indegree of the “from” node, the outdegree of the “from” node, the indegree of the “to” node, and the outdegree of the “to” node.\nWe can adapt the code we used for the undirected case for this purpose. First, we create an edge list data frame using the igraph function as_data_frame:\n\n   library(dplyr)\n   g.el <- igraph::as_data_frame(g) %>% \n      rename(fr = from)\n   head(g.el)\n\n  fr to\n1  1  2\n2  1 17\n3  1 20\n4  2  1\n5  2  6\n6  2 17\n\n\nNote that we have to specify that this is in an igraph function by typing igraph:: in front of the as_data_frame because there is an (older) dplyr function with the same name that was used for data wrangling.\nSecond, we create data frames containing the in and outdegrees of each node in the network:\n\n    deg.dat.fr <- data.frame(fr = 1:vcount(g), o.d, i.d)\n    deg.dat.to <- data.frame(to = 1:vcount(g), o.d, i.d)\n\nThird, we merge this info into the edge list data frame to get the in and outdegrees of the from and to nodes in the directed edge:\n\n   d.el <- g.el %>% \n      left_join(deg.dat.fr) %>% \n      rename(o.d.fr = o.d, i.d.fr = i.d) %>% \n      left_join(deg.dat.to, by = \"to\") %>% \n      rename(o.d.to = o.d, i.d.to = i.d) \n   head(d.el)\n\n  fr to o.d.fr i.d.fr o.d.to i.d.to\n1  1  2      3     22      7     23\n2  1 17      3     22     21     26\n3  1 20      3     22     11     22\n4  2  1      7     23      3     22\n5  2  6      7     23      0     21\n6  2 17      7     23     21     26\n\n\nNow we can compute the four different flavors of the degree correlation for directed graphs:\n\n   round(cor(d.el$o.d.fr, d.el$o.d.to), 4) #out-out correlation\n\n[1] -0.0054\n\n   round(cor(d.el$i.d.fr, d.el$i.d.to), 4) #in-in correlation\n\n[1] 0.187\n\n   round(cor(d.el$i.d.fr, d.el$o.d.to), 4) #in-out correlation\n\n[1] -0.0283\n\n   round(cor(d.el$o.d.fr, d.el$i.d.to), 4) #out-in correlation\n\n[1] -0.0843\n\n\nThese results tell us that there is not much degree assortativity going on in the law advice network, except for a slight tendency of people who receive advice from lots of others to give advice to people who also receive advice from a lot of other people (the “in-in” correlation)\nNote that by default, the assortativity_degree function in igraph only returns the out-in correlation for directed graphs:\n\n   round(assortativity_degree(g, directed = TRUE), 4)\n\n[1] -0.0843\n\n\nThat is, assortativity_degree checks if more active senders are more likely to send ties to people who are popular receivers of ties."
  },
  {
    "objectID": "blondel.html",
    "href": "blondel.html",
    "title": "Role Similarity Across Graphs",
    "section": "",
    "text": "Sometimes we may want to figure out how similar a given node’s position in one social network is to that of another node in a different network. This calls for a method that could allow us to compare how similar a node in one graph is to other nodes in another graph.\nA particularly interesting version of this problem arises when we have information on the same set of nodes across different set of relations. In that case, we may be interested in answering the question as to whether nodes occupy similar or dissimilar positions across the networks defined by the different relations.\nBlondel et al. (2004) describe an approach that can help us make headway on this problem. They use a similar iterative procedure that we saw can be used to compute status scores from directed graphs (like PageRank and HITS) but this time to compute similarity scores between pairs of nodes across graphs.\nThe idea, just like with the status scores, is that the two set of nodes in each graph start with the same set of similarity scores, and then we update them as we traverse the connectivity structure of the two graphs.\nSo let’s say the adjacency matrix of the first graph is \\(\\mathbf{A}\\) and that of the second graph is \\(\\mathbf{B}\\). The first graph has \\(n_A\\) number of nodes and the corresponding quantity in the second graph is \\(n_B\\) our target similarity matrix \\(\\mathbf{Z}\\), comparing the node sets in the two graphs, will therefore be of dimensions \\(n_B \\times n_A\\).\nWe initialize \\(z_{ij}(0) = 1\\) for all \\(i\\) and \\(j\\); that is, \\(\\mathbf{Z}(0)\\) is a matrix full of ones. At each time step subsequent to that, we fill up the \\(\\mathbf{Z}\\) matrix with new values according to:\n\\[\n   \\mathbf{Z}(t + 1) = \\mathbf{B}\\mathbf{Z(t)}\\mathbf{A}^T + \\mathbf{B}^T\\mathbf{Z(t)}\\mathbf{A}\n\\]\nTo ensure convergence, we then normalize the \\(\\mathbf{Z}\\) matrix after every update using our trusty Euclidean norm:\n\\[\n\\mathbf{Z}(t > 0) = \\frac{\\mathbf{Z}}{||\\mathbf{Z}||_2}\n\\]"
  },
  {
    "objectID": "blondel.html#computing-node-similarities-across-different-graphs",
    "href": "blondel.html#computing-node-similarities-across-different-graphs",
    "title": "Role Similarity Across Graphs",
    "section": "Computing Node Similarities Across Different Graphs",
    "text": "Computing Node Similarities Across Different Graphs\nLet us see how this would work with real data. We will compare two subgraphs of the larger law_advice network (Lazega 2001) from the networkdata package. This is a directed advice-seeking network so a node goes from advisee to adviser.\nWe create two subgraphs. One composed of older male partners (aged fifty or older) and the other composed of the women in the firm (both parterns and associates). They look lik this:\n\n\n\n\n\n\nOlder Men Partners\n\n\n\n\n\n\n\n\n\nWomen Lawyers\n\n\n\n\n\nA function to compute the Blondel similarity as described earlier can be written as:\n\n   blondel.sim <- function(A, B) {\n      K <- matrix(1, nrow(B), nrow(A))\n      if (is.null(rownames(A)) == TRUE) {\n         rownames(A) <- 1:nrow(A)\n         colnames(A) <- 1:nrow(A)\n         }\n      if (is.null(rownames(B)) == TRUE) {\n         rownames(B) <- 1:nrow(B)\n         colnames(B) <- 1:nrow(B)\n         }\n      k <- 1\n      diff <- 1\n      old.diff <- 2\n      while (diff != old.diff | k %% 2 == 0) {\n         old.diff <- diff\n         K.old <- K\n         K <- (B %*% K.old %*% t(A)) + (t(B) %*% K.old %*% A)\n         K <- K/norm(K, type = \"F\")\n         diff <- abs(sum(abs(K)) - sum(abs(K.old)))\n         k <- k + 1\n      }\n   for (j in 1:ncol(K)) {\n      K[, j] <- K[, j]/max(K[, j])\n      }\n   rownames(K) <- rownames(B)\n   colnames(K) <- rownames(A)\n   return(list(K = K, k = k, diff = diff))\n   }\n\nWhich is modeled after our status game function but instead of computing a vector of scores we are populating a whole matrix!\nThe basic task is to figure out which nodes from the first matrix are most similar to which nodes from the second. That is, given these two networks can be identify actors who play similar roles in each?\nAnd here are the results presented in tabular form:\n\n   library(kableExtra)\n   A <- as.matrix(as_adjacency_matrix(g1))\n   B <- as.matrix(as_adjacency_matrix(g2))\n   K <- round(blondel.sim(A, B)[[1]], 2)\n   kbl(K, align = \"c\", format = \"html\", row.names = TRUE) %>% \n      column_spec(1, bold = TRUE) %>% \n      kable_styling(full_width = TRUE,\n                     bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\n \n  \n      \n    1 \n    2 \n    3 \n    4 \n    5 \n    6 \n    7 \n    8 \n    9 \n    10 \n    11 \n    12 \n    13 \n  \n \n\n  \n    1 \n    0.25 \n    0.22 \n    0.41 \n    0.48 \n    0.45 \n    0.11 \n    0.47 \n    0.24 \n    0.20 \n    0.30 \n    0.47 \n    0.25 \n    0.36 \n  \n  \n    2 \n    0.39 \n    0.38 \n    0.38 \n    0.27 \n    0.16 \n    0.37 \n    0.15 \n    0.38 \n    0.34 \n    0.38 \n    0.36 \n    0.34 \n    0.34 \n  \n  \n    3 \n    0.64 \n    0.63 \n    0.54 \n    0.33 \n    0.09 \n    0.67 \n    0.10 \n    0.63 \n    0.56 \n    0.58 \n    0.51 \n    0.55 \n    0.51 \n  \n  \n    4 \n    0.65 \n    0.66 \n    0.52 \n    0.26 \n    0.00 \n    0.71 \n    0.00 \n    0.63 \n    0.60 \n    0.59 \n    0.46 \n    0.58 \n    0.50 \n  \n  \n    5 \n    1.00 \n    1.00 \n    0.99 \n    0.73 \n    0.28 \n    1.00 \n    0.34 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n  \n  \n    6 \n    0.66 \n    0.67 \n    0.56 \n    0.33 \n    0.04 \n    0.71 \n    0.06 \n    0.65 \n    0.64 \n    0.62 \n    0.53 \n    0.62 \n    0.57 \n  \n  \n    7 \n    0.05 \n    0.04 \n    0.12 \n    0.14 \n    0.16 \n    0.00 \n    0.15 \n    0.04 \n    0.04 \n    0.08 \n    0.12 \n    0.05 \n    0.09 \n  \n  \n    8 \n    0.32 \n    0.31 \n    0.37 \n    0.32 \n    0.24 \n    0.26 \n    0.24 \n    0.30 \n    0.29 \n    0.34 \n    0.37 \n    0.31 \n    0.34 \n  \n  \n    9 \n    0.49 \n    0.44 \n    0.88 \n    1.00 \n    1.00 \n    0.20 \n    1.00 \n    0.47 \n    0.40 \n    0.63 \n    0.96 \n    0.50 \n    0.71 \n  \n  \n    10 \n    0.31 \n    0.28 \n    0.73 \n    0.89 \n    0.89 \n    0.04 \n    0.91 \n    0.29 \n    0.30 \n    0.47 \n    0.82 \n    0.39 \n    0.60 \n  \n  \n    11 \n    0.86 \n    0.86 \n    1.00 \n    0.82 \n    0.58 \n    0.76 \n    0.57 \n    0.86 \n    0.79 \n    0.92 \n    0.97 \n    0.82 \n    0.86 \n  \n  \n    12 \n    0.15 \n    0.15 \n    0.18 \n    0.16 \n    0.04 \n    0.15 \n    0.08 \n    0.15 \n    0.20 \n    0.17 \n    0.21 \n    0.20 \n    0.23 \n  \n  \n    13 \n    0.34 \n    0.34 \n    0.42 \n    0.37 \n    0.26 \n    0.29 \n    0.28 \n    0.33 \n    0.34 \n    0.38 \n    0.43 \n    0.36 \n    0.40 \n  \n  \n    14 \n    0.18 \n    0.17 \n    0.32 \n    0.35 \n    0.33 \n    0.09 \n    0.34 \n    0.17 \n    0.16 \n    0.23 \n    0.35 \n    0.19 \n    0.27 \n  \n  \n    15 \n    0.40 \n    0.41 \n    0.41 \n    0.28 \n    0.16 \n    0.39 \n    0.15 \n    0.40 \n    0.38 \n    0.41 \n    0.37 \n    0.38 \n    0.36 \n  \n  \n    16 \n    0.53 \n    0.51 \n    0.92 \n    0.99 \n    0.87 \n    0.29 \n    0.91 \n    0.52 \n    0.54 \n    0.69 \n    1.00 \n    0.61 \n    0.80 \n  \n  \n    17 \n    0.46 \n    0.44 \n    0.70 \n    0.74 \n    0.49 \n    0.33 \n    0.58 \n    0.46 \n    0.53 \n    0.55 \n    0.81 \n    0.57 \n    0.71 \n  \n  \n    18 \n    0.34 \n    0.31 \n    0.67 \n    0.81 \n    0.63 \n    0.15 \n    0.72 \n    0.34 \n    0.41 \n    0.46 \n    0.82 \n    0.47 \n    0.67 \n  \n\n\n\n\n\nIn the table each column is normalized by its maximum, so a 1.0 in that column tells us that that node (from the first network) is maximally similar to the corresponding row node (from the second network).\nFor instance, node 5 in the women’s lawyers graph (a highly central node in terms of being an adviser) is most similar to node 1 in the older men partner’s graph (also a highly central node in terms of being an adviser).\nNode 9 in the women lawyer’s graph, who’s mostly an advise-seeker, is most similar to node 5 in the older men partner graph who’s also an advise-seeker. So it looks like it works!"
  },
  {
    "objectID": "blondel.html#equivalence-to-hits",
    "href": "blondel.html#equivalence-to-hits",
    "title": "Role Similarity Across Graphs",
    "section": "Equivalence to HITS",
    "text": "Equivalence to HITS\nOne neat thing that Blondel et al. (2004) show is that we can also take a network and compare it to ideal-typical small graphs and get scores for how much each node in the observed network resembles each of the nodes in the hypothetical ideal-typical structure.\nMore specifically, they show that if we can run their algorithm to compare any network to the following two-node graph:\n\n   g <- make_empty_graph(2)\n   g <- add_edges(g, c(1,2))\n   V(g)$name <- c(\"Hub\", \"Authority\")\n   plot(g, \n        edge.arrow.size=1, \n        vertex.color=\"tan2\", \n        vertex.size=12, vertex.frame.color=\"lightgray\", \n        vertex.label.color=\"black\", vertex.label.cex=1.5, \n        vertex.label.dist=-3)\n\n\n\n\nIn which case, the result “similarity” scores, will be equivalent to the Hub and Authority scores!\nWe can check that this is the case for the women’s lawyers advice graph:\n\n   A <- as.matrix(as_adjacency_matrix(g))\n   B <- as.matrix(as_adjacency_matrix(g2))\n   K <- round(blondel.sim(A, B)[[1]], 4)\n   tab <- cbind(K, Hub.Score = round(hits_scores(g2)$hub, 4), \n         Auth.Score = round(hits_scores(g2)$authority, 4))\n   kbl(tab, align = \"c\", format = \"html\", row.names = TRUE) %>% \n      column_spec(1, bold = TRUE) %>% \n      kable_styling(full_width = TRUE,\n                     bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\n \n  \n      \n    Hub \n    Authority \n    Hub.Score \n    Auth.Score \n  \n \n\n  \n    1 \n    0.3810 \n    0.0000 \n    0.3810 \n    0.0000 \n  \n  \n    2 \n    0.1434 \n    0.3415 \n    0.1434 \n    0.3415 \n  \n  \n    3 \n    0.0813 \n    0.6021 \n    0.0813 \n    0.6021 \n  \n  \n    4 \n    0.0000 \n    0.7826 \n    0.0000 \n    0.7826 \n  \n  \n    5 \n    0.1654 \n    1.0000 \n    0.1654 \n    1.0000 \n  \n  \n    6 \n    0.0000 \n    0.7826 \n    0.0000 \n    0.7826 \n  \n  \n    7 \n    0.1434 \n    0.0000 \n    0.1434 \n    0.0000 \n  \n  \n    8 \n    0.2235 \n    0.2858 \n    0.2235 \n    0.2858 \n  \n  \n    9 \n    1.0000 \n    0.0386 \n    1.0000 \n    0.0386 \n  \n  \n    10 \n    0.8973 \n    0.0000 \n    0.8973 \n    0.0000 \n  \n  \n    11 \n    0.6110 \n    0.6559 \n    0.6110 \n    0.6559 \n  \n  \n    12 \n    0.0000 \n    0.2095 \n    0.0000 \n    0.2095 \n  \n  \n    13 \n    0.2239 \n    0.3364 \n    0.2239 \n    0.3364 \n  \n  \n    14 \n    0.3197 \n    0.0523 \n    0.3197 \n    0.0523 \n  \n  \n    15 \n    0.1434 \n    0.4036 \n    0.1434 \n    0.4036 \n  \n  \n    16 \n    0.8432 \n    0.2174 \n    0.8432 \n    0.2174 \n  \n  \n    17 \n    0.4089 \n    0.3188 \n    0.4089 \n    0.3188 \n  \n  \n    18 \n    0.5222 \n    0.0955 \n    0.5222 \n    0.0955 \n  \n\n\n\n\n\nThe first two columns are the scores using the function to compute the Blondel et al. similarity to each of the two nodes in the Hub/Authority micro-graph and the third and fourth columns are the scores we get from the igraph function hits scores, which as we can see, are identical."
  },
  {
    "objectID": "blondel.html#computing-a-brokerage-score",
    "href": "blondel.html#computing-a-brokerage-score",
    "title": "Role Similarity Across Graphs",
    "section": "Computing a Brokerage Score",
    "text": "Computing a Brokerage Score\nOf course in a directed graph, there are more than two ideal typical “roles.” In addition to “sender” (Hub) or “receiver” (Authority) we may also have “intermediaries” or “pass along” nodes. We can thus get an “intermediary” score for each node by comparing any network to the following three-node graph:\n\n   g <- make_empty_graph(3)\n   g <- add_edges(g, c(1,2, 2,3))\n   V(g)$name <- c(\"Hub\", \"Broker\", \"Authority\")\n   plot(g, \n        edge.arrow.size=1, \n        vertex.color=\"tan2\", \n        vertex.size=12, vertex.frame.color=\"lightgray\", \n        vertex.label.color=\"black\", vertex.label.cex=1.5, \n        vertex.label.dist=2)\n\n\n\n\nHere are the results for the women lawyers graph:\n\n   A <- as.matrix(as_adjacency_matrix(g))\n   B <- as.matrix(as_adjacency_matrix(g2))\n   K <- round(blondel.sim(A, B)[[1]], 4)\n   kbl(K, align = \"c\", format = \"html\", row.names = TRUE) %>% \n      column_spec(1, bold = TRUE) %>% \n      kable_styling(full_width = TRUE,\n                     bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\n \n  \n      \n    Hub \n    Broker \n    Authority \n  \n \n\n  \n    1 \n    0.5190 \n    0.3321 \n    0.1135 \n  \n  \n    2 \n    0.1318 \n    0.3472 \n    0.3600 \n  \n  \n    3 \n    0.0998 \n    0.4586 \n    0.6923 \n  \n  \n    4 \n    0.0000 \n    0.4888 \n    0.7547 \n  \n  \n    5 \n    0.5241 \n    0.8968 \n    1.0000 \n  \n  \n    6 \n    0.0954 \n    0.5091 \n    0.7547 \n  \n  \n    7 \n    0.1318 \n    0.1234 \n    0.0000 \n  \n  \n    8 \n    0.2501 \n    0.3584 \n    0.2755 \n  \n  \n    9 \n    1.0000 \n    0.8236 \n    0.2000 \n  \n  \n    10 \n    0.9913 \n    0.6859 \n    0.0318 \n  \n  \n    11 \n    0.5445 \n    1.0000 \n    0.7371 \n  \n  \n    12 \n    0.1971 \n    0.1427 \n    0.1530 \n  \n  \n    13 \n    0.3175 \n    0.4117 \n    0.2970 \n  \n  \n    14 \n    0.3710 \n    0.2875 \n    0.0918 \n  \n  \n    15 \n    0.1318 \n    0.4205 \n    0.3780 \n  \n  \n    16 \n    0.9998 \n    0.8747 \n    0.2453 \n  \n  \n    17 \n    0.7783 \n    0.5841 \n    0.3102 \n  \n  \n    18 \n    0.9644 \n    0.5162 \n    0.1302 \n  \n\n\n\n\n\nColumns one and three gives us versions of the Hub and Authority scores (respectively), but column two now gives us a “score” for how much the row node resembles and intermediary (or broker) in the network. We can see that the “purest” broker in the women’s advice network is node 11."
  },
  {
    "objectID": "centrality.html",
    "href": "centrality.html",
    "title": "Centrality",
    "section": "",
    "text": "In this handout we will go through the basic centrality metrics. Particularly, the “big three” according to Freeman (1979), namely, degree, closeness (in two flavors) and betweenness.\nWe first load our trusty Pulp Fiction data set from the networkdata package, which is an undirected graph of character scene co-appearances in the film:"
  },
  {
    "objectID": "centrality.html#degree-centrality",
    "href": "centrality.html#degree-centrality",
    "title": "Centrality",
    "section": "Degree Centrality",
    "text": "Degree Centrality\nDegree centrality is the simplest and most straightforward measure. In fact, we are already computed in the lecture notes on basic network statistics since it is the same as obtaining the graph’s degree sequence. So the igraph function degree would do it as we already saw.\nHere we follow a different approach using the row (or column) sums of the graph’s adjacency matrix:\n\n   A <- as_adjacency_matrix(g)\n   A <- as.matrix(A)\n   rowSums(A)\n\n          BRETT           BUDDY           BUTCH      CAPT KOONS     ED SULLIVAN \n              7               2              17               5               2 \n   ENGLISH DAVE       ESMARELDA        FABIENNE      FOURTH MAN       GAWKER #2 \n              4               1               3               2               3 \n    HONEY BUNNY          JIMMIE            JODY           JULES           LANCE \n              8               3               4              16               4 \n        MANAGER       MARSELLUS          MARVIN         MAYNARD             MIA \n              5              10               6               3              11 \n         MOTHER          PATRON      PEDESTRIAN        PREACHER         PUMPKIN \n              5               5               3               3               8 \n         RAQUEL           ROGER SPORTSCASTER #1 SPORTSCASTER #2        THE GIMP \n              3               6               2               1               2 \n       THE WOLF         VINCENT        WAITRESS         WINSTON           WOMAN \n              3              25               4               3               5 \n      YOUNG MAN     YOUNG WOMAN             ZED \n              4               4               2 \n\n\nThe igraph function as_adjancency_matrix doesn’t quite return a regular R matrix object, so we have to further coerce the resulting object into a numerical matrix containing zeroes and ones using the as.matrix function in line 2. Then we can apply the native rowSums function to obtain each node’s degree. Note that this is same output we got using the degree function before."
  },
  {
    "objectID": "centrality.html#indegree-and-outdegree",
    "href": "centrality.html#indegree-and-outdegree",
    "title": "Centrality",
    "section": "Indegree and Outdegree",
    "text": "Indegree and Outdegree\nThe movie network is based on the relationship of co-appearance in a scene which by nature lacks any natural directionality (it’s a symmetric relation) and can therefore be represented in an undirected graph. The concepts of in and outdegree, by contrast, are only applicable to directed relations. So to illustrate them, we need to switch to a different source of data.\nWe pick an advice network which is a classical directed kind of (asymmetric) relation. I can give advice to you, but that doesn’t necessarily mean you can give advice to me. The networkdata package contains one such data set collected in the late 80s early 1990s in a New England law firm (see the description here), called law_advice:\n\n   d.g <- law_advice\n   V(d.g)\n\n+ 71/71 vertices, from d1a9da7:\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50\n[51] 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n\n   vertex_attr(d.g)\n\n$status\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2\n[39] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n\n$gender\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 1 1 1 1 2 1 1 1 2\n[39] 2 1 1 1 2 2 1 2 1 2 1 1 2 1 1 1 1 1 2 1 2 2 2 1 1 2 1 1 2 1 2 1 2\n\n$office\n [1] 1 1 2 1 2 2 2 1 1 1 1 1 1 2 3 1 1 2 1 1 1 1 1 1 2 1 1 2 1 2 2 2 2 1 2 1 3 1\n[39] 1 1 1 1 1 3 1 2 3 1 1 2 2 1 1 1 1 1 1 2 2 1 1 1 2 1 1 1 1 1 1 1 1\n\n$seniority\n [1] 31 32 13 31 31 29 29 28 25 25 23 24 22  1 21 20 23 18 19 19 17  9 16 15 15\n[26] 15 13 11 10  7  8  8  8  8  8  5  5  7  6  6  5  4  5  5  3  3  3  1  4  3\n[51]  4  4 10  3  3  3  3  3  2  2  2  2  2  2  2  1  1  1  1  1  1\n\n$age\n [1] 64 62 67 59 59 55 63 53 53 53 50 52 57 56 48 46 50 45 46 49 43 49 45 44 43\n[26] 41 47 38 38 39 34 33 37 36 33 43 44 53 37 34 31 31 47 53 38 42 38 35 36 31\n[51] 29 29 38 29 34 38 33 33 30 31 34 32 29 45 28 43 35 26 38 31 26\n\n$practice\n [1] 1 2 1 2 1 1 2 1 2 2 1 2 1 2 2 2 2 1 2 1 1 1 1 1 2 1 1 2 2 1 1 1 1 2 2 1 2 1\n[39] 1 1 1 2 1 2 2 2 1 2 1 2 1 1 2 1 1 1 1 1 1 2 2 2 2 2 1 1 1 1 1 2 1\n\n$law_school\n [1] 1 1 1 3 2 1 3 3 1 3 1 2 2 1 3 1 1 2 1 1 2 3 2 2 2 3 1 2 3 3 2 3 3 2 3 3 3 2\n[39] 1 1 2 2 2 1 3 2 3 3 2 2 3 3 3 3 3 2 2 3 2 2 3 2 2 2 3 3 2 3 3 2 2\n\n\nWe can see that the graph has 71 vertices, and that there are various attributes associated with each vertex, like gender, age, seniority, status in the law firm, etc. We can query those attributes using the igraph function vertex_attr, which takes the graph object as input.\n\nSubsetting the Graph According to a Node Attribute\nTo keep things manageable, we will restrict our analysis to partners. To do that we need to select the subgraph that only includes the vertices with value of 1 in the “status” vertex attribute. From the data description, we know the first 36 nodes (with value of 1 in the status attribute) are the law firm’s partners (the rest are associates). In igraph we can do this as using the subgraph function:\n\n   d.g <- subgraph(d.g, 1:36)\n   V(d.g)\n\n+ 36/36 vertices, from 14f08a2:\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30 31 32 33 34 35 36\n\n   V(d.g)$status\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n\nThe first line just tells igraph to generate the subgraph containing the first 36 nodes (the partners). The subgraph function thus takes two main inputs: The graph object, and then a vector of node ids (or node labels) telling the function which nodes to select to create the node-induced subgraph.\nOf course we already knew from the data description that the first 36 nodes where the partners. But let’s say we have a large data set and we don’t know which nodes are the partners. A smarter way of selecting a subgraph based on a node attribute is as follows:\n\n   partners <- which(V(law_advice)$status == 1)\n   d.g <- subgraph(law_advice, partners)\n   V(d.g)\n\n+ 36/36 vertices, from 14f709f:\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30 31 32 33 34 35 36\n\n\nThe first line using the native R vector function which allowing us to subset a vector based on a logical condition. The function takes a vector followed by a logical condition as input, and returns the position of the vector elements that meet that condition. In this case, we took the vector of values for the attribute of status and selected the node ids where status is equal to 1. We then fed that vector to the subgraph function in line 2.\nWe could do this with any other attribute:\n\n   older <- which(V(law_advice)$age > 50)\n   older\n\n [1]  1  2  3  4  5  6  7  8  9 10 12 13 14 38 44\n\n   og <- subgraph(law_advice, older)\n   V(og)\n\n+ 15/15 vertices, from 14fd537:\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15\n\n\nHere we selected the subgraph (called “og”, get it, get it) formed by the subset of nodes over the age of 50 at the firm. The values of the vector older tell us which of the 71 members meet the relevant condition.\n\n\nComputing in and outdegree\nOK, going back to the partners subgraph, we can now create our (asymmetric) adjacency matrix and compute the row and column sums:\n\n   d.A <- as_adjacency_matrix(d.g)\n   d.A <- as.matrix(d.A)\n   rowSums(d.A)\n\n [1]  3  7  7 17  4  0  4  2  3  7  5 18 11 13 10 19 17  5 21 10  9 12  9 16  8\n[26] 22 18 22 13 15 16  9 15  6 15  7\n\n   colSums(d.A)\n\n [1] 18 17  8 14 10 17  4  8 10  6 11 14 14 12 15 13 21  7  7 17 11 10  2 14  7\n[26] 20  2 14 12 11  8 13  2 16  8  2\n\n\nNote that in contrast to the undirected case the row and column sums give you two different sets of numbers. The row sums provide the directed graph’s outdegree set (number of outgoing links incident to each node), and the column sums provide the graph’s indegree set (number of incoming links incident to each node). So if you are high in the first vector, you are an advice giver (perhaps indicating informal status or experience) and if you are high in the second you are advice taker.\nOf course igraph has a dedicated function for this, which is just our old friend degree with an extra option mode, indicating whether you want the in or outdegrees:\n\n   d.o <- degree(d.g, mode = \"out\")\n   d.i <- degree(d.g, mode = \"in\")\n   d.o\n\n [1]  3  7  7 17  4  0  4  2  3  7  5 18 11 13 10 19 17  5 21 10  9 12  9 16  8\n[26] 22 18 22 13 15 16  9 15  6 15  7\n\n   d.i\n\n [1] 18 17  8 14 10 17  4  8 10  6 11 14 14 12 15 13 21  7  7 17 11 10  2 14  7\n[26] 20  2 14 12 11  8 13  2 16  8  2\n\n\nNote that the graph attributes are just vectors of values, and can be accessed from the graph object using the $ operator attached to the V() function as we did above.\nSo if we wanted to figure out the correlation between some vertex attribute and in or out degree centrality, all we need to do is correlate the two vectors:\n\n   r <- cor(d.o, V(d.g)$age)\n   round(r, 2)\n\n[1] -0.43\n\n\nWhich tells us that at least in this case, younger partners are more sought after as sources of advice than older partners."
  },
  {
    "objectID": "centrality.html#closeness-centrality",
    "href": "centrality.html#closeness-centrality",
    "title": "Centrality",
    "section": "Closeness Centrality",
    "text": "Closeness Centrality\nRecall that the closeness centrality is defined as the inverse of the sum of the lengths of shortest paths from each node to every other node. That means that to compute it, we first need to calculate the geodesic distance matrix. This is matrix in which each entry \\(g_{ij}\\) records the length of the shortest path(s) between row node \\(i\\) and column node \\(j\\). Then, we sum the rows (or columns) of this symmetric matrix and then we obtain the inverse to get the closeness of each node:\n\n   S <- distances(g) #length of shortest paths matrix\n   d.sum <- rowSums(S)\n   close1 <- round(1/d.sum, 4)\n   close1\n\n          BRETT           BUDDY           BUTCH      CAPT KOONS     ED SULLIVAN \n         0.0125          0.0108          0.0143          0.0125          0.0108 \n   ENGLISH DAVE       ESMARELDA        FABIENNE      FOURTH MAN       GAWKER #2 \n         0.0105          0.0070          0.0096          0.0104          0.0097 \n    HONEY BUNNY          JIMMIE            JODY           JULES           LANCE \n         0.0114          0.0093          0.0093          0.0132          0.0093 \n        MANAGER       MARSELLUS          MARVIN         MAYNARD             MIA \n         0.0111          0.0104          0.0103          0.0098          0.0115 \n         MOTHER          PATRON      PEDESTRIAN        PREACHER         PUMPKIN \n         0.0125          0.0111          0.0097          0.0097          0.0114 \n         RAQUEL           ROGER SPORTSCASTER #1 SPORTSCASTER #2        THE GIMP \n         0.0105          0.0125          0.0096          0.0057          0.0073 \n       THE WOLF         VINCENT        WAITRESS         WINSTON           WOMAN \n         0.0056          0.0139          0.0083          0.0105          0.0125 \n      YOUNG MAN     YOUNG WOMAN             ZED \n         0.0083          0.0083          0.0073 \n\n\nOf course, we could have just used the available function in igraph and computed the closeness centrality directly from the graph object using the function closeness:\n\n   close2 <- round(closeness(g), 4)\n   close2\n\n          BRETT           BUDDY           BUTCH      CAPT KOONS     ED SULLIVAN \n         0.0125          0.0108          0.0143          0.0125          0.0108 \n   ENGLISH DAVE       ESMARELDA        FABIENNE      FOURTH MAN       GAWKER #2 \n         0.0105          0.0070          0.0096          0.0104          0.0097 \n    HONEY BUNNY          JIMMIE            JODY           JULES           LANCE \n         0.0114          0.0093          0.0093          0.0132          0.0093 \n        MANAGER       MARSELLUS          MARVIN         MAYNARD             MIA \n         0.0111          0.0104          0.0103          0.0098          0.0115 \n         MOTHER          PATRON      PEDESTRIAN        PREACHER         PUMPKIN \n         0.0125          0.0111          0.0097          0.0097          0.0114 \n         RAQUEL           ROGER SPORTSCASTER #1 SPORTSCASTER #2        THE GIMP \n         0.0105          0.0125          0.0096          0.0057          0.0073 \n       THE WOLF         VINCENT        WAITRESS         WINSTON           WOMAN \n         0.0056          0.0139          0.0083          0.0105          0.0125 \n      YOUNG MAN     YOUNG WOMAN             ZED \n         0.0083          0.0083          0.0073 \n\n\nOnce we have the closeness centrality values, we are interested in who are the top nodes. The following code creates a table with the top five:\n\n   library(kableExtra)\n   close2 <- sort(close2, decreasing = TRUE)\n   close2 <- data.frame(close2[1:5])\n   kbl(close2, format = \"pipe\", align = c(\"l\", \"c\"),\n       col.names = c(\"Character\", \"Closeness\"),\n       caption = \"Top Five Closeness Characters in Pulp Fiction Network.\") %>% \n   kable_styling(bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\nTop Five Closeness Characters in Pulp Fiction Network.\n \n  \n    Character \n    Closeness \n  \n \n\n  \n    BUTCH \n    0.0143 \n  \n  \n    VINCENT \n    0.0139 \n  \n  \n    JULES \n    0.0132 \n  \n  \n    BRETT \n    0.0125 \n  \n  \n    CAPT KOONS \n    0.0125 \n  \n\n\n\n\n\nIt makes sense that the three main characters are also the ones that are at closest distances from everyone else!"
  },
  {
    "objectID": "centrality.html#edge-closeness",
    "href": "centrality.html#edge-closeness",
    "title": "Centrality",
    "section": "Edge Closeness",
    "text": "Edge Closeness\nBröhl and Lehnertz (2022) define the closeness of an edge as a function of the closeness of the two nodes incident to it. An edge \\(e_{jk}\\) linking vertex \\(v_j\\) to \\(v_k\\) has high closeness whenever vertices \\(v_j\\) and \\(v_k\\) also have high closeness.\nMore specifically, the closeness centrality of an edge is proportional to the ratio of the product of the closeness of the two nodes incident to it divided by their sum:\n\\[\nC(e_{jk}) = (E - 1)\\frac{C(v_j) \\times C(v_k)}{C(v_j)+C(v_k)}\n\\]\nNote that the equation normalizes the ratio of the product to the sum of the vertex closeness centralities by the number of edges minus one.\nTo compute edge closeness in a real network, we can use the same approach to data wrangling we used to compute the degree correlation in Handout 1. The goal is to create an edge list data frame containing five columns. The ids of the two nodes in the edge, the closeness centralities of the two nodes in the edge, and the closeness centrality of the edge calculated according to the above equation.\nIn the Pulp Fiction network this looks like:\n\n   library(dplyr)\n   g.el <- as_edgelist(g) #transforming graph to edgelist\n   c <- round(closeness(g), 3)  #closeness centrality vector\n   c.dat <- data.frame(name1 = names(c), name2 = names(c), c)\n   el.temp <- data.frame(name2 = g.el[, 2]) %>% \n      left_join(c.dat, by = \"name2\") %>% \n      dplyr::select(c(\"name2\", \"c\")) %>% \n      rename(c2 = c) \n   c.el <- data.frame(name1 = g.el[, 1]) %>% \n      left_join(c.dat, by = \"name1\") %>% \n      dplyr::select(c(\"name1\", \"c\")) %>% \n      rename(c1 = c) %>% \n      cbind(el.temp) %>% \n      mutate(e.clos = round((ecount(g)-1)*(c1*c2)/(c+c2), 3))\nhead(c.el)\n\n  name1    c1     name2    c2 e.clos\n1 BRETT 0.013 MARSELLUS 0.010  0.571\n2 BRETT 0.013    MARVIN 0.010  0.625\n3 BRETT 0.013     ROGER 0.013  0.632\n4 BRETT 0.013   VINCENT 0.014  0.681\n5 BUDDY 0.011       MIA 0.011  0.555\n6 BUDDY 0.011   VINCENT 0.014  0.622\n\n\nTo create a table of the top five closeness centrality edges, we just order the data frame by the last column and table it:\n\n   c.el <- c.el[order(c.el$e.clos, decreasing = TRUE), ] %>% \n      dplyr::select(c(\"name1\", \"name2\", \"e.clos\"))\n\n   kbl(c.el[1:5, ], format = \"pipe\", align = c(\"l\", \"l\", \"c\"),\n       col.names = c(\"i\", \"j\", \"Edge Clos.\"), row.names = FALSE,\n       caption = \"Edges Sorted by Closeness in the Pulp Fiction Network\") %>% \n   kable_styling(bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\nEdges Sorted by Closeness in the Pulp Fiction Network\n \n  \n    i \n    j \n    Edge Clos. \n  \n \n\n  \n    BUTCH \n    VINCENT \n    0.900 \n  \n  \n    BRETT \n    BUTCH \n    0.875 \n  \n  \n    MOTHER \n    VINCENT \n    0.875 \n  \n  \n    BUTCH \n    MIA \n    0.864 \n  \n  \n    JULES \n    PUMPKIN \n    0.850 \n  \n\n\n\n\n\nInterestingly, the top closeness edges tend to bring somewhat strange bedfellows together, characters that themselves don’t spend much time together in the film (e.g., the Butch/Vincent interaction is relatively brief and somewhat embarrassing for Vincent) but who themselves can reach other character clusters in the film via relatively short paths."
  },
  {
    "objectID": "centrality.html#closeness-centrality-in-directed-graphs",
    "href": "centrality.html#closeness-centrality-in-directed-graphs",
    "title": "Centrality",
    "section": "Closeness Centrality in Directed Graphs",
    "text": "Closeness Centrality in Directed Graphs\nWhat about closeness centrality for a directed network? Let us see how this works using a subgraph of the advice network, this time selecting just women under the age of forty:\n\n   women <- which(V(law_advice)$gender == 2)\n   wg <- subgraph(law_advice, women)\n   young <- which(V(wg)$age < 40)\n   wg <- subgraph(wg, young)\n   V(wg)\n\n+ 12/12 vertices, from 159163f:\n [1]  1  2  3  4  5  6  7  8  9 10 11 12\n\n\nThis network is small enough that a plot could be informative about its structure. Let us plot it using the package ggraph, a visualization package that follows the same principles as the ggplot grammar of graphics but for network graphs (see here).\n\n   #install.packages(\"ggraph\")\n   library(ggraph)\n    p <- ggraph(wg, layout = 'auto')\n    p <- p + geom_edge_parallel(color = \"steelblue\", edge_width = 0.5,\n                                arrow = arrow(length = unit(2.5, 'mm')),\n                                end_cap = circle(4, 'mm'), \n                                sep = unit(3, 'mm'))\n    p <- p + geom_node_point(aes(x = x, y = y), size = 8, color = \"tan2\") \n    p <- p + geom_node_text(aes(label = 1:vcount(wg)), size = 4, color = \"white\")\n    p <- p + theme_graph() \n    p\n\n\n\n\nWomen lawyers advice network\n\n\n\n\nNow a question we might ask is who has the greatest closeness centrality in this advice network. We could proceed as usual and compute the geodesic distances between actors:\n\n   S <- distances(wg)\n   S\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]\n [1,]    0    1    2    1    3    3    4    2    2     3     3     3\n [2,]    1    0    2    1    2    3    3    1    1     3     3     3\n [3,]    2    2    0    1    1    1    2    2    3     1     1     1\n [4,]    1    1    1    0    2    2    3    2    2     2     2     2\n [5,]    3    2    1    2    0    1    1    1    2     2     2     2\n [6,]    3    3    1    2    1    0    2    2    3     1     2     1\n [7,]    4    3    2    3    1    2    0    2    3     3     3     3\n [8,]    2    1    2    2    1    2    2    0    1     3     3     3\n [9,]    2    1    3    2    2    3    3    1    0     4     4     4\n[10,]    3    3    1    2    2    1    3    3    4     0     1     1\n[11,]    3    3    1    2    2    2    3    3    4     1     0     1\n[12,]    3    3    1    2    2    1    3    3    4     1     1     0\n\n\nNote that this is not quite right. In igraph the default settings of the distance function treats the graph as undirected. So it doesn’t use the strict directed paths, but it just treats them all as semi-paths ignoring direction. That is why, for instance, it counts node 1 as being “adjacent” to node 4 even though there is only one incoming link from 4 to 1 and why the whole matrix is symmetric, when we know from just eyeballing the network that there is a lot of asymmetry in terms of who can reach who via directed paths.\nTo get the actual directed distance matrix, we need to specify the “mode” option, asking whether we want in or out paths. Here, let’s select out-paths:\n\n   S <- distances(wg, mode = \"out\")\n   S\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]\n [1,]    0    1  Inf  Inf  Inf  Inf  Inf  Inf  Inf   Inf   Inf   Inf\n [2,]    1    0  Inf  Inf  Inf  Inf  Inf  Inf  Inf   Inf   Inf   Inf\n [3,]    2    2    0    1  Inf    1  Inf  Inf  Inf   Inf   Inf   Inf\n [4,]    1    1    1    0  Inf    2  Inf  Inf  Inf   Inf   Inf   Inf\n [5,]    3    2    1    2    0    1    1    1    2   Inf   Inf   Inf\n [6,]    3    3    1    2  Inf    0  Inf  Inf  Inf   Inf   Inf   Inf\n [7,]    4    3    2    3    1    2    0    2    3   Inf   Inf   Inf\n [8,]    2    1  Inf  Inf  Inf  Inf  Inf    0    1   Inf   Inf   Inf\n [9,]    2    1  Inf  Inf  Inf  Inf  Inf    1    0   Inf   Inf   Inf\n[10,]    3    3    1    2  Inf    1  Inf  Inf  Inf     0     1     2\n[11,]    3    3    1    2  Inf    2  Inf  Inf  Inf     1     0     1\n[12,]    3    3    1    2  Inf    1  Inf  Inf  Inf     1     1     0\n\n\nThis is better but introduces a problem. The directed graph is not strongly connected, so it means that some nodes cannot reach other ones via a directed path of any length. That means that the geodesic distances from a node to an unreachable node is coded as “infinite” (Inf). The problem with infinity is that it gets in the way of calculating sums of distances, a requirement for the closeness centrality.\n\n   S <- distances(wg, mode = \"out\")\n   rowSums(S)\n\n [1] Inf Inf Inf Inf Inf Inf Inf Inf Inf Inf Inf Inf\n\n\nAdding infinity to a number just returns infinity so all the rows with at least one “Inf” in the distance matrix get an Inf for the row sum. In this case that’s all of them. A bummer.\n\nHarmonic Centrality\nBut dont’ worry there’s a patch. It is called the harmonic centrality (Rochat 2009).1 This is a variation on the closeness centrality that works whether you are working with connected or disconnected graphs (or in the case of directed graphs regardless of whether the graph is strongly or weakly connected), and therefore regardless of whether the geodesic distance matrix contains Infs.2\nThe main difference between the harmonic and regular closeness centrality is that instead of calculating the inverse of the sum of the distances for each node, we calculate the sum of the inverses:\n\n   S <- distances(wg, mode = \"out\")\n   S = round(1/S, 2) \n   diag(S) <- 0 #setting diagonals to zero\n   S\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]\n [1,] 0.00 1.00  0.0 0.00    0  0.0    0  0.0 0.00     0     0   0.0\n [2,] 1.00 0.00  0.0 0.00    0  0.0    0  0.0 0.00     0     0   0.0\n [3,] 0.50 0.50  0.0 1.00    0  1.0    0  0.0 0.00     0     0   0.0\n [4,] 1.00 1.00  1.0 0.00    0  0.5    0  0.0 0.00     0     0   0.0\n [5,] 0.33 0.50  1.0 0.50    0  1.0    1  1.0 0.50     0     0   0.0\n [6,] 0.33 0.33  1.0 0.50    0  0.0    0  0.0 0.00     0     0   0.0\n [7,] 0.25 0.33  0.5 0.33    1  0.5    0  0.5 0.33     0     0   0.0\n [8,] 0.50 1.00  0.0 0.00    0  0.0    0  0.0 1.00     0     0   0.0\n [9,] 0.50 1.00  0.0 0.00    0  0.0    0  1.0 0.00     0     0   0.0\n[10,] 0.33 0.33  1.0 0.50    0  1.0    0  0.0 0.00     0     1   0.5\n[11,] 0.33 0.33  1.0 0.50    0  0.5    0  0.0 0.00     1     0   1.0\n[12,] 0.33 0.33  1.0 0.50    0  1.0    0  0.0 0.00     1     1   0.0\n\n\nNote that in this matrix of inverse distances, the closest (adjacent) nodes get the maximum score of one, and nodes farther apart when smaller scores (approaching zero). More importantly, those pesky Infs disappear (!) because unreachable directed pairs of nodes get the lowest score, corresponding to \\(1/\\infty = 0\\). Turns out the mathematics of infinity weren’t our enemy after all.\nAlso note that the reachability relation expressed in this matrix is asymmetric: So node 4 and reach node 1 (there is a directed tie from 4 to 1), but node 1 cannot reach 4. This is precisely what we want.\nOnce we have this matrix of inverse distances, we can then we can compute the harmonic centrality the same way as regular closeness by adding up the row scores for each node and dividing by the number of nodes minus one (to get the average):\n\n   d.harm <- rowSums(S)\n   d.harm <- d.harm/(vcount(wg) - 1)\n   d.harm <- round(d.harm, 2)\n   d.harm\n\n [1] 0.09 0.09 0.27 0.32 0.53 0.20 0.34 0.23 0.23 0.42 0.42 0.47\n\n\nWe can see that the highest harmonic closeness centrality node is 5, followed by 12. Here’s a plot of the network highlighting the highest harmonic (closeness) centrality node.\n\n   col <- rep(\"tan2\", vcount(wg)) #creating node color vector\n   col[which(d.harm == max(d.harm))] <- \"red\" #changing color of max centrality node to red\n   p <- p + geom_node_point(aes(x = x, y = y), size = 8, color = col)\n   p <- p + geom_node_text(aes(label = 1:vcount(wg)), size = 4, color = \"white\")\n   p\n\n\n\n\nWomen lawyers advice network with highest closeness centrality node in red\n\n\n\n\nOf course, igraph has a built in function to calculate the harmonic centrality called (you guessed it) harmonic_centrality:\n\n   d.harm <- harmonic_centrality(wg, normalized = TRUE)\n   d.harm <- round(d.harm, 2)\n   d.harm\n\n [1] 0.09 0.09 0.27 0.32 0.53 0.20 0.34 0.23 0.23 0.42 0.42 0.47\n\n\nWhich gives us the same results."
  },
  {
    "objectID": "centrality.html#generalized-harmonic-centrality",
    "href": "centrality.html#generalized-harmonic-centrality",
    "title": "Centrality",
    "section": "Generalized Harmonic Centrality",
    "text": "Generalized Harmonic Centrality\nAgneessens, Borgatti, and Everett (2017) propose a “generalized” version of the harmonic centrality that yields plain old degree centrality and the regular harmonic centrality as special cases. The key is to introduce a parameter \\(\\delta\\) governing how much weight we give to shortest paths based on distance. Let’s see how this works.\nRecall that the harmonic centrality we defined earlier is given by:\n\\[\n\\frac{\\sum_{j \\neq i} (g_{ij})^{-1}}{n-1}\n\\]\nFor any node \\(i\\), where \\(g_{ij}\\) is the geodesic distance between \\(i\\) and every other node in the graph \\(j\\), which could be “infinite” if there is no path linking them.\nAgneessens et al’s tweak is to instead compute:\n\\[\n\\frac{\\sum_{j \\neq i} (g_{ij})^{-\\delta}}{n-1}\n\\]\nWhere \\(\\delta\\) is a free parameter chosen by the researcher with the restriction that \\(\\delta \\geq 0\\) (if you want to calculate a closeness measure as we will see below).\nWhen \\(\\delta = \\infty\\) the numerator element \\(1/(g_{ij})^{\\infty} = 1\\) only when nodes are adjacent and \\(g_{ij} = 1\\) (because \\(1^{\\infty} = 1\\)); otherwise, for \\(g_{ij} > 1\\) then \\(1/(g_{ij})^{\\infty} = 0\\), and therefore the generalized harmonic centrality just becomes a (normalized) version of degree centrality. Alternatively, when \\(\\delta = 1\\) we just get the plain old harmonic centrality we defined earlier.\nThe interesting cases come from \\(1 > \\delta < \\infty\\) and \\(0 > \\delta < 1\\). In the first case, nodes at shorter distances are weighted more (like in the standard harmonic centrality measure) as \\(\\delta\\) becomes bigger and bigger then the generalized harmonic centrality approximates degree. For values below one, as \\(\\delta\\) approaches zero, then indirect connections to nodes of greater length are discounted less, and thus count for “more” in defining your generalized harmonic centrality score.\nLet us see a real-world example of the generalized harmonic centrality in action:\nFirst, we create a custom function to compute the generalized harmonic centrality:\n\n   g.harm <- function(x, d) {\n      library(igraph)\n      S <- distances(x) #get distances from graph object\n      S <- 1/S^d #matrix of generalized inverse distances\n      diag(S) <- 0 #set diagonals to zero\n      c <- rowSums(S)/(vcount(x) - 1) #summing and averaging\n      return(c)\n   }\n\nSecond, we compute three versions of the harmonic centrality, with \\(\\delta = 5\\), \\(\\delta = 0.05\\), and \\(\\delta = -5\\), using the full (unrestricted by age) subgraph of the law_advice network composed of the women lawyers at the firm, with relations constrained to be undirected:\n\n   women <- which(V(law_advice)$gender == 2)\n   wg <- subgraph(law_advice, women)\n   wg <- as.undirected(wg)\n   c1 <- g.harm(wg, d = 5)\n   c2 <- g.harm(wg, d = 0.05)\n   c3 <- g.harm(wg, d = -5)\n\n\nThe first version of the harmonic centrality in line 5, with a positive value of \\(\\delta\\) above zero, will compute centrality scores emphasizing direct (one-step) connections, thus coming closer to degree.\nThe second version, in line 6, with a value of \\(\\delta\\) close to zero, will give comparatively more emphasis to indirect connections weighing longer paths almost as much as shorter paths (but always a little less), thus being more similar to closeness centrality.\nFinally, the last version, in line 7, with \\(\\delta < 0\\), will weigh longer paths more than shorter ones, serving as a measure of eccentricity (farness from others) not closeness.\n\n\n\n\n\n\nFull women lawyers advice network\n\n\n\n\nAbove is a plot of the women lawyers network showing the top node for each of the centralities:\n\nIn red we have node 3 who has the largest degree (\\(k(3) = 8\\)) and thus comes out on top using the generalized harmonic centrality version emphasizing direct connections (\\(\\delta > 1\\)).\nThen in blue we have node 9 who can reach the most others via the shortest paths, and thus comes out on top when the generalized harmonic centrality emphasizes indirect connectivity.\nFinally, in purple we have node 12, which is farthest from everyone else, and thus comes out on “top” when longer indirect connections count for more (\\(\\delta < 0)\\).\n\nAs we said earlier, both regular harmonic centrality and degree are special cases of the generalized measure. We can check this by setting \\(\\delta\\) to either one or infinity.\nWhen we set \\(\\delta=1\\) the generalized harmonic centrality is the same as (normalized by number of nodes minus one) the regular harmonic centrality:\n\n   g.harm(wg, d = 1)\n\n [1] 0.5980392 0.5343137 0.7058824 0.5980392 0.6568627 0.6274510 0.4264706\n [8] 0.5392157 0.6960784 0.6470588 0.6666667 0.4068627 0.5882353 0.5196078\n[15] 0.5833333 0.6029412 0.5588235 0.5441176\n\n   harmonic_centrality(wg, normalized = TRUE)\n\n [1] 0.5980392 0.5343137 0.7058824 0.5980392 0.6568627 0.6274510 0.4264706\n [8] 0.5392157 0.6960784 0.6470588 0.6666667 0.4068627 0.5882353 0.5196078\n[15] 0.5833333 0.6029412 0.5588235 0.5441176\n\n\nWhen we set \\(\\delta=\\infty\\) the generalized harmonic centrality is the same as (normalized by number of nodes minus one) degree centrality:\n\n   g.harm(wg, d = Inf)\n\n [1] 0.23529412 0.17647059 0.47058824 0.23529412 0.35294118 0.29411765\n [7] 0.05882353 0.17647059 0.41176471 0.35294118 0.41176471 0.05882353\n[13] 0.23529412 0.17647059 0.23529412 0.35294118 0.23529412 0.23529412\n\n   degree(wg, normalized = TRUE)\n\n [1] 0.23529412 0.17647059 0.47058824 0.23529412 0.35294118 0.29411765\n [7] 0.05882353 0.17647059 0.41176471 0.35294118 0.41176471 0.05882353\n[13] 0.23529412 0.17647059 0.23529412 0.35294118 0.23529412 0.23529412"
  },
  {
    "objectID": "centrality.html#betweenness",
    "href": "centrality.html#betweenness",
    "title": "Centrality",
    "section": "Betweenness",
    "text": "Betweenness\nWe finally come to betweenness centrality. Recall that the key conceptual distinction between closeness and betweenness according to Freeman (1979) is that between (pun intended) the capacity to reach others quickly (e.g., via the shortest paths) and the capacity to intermediate among those same paths. High betweenness nodes control the flow of information in the network between other nodes.\nThis is evident in the way betweenness is calculated. Recall that the betweenness of a node k relative to any pair of nodes i and j in the network is simply:\n\\[\n\\frac{\\sigma_{i(k)j}}{\\sigma_{ij}}\n\\]\nWhere the denominator of the fraction (\\(\\sigma_{ij}\\)) is a count of the total number of shortest paths that start and end with nodes i and j and the numerator of the fraction (\\(\\sigma_{i(k)j}\\)) is the subset of those paths that include node k as an inner node.\nAs Freeman (1979) also notes because this is a ratio, it can range from zero to one, with everything in between. As such the betweenness centrality of a node relative to any two others has an intuitive interpretation as a probability, namely the probability that if you send something from i to j it has to go through k. This probability is 1.0 if k stands in every shortest path between i and j and zero if they stand in none of the shortest paths indirectly connecting i and j.\nThe betweenness of a given node is just the sum all of these probabilities across every pair of nodes in the graph for each node:\n\\[\n\\sum_{i \\neq j, i \\neq n, j \\neq v} \\frac{\\sigma_{i(k)j}}{\\sigma_{ij}}\n\\]\nBelow we can see a point and line diagram of the undirected Pulp Fiction network we have been working with.\n\n\n\n\n\nPulp Fiction character schene co-appearance network.\n\n\n\n\nWe should expect a character to have high betweenness in this network to the extent that they appear in scenes with characters who themselves don’t appear in any scenes together, thus inter-mediating between different parts of the story. Characters who only appear in one scene with some others (like The Wolf or The Gimp) are likely to be low in betweenness.\nLet’s create a top ten table of betweenness for the Pulp Fiction network. We use the igraph function betweenness to calculate the scores:\n\n   pulp.bet <- betweenness(g)\n   top.5.bet <- sort(pulp.bet, decreasing = TRUE)[1:10]\n   kbl(round(top.5.bet, 2), format = \"pipe\", align = c(\"l\", \"c\"),\n       col.names = c(\"Character\", \"Betweenness\"),\n       caption = \"Top Five Betweenness Characters in Pulp Fiction Network.\") %>% \n   kable_styling(bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\nTop Five Betweenness Characters in Pulp Fiction Network.\n \n  \n    Character \n    Betweenness \n  \n \n\n  \n    BUTCH \n    275.52 \n  \n  \n    VINCENT \n    230.19 \n  \n  \n    JULES \n    142.11 \n  \n  \n    MIA \n    76.68 \n  \n  \n    MAYNARD \n    70.00 \n  \n  \n    HONEY BUNNY \n    49.97 \n  \n  \n    PUMPKIN \n    49.97 \n  \n  \n    SPORTSCASTER #1 \n    36.00 \n  \n  \n    BRETT \n    29.85 \n  \n  \n    PREACHER \n    28.23 \n  \n\n\n\n\n\nUnsurprisingly, the top four characters are also the highest in betweenness. Somewhat surprisingly, the main antagonist of the story (the pawn shop owner) is also up there. After that we see a big drop in the bottom five of the top ten.\nNow let us examine betweenness centrality in our directed young women lawyers advice network:\n\n   women <- which(V(law_advice)$gender == 2)\n   wg <- subgraph(law_advice, women)\n   young <- which(V(wg)$age < 40)\n   wg <- subgraph(wg, young)\n   w.bet <- betweenness(wg)\n   w.bet\n\n [1]  0.0000000  3.0000000 16.3333333 11.0000000  7.0000000  0.0000000\n [7]  0.0000000  5.0000000  0.0000000  0.3333333  1.0000000  0.3333333\n\n\nHere we see that node 3 is the highest in betweenness, pictured below:\n\n\n\n\n\nWomen lawyers advice network with highest closeness centrality node in blue and highest betweenness centrality node in red\n\n\n\n\nThis result makes sense. Node 3 intermediates all the connections linking the tightly knit group of nodes on the left side (6, 10, 11, 12) with the rest of the network. Also if nodes 5 and 7 need to pass something along to the rest, they have to use 3 at least half time. Node 4 also needs 3 to reach 6.\nThis result nicely illustrates the difference between closeness and betweenness."
  },
  {
    "objectID": "centrality.html#edge-betweenness",
    "href": "centrality.html#edge-betweenness",
    "title": "Centrality",
    "section": "Edge Betweenness",
    "text": "Edge Betweenness\nEdge betweenness is defined in similar fashion as node betweenness:\n\\[\n\\sum_{i \\neq j} \\frac{\\sigma_{i(e)j}}{\\sigma_{ij}}\n\\]\nWhere \\(\\sigma_{i(e)j}\\) is a count of the number of shortest paths between i and j that feature edge e as an intermediary link. This tells us that the betweenness of an edge e is the sum of the ratios of the number of times that edge appears in the middle of a shortest path connecting every pair of nodes in the graph i and j divided by the total number of shortest paths linking each pair of nodes.\nLike before, the edge betweenness with respect to a specific pair of nodes in the graph is a probability: Namely, that if you send something–using a shortest path–from any node i to any other node j it has to go through edge e. The resulting edge betweenness scores is the sum of these probabilities across every possible pair of nodes for each edge in the graph.\nFor this example, we will work with a simplified version of the women lawyers advice network, in which we transform it into an undirected graph. We use the igraph function as.undirected for that:\n\n   wg <- as.undirected(wg, mode = \"collapse\")\n\nThe “collapse” value in the “mode” argument tells as.undirected to link every connected dyad in the original directed graph using an undirected edge. It does that by removing the directional arrow of the single directed links and collapsing (hence the name) all the bi-directional links into a single undirected one.\nThe resulting undirected graph looks like this:\n\n\n\n\n\nLooking at this point and line plot of the women lawyers advice network, which edge do you think has the top betweenness?\nWell no need to figure that out via eyeballing! We can just use the igraph function edge_betweenness:\n\n   w.ebet <- edge_betweenness(wg)\n\nThe edge_betweenness function takes the igraph graph object as input and produces a vector of edge betweenness values of the same length as the number of edges in the graph, which happens to be 20 in this case.\nUsing this information, we can then create a table of the top ten edges ordered by betweenness:\n\n   edges <- as_edgelist(wg) #creating an edgelist\n   etab <- data.frame(edges, bet = round(w.ebet, 2)) #adding bet. scores to edgelist\n   etab <- etab[order(etab$bet, decreasing = TRUE), ] #ordering by bet.\n   kbl(etab[1:10, ], format = \"pipe\", align = c(\"l\", \"l\", \"c\"),\n       col.names = c(\"i\", \"j\", \"Edge Bet.\"), row.names = FALSE,\n       caption = \"Edges Sorted by Betweenness in the Women Lawyers Advice Network\") %>% \n   kable_styling(bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\nEdges Sorted by Betweenness in the Women Lawyers Advice Network\n \n  \n    i \n    j \n    Edge Bet. \n  \n \n\n  \n    3 \n    4 \n    19.17 \n  \n  \n    5 \n    8 \n    15.83 \n  \n  \n    3 \n    5 \n    13.67 \n  \n  \n    5 \n    7 \n    11.00 \n  \n  \n    2 \n    4 \n    9.17 \n  \n  \n    3 \n    11 \n    8.33 \n  \n  \n    5 \n    6 \n    8.17 \n  \n  \n    1 \n    4 \n    7.00 \n  \n  \n    2 \n    8 \n    6.50 \n  \n  \n    8 \n    9 \n    6.33 \n  \n\n\n\n\n\nNot surprisingly, the top edges are the ones linking nodes 3 and 4 and nodes 5 and 8.\n\nDisconnecting a Graph Via Bridge Removal\nHigh betweenness edges are likely to function as bridges being the only point of indirect connectivity between most nodes in the social structure. That means that an easy way to disconnect a connected graph is to remove the bridges (Girvan and Newman 2002).\nIn igraph we can produce an edge deleted subgraph of an original graph using the “minus” operator, along with the edge function like this:\n\n   del.g <- wg - edge(\"3|4\")\n   del.g <- del.g - edge(\"5|8\")\n\nThe first line creates a new graph object (a subgraph) which equals the original graph minus the edge linking nodes 3 and 4. The second line takes this last subgraph and further deletes the edge linking nodes 5 and 8.\nThe resulting subgraph, minus the top two high-betweenness edges, looks like:\n\n\n\n\n\nWhich is indeed disconnected!"
  },
  {
    "objectID": "centrality.html#induced-betweenness",
    "href": "centrality.html#induced-betweenness",
    "title": "Centrality",
    "section": "Induced Betweenness",
    "text": "Induced Betweenness\nBorgatti and Everett (2020, 340–45) argue that another way of thinking about centrality of a node (or edge) is to calculate the difference that removing that node makes for some graph property in the network. They further suggest that the sum of the centrality scores of each node is just such a property, proposing that betweenness is particularly interesting in this regard. Let’s see how this works.\nWe will use the undirected version of the women lawyers advice network for this example. Let’s say we are interested in the difference that node 10 makes for the betweenness centralities of everyone else. In that case we would proceed as follows:\n\n   bet <- betweenness(wg) #original centrality scores\n   Sbet <- sum(bet) #sum of original centrality scores\n   wg.d <- wg - vertex(\"10\") #removing vertex 10 from the graph\n   bet.d <- betweenness(wg.d) #centrality scores of node deleted subgraph\n   Sbet.d <- sum(bet.d) #sum of centrality scores of node deleted subgraph\n   total.c <- Sbet - Sbet.d #total centrality\n   indirect.c <- total.c - bet[10] #indirect centrality\n   indirect.c\n\n[1] 12.66667\n\n\nLine 1 just calculates the regular betweenness centrality vector for the graph. Line 2 sums up all of the entries of this vector. Line 3 creates a node deleted subgraph by removing node 10. This is done using the “minus” operator and the igraph function vertex, which works just like the edge function we used earlier to create an edge deleted subgraph, except it takes a node id or name as input.\nLines 4-5 just recalculate the sum of betweenness centralities in the subgraph that excludes node 10. Then in line 6 we subtract the sum of centralities of the node deleted subgraph from the sum of centralities of the original graph. If this number, which Borgatti and Everett call the “total” centrality, is large and positive then that means that node 10 makes a difference for the centrality of others.\nHowever, part of that difference is node 10’s own “direct” centrality, so to get a more accurate sense of node 10’s impact on other people’s centrality we need to subtract node 10’s direct centrality from the total number, which we do in line 7 to get node 10’s “indirect” centrality. The result is shown in the last line, which indicates that node 10 has a pretty big impact on other people’s betweenness centralities, net of their own (which is pretty small).\nNow all we need to do is do the same for each node to create a vector of indirect betweenness centralities. So we incorporate the code above into a short loop through all vertices:\n\n   total.c <- 0 #empty vector\n   indirect.c <- 0 #empty vector\n   for (i in 1:vcount(wg)) {\n      wg.d <- wg - vertex(i)\n      bet.d <- betweenness(wg.d) #centrality scores of node deleted subgraph\n      Sbet.d <- sum(bet.d) #sum of centrality scores of node deleted subgraph\n      total.c[i] <- Sbet - Sbet.d #total centrality\n   indirect.c[i] <- total.c[i] - bet[i] #total minus direct\n   }\n\nWe can now list the total, direct, and indirect betweenness centralities for the women lawyers graph using a nice table:\n\n   i.bet <- data.frame(n = 1:vcount(wg), total.c, round(betweenness(wg), 1), round(indirect.c, 1))\n   kbl(i.bet, format = \"pipe\", align = c(\"l\", \"c\", \"c\", \"c\"),\n       col.names = c(\"Node\", \"Total\", \"Direct\", \"Indirect\"), row.names = FALSE,\n       caption = \"Induced Betweenness Scores in the Women Lawyers Advice Network\") %>% \n   kable_styling(bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\nInduced Betweenness Scores in the Women Lawyers Advice Network\n \n  \n    Node \n    Total \n    Direct \n    Indirect \n  \n \n\n  \n    1 \n    16 \n    0.0 \n    16.0 \n  \n  \n    2 \n    4 \n    6.7 \n    -2.7 \n  \n  \n    3 \n    -24 \n    23.2 \n    -47.2 \n  \n  \n    4 \n    -4 \n    12.2 \n    -16.2 \n  \n  \n    5 \n    19 \n    18.8 \n    0.2 \n  \n  \n    6 \n    10 \n    3.7 \n    6.3 \n  \n  \n    7 \n    18 \n    0.0 \n    18.0 \n  \n  \n    8 \n    4 \n    8.8 \n    -4.8 \n  \n  \n    9 \n    18 \n    0.0 \n    18.0 \n  \n  \n    10 \n    13 \n    0.3 \n    12.7 \n  \n  \n    11 \n    14 \n    0.0 \n    14.0 \n  \n  \n    12 \n    13 \n    0.3 \n    12.7 \n  \n\n\n\n\n\nThis approach to decomposing betweenness centrality provides a new way to categorize actors in a network:\n\nOn the one hand, we have actors like nodes 3 and 4 who “hog” centrality from others. Perhaps these are the prototypical high betweenness actors who monopolize the flow through the network. Their own direct centrality is high, but their indirect centrality is negative, suggesting that others become more central when they are removed from the graph as they can now become intermediaries themselves.\nIn contrast, we also have actors like node 5 who are high centrality themselves, but who’s removal from the network does not affect anyone else’s centrality. These actors are high betweenness but themselves don’t monopolize the flow of information in the network.\nThen we have actors (like nodes 9-12) who have low centrality, but whose removal from the network makes a positive difference for other people’s centrality, which overall decreases when they are removed from the network.\nFinally, we have actors line nodes 2 and 8, who are not particularly central, but who also hog centrality from others, in that removing them from the network also increases other people’s centrality (although not such an extent as the hogs)."
  },
  {
    "objectID": "community.html",
    "href": "community.html",
    "title": "Community Structure",
    "section": "",
    "text": "What are communities? In networks, communities are subset of nodes that have more interactions or connectivity within themselves than they do outside of themselves (these are sometimes called “modules” outside of sociology). Communities thus exemplify the sociological concept of a group.\nA network has community structure if it contains many such subsets or groups of nodes that interact preferentially among themselves. Not all networks have to have community structure; a network in which all nodes interact with equal propensity doesn’t have communities.\nSo whether a network has community structure, and whether a given guess as to what these communities are yields actual communities (cluster of nodes that interact more among themselves than they do with outsiders) is an empirical question than needs to be answered with data.\nBut first, we need to develop a criterion for whether a given partition of the network into mutually exclusive node subsets is actually producing communities as we defined them earlier. This criterion should be independent of particular methods and algorithms that claim to find communities, so that way we can compare them with one another and see whether the partitions they recommend yield actual communities.\nMark Newman (2006b), who has done the most influential work in this area, proposed such a criterion and called it the modularity of a partition (e.g., the extent to which a partition has identified the “modules” or subsets of the network)."
  },
  {
    "objectID": "community.html#a-bag-of-links",
    "href": "community.html#a-bag-of-links",
    "title": "Community Structure",
    "section": "A bag of links",
    "text": "A bag of links\nAn intuitive way to understand the modularity is as follows. Imagine we have an idea of what the communities in a network are. This could be given by some special community partition method, our intuition, or some exogenous coloring on the nodes (e.g., as given by a node attribute like race, gender, position in the organization, etc.). The membership of each node in each community is thus stored in a vector, \\(C_i = k\\) if node \\(i\\) belongs to community \\(k\\).\nOur job is to decide whether that community partition is a good one. One way to proceed is to imagine that we take the network in question, and we throw each observed link into a bag. Then our modularity measure should give us an idea of the probability that if we drew a link at random, the nodes at the end of each link belong to same community (or have the same color if the communities are defined by exogenous attributes). If the probability is high, then the network has community structure. If the probability is no better than we would expect given a null model that says there is nothing going on in the network connectivity-wise except random chance, then the modularity should be low (and we decide that the partition we chose actually does not divide the network into meaningful communities).\nLet’s assume our bag of links is full of directed links and we draw a bunch of them at random. Let’s call the node at the starting end of the link \\(s\\) and the node at the destination end of the link \\(d\\). Then we can measure the modularity—let’s call it \\(Q\\)—of a given partition of the network into \\(m\\) clusters as follows:\n\\[\nQ = \\sum_{k = 1}^m \\left[P(s \\in k \\land d \\in k) - P(s \\in k) \\times P(d \\in k) \\right]\n\\]\nIn this equation, \\(P(s \\in k \\land d \\in k)\\) is the probability that a link drawn at random has a source and a destination node that belong to the same community \\(k\\); \\(P(s \\in k)\\) is just the probability of drawing a link that has a starting node in community \\(k\\) (regardless of the membership of the destination node) and \\(P(d \\in k)\\) is the probability of drawing a link that has a destination node that belongs to community \\(k\\) (regardless of the membership fo the source node).\nIf you remember your elementary probability theory, you know that the joint probability of two events that are assumed to be independent of one another is just the product of their individual probabilities. So that means that \\(P(s \\in k) \\times P(d \\in k)\\) is the expected probability of finding that both the source and destination node are in the same community \\(k\\) in a network in which communities don’t matter for link formation (because the two probabilities are assumed to be independent).\nSo the formula for the modularity subtracts the observed probability of finding links with two nodes in the same community from what we would expect if communities didn’t matter. So it measures the extent to which we find within-community links in the network beyond what we would expect by chance. Obviously, the higher this number, the higher the deviation from chance is, and the more community structure there is in the network.\nLet’s move to a real example. Figure 1 shows two plots of the advice network from the law_friends data (Lazega 2001) from the networkdata package. The first shows the nodes colored by status in the firm (partners in tan versus associates in blue) and the other shows the nodes colored by gender (men in tan versus women in blue). It is pretty evident that there is more community partitioning in the network by status than by gender; that is, friendship nominations tend to go from people of a given rank to people of the same rank, which makes sense. So a measure of modularity should be higher when using status as our partition than when using gender.\n\n\n\n\n\n\n\n(a) Nodes colored by status (Partner/Associate)\n\n\n\n\n\n\n\n(b) Nodes colored by Gender (Men/Women)\n\n\n\n\nFigure 1: Law Firm Friendship Network\n\n\nLet’s see an example of the modularity computed using our “bag of links” framework. Below is a quick function that uses dplyr code to take a graph as input and return and edge list data frame containing the “community membership” of each node by some characteristic given by the second input into the function.\n\n   link.bag <- function(x, c) {\n      library(dplyr)\n      g.el <- data.frame(as_edgelist(x))\n      names(g.el) <- c(\"vi\", \"vj\")\n      comm.dat <- data.frame(vi = as.vector(V(x)), \n                             vj = as.vector(V(x)), \n                             c = as.vector(c))\n      el.temp <- data.frame(vj = g.el[, 2]) %>% \n         left_join(comm.dat, by = \"vj\") %>% \n         dplyr::select(c(\"vj\", \"c\")) %>% \n         rename(c2 = c) \n      d.el <- data.frame(vi = g.el[, 1]) %>% \n         left_join(comm.dat, by = \"vi\") %>% \n         dplyr::select(c(\"vi\", \"c\")) %>% \n         rename(c1 = c) %>% \n         cbind(el.temp)\n   return(d.el)\n   }\n\nSo if we wanted an edge list data frame containing each node’s status membership in the law_advice network, we would just type:\n\n   link.dat <- link.bag(g, V(g)$status)\n   head(link.dat)\n\n  vi c1 vj c2\n1  1  1  2  1\n2  1  1  4  1\n3  1  1  8  1\n4  1  1 17  1\n5  2  1 16  1\n6  2  1 17  1\n\n\nNote than an edge list is already a “bag of links” so we can obtain the three probabilities we need to compute the modularity of a partition directly from the edge list data frame. Here’s a function that does this:\n\n   mod.Q1 <- function(x, c1 = 2, c2 = 4) {\n      Q <- 0\n      comms <- unique(x[, c1])\n      for (k in comms) {\n         e.same <- x[x[, c1] == k & x[, c2] == k, ]\n         e.sour <- x[x[, c1] == k, ]\n         e.dest <- x[x[, c2] == k, ]\n         e.total <- nrow(x)\n         p.same <- nrow(e.same)/e.total\n         p.sour <- nrow(e.sour)/e.total\n         p.dest <- nrow(e.dest)/e.total\n         Q <- Q + (p.same - (p.sour * p.dest))\n      }\n   return(Q)\n   }\n\nThis function takes the edge list data frame as input. Optionally, you can specify the two columns containing community membership info for the source and destination node in each link (in this case this happens to be the second and fourth columns). The Function works like this:\n\nThe probability of a link drawn randomly from the bag is just the number of links where the source and destination links belong to the same community (e.same, computed in line 5) divided by the total number of links (which is the number of rows in the edge list data frame), this ratio is computed in line 9 (p.same).\nThe overall probability of a link containing a source node in community \\(k\\) is computed in line 10 (p.sour), and the overall probability of a link containing a destination node in community \\(k\\) is computed in line 11 (p.dest).\nThe actual modularity is computed step by step by summation across levels of the community indicator variable in line 12 which is the sum of the difference between p.same and the product of p.sour times p.dest across all communities \\(m\\) (in this case \\(m = 2\\) as there are only two levels of status).\n\nSo if wanted to check if status was more powerful in structuring the community organization of the law_friends network than gender, we would just type:\n\n   mod.Q1(link.bag(g, V(g)$status))\n\n[1] 0.2715221\n\n   mod.Q1( link.bag(g, V(g)$gender))\n\n[1] 0.07495538\n\n\nWhich indeed confirms that status is a more powerful group formation principle than gender in this network."
  },
  {
    "objectID": "community.html#modularity-from-the-adjancency-matrix",
    "href": "community.html#modularity-from-the-adjancency-matrix",
    "title": "Community Structure",
    "section": "Modularity from the Adjancency Matrix",
    "text": "Modularity from the Adjancency Matrix\nNote that while the “bag of links” idea is good for showing the basic probabilistic principle behind the modularity in a network, we can compute directly from the adjacency matrix without going through the edge list data frame construction step.\nA function that does this looks like:\n\n   mod.Q2 <- function(x, c) {\n   A <- as.matrix(as_adjacency_matrix(x))\n   vol <- sum(A)\n   Q <- 0\n   for (k in unique(c)) {\n      A.sub <- A[which(c == k), which(c == k)]\n      vol.k <- sum(A.sub)\n      A.i <- A[which(c == k), ]\n      A.j <- A[, which(c == k)]\n      Q <- Q + ((vol.k/vol) - ((sum(A.i) * sum(A.j))/vol^2))\n      }\n   return(Q)\n   }\n\nThis function just takes a graph and a vector indicating the community membership of each node and returns the modularity as output. Like before the modularity is the difference in the probability of observing a within-community link between two nodes (given by the ratio of the number of links in the sub-adjacency matrix containing only within community-nodes—obtained in line 7—and the overall number of links in the adjacency matrix, computed in line 3) and the expected probability of a link between a source node with community membership \\(k\\) and a destination node with the same community membership.\nThis last quantity is given by the product of the sum links in a the sub-adjacency matrix with row nodes that belong to community \\(k\\) (the source nodes) and the number of links in the sub adjacency matrix with the column nodes belonging to community \\(k\\) (the destination nodes) divided by the square of the total number of links observed the network.\nIn formulese:\n\\[\\begin{equation}\nQ = \\sum_{k=1}^m \\left[\\frac{\\sum_{i \\in k} \\sum_{j \\in k} a_{ij}}{\\sum_i \\sum_j a_{ij}}  - \\frac{(\\sum_{i \\in k} \\sum_j a_{ij})(\\sum_i \\sum_{j \\in k} a_{ij})}{(\\sum_i \\sum_j a_{ij})^2} \\right]\n\\end{equation}\\]\nWhere \\(\\sum_i \\sum_ja_{ij}\\) is the sum of all the entries in the network adjacency matrix, \\(\\sum_{i \\in k} \\sum_{j \\in k} a_{ij}\\) is the sum of all the entries of the sub-adjacency matrix where both the row and column nodes come from community \\(k\\), \\(\\sum_{i \\in k} \\sum_j a_{ij}\\) is the sum of all the entries in the sub-adjacency matrix where only the row nodes come from community \\(k\\), and \\(\\sum_i \\sum_{j \\in k} a_{ij}\\) is the sum of all the entries in the sub-adjacency matrix where only the column nodes come from community \\(k\\).\nWe can easily see that this approach gives us the same answer as the bag of links version:\n\n   mod.Q2(g, V(g)$status)\n\n[1] 0.2715221\n\n   mod.Q2(g, V(g)$gender)\n\n[1] 0.07495538\n\n\nOf course, you don’t even have to use a custom function like the above, because igraph has one, called (you guessed it) modularity:\n\n   modularity(g, V(g)$status)\n\n[1] 0.2715221\n\n   modularity(g, V(g)$gender)\n\n[1] 0.07495538\n\n\nBut at least now you know what’s going on inside of it!"
  },
  {
    "objectID": "community.html#the-modularity-matrix",
    "href": "community.html#the-modularity-matrix",
    "title": "Community Structure",
    "section": "The Modularity Matrix",
    "text": "The Modularity Matrix\nObviously, the modularity wouldn’t be a famous method if it was just a way of measuring the goodness of a community partition produced by other methods. It itself can be used to find community partitions by using a method that somehow produces node partition that find the largest values that it can take in a graph.\nA useful tool in this quest is the modularity matrix \\(\\mathbf{B}\\) (Newman 2006b), which is defined as a variation on the adjacency matrix \\(\\mathbf{A}\\), each cell of the modularity matrix \\(b_{ij}\\) takes the value:\n\\[\nb_{ij} = a_{ij} - \\frac{k^{out}_ik^{in}_j}{\\sum_i\\sum_j a_{ij}}\n\\]\nWhere \\(k^{out}_i\\) is node \\(i\\)’s outdegree and \\(k^{in}_j\\) is node j’s indegree. Note that the modularity matrix has the same “observed minus expected” structure as the formulas for the modularity. In this case we compare whether we see a link from \\(i\\) to \\(j\\) as given by \\(a_{ij}\\) against the chances of observing a link in a graph in which nodes connect at random with probability proportional to their degrees (as given by the right-hand side fraction).\nNote that if the graph is undirected, the modularity matrix is just:\n\\[\nb_{ij} = a_{ij} - \\frac{k_ik_j}{\\sum_i\\sum_j a_{ij}}\n\\]\nA function to compute the modularity matrix by looping through every element of the adjacency matrix for a directed graph looks like:\n\n   mod.mat <- function(x){\n      A <- as.matrix(as_adjacency_matrix(x))\n      od <- rowSums(A) #outdegrees\n      id <- colSums(A) #indegrees\n      vol <- sum(A)\n      n <- nrow(A)\n      B <- matrix(0, n, n)\n      for (i in 1:n){\n         for (j in 1:n) {\n            B[i,j] <- A[i,j] - ((od[i]*id[j])/vol)\n         }\n      }\n   return(B)\n   }\n\nPeeking inside the resulting matrix:\n\n   round(mod.mat(g)[1:10, 1:10], 3)\n\n        [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]   [,8]   [,9]  [,10]\n [1,] -0.035  0.930 -0.028  0.902 -0.035 -0.014 -0.014  0.951 -0.098 -0.028\n [2,] -0.035 -0.070 -0.028 -0.098 -0.035 -0.014 -0.014 -0.049 -0.098 -0.028\n [3,]  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n [4,] -0.131  0.739  0.895 -0.366 -0.131 -0.052 -0.052 -0.183  0.634 -0.105\n [5,] -0.026 -0.052 -0.021 -0.073 -0.026 -0.010  0.990 -0.037 -0.073 -0.021\n [6,]  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n [7,] -0.017 -0.035  0.986 -0.049  0.983 -0.007 -0.007 -0.024 -0.049 -0.014\n [8,] -0.009 -0.017 -0.007 -0.024 -0.009 -0.003 -0.003 -0.012 -0.024 -0.007\n [9,] -0.052 -0.105 -0.042  0.854 -0.052 -0.021 -0.021 -0.073 -0.146 -0.042\n[10,] -0.122  0.756 -0.098 -0.341 -0.122 -0.049 -0.049  0.829  0.659 -0.098\n\n\nInterestingly, the matrix has some negative entries, some positive entries and some close to or actually zero. We interpret these as follows: If an entry is negative it means that a link between the row and column nodes is much less likely to happen than expected given the each node’s degrees, a positive entry indicates the opposite; a larger than expected chance of a link forming. Entries close to zero indicate those nodes have odds close to random chance of forming a tie.\nOf course igraph has a function, called modularity_matrix, which gives us the same result:\n\n   B <- modularity_matrix(g)\n   round(B[1:10, 1:10], 3)\n\n        [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]   [,8]   [,9]  [,10]\n [1,] -0.035  0.930 -0.028  0.902 -0.035 -0.014 -0.014  0.951 -0.098 -0.028\n [2,] -0.035 -0.070 -0.028 -0.098 -0.035 -0.014 -0.014 -0.049 -0.098 -0.028\n [3,]  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n [4,] -0.131  0.739  0.895 -0.366 -0.131 -0.052 -0.052 -0.183  0.634 -0.105\n [5,] -0.026 -0.052 -0.021 -0.073 -0.026 -0.010  0.990 -0.037 -0.073 -0.021\n [6,]  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n [7,] -0.017 -0.035  0.986 -0.049  0.983 -0.007 -0.007 -0.024 -0.049 -0.014\n [8,] -0.009 -0.017 -0.007 -0.024 -0.009 -0.003 -0.003 -0.012 -0.024 -0.007\n [9,] -0.052 -0.105 -0.042  0.854 -0.052 -0.021 -0.021 -0.073 -0.146 -0.042\n[10,] -0.122  0.756 -0.098 -0.341 -0.122 -0.049 -0.049  0.829  0.659 -0.098\n\n\nIn the case of an undirected graph, computing the modularity is even simpler:\n\n   mod.mat.u <- function(x){\n      A <- as.matrix(as_adjacency_matrix(x))\n      d <- rowSums(A) #degrees\n      vol <- sum(A)\n      n <- nrow(A)\n      B <- A - (d %*% t(d)/vol)\n   return(B)\n   }\n\nLet’s see the function in action:\n\n   ug <- as_undirected(g, mode = \"collapse\")\n   Bu <- mod.mat.u(ug)\n   round(Bu[1:10, 1:10], 2)\n\n       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10]\n [1,] -0.08  0.90 -0.04  0.78 -0.06 -0.02 -0.03  0.93 -0.14 -0.14\n [2,]  0.90 -0.13 -0.05  0.72 -0.08 -0.03 -0.04 -0.09 -0.18  0.82\n [3,] -0.04 -0.05 -0.02  0.89 -0.03 -0.01  0.98 -0.04 -0.07 -0.07\n [4,]  0.78  0.72  0.89 -0.61 -0.17 -0.06 -0.08 -0.19  0.61 -0.39\n [5,] -0.06 -0.08 -0.03 -0.17 -0.05 -0.02  0.98 -0.05 -0.11 -0.11\n [6,] -0.02 -0.03 -0.01 -0.06 -0.02 -0.01 -0.01 -0.02 -0.04 -0.04\n [7,] -0.03 -0.04  0.98 -0.08  0.98 -0.01 -0.01 -0.03 -0.05 -0.05\n [8,]  0.93 -0.09 -0.04 -0.19 -0.05 -0.02 -0.03 -0.06 -0.12  0.88\n [9,] -0.14 -0.18 -0.07  0.61 -0.11 -0.04 -0.05 -0.12 -0.25  0.75\n[10,] -0.14  0.82 -0.07 -0.39 -0.11 -0.04 -0.05  0.88  0.75 -0.25\n\n\nWhich gives us the same results as using igraph:\n\n   Bu <- modularity_matrix(ug, directed = FALSE)\n   round(Bu[1:10, 1:10], 2)\n\n       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10]\n [1,] -0.08  0.90 -0.04  0.78 -0.06 -0.02 -0.03  0.93 -0.14 -0.14\n [2,]  0.90 -0.13 -0.05  0.72 -0.08 -0.03 -0.04 -0.09 -0.18  0.82\n [3,] -0.04 -0.05 -0.02  0.89 -0.03 -0.01  0.98 -0.04 -0.07 -0.07\n [4,]  0.78  0.72  0.89 -0.61 -0.17 -0.06 -0.08 -0.19  0.61 -0.39\n [5,] -0.06 -0.08 -0.03 -0.17 -0.05 -0.02  0.98 -0.05 -0.11 -0.11\n [6,] -0.02 -0.03 -0.01 -0.06 -0.02 -0.01 -0.01 -0.02 -0.04 -0.04\n [7,] -0.03 -0.04  0.98 -0.08  0.98 -0.01 -0.01 -0.03 -0.05 -0.05\n [8,]  0.93 -0.09 -0.04 -0.19 -0.05 -0.02 -0.03 -0.06 -0.12  0.88\n [9,] -0.14 -0.18 -0.07  0.61 -0.11 -0.04 -0.05 -0.12 -0.25  0.75\n[10,] -0.14  0.82 -0.07 -0.39 -0.11 -0.04 -0.05  0.88  0.75 -0.25\n\n\nThe modularity matrix has some interesting properties. For instance, both its rows and columns sum to zero:\n\n   round(rowSums(Bu), 2)\n\n [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[39] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n\n   round(colSums(Bu), 2)\n\n [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[39] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n\n\nWhich makes \\(\\mathbf{B}\\) doubly centered a neat but so far useless property."
  },
  {
    "objectID": "community.html#using-the-modularity-matrix-to-find-the-modularity-of-a-partition",
    "href": "community.html#using-the-modularity-matrix-to-find-the-modularity-of-a-partition",
    "title": "Community Structure",
    "section": "Using the Modularity Matrix to Find the Modularity of a Partition",
    "text": "Using the Modularity Matrix to Find the Modularity of a Partition\nA more interesting property of the modularity matrix is that we can use it to compute the actual modularity of a given binary partition of the network into clusters. Let’s take status in the law_friends network as an example.\nFirst we create a new vector \\(\\mathbf{s}\\) that equals one when node \\(i\\) is in the first group (partners) and minus one when node \\(i\\) is in the second group (associates):\n\n   s <- V(g)$status\n   s[which(s==1)] <- 1\n   s[which(s==2)] <- -1\n   s\n\n [1]  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n[26]  1  1  1  1  1  1  1  1  1  1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n[51] -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n\n\nOnce we have this vector, the modularity is just:\n\\[\nQ = \\frac{\\mathbf{s}^T\\mathbf{B}\\mathbf{s}}{2\\sum_i\\sum_ja_{ij}}\n\\]\nWhich we can readily check in R:\n\n   A <- as.matrix(as_adjacency_matrix(g))\n   as.numeric(t(s) %*% B %*% s)/(2 * sum(A))\n\n[1] 0.2715221\n\n\nYou can think of the \\(\\mathbf{s}\\) vector as identifying the binary community membership via contrast coding.\nWe can also use the modularity matrix to find the modularity of a partition via dummy coding. To do this, we create two vectors for each level of the group membership variable with \\(u^{(1)}_i = 1\\) when when node \\(i\\) is a partner (otherwise \\(u^{(1)}_i = 0\\)), and \\(u^{(2)}_i = 1\\) when when node \\(i\\) is an associate (otherwise \\(u^{(2)}_i = 0\\)).\nIn R we can do this as follows:\n\n   u1 <- rep(0, length(V(g)$status))\n   u2 <- rep(0, length(V(g)$status))\n   u1[which(V(g)$status==1)] <- 1\n   u2[which(V(g)$status==2)] <- 1\n\nWe then join the two vectors into a matrix \\(\\mathbf{U}\\) of dimensions \\(n \\times 2\\):\n\n   U <- cbind(u1, u2)\n   head(U)\n\n     u1 u2\n[1,]  1  0\n[2,]  1  0\n[3,]  1  0\n[4,]  1  0\n[5,]  1  0\n[6,]  1  0\n\n   tail(U)\n\n      u1 u2\n[63,]  0  1\n[64,]  0  1\n[65,]  0  1\n[66,]  0  1\n[67,]  0  1\n[68,]  0  1\n\n\nOnce we have this matrix, the modularity of the partition is given by:\n\\[\n\\frac{tr(U^TBU)}{\\sum_i\\sum_ja_{ij}}\n\\]\nWhere \\(tr\\) denotes the trace operation in a matrix, namely, taking the sum of its diagonal elements.\nWe can check that this gives us the correct solution in R as follows:\n\n   sum(diag(t(U) %*% B %*% U))/sum(A)\n\n[1] 0.2715221\n\n\nNeat! It makes sense (given the name of the matrix) that we can compute the modularity from \\(\\mathbf{B}\\), and we can do it in multiple ways."
  },
  {
    "objectID": "community.html#using-the-modularity-matrix-to-find-communities",
    "href": "community.html#using-the-modularity-matrix-to-find-communities",
    "title": "Community Structure",
    "section": "Using the Modularity Matrix to Find Communities",
    "text": "Using the Modularity Matrix to Find Communities\nHere’s an even more useful property of the modularity matrix. Remember the network distribution game we played to defined our various reflective status measure in our discussion of prestige?\n\n   status1 <- function(w) {\n      x <- rep(1, nrow(w)) #initial status vector set to all ones of length equal to the number of nodes\n      d <- 1 #initial delta\n      k <- 0 #initializing counter\n      while (d > 1e-10) {\n          o.x <- x #old status scores\n          x <- w %*% o.x #new scores a function of old scores and adjacency matrix\n          x <- x/norm(x, type = \"E\") #normalizing new status scores\n          d <- abs(sum(abs(x) - abs(o.x))) #delta between new and old scores\n          k <- k + 1 #incrementing while counter\n      }\n   return(as.vector(x))\n   }\n\nWhat if we played it with the modularity matrix?\n\n   s <- status1(Bu)\n   round(s,2)\n\n [1] -0.10 -0.14 -0.02 -0.21 -0.03  0.00  0.01 -0.05 -0.21 -0.18 -0.16 -0.24\n[13] -0.13 -0.06 -0.04 -0.12 -0.24  0.08 -0.03 -0.17 -0.18 -0.15 -0.09 -0.14\n[25] -0.11 -0.17 -0.19  0.04 -0.10 -0.02  0.11  0.06  0.06 -0.06  0.10  0.01\n[37] -0.05  0.04 -0.05  0.12  0.19 -0.02  0.07  0.05  0.02  0.08  0.08  0.09\n[49]  0.17  0.00  0.20  0.06  0.20  0.13  0.20  0.09  0.08  0.05  0.06  0.02\n[61]  0.24  0.14  0.17  0.06  0.13  0.09  0.09  0.09\n\n\nThis seems more interesting. The resulting “status” vector has both negative and positive entries. What if I told you that this vector is actual a partition of the original graph into two communities?\nNot only that, I have even better news. This is the partition that maximizes the modularity in the graph, the best two-group partition that has the most edges within groups and the least edges going across groups.1\nLet’s see some evidence for these claims in real data.\nFirst, let’s turn the “status” vector into a community indicator vector. We assign all nodes with scores larger than zero to one community and nodes with scores equal to zero or less to the other one:\n\n   C <- rep(0, length(s))\n   C[which(s<=0)] <- 1\n   C[which(s >0)] <- 2\n   names(C) <- 1:length(C)\n   C\n\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \n 1  1  1  1  1  2  2  1  1  1  1  1  1  1  1  1  1  2  1  1  1  1  1  1  1  1 \n27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 \n 1  2  1  1  2  2  2  1  2  2  1  2  1  2  2  1  2  2  2  2  2  2  2  2  2  2 \n53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 \n 2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2 \n\n\nAnd for the big check:\n\n   modularity(ug, C)\n\n[1] 0.2935153\n\n\nWhich is a pretty big score, at least larger than we obtained earlier using the exogenous indicator of status in the law firm as the basis for a binary partition. Figure 2 shows visual evidence that this is indeed an optimal binary partition of the law_friends network.\nOf course, you may also remember from the Status and Prestige lesson that the little status game we played above is also a method (called the “power” method) of computing the leading eigenvector (the one associated with the largest eigenvalue) of a matrix. So turns out that the modularity maximizing bi-partition of a graph is just given by this vector:\n\\[\n\\mathbf{B}\\mathbf{v} = \\lambda\\mathbf{v}\n\\]\nSolving for \\(\\mathbf{v}\\) in the equation above will give us the community memberships of the nodes. In R you can do this using the native eigen function like we did for the status scores:\n\n   v <- eigen(Bu)$vectors[, 1] \n   round(v, 2)\n\n [1] -0.10 -0.14 -0.02 -0.21 -0.03  0.00  0.01 -0.05 -0.21 -0.18 -0.16 -0.24\n[13] -0.13 -0.06 -0.04 -0.12 -0.24  0.08 -0.03 -0.17 -0.18 -0.15 -0.09 -0.14\n[25] -0.11 -0.17 -0.19  0.04 -0.10 -0.02  0.11  0.06  0.06 -0.06  0.10  0.01\n[37] -0.05  0.04 -0.05  0.12  0.19 -0.02  0.07  0.05  0.02  0.08  0.08  0.09\n[49]  0.17  0.00  0.20  0.06  0.20  0.13  0.20  0.09  0.08  0.05  0.06  0.02\n[61]  0.24  0.14  0.17  0.06  0.13  0.09  0.09  0.09\n\n\nWhich look like the same values we obtained earlier.\nWe can package all of the steps above into a simple function:\n\n   split.two <- function(x) {\n      e <- eigen(x)$vectors[, 1]\n      v <- rep(0, length(e))\n      v[e<= 0] <- 1\n      v[e > 0] <- 2\n      names(v) <- 1:length(v)\n      c1 <- rep(0, length(v))\n      c2 <- rep(0, length(v))\n      c1[which(v == 1)] <- 1\n      c2[which(v == 2)] <- 1\n      dummy <- cbind(c1, c2)\n   return(list(v = v, dummy = dummy))\n   }\n\nAnd check to see if it works:\n\n   split.two(Bu)$v\n\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \n 1  1  1  1  1  2  2  1  1  1  1  1  1  1  1  1  1  2  1  1  1  1  1  1  1  1 \n27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 \n 1  2  1  1  2  2  2  1  2  2  1  2  1  2  2  1  2  2  2  2  2  2  2  2  2  2 \n53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 \n 2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2 \n\n\nIndeed it splits the nodes in a graph into two modularity maximizing communities.\nIt is clear that this approach hints at a divisive partitioning strategy, where we begin with two communities and then try to split one (or both) of those communities into two more like with CONCOR.\nHowever, unlike CONCOR, We don’t want to be unprincipled here because splitting a well-defined community into two just for the hell of it, can actually lead to worse modularity than keeping the original larger communities.\nSo we need an approach that tries out splitting one of the original two communities into two smaller ones, checks the new modularity, compares it with the old, and only goes ahead with the partition if the new modularity is larger than the old one.\nHere’s one way of doing that:\n\n   split.mult <- function(B, vol, u, v, s = 0.01) {\n      split <- list()\n      delta.Q <- rep(0, max(v))\n      for (i in 1:max(v)) {\n         old.Q <- t(u[, i]) %*% B %*% u[, i] #Using dummy coding to compute Q\n         sub <- which(u[, i] == 1)\n         new.B <- B[sub, sub]\n         new.u <- split.two(new.B)$dummy\n         new.v <- split.two(new.B)$v\n         Q1 <- t(new.u[, 1]) %*% new.B %*% new.u[, 1] #Using dummy coding to compute Q\n         Q2 <- t(new.u[, 2]) %*% new.B %*% new.u[, 2] #Using dummy coding to compute Q\n         new.Q <- Q1 + Q2\n         delta.Q[i] <- (new.Q - old.Q) * (1/vol)\n         if (delta.Q[i] > s) {\n            split[[i]] <- new.v\n            names(split[[i]]) <- which(u[, i] == 1)\n            }\n         else if (delta.Q[i] <= s) {\n            split[[i]] <- NULL\n            }\n      }\n   return(list(delta.Q = round(delta.Q, 3), v = v, split = split))\n   }\n\nThis function uses the output of the split.two function to check if any of the two sub-communities should be split in two further ones by comparing the modularities pre and post second partition.\nHere are the results applied to the law_friends network:\n\n   s <- split.mult(B = Bu, \n                   vol = sum(as.matrix(as_adjacency_matrix(ug))), \n                   u = split.two(Bu)$dummy, \n                   v = split.two(Bu)$v)\n   s\n\n$delta.Q\n[1] 0.000 0.048\n\n$v\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \n 1  1  1  1  1  2  2  1  1  1  1  1  1  1  1  1  1  2  1  1  1  1  1  1  1  1 \n27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 \n 1  2  1  1  2  2  2  1  2  2  1  2  1  2  2  1  2  2  2  2  2  2  2  2  2  2 \n53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 \n 2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2 \n\n$split\n$split[[1]]\nNULL\n\n$split[[2]]\n 6  7 18 28 31 32 33 35 36 38 40 41 43 44 45 46 47 48 49 50 51 52 53 54 55 56 \n 2  2  2  2  2  2  2  2  2  1  1  1  1  1  1  1  2  2  1  1  1  1  1  1  2  2 \n57 58 59 60 61 62 63 64 65 66 67 68 \n 2  1  1  2  1  1  1  1  1  1  1  1 \n\n\nThe delta.Q vector stores the results of our modularity checks. In this case, the test in the first community failed; the modularity didn’t change (the difference between the old and the new is zero) when we tried to split into two smaller communities. This is usually indicative that the original community was a well-defined group with lots of within-cluster ties so that splitting it makes no difference (that’s the orange nodes in the figure above).\nNevertheless, the test in the second community returned a positive result, indicating we should split it in two. Because we left one of the original communities alone, the resulting network will have three communities after splitting the second one in two.\nNote that the function returns the original two-split named vector of communities assignments v, a NULL result for the first split, and a new vector assigning nodes in the second original split to two other communities (split[[2]]).\nWe now create a new three-community vector from these results:\n\n   s$v[names(s$split[[2]])] <- s$split[[2]] + 1\n   s$v\n\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \n 1  1  1  1  1  3  3  1  1  1  1  1  1  1  1  1  1  3  1  1  1  1  1  1  1  1 \n27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 \n 1  3  1  1  3  3  3  1  3  3  1  2  1  2  2  1  2  2  2  2  3  3  2  2  2  2 \n53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 \n 2  2  3  3  3  2  2  3  2  2  2  2  2  2  2  2 \n\n\nFigure 3 shows the results of the three-community partition.\n\n\n\n\n\n\nFigure 2: Law Firm Friendship Network by The Best Binary Partition Using the Leading Eigenvector of the Modularity Matrix Approach\n\n\n\n\n\n\n\nFigure 3: Law Firm Friendship Network by The Best Three-Community Partition Using the Leading Eigenvector of the Modularity Matrix Approach\n\n\n\n\n\nNow let’s say we wanted to check if we should further split any of these three communities into two. All we need to do is create “dummy” variables for the new three community partition and feed these (and the three community membership vector) to the split.mult function, while keeping the original modularity matrix (Newman 2006a):\n\n   c1 <- rep(0, length(s$v))\n   c2 <- rep(0, length(s$v))\n   c3 <- rep(0, length(s$v))\n   c1[which(s$v==1)] <- 1\n   c2[which(s$v==2)] <- 1\n   c3[which(s$v==3)] <- 1\n   dummies <- cbind(c1, c2, c3)\n   s <- split.mult(B = Bu, \n                   vol = sum(as.matrix(as_adjacency_matrix(ug))), \n                   u = dummies, \n                   v = s$v)\n   s\n\n$delta.Q\n[1]  0.000  0.000 -0.002\n\n$v\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \n 1  1  1  1  1  3  3  1  1  1  1  1  1  1  1  1  1  3  1  1  1  1  1  1  1  1 \n27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 \n 1  3  1  1  3  3  3  1  3  3  1  2  1  2  2  1  2  2  2  2  3  3  2  2  2  2 \n53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 \n 2  2  3  3  3  2  2  3  2  2  2  2  2  2  2  2 \n\n$split\nlist()\n\n\nHere, the the three modularity split checks failed, as given by the delta.Q vector. So no further splits were made (hence the split object is an empty list). This tells us that according to this method, three communities is the optimal partition.\nOf course, if you are working on your project, you don’t have to mess with all of the functions above, because igraph has implemented Newman’s leading eigenvector (of the modularity matrix) community detection method (Newman 2006a) in a single function called cluster_leading_eigen.\nTo use it, just feed it the relevant graph:\n\n   le.res <- cluster_leading_eigen(ug)\n\nAnd examine the membership object stored in the results list:\n\n   names(le.res$membership) <- 1:length(le.res$membership)\n   le.res$membership\n\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \n 1  1  1  1  1  2  2  1  1  1  1  1  1  1  1  1  1  2  1  1  1  1  1  1  1  1 \n27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 \n 1  2  1  1  2  2  2  1  2  2  1  3  1  3  3  1  3  2  3  3  2  2  3  3  3  3 \n53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 \n 3  3  2  2  2  3  3  2  3  3  3  3  3  3  3  3 \n\n\nWhich returns identical community assignments as above (but with the labels between communities 2 and 3 flipped)."
  },
  {
    "objectID": "community.html#communities-versus-exogeneous-attributes",
    "href": "community.html#communities-versus-exogeneous-attributes",
    "title": "Community Structure",
    "section": "Communities versus Exogeneous Attributes",
    "text": "Communities versus Exogeneous Attributes\nRemember we began this handout using an exogenous criterion (law firm status as partner or associate) to examine the community structure of the network (which does pretty well according to the modularity). But now we have also used a pure link-based approach to finding communities. How do they compare? Here’s a way to find out:\n\n   library(kableExtra)\n   t <- table(V(ug)$status, le.res$membership)\n   kbl(t, row.names = TRUE, align = \"c\", format = \"html\") %>%\n      column_spec(1, bold = TRUE) %>% \n      kable_styling(full_width = TRUE,\n                     bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\n \n  \n      \n    1 \n    2 \n    3 \n  \n \n\n  \n    1 \n    27 \n    9 \n    0 \n  \n  \n    2 \n    3 \n    7 \n    22 \n  \n\n\n\n\n\nThis is just a contingency table listing status in the rows (1 = partner) and the three leading eigenvector communities in the columns.\nWe find that community one (the most coherent; in orange) is composed of 89% of partners. So it is a high status clique. Community three (in blue) on the other hand is composed of 100% of associates, a less dense, but completely (low) status homogeneous clique. Finally community two (green) is a status-mixed clique, composed of nine partners and seven associates.\nSo in this network, both status and endogenous group dynamics seem to be at play."
  },
  {
    "objectID": "community.html#an-agglomerative-approach-based-on-the-modularity",
    "href": "community.html#an-agglomerative-approach-based-on-the-modularity",
    "title": "Community Structure",
    "section": "An Agglomerative Approach Based on the Modularity",
    "text": "An Agglomerative Approach Based on the Modularity\nThe leading eigenvector approach discussed above is a divisive clustering algorithm. All nodes begin in one community, then we split in two, then we try to split one of those two into two other ones (if the modularity increases) and so forth.\nBut as we noted in the previous handout, another way to cluster is bottom up. All nodes begin in their own singleton cluster, and we join nodes if they maximize some criterion, in this case, the modularity. We then join new nodes to that group if they increase the modularity, otherwise we try other joins, merging communities as we go along, until we can’t increase the modularity any longer.\nThis agglomerative approach to clustering is not guaranteed to reach some kind of global maximum, but it can work in most applied settings even at the risk of getting stuck in a local optimum.\nNewman (2004) developed an agglomerative algorithm that optimizes the modularity, a more recent version of which is included in an igraph function called cluster_fast_greedy (Clauset, Newman, and Moore 2004).\nLet’s try it out to see how it works:\n\n   fg.res <- cluster_fast_greedy(ug)\n\nThe resulting list object has various sub-objects as components. One of them is called merges which gives a history of the various merges that resulted in the final communities. This means that we can view the results as a dendrogram just like we did with structural similarity analyses based on hierarchical clustering of a distance matrix.\nTo do this we just transform the resulting object into an hclust object and plot:\n\n   hc <- as.hclust(fg.res)\n   par(cex = 0.5) #smaller labels\n   plot(hc)\n\n\n\n\nWhich gives us the sense that the network is divided into three communities like we saw earlier. If wanted to see which nodes are in which, we could just cut the dendrogram at \\(k = 3\\):\n\n   library(dendextend)\n   cutree(hc, k = 3)\n\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \n 1  1  2  1  2  2  2  1  1  1  1  1  1  2  1  1  1  2  1  1  1  1  1  1  2  1 \n27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 \n 1  2  1  2  2  2  2  1  2  2  1  3  1  3  3  3  3  3  3  3  2  2  3  1  3  3 \n53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 \n 3  3  3  2  3  3  3  2  3  3  3  3  3  3  3  3 \n\n\nWhich provides the relevant assignment to communities (if we wanted more communities, we would cut the tree at a lower height).\nOf course, this information was already stored in our results in the membership object:\n\n   names(fg.res$membership) <- 1:68\n   fg.res$membership\n\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \n 1  1  2  1  2  2  2  1  1  1  1  1  1  2  1  1  1  2  1  1  1  1  1  1  2  1 \n27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 \n 1  2  1  2  2  2  2  1  2  2  1  3  1  3  3  3  3  3  3  3  2  2  3  1  3  3 \n53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 \n 3  3  3  2  3  3  3  2  3  3  3  3  3  3  3  3 \n\n\nFigure 4 visualizes the resulting three-community partition, which is pretty close to what we got using the leading eigenvector method."
  },
  {
    "objectID": "community.html#community-detection-using-edge-betweenness",
    "href": "community.html#community-detection-using-edge-betweenness",
    "title": "Community Structure",
    "section": "Community Detection Using Edge Betweenness",
    "text": "Community Detection Using Edge Betweenness\nA final approach to community detection with good sociological bona fides relies on edge betweenness, a concept we covered on the centrality handout. And edge has high betweenness if many other pairs of nodes need to use to reach one another via shortest paths.\nIt follows that if we iteratively identify high betweenness edges, removing the top one, recalculate the edge betweenness on the resulting edge-deleted subgraph, and remove that top one (or flip a coin if two are tied) we have an algorithm for identifying communities as those subgraphs that end up being disconnected after removing the bridges (Girvan and Newman 2002).\nNote that like the leading eigenvector method, this approach is divisive with all nodes starting in a single cluster, then that cluster splitting in two, and so forth. This algorithm is implemented in igraph in the function cluster_edge_betweenness.\nLet’s see how it works:\n\n   eb.res <- cluster_edge_betweenness(ug, directed = FALSE)\n\nInterestingly, even though this is a divisive algorithm, the results can be presented as a dendrogram, but storing top down divisions rather than bottom up merges:\n\n   hc <- as.hclust(eb.res)\n   par(cex = 0.5) #smaller labels\n   plot(hc)\n\n\n\n\nNote that the edge betweenness provides a different picture of the community structure of the network. This is not surprising because it is based on graph theoretic principles and not on null model principles like the modularity: The picture here is of two broad communities (in the middle) surrounded by many smaller islands, including some singletons.\nWe can, of course, combine the modularity and edge-betweenness criteria by trying out dendrogram cuts until we find one with the highest modularity.\nA simple function that does that looks like:\n\n   top.mod <- function(x, g, m) {\n      k <- 1:m\n      Q <- rep(0, m)\n      tab <- cbind(k, Q)\n      for (i in 2:m) {\n         c <- cutree(x, i)\n         tab[i, 2] <- round(modularity(g, c, directed = FALSE), 3)\n      }\n   return(tab)\n   }\n   top.mod(hc, ug, 25)\n\n       k     Q\n [1,]  1 0.000\n [2,]  2 0.000\n [3,]  3 0.067\n [4,]  4 0.069\n [5,]  5 0.073\n [6,]  6 0.073\n [7,]  7 0.072\n [8,]  8 0.071\n [9,]  9 0.070\n[10,] 10 0.167\n[11,] 11 0.166\n[12,] 12 0.181\n[13,] 13 0.178\n[14,] 14 0.183\n[15,] 15 0.179\n[16,] 16 0.183\n[17,] 17 0.182\n[18,] 18 0.188\n[19,] 19 0.193\n[20,] 20 0.194\n[21,] 21 0.192\n[22,] 22 0.188\n[23,] 23 0.208\n[24,] 24 0.201\n[25,] 25 0.195\n\n\nWhich seems to recommend we divide the graph into 23 communities! This is, of course, what is stored in the membership object:\n\n   names(eb.res$membership) <- 1:68\n   eb.res$membership\n\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \n 1  1  2  1  3  4  2  1  1  1  1  1  1  5  6  1  1  3  7  1  1  1  1  1  1  1 \n27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 \n 1  3  1  8  1  3  3  1  3  1  1  1  1  1  1  1  9  7 10 11  3  3 12 13 14 15 \n53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 \n16 17  3 18  7 19 20 21 22  1 23 23 23 23 23 23 \n\n\nAnother thing we can do with the edge_betweenness function output is that we can highlight the edges identified as bridges during the divisive process, as this information is stored in the object bridges. We can use this vector of edge ids to create an edge color vector that will highlight the bridges in red.\n\n\n\n\n   set.seed(123)\n   V(ug)$color <- fg.res$membership\n   plot(ug, \n     vertex.size=6, vertex.frame.color=\"lightgray\", \n     vertex.label = NA, edge.arrow.size = 0.25,\n     vertex.label.dist=1, edge.curved=0.2)   \n   \n   set.seed(123)\n   V(ug)$color <- c25[eb.res$membership]\n   e.col <- rep(\"lightblue\", ecount(ug))\n   e.col[eb.res$bridges] <- \"red\"\n   E(ug)$color <- e.col\n   plot(ug, \n     vertex.size=6, vertex.frame.color=\"lightgray\", \n     vertex.label = NA, edge.arrow.size = 0.25,\n     vertex.label.dist=1, edge.curved=0.2)\n\n\n\n\n\n\nFigure 4: Law Firm Friendship Network Partitioned According to Agglomerative Clustering Based on Optimizing the Modularity\n\n\n\n\n\n\n\nFigure 5: Law Firm Friendship Network Partitioned According to the Edge Betweenness Algorithm with Number of Communities that Maximize the Modularity (Bridge Edges in Red)\n\n\n\n\n\n\nThe resulting twenty-three-community partition based on the edge betweenness is shown in Figure 5."
  },
  {
    "objectID": "cube.html",
    "href": "cube.html",
    "title": "The Centrality Cube",
    "section": "",
    "text": "Brandes, Borgatti, and Freeman (2016) discuss the centrality “cube,” an interesting and intuitive way to understand the way betweenness centrality works, as well as the dual connection between closeness and betweenness.\nLet us illustrate using a simple example. We begin by creating an Erdos-Renyi graph with eight nodes and connection probability \\(p = 0.5\\):\nThe resulting graph looks like:\nThe basic innovation behind the centrality cube is to store the intermediation information among every node triplet in the graph \\(s\\), \\(r\\), \\(b\\) (standing for “sender,” receiver,” and “broker”) in a three dimensional array rather than the usual two dimensional matrix.\nThe three dimensional array can be thought of as a “cube” by stacking multiple reachability matrices between every pair \\(s\\) and \\(r\\) along a three dimensional dimension \\(b\\). So each “b-slice” of the cube will contain the number of times node \\(b\\) stands in a shortest path between \\(s\\) and \\(r\\) divided by the total number of paths between \\(s\\) and \\(r\\) which as you recall computes the pairwise betweenness of \\(b\\) with respect to \\(s\\) and \\(r\\).\nLet’s see how that works."
  },
  {
    "objectID": "cube.html#building-the-cube",
    "href": "cube.html#building-the-cube",
    "title": "The Centrality Cube",
    "section": "Building the Cube",
    "text": "Building the Cube\nWe begin by writing a simple user-defined function to count the total number of shortest paths between each pair of nodes:\n\n   nsp <- function(x) {\n      n <- vcount(g)\n      S <- matrix(0, n, n)\n      for (i in 1:n) {\n            for (j in 1:n) {\n                  if (j %in% neighbors(x, i) == FALSE) {\n                     S[i, j] <- length(all_shortest_paths(x, i, j)$vpaths)\n               }\n            }\n         }\n      return(S)\n   }\n\nThe function is called nsp and takes a graph as input and returns and matrix called \\(\\mathbf{S}\\) with entries \\(s_{ij}\\) equal to the total number of shortest paths between \\(i\\) and \\(j\\). This is done by computing the length of the list returned by the all_shortest_paths function in igraph for each pair of non-adjacent nodes. This is done in two steps.\n\nFirst, we check whether \\(j\\) is a neighbor of \\(i\\) using the neighbors function in igraph. The neighbors function takes a graph and a node id as input and returns a vector of that node’s neighbors in the graph. We want the function to update the \\(S\\) matrix only when \\(i\\) and \\(j\\) are not adjacent (indirectly connected).\nSecond, we use the all_shortest_paths function to actually compute the number of shortest paths between \\(i\\) and \\(j\\). This function takes three inputs: (1) A graph object, (2) a sender node id, and (3) a receiver node id (which can be a vector of receiver nodes), and returns a list of the paths between the sender and receiver nodes in the form of vectors of node ids defining each path as elements of a list called “vpaths.”\n\nNow we are ready to write a user defined function to build the cube. Here’s a not-so-efficient (programming wise) but working example:\n\n   cube <- function(g) {\n      n <- vcount(g)\n      c <- array(rep(n^3, 0), c(n, n, n))\n      S <- nsp(g)\n      for (b in 1:n) {\n         for (s in 1:n) {\n            for (r in 1:n) {\n               if (s != r & r %in% neighbors(g, s) == FALSE) {\n                  p.sr <- all_shortest_paths(g, s, r)$vpaths\n                  b.sr <- lapply(p.sr, function(x) {x[-c(1, length(x))]}) \n                  c[s, r, b] <- sum(as.numeric(sapply(b.sr, function(x) {b %in% x})))\n                  c[s, r, b] <- c[s, r, b]/S[s, r]\n               }\n            }\n         }\n      }\n   c[is.na(c)] <- 0\n   return(c)\n   }\n\nIn line 1, we name the function cube. Line 3 initializes the array in R. It takes a string of zeros of length \\(n^3\\) where \\(n\\) is the number of nodes and crams them into an \\(n \\times n \\times n\\) array. In this case, since \\(n = 8\\), this means eight empty matrices of dimensions \\(8 \\times 8\\) stacked together to form our cube full of zeros. The \\(ijk^{th}\\) cell of the array corresponds to sender \\(i\\), receiver \\(j\\) and broker node \\(k\\). Line 4 computes the matrix \\(S\\) containing the number of shortest paths between every sender and receiver node in the graph.\nLines 5-15 populate the cube with the required information using an (inefficient) triple for loop. As noted some, useful igraph functions come into play here:\n\nIn line 8 the if conditional inside the triple loop uses the neighbors function in igraph and checks that node \\(r\\) is not a neighbor of \\(s\\) (if they are directly connected then node \\(b\\) cannot be a broker).\nAfter we check that \\(s\\) and \\(r\\) are not neighbors, we use the all_shortest_paths function in igraph to get all the shortest paths between \\(s\\) and \\(r\\) in line 9.\nLine 10 uses some lapply magic to drop the source and receiver nodes from the list of node ids vectors returned by the all_shortest_paths function.\nLine 11 uses additional sapply magic and the base R function %in% to check how many times each broker node \\(b\\) shows up in that list of shortest paths as an inner node between \\(s\\) and \\(r\\); we put that number in the \\(ijk^{th}\\) cell of the array, and loop through all triplets until we are done.\nLine 12 takes the number computed in line 11 and divides by the total number of shortest paths between \\(s\\) and \\(r\\) which is the betweenness ratio we are seeking."
  },
  {
    "objectID": "cube.html#exploring-the-cube",
    "href": "cube.html#exploring-the-cube",
    "title": "The Centrality Cube",
    "section": "Exploring the Cube",
    "text": "Exploring the Cube\nOnce we have our array, we can create all kinds of interesting sub-matrices containing the intermediation information in the graph by summing rows and columns of the array along different dimensions.\nFirst, let us see what’s in the cube. We can query specific two-dimensional sub-matrices using an extension of the usual format for querying matrices in R for three-dimensional arrays. For instance this:\n\n   srb <- cube(g)\n   round(srb[, , 2], 2)\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n[1,]  0.0    0 0.00    0 0.00  0.5    0 0.00\n[2,]  0.0    0 0.00    0 0.00  0.0    0 0.00\n[3,]  0.0    0 0.00    0 0.00  0.0    0 0.33\n[4,]  0.0    0 0.00    0 0.00  0.0    0 0.00\n[5,]  0.0    0 0.00    0 0.00  0.0    0 0.33\n[6,]  0.5    0 0.00    0 0.00  0.0    0 1.00\n[7,]  0.0    0 0.00    0 0.00  0.0    0 0.00\n[8,]  0.0    0 0.33    0 0.33  1.0    0 0.00\n\n\nCreates a three-dimensional matrix of pairwise betweenness probabilities and assigns it to the srb object in line 1, and looks at the \\(s_{\\bullet} \\times r_{\\bullet} \\times b_2\\) entry in line 2.\nEach entry in the matrix is the probability that node 2 stands on a shortest path between the row sender and the column receiver node. For instance, the 0.33 in the entry corresponding to row 5 and column 8 tells us that node 2 stands in one third of the shortest paths between nodes 5 and 8 (there are 3 distinct shortest paths between 5 and 8).\nBecause each sub-matrix in the cube is a matrix, we can do the usual matrix operations on them. For instance, let’s take the row sums of the \\(s\\) to \\(r\\) matrix corresponding to node 3 as the broker. This can be done like this:\n\n   d.3 <- rowSums(srb[ , , 3])\n   names(d.3) <- 1:vcount(g)\n   d.3\n\n  1   2   3   4   5   6   7   8 \n1.5 2.0 0.0 3.0 6.0 3.5 3.0 1.0 \n\n\nAs Brandes, Borgatti, and Freeman (2016), note this vector gives us the dependence of each node in the graph on node 3. Obviously node 3 doesn’t depend on itself so there is a zero on the third spot in the vector. As is clear from the plot, node 5 is the most dependent on 3 for intermediation with the rest of the nodes in the graph.\nWe can also pick a particular sender and receiver node and sum all their dyadic entries in the cube across the third (broker) dimension:\n\n   sum(srb[1, 6, ])\n\n[1] 2\n\n\nThis number is equivalent to the geodesic distance between the nodes minus one:\n\n   distances(g)[1, 6] - 1\n\n[1] 2"
  },
  {
    "objectID": "cube.html#betweenness-and-closeness-in-the-cube",
    "href": "cube.html#betweenness-and-closeness-in-the-cube",
    "title": "The Centrality Cube",
    "section": "Betweenness and Closeness in the Cube",
    "text": "Betweenness and Closeness in the Cube\nThe betweenness centrality of each node is encoded in the cube, because we already computed the main ratio that the measure depends on. For instance, let’s look at the matrix composed by taking the slice of cube that corresponds to node 3 as a broker:\n\n   srb[, , 3]\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n[1,]  0.0  0.0    0  0.0    1  0.5  0.0    0\n[2,]  0.0  0.0    0  0.5    1  0.0  0.5    0\n[3,]  0.0  0.0    0  0.0    0  0.0  0.0    0\n[4,]  0.0  0.5    0  0.0    1  1.0  0.5    0\n[5,]  1.0  1.0    0  1.0    0  1.0  1.0    1\n[6,]  0.5  0.0    0  1.0    1  0.0  1.0    0\n[7,]  0.0  0.5    0  0.5    1  1.0  0.0    0\n[8,]  0.0  0.0    0  0.0    1  0.0  0.0    0\n\n\nThe sum of the all the cells in this matrix (divided by two) correspond to node 3’s betweenness centrality:\n\n   sum(srb[, , 3])/2\n\n[1] 10\n\n   betweenness(g)[3]\n\n[1] 10\n\n\nSo to get each node’s betweenness we just can just sum up the entries in each of the cube’s sub-matrices:\n\n   b.cube <- round(colSums(srb, dims = 2)/2, 2)\n   b.igraph <- round(betweenness(g), 2)\n   names(b.cube) <- 1:vcount(g)\n   names(b.igraph) <- 1:vcount(g)\n   b.cube\n\n    1     2     3     4     5     6     7     8 \n 0.00  2.17 10.00  0.67  0.00  0.00  3.17  4.00 \n\n   b.igraph\n\n    1     2     3     4     5     6     7     8 \n 0.00  2.17 10.00  0.67  0.00  0.00  3.17  4.00 \n\n\nNote the neat trick of using the argument dims = 2 in the usual colSums command. This tells colSums that we are dealing with a three dimensional matrix, and that what we want is the sum of the columns across the cube’s third dimension (the brokers). Note also that we divide the cube betweenness by two because we are summing identical entries across the upper and lower triangle of the symmetric dyadic brokerage matrices inside the cube (not surprisingly, node 3 is the top betweenness centrality node).\nAs Brandes, Borgatti, and Freeman (2016) point out, using the cube info, we can build a matrix of dependencies between each pair of nodes. In this matrix, the rows correspond to a sender (or receiver) node, the columns to a broker node and the \\(sb^{th}\\) entry contains the sum of the proportion of paths containing the broker nodes that starts with the sender node and end with some other node in the graph.\nHere’s a function that uses the cube info to build the dependency matrix that Brandes, Borgatti, and Freeman (2016) talk about using the cube as input:\n\n   dep.ij <- function(c) {\n      n <- nrow(c)\n      dep.ij <- rowSums(c[, , 1])\n      for (i in 2:n) {\n         dep.ij <- cbind(dep.ij,  rowSums(c[, , i]))\n         \n         }\n      rownames(dep.ij) <- 1:n\n      colnames(dep.ij) <- 1:n\n      return(dep.ij)\n   }\n\nThis function just takes the various vectors formed by the row sums of the sender-receiver matrix across each value of the third dimension (which is just each node in the graph when playing the broker role). It then returns a regular old \\(n \\times n\\) containing the info.\nHere’s the result when applied to our little example:\n\n   library(kableExtra)\n   kbl(round(dep.ij(srb), 2), \n       format = \"html\", align = \"c\", row.names = TRUE,\n       caption = \"Dependence Matrix.\") %>% \n      column_spec(1, bold = TRUE) %>%\n   kable_styling(bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\nDependence Matrix.\n \n  \n      \n    1 \n    2 \n    3 \n    4 \n    5 \n    6 \n    7 \n    8 \n  \n \n\n  \n    1 \n    0 \n    0.50 \n    1.5 \n    0.00 \n    0 \n    0 \n    2.50 \n    2.5 \n  \n  \n    2 \n    0 \n    0.00 \n    2.0 \n    0.00 \n    0 \n    0 \n    0.00 \n    2.0 \n  \n  \n    3 \n    0 \n    0.33 \n    0.0 \n    0.33 \n    0 \n    0 \n    1.33 \n    0.0 \n  \n  \n    4 \n    0 \n    0.00 \n    3.0 \n    0.00 \n    0 \n    0 \n    0.00 \n    2.0 \n  \n  \n    5 \n    0 \n    0.33 \n    6.0 \n    0.33 \n    0 \n    0 \n    1.33 \n    0.0 \n  \n  \n    6 \n    0 \n    1.50 \n    3.5 \n    0.00 \n    0 \n    0 \n    0.50 \n    0.5 \n  \n  \n    7 \n    0 \n    0.00 \n    3.0 \n    0.00 \n    0 \n    0 \n    0.00 \n    1.0 \n  \n  \n    8 \n    0 \n    1.67 \n    1.0 \n    0.67 \n    0 \n    0 \n    0.67 \n    0.0 \n  \n\n\n\n\n\nNote that this is valued matrix that is also asymmetric. Take for instance, node 3. Every node in the graph depends on node 3 for access to other nodes, but node 3 does not depend on nodes 1, 5, 6, or 8.\nInterestingly, as Brandes, Borgatti, and Freeman (2016) also show, the betweenness centrality also can be calculated from the dependency matrix! All we need to do is compute the column sums, equivalent to in-degree in the directed dependence network:\n\n   round(colSums(dep.ij(srb))/2, 2)\n\n    1     2     3     4     5     6     7     8 \n 0.00  2.17 10.00  0.67  0.00  0.00  3.17  4.00 \n\n\nEven more interestingly, closeness centrality is also in the dependence matrix! It is given by the outdegree of each actor in the directed dependence network, corresponding to the row sums of the matrix (shifted by a constant given by \\(n-1\\)).\n\n   c.c <- rowSums(distances(g))\n   c.d <- rowSums(dep.ij(srb)) + (vcount(g) - 1)\n   names(c.c) <- 1:vcount(g)\n   names(c.d) <- 1:vcount(g)\n   round(1/c.c, 3)\n\n    1     2     3     4     5     6     7     8 \n0.071 0.091 0.111 0.083 0.067 0.077 0.091 0.091 \n\n   round(1/c.d, 3)\n\n    1     2     3     4     5     6     7     8 \n0.071 0.091 0.111 0.083 0.067 0.077 0.091 0.091 \n\n\nHere we see that node 3 is also the top in closeness, followed closely (pun intended) by nodes 2, 7, and 8. This makes sense because an actor with high closeness is one that has low dependence on key nodes to be able to reach others.\nOf course, closeness is also in the cube because of the mathematical relationship we saw earlier between the sum of entries between senders and receivers across brokers in the cube and the geodesic distance.\nFor instance, let’s get the matrix corresponding to node 3’s role as sender across all brokers and receivers:\n\n   round(srb[3, , ], 2)\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n[1,]    0 0.00    0 0.00    0    0 1.00    0\n[2,]    0 0.00    0 0.00    0    0 0.00    0\n[3,]    0 0.00    0 0.00    0    0 0.00    0\n[4,]    0 0.00    0 0.00    0    0 0.00    0\n[5,]    0 0.00    0 0.00    0    0 0.00    0\n[6,]    0 0.00    0 0.00    0    0 0.00    0\n[7,]    0 0.00    0 0.00    0    0 0.00    0\n[8,]    0 0.33    0 0.33    0    0 0.33    0\n\n\nThe entries of this matrix give us the probability that node 3 is the sender, whenever the row node is the is the broker (an inner node in the path) and the column node is the receiver.\nFor instance, the value 0.3 in row 8 and column 1 tells us that node 3 is the sender node in one third of the paths that end in node 1 and feature node 8 as a broker.\nInterestingly, the sums of the entries in this matrix are equivalent to the sum of the geodesic distances between node 3 and every other node in the graph shifted by a constant (\\(n- 1\\)):\n\n   sum(srb[3, , ]) + (vcount(g) - 1)\n\n[1] 9\n\n   sum(distances(g)[3, ])\n\n[1] 9\n\n\nSo the closeness centrality can be computed from the cube as follows:\n\n   c <- 0\n   n <- vcount(g)\n   for (i in 1:n) {\n      c[i] <- 1/(sum(srb[i, , ]) + (n - 1))\n   }\n   round(c, 3)\n\n[1] 0.071 0.091 0.111 0.083 0.067 0.077 0.091 0.091\n\n\nWhich is the same as:\n\n   round(closeness(g), 3)\n\n[1] 0.071 0.091 0.111 0.083 0.067 0.077 0.091 0.091"
  },
  {
    "objectID": "ego.html",
    "href": "ego.html",
    "title": "Analyzing Ego Networks",
    "section": "",
    "text": "An ego-network, is just a subgraph of a larger network–referred to as an ego graph–that includes a node of interest (“ego”), all of the connections between ego and their neighbors (called “alters”) and usually all of the connections between each of the alters, excluding the connections alters may have with other nodes who are not directly tied to ego.\nEgo network data is social network data collected in such way (e.g., using standard social survey techniques) that you capture the ego networks of some set of people, usually a convenience sample or, more rarely, a probability sample of some population.\nNote that if you collect traditional (“whole”) network data (e.g., all the people in a classroom) you have ego network data for that settings (the ego subgraphs of each node as just defined), but not the reverse; ego-network data cannot be converted into “whole” network data, although it can approximate it.\nOnce you have ego network data you can analyze each ego graph using the standard techniques we learned so far (if you are only interested in the structural characteristics of the ego graph).\nIf you have attributes on each alter, you can alternatively compute measures of diversity to or homophily to get a sense of how likely ego is to connect to similar or diverse others."
  },
  {
    "objectID": "ego.html#structural-measures",
    "href": "ego.html#structural-measures",
    "title": "Analyzing Ego Networks",
    "section": "Structural Measures",
    "text": "Structural Measures\n\nThe Clustering Coefficient\nPerhaps the most basic structural characteristic of an ego network is the density of the subgraph formed by all of the connections between the alters. This is called ego’s clustering coefficient.\nLet’s see how it works. First we load up the New Hope Star Wars social network included in the networkdata package (Gabasova 2016):\n\n   library(networkdata)\n   g <- starwars[[4]]\n\nThe starwars object contain a list of igraph network data sets corresponding to seven of the Star Wars films (the three originals, three prequels, and The Force Awakens). They are ordered in terms of the film chronology, so Episode IV, A New Hope is the fourth element in the list.\nAs we said an ego graph is just a subgraph centered on a particular actor. So R2-D2’s ego graph is just:\n\n   library(igraph)\n   N <- neighbors(g, \"R2-D2\")\n   r2 <- subgraph(g, c(\"R2-D2\", names(N))) #ego graph\n\nAnd we can just plot it like we would any igraph object:\n\n   V(r2)$color <- rep(2, vcount(r2)) #alters appear in blue\n   V(r2)$color[which(V(r2)$name == \"R2-D2\")] <- 1 #ego appears in tan  \n   plot(r2, \n     vertex.size=10, vertex.frame.color=\"lightgray\", \n     vertex.label.dist=2, \n     layout = layout_as_star(r2, center = \"R2-D2\"),\n     vertex.label.cex = 1.25, edge.color = \"lightgray\")\n\n\n\n\nR2-D2’s Ego Network.\n\n\n\n\nNote that we use the layout_as_star() for the layout, so that the ego is put in the center of the star graph surrounded by their alters, we can specify who the star node is using the option center.\nR2-D2’s clustering coefficient is just the density of the graph that includes only the alters:\n\n   alters <- r2 - vertex(\"R2-D2\")\n   C <- round(edge_density(alters), 2)\n   C\n\n[1] 0.67\n\n\nThe clustering coefficient \\(C_i\\) for an ego \\(i\\) ranges from zero to one. \\(C = 0\\) means that none of ego’s alters are connected to one another and \\(C = 1\\) means that all of ego’s alters are connected to one another. In this case, \\(C = 0.67\\) means that 67% of R2-D2’s alters are connected (e.g., co-appear in scenes) to one another.\nTypically we would want to compute the clustering coefficient of every node in a graph. This can be done using our trusty lapply and sapply meta-functions.\nWe proceed in three steps:\nFirst, we turn the code we used to find R2-D2’s ego graph into a function:\n\n   create.ego <- function(x, w) {\n      alter.net <- subgraph(w, neighbors(w, x))\n      return(alter.net)\n      }\n\nThen we lapply the function to each node in the network:\n\n   ego.graphs <- lapply(V(g)$name, create.ego, w = g)\n   head(ego.graphs)\n\n[[1]]\nIGRAPH 810ef73 UNW- 9 24 -- Episode IV – A New Hope\n+ attr: name (g/c), name (v/c), height (v/n), mass (v/n), hair_color\n| (v/c), skin_color (v/c), eye_color (v/c), birth_year (v/n), sex\n| (v/c), homeworld (v/c), species (v/c), weight (e/n)\n+ edges from 810ef73 (vertex names):\n [1] CHEWBACCA--C-3PO   CHEWBACCA--LUKE    C-3PO    --LUKE    C-3PO    --BIGGS  \n [5] LUKE     --BIGGS   CHEWBACCA--LEIA    C-3PO    --LEIA    LUKE     --LEIA   \n [9] BIGGS    --LEIA    C-3PO    --BERU    LUKE     --BERU    LEIA     --BERU   \n[13] C-3PO    --OWEN    LUKE     --OWEN    BERU     --OWEN    CHEWBACCA--OBI-WAN\n[17] C-3PO    --OBI-WAN LUKE     --OBI-WAN LEIA     --OBI-WAN CHEWBACCA--HAN    \n[21] C-3PO    --HAN     LUKE     --HAN     LEIA     --HAN     OBI-WAN  --HAN    \n\n[[2]]\nIGRAPH 810f581 UNW- 6 15 -- Episode IV – A New Hope\n+ attr: name (g/c), name (v/c), height (v/n), mass (v/n), hair_color\n| (v/c), skin_color (v/c), eye_color (v/c), birth_year (v/n), sex\n| (v/c), homeworld (v/c), species (v/c), weight (e/n)\n+ edges from 810f581 (vertex names):\n [1] R2-D2  --C-3PO   R2-D2  --LUKE    C-3PO  --LUKE    R2-D2  --LEIA   \n [5] C-3PO  --LEIA    LUKE   --LEIA    R2-D2  --OBI-WAN C-3PO  --OBI-WAN\n [9] LUKE   --OBI-WAN LEIA   --OBI-WAN R2-D2  --HAN     C-3PO  --HAN    \n[13] LUKE   --HAN     LEIA   --HAN     OBI-WAN--HAN    \n\n[[3]]\nIGRAPH 810f684 UNW- 10 27 -- Episode IV – A New Hope\n+ attr: name (g/c), name (v/c), height (v/n), mass (v/n), hair_color\n| (v/c), skin_color (v/c), eye_color (v/c), birth_year (v/n), sex\n| (v/c), homeworld (v/c), species (v/c), weight (e/n)\n+ edges from 810f684 (vertex names):\n [1] R2-D2    --CHEWBACCA R2-D2    --LUKE      CHEWBACCA--LUKE     \n [4] R2-D2    --BIGGS     LUKE     --BIGGS     R2-D2    --LEIA     \n [7] CHEWBACCA--LEIA      LUKE     --LEIA      BIGGS    --LEIA     \n[10] R2-D2    --BERU      LUKE     --BERU      LEIA     --BERU     \n[13] R2-D2    --OWEN      LUKE     --OWEN      BERU     --OWEN     \n[16] R2-D2    --OBI-WAN   CHEWBACCA--OBI-WAN   LUKE     --OBI-WAN  \n+ ... omitted several edges\n\n[[4]]\nIGRAPH 810f78c UNW- 15 36 -- Episode IV – A New Hope\n+ attr: name (g/c), name (v/c), height (v/n), mass (v/n), hair_color\n| (v/c), skin_color (v/c), eye_color (v/c), birth_year (v/n), sex\n| (v/c), homeworld (v/c), species (v/c), weight (e/n)\n+ edges from 810f78c (vertex names):\n [1] R2-D2    --CHEWBACCA R2-D2    --C-3PO     R2-D2    --BERU     \n [4] R2-D2    --OWEN      R2-D2    --OBI-WAN   R2-D2    --LEIA     \n [7] R2-D2    --BIGGS     R2-D2    --HAN       CHEWBACCA--OBI-WAN  \n[10] CHEWBACCA--C-3PO     CHEWBACCA--HAN       CHEWBACCA--LEIA     \n[13] CAMIE    --BIGGS     BERU     --OWEN      C-3PO    --BERU     \n[16] C-3PO    --OWEN      C-3PO    --LEIA      LEIA     --BERU     \n+ ... omitted several edges\n\n[[5]]\nIGRAPH 810f882 UNW- 4 4 -- Episode IV – A New Hope\n+ attr: name (g/c), name (v/c), height (v/n), mass (v/n), hair_color\n| (v/c), skin_color (v/c), eye_color (v/c), birth_year (v/n), sex\n| (v/c), homeworld (v/c), species (v/c), weight (e/n)\n+ edges from 810f882 (vertex names):\n[1] LEIA --OBI-WAN LEIA --MOTTI   LEIA --TARKIN  MOTTI--TARKIN \n\n[[6]]\nIGRAPH 810f975 UNW- 2 1 -- Episode IV – A New Hope\n+ attr: name (g/c), name (v/c), height (v/n), mass (v/n), hair_color\n| (v/c), skin_color (v/c), eye_color (v/c), birth_year (v/n), sex\n| (v/c), homeworld (v/c), species (v/c), weight (e/n)\n+ edge from 810f975 (vertex names):\n[1] LUKE--BIGGS\n\n\nThe result is a list object with \\(|V| = 21\\) ego subgraphs composed of each node’s alters and their connections.\nFinally, to find out the clustering coefficient of each node, we just sapply the igraph function edge_density to each member of the ego.graphs list:\n\n   C <- sapply(ego.graphs, edge_density)\n   names(C) <- V(g)$name\n   round(C, 2)\n\n      R2-D2   CHEWBACCA       C-3PO        LUKE DARTH VADER       CAMIE \n       0.67        1.00        0.60        0.34        0.67        1.00 \n      BIGGS        LEIA        BERU        OWEN     OBI-WAN       MOTTI \n       0.54        0.42        0.90        1.00        0.76        1.00 \n     TARKIN         HAN      GREEDO       JABBA     DODONNA GOLD LEADER \n       1.00        0.54         NaN         NaN        1.00        0.80 \n      WEDGE  RED LEADER     RED TEN \n       0.80        0.57        1.00 \n\n\nNote we have a couple of NaN values in the slots corresponding to Greedo and Jabba in the clustering coefficient vector.\nLet’s check out why:\n\n   degree(g)\n\n      R2-D2   CHEWBACCA       C-3PO        LUKE DARTH VADER       CAMIE \n          9           6          10          15           4           2 \n      BIGGS        LEIA        BERU        OWEN     OBI-WAN       MOTTI \n          8          12           5           4           7           3 \n     TARKIN         HAN      GREEDO       JABBA     DODONNA GOLD LEADER \n          3           8           1           1           3           5 \n      WEDGE  RED LEADER     RED TEN \n          5           7           2 \n\n\nHere we see the problem is that both Greedo and Jabba are singleton nodes (with degree equal to one), so it doesn’t make sense to analyze their clustering coefficients because their ego graph is just an isolated node!\nWe can just drop them and re-analyze:\n\n   g <- subgraph(g, degree(g)> 1)\n   ego.graphs <- lapply(V(g)$name, create.ego, w = g)\n   C <- sapply(ego.graphs, edge_density)\n   names(C) <- V(g)$name\n   round(C, 2)\n\n      R2-D2   CHEWBACCA       C-3PO        LUKE DARTH VADER       CAMIE \n       0.67        1.00        0.60        0.34        0.67        1.00 \n      BIGGS        LEIA        BERU        OWEN     OBI-WAN       MOTTI \n       0.54        0.42        0.90        1.00        0.76        1.00 \n     TARKIN         HAN     DODONNA GOLD LEADER       WEDGE  RED LEADER \n       1.00        1.00        1.00        0.80        0.80        0.57 \n    RED TEN \n       1.00 \n\n\nMuch better!\nNote that in this analysis, Luke has the lowest clustering coefficient (\\(C = 0.34\\)) this usually indicates an ego whose alters are partitioned into distinct clusters (and hence they are not connected to one another), and ego is a mediator or broker between those clusters.\nLet’s see what that looks like:\n\n   set.seed(456)\n   N <- neighbors(g, \"LUKE\")\n   luke.alters <- subgraph(g, N)  \n   V(luke.alters)$color <- cluster_leading_eigen(luke.alters)$membership\n   luke <- subgraph(g, c(\"LUKE\", names(N)))\n   luke <- simplify(union(luke, luke.alters))\n   V(luke)$color[which(is.na(V(luke)$color))] <- \"red\"\n   plot(luke, \n     vertex.size=6, vertex.frame.color=\"lightgray\", \n     vertex.label.dist=1.25,  \n     vertex.label.cex = 0.75, edge.color = \"lightgray\")\n\n\n\n\nLuke’s ego network with alter nodes colored by community assingment via Newman’s leading eigenvector method.\n\n\n\n\nHere we can see that Luke mediates between the Rebel Pilot community on the left and the Obi-Wan, Leia, Chewbacca, Han Solo and Droid communities on the right. Note that we didn’t have to use that layout_as_star option because when ego’s network has community structure, the traditional spring embedding layout will put ego at the center.\nNote that in the ego graph that includes ego, each connected alter is a triangle in the ego graph. So the clustering coefficient is simply a count of the number of undirected triangles that are centered on ego, or the number of cycles of length three centered on ego.\nSo that means that the diagonals of the cube of the adjacency matrix also contain the information needed to compute the clustering coefficient:\n\n   A <- as.matrix(as_adjacency_matrix(g))\n   A3 <- A %*% A %*% A\n   diag(A3)\n\n      R2-D2   CHEWBACCA       C-3PO        LUKE DARTH VADER       CAMIE \n         48          30          54          72           8           2 \n      BIGGS        LEIA        BERU        OWEN     OBI-WAN       MOTTI \n         30          56          18          12          32           6 \n     TARKIN         HAN     DODONNA GOLD LEADER       WEDGE  RED LEADER \n          6          30           6          16          16          24 \n    RED TEN \n          2 \n\n\nSo all we need to do is divide these numbers by the maximum possible number of undirected triangles that could be centered on a node, which is \\(k_i(k_i - 1)\\) where \\(k_i\\) is ego’s degree:\n\n   k <- degree(g)\n   C <- diag(A3)/(k*(k - 1))\n   round(C, 2)\n\n      R2-D2   CHEWBACCA       C-3PO        LUKE DARTH VADER       CAMIE \n       0.67        1.00        0.60        0.34        0.67        1.00 \n      BIGGS        LEIA        BERU        OWEN     OBI-WAN       MOTTI \n       0.54        0.42        0.90        1.00        0.76        1.00 \n     TARKIN         HAN     DODONNA GOLD LEADER       WEDGE  RED LEADER \n       1.00        1.00        1.00        0.80        0.80        0.57 \n    RED TEN \n       1.00 \n\n\nWhich gives us the answer as before!\nFinally, from each ego’s clustering coefficient (sometimes called the local clustering coefficient of each node) we can compute the graph’s global clustering coefficient which is just the average this quantity across each node in the graph:\n\\[\nC(G) = \\frac{1}{N}\\sum_iC_i\n\\]\nIn R:\n\n   C.glob <- mean(C)\n   round(C.glob, 2)\n\n[1] 0.79\n\n\nWhich indicates a fairly clustered graph.\nIn igraph we can use the function transitivity to compute the local and global clustering coefficients, which can be specified using the argument type. For the local version, the function also expects a list of vertices:\n\n   round(transitivity(g, V(g)$name, type = \"local\"), 2)\n\n      R2-D2   CHEWBACCA       C-3PO        LUKE DARTH VADER       CAMIE \n       0.67        1.00        0.60        0.34        0.67        1.00 \n      BIGGS        LEIA        BERU        OWEN     OBI-WAN       MOTTI \n       0.54        0.42        0.90        1.00        0.76        1.00 \n     TARKIN         HAN     DODONNA GOLD LEADER       WEDGE  RED LEADER \n       1.00        1.00        1.00        0.80        0.80        0.57 \n    RED TEN \n       1.00 \n\n\nAnd the graph’s global clustering coefficient is:\n\n   round(transitivity(g, type = \"average\"), 2)\n\n[1] 0.79"
  },
  {
    "objectID": "ego.html#effective-size",
    "href": "ego.html#effective-size",
    "title": "Analyzing Ego Networks",
    "section": "Effective Size",
    "text": "Effective Size\nAnother structural measure at the ego-network level, closely related to the clustering coefficient, is the effective size (Burt 1992).\nThe basic idea here is that if you have two alters in your network who are themselves connected to one another then they are redundant, that is the information you get from one can be substituted for the information you can get from the other (because you can always find the information of one via the other).\nThis means that instead of counting them as two alters, we should (dis)count them as one (or if you were engineering your ego network to optimize effective size, you would drop one and keep the other).\nFor an ego, the effective size \\(ES\\) is then given by:\n\\[\nES_i = k_i - \\bar{k}_{j \\in N(i)}\n\\]\nWhere \\(k_i\\) is the ego network size (ego’s degree) and \\(\\bar{k}_{j \\in N(i)}\\) is the average degree of the subgraph formed by ego’s alters (excluding ego).\nSo to calculate \\(ES\\) in our network all we have to do is calculate the average degree of each ego graph and the subtract that from ego’s degree. We can obtain the average degree of each ego using the first two lapply and sapply lines in the following:\n\n   mk <- lapply(ego.graphs, degree) #degree sequence of each ego\n   mk <- sapply(mk, mean) \n   names(mk) <- V(g)$name\n   round(mk, 2)\n\n      R2-D2   CHEWBACCA       C-3PO        LUKE DARTH VADER       CAMIE \n       5.33        5.00        5.40        4.80        2.00        1.00 \n      BIGGS        LEIA        BERU        OWEN     OBI-WAN       MOTTI \n       3.75        4.67        3.60        3.00        4.57        2.00 \n     TARKIN         HAN     DODONNA GOLD LEADER       WEDGE  RED LEADER \n       2.00        5.00        2.00        3.20        3.20        3.43 \n    RED TEN \n       1.00 \n\n\nWe can then compute ES as follows:\n\n   ES <- degree(g) - mk\n   round(ES, 1)\n\n      R2-D2   CHEWBACCA       C-3PO        LUKE DARTH VADER       CAMIE \n        3.7         1.0         4.6        10.2         2.0         1.0 \n      BIGGS        LEIA        BERU        OWEN     OBI-WAN       MOTTI \n        4.2         7.3         1.4         1.0         2.4         1.0 \n     TARKIN         HAN     DODONNA GOLD LEADER       WEDGE  RED LEADER \n        1.0         1.0         1.0         1.8         1.8         3.6 \n    RED TEN \n        1.0 \n\n\nNote that for nodes with the maximum clustering coefficient (\\(C_i = 1\\)) (like Han) the \\(ES\\) measure reaches its minimum value of 1.0. The reason for this is that if the alter-to-alter network for an ego is a complete graph, then the average degree of that graph will necessarily be equal to the number of nodes in the alter-to-alter graph (ego’s degree) minus one.\nGenerally, \\(ES\\) will be smaller than ego’s degree, except in the case where ego is at the center of a star graph with \\(C_i = 0\\) (none of their contacts are connected to one another), in which case \\(ES\\) will be equal to ego’s degree (the effective size will be equal to the actual size).\nNote that if two nodes have the same degree, but different clustering coefficients, then the effective size of their networks will be different, with the node with the larger clustering coefficient having a smaller effective size.\nHere’s an example from A New Hope:\n\n\n\n\n\n\nk = 7, C = 0.76, ES = 2.4\n\n\n\n\n\n\n\nk = 7, C = 0.57, ES = 3.6\n\n\n\n\n\n\nTwo Ego Networks of the Same Size but Different Effective Size\n\n\n\nHere we can see that even though both Obi-Wan and Red Leader have an ego network of the same size (\\(k = 7\\)), a larger proportion of Obi-Wan’s contacts (\\(C = 0.76\\)) are connected to one another that Red Leader’s (\\(C = 0.57\\)), which means that Obi-Wan’s effective size (\\(ES = 2.4\\)) ends up being smaller than Red Leader’s (\\(ES = 3.6\\)).\n\nEgo-Network Betweenness\nAn alternative structural measure of ego’s position in the ego network, also closely related to the clustering coefficient, is ego network betweenness. As Everett and Borgatti (2005) note, in an ego network betweenness is determined by the number of paths of length two that involve ego, that is, by the number of disconnected alters.\nTherefore, if \\(\\mathbf{A}\\) is the adjacency matrix recording the links between the alters, then \\(\\mathbf{A}^2\\) will contain the number of paths of length two between each pair of alters. Because we are only interested in the number of paths of length two between each pair of disconnected alters, we multiply (element-wise) this matrix by the adjacency matrix corresponding to the graph complement (a matrix with a one for every zero in the original adjacency matrix and a zero for each one). We then take the sum of the reciprocals of one of the triangles (excluding the main diagonal) of the resulting matrix to find the betweenness of ego.\nIn math:\n\\[\nC_B(Ego) = \\sum_{i < j}\\left[\\mathbf{A}^2 \\bullet (\\mathbf{J} - \\mathbf{A})\\right]_{ij}^{-1}\n\\]\nWhere \\(\\mathbf{J}\\) is a matrix full of ones of the same dimensions as \\(\\mathbf{A}\\) and \\(\\bullet\\) indicates element-wise matrix multiplication.\nA simple function that does this looks like:\n\n   ego.bet <- function(x, n) {\n      N <- neighbors(x, n)\n      alter.net <- subgraph(x, c(n, names(N)))\n      A <- as.matrix(as_adjacency_matrix(alter.net))\n      A2 <- A %*% A\n      J <- matrix(1, nrow(A), ncol(A))\n      cb <- A2 * (J - A)\n      cb <- 1/cb\n      cb[is.infinite(cb)] <- 0\n      cb <- sum(cb[upper.tri(cb)])\n      return(cb)\n   }\n\nLet’s see how it works for Luke:\n\n   round(ego.bet(g, \"LUKE\"), 2)\n\n[1] 45.92\n\n\nWhich says that Luke has pretty high ego-network betweenness.\nWe can, of course, compute it for everyone in the network like before:\n\n   round(sapply(V(g)$name, ego.bet, x = g), 2)\n\n      R2-D2   CHEWBACCA       C-3PO        LUKE DARTH VADER       CAMIE \n       3.33        0.00        5.42       45.92        1.00        0.00 \n      BIGGS        LEIA        BERU        OWEN     OBI-WAN       MOTTI \n       5.58       25.83        0.25        0.00        2.50        0.00 \n     TARKIN         HAN     DODONNA GOLD LEADER       WEDGE  RED LEADER \n       0.00        0.00        0.00        0.67        0.67        3.83 \n    RED TEN \n       0.00 \n\n\nWhich confirms our original impression of Luke as the highest ego-network betweenness character with Leia in second place.\nWhat does it mean to have an ego-network betweenness of zero? Well, this is only possible if your clustering coefficient is 1.0, that is, when all of your alters are directly connected to one another. This is evident in Chewbacca’s ego network:\n\n   N <- neighbors(g, \"CHEWBACCA\")\n   chew <- subgraph(g, c(\"CHEWBACCA\", names(N)))\n   V(chew)$color <- rep(2, vcount(chew))\n   V(chew)$color[which(V(chew)$name == \"CHEWBACCA\")] <- 1 \n   plot(chew, \n     vertex.size=10, vertex.frame.color=\"lightgray\", \n     vertex.label.dist=2, \n     layout = layout_as_star(chew, center = \"CHEWBACCA\"),\n     vertex.label.cex = 1.25, edge.color = \"lightgray\")\n\n\n\n\nChewbacca’s Ego Network.\n\n\n\n\nWhich is a complete clique of size seven."
  },
  {
    "objectID": "ego.html#compositional-measures",
    "href": "ego.html#compositional-measures",
    "title": "Analyzing Ego Networks",
    "section": "Compositional Measures",
    "text": "Compositional Measures\n\nEgo Network Diversity\nIf we have information on the categorical vertex attributes of ego’s alters we may be interested in how diverse are ego’s choices across those attributes.\nThe most common measure is Blau’s Diversity Index (\\(H\\)). For a categorical attribute with \\(m\\) levels, this is given by:\n\\[\nH = 1 - \\sum_{k=1}^m p_k^2\n\\]\nWhere \\(p_k\\) is the proportion of ego’s alters that fall under category level \\(k\\).\nThe \\(H\\) measure ranges from a minimum of \\(H = 0\\) (all of ego’s alters belong to a single category) to a maximum of \\(H = 1- \\frac{1}{m}\\) (all of ego’s alters belong to a different category).\nWhen the Blau diversity index is normalized by its theoretical maximum, it is sometimes referred to as the Index of Qualitative Variation or \\(IQV\\):\n\\[\nIQV = \\frac{1 - \\sum_{k=1}^m p_k^2}{1-\\frac{1}{m}}\n\\]\nThe main difference between \\(H\\) and \\(IQV\\) is that the latter has a maximum of \\(IQV = 1.0\\) indicating the top diversity that can be observed for a categorical attribute with \\(m\\) categories.\nLet’s see how this would work.\nFirst, let’s switch to the Attack of the Clones Star Wars graph:\n\n   g <- starwars[[2]]\n   g <- subgraph(g, degree(g)> 1) #removing singletons\n\nNow, we will pick the vertex attribute homeworld and try to measure how diverse is each character’s ego network on this score.\nTo do that, we need to get the proportion of characters from each homeworld in the network.\nLet’s check out this vertex attribute:\n\n   V(g)$homeworld\n\n [1] \"Naboo\"          \"Naboo\"          \"Naboo\"          NA              \n [5] NA               \"Haruun Kal\"     NA               \"Cerea\"         \n [9] \"Alderaan\"       \"Naboo\"          \"Stewjon\"        \"Tatooine\"      \n[13] \"Naboo\"          NA               NA               NA              \n[17] \"Kamino\"         \"Kamino\"         \"Kamino\"         \"Concord Dawn\"  \n[21] \"Tatooine\"       \"Tatooine\"       \"Tatooine\"       \"Tatooine\"      \n[25] \"Serenno\"        NA               \"Geonosis\"       \"Cato Neimoidia\"\n\n\nThere are some NA values here, so let’s create a residual category called “other”:\n\n   V(g)$homeworld[is.na(V(g)$homeworld)] <- \"Other\"\n   V(g)$homeworld\n\n [1] \"Naboo\"          \"Naboo\"          \"Naboo\"          \"Other\"         \n [5] \"Other\"          \"Haruun Kal\"     \"Other\"          \"Cerea\"         \n [9] \"Alderaan\"       \"Naboo\"          \"Stewjon\"        \"Tatooine\"      \n[13] \"Naboo\"          \"Other\"          \"Other\"          \"Other\"         \n[17] \"Kamino\"         \"Kamino\"         \"Kamino\"         \"Concord Dawn\"  \n[21] \"Tatooine\"       \"Tatooine\"       \"Tatooine\"       \"Tatooine\"      \n[25] \"Serenno\"        \"Other\"          \"Geonosis\"       \"Cato Neimoidia\"\n\n\nGreat! Now we can use the native R function table to get the relevant proportions.\nThe function table gives us the count of characters in each category, and then we divide by the total number of actors in the network, given by vcount:\n\n   p.hw <- round(table(V(g)$homeworld)/vcount(g), 3)\n   p.hw\n\n\n      Alderaan Cato Neimoidia          Cerea   Concord Dawn       Geonosis \n         0.036          0.036          0.036          0.036          0.036 \n    Haruun Kal         Kamino          Naboo          Other        Serenno \n         0.036          0.107          0.179          0.250          0.036 \n       Stewjon       Tatooine \n         0.036          0.179 \n\n\nNow that we know how to get the proportions we need, we can write a custom function that will compute \\(H\\) (or its normalized counterpart the \\(IQV\\)) for a given ego network for any given attribute:\n\n   blau <- function(n, w, a, norm = FALSE) {\n      x <- subgraph(w, neighbors(w, n)) #ego subgraph\n      p <- table(vertex_attr(x, a))/vcount(x) #proportion of alters in each category of the attribute a\n      m <- length(p)\n      res <- 1 - sum(p^2) #blau diversity\n      if (norm == TRUE) {res <- res /(1 - 1/m)} #IQV\n      return(res)\n      }\n\nThis function takes three inputs: The name of the ego n, the graph object w, and the name of the attribute a. It returns the Blau diversity index score for that ego on that attribute by default; when norm is set to TRUE it returns the normalized Blau score (a.k.a. the \\(IQV\\)).\nLet’s see Padme’s Home World ego network diversity score:\n\n   round(blau(\"PADME\", g, \"homeworld\"), 3)\n\n[1] 0.796\n\n\nWhich says that Padme has a fairly diverse ego network when it comes to Home World.\nWe can, of course, just use sapply to compute everyone’s Home World ego network diversity score:\n\n   round(sapply(V(g)$name, blau, w = g, a = \"homeworld\"), 3)\n\n          R2-D2   CAPTAIN TYPHO         EMPEROR SENATOR ASK AAK    ORN FREE TAA \n          0.444           0.667           0.815           0.776           0.444 \n     MACE WINDU            YODA    KI-ADI-MUNDI     BAIL ORGANA         JAR JAR \n          0.820           0.840           0.750           0.776           0.780 \n        OBI-WAN          ANAKIN           PADME            SOLA           JOBAL \n          0.858           0.809           0.796           0.625           0.625 \n          RUWEE         TAUN WE         LAMA SU       BOBA FETT      JANGO FETT \n          0.625           0.625           0.500           0.667           0.625 \n          C-3PO            OWEN            BERU          CLIEGG     COUNT DOOKU \n          0.444           0.320           0.320           0.320           0.860 \n        SUN RIT          POGGLE     NUTE GUNRAY \n          0.833           0.833           0.833 \n\n\nIn this network Count Dooku stands out as having a very diverse ego network by Home World, while Owen sports a very homogeneous ego network on the same attribute.\nLet’s see a side-by-side comparison:\n\n\n\n\n\n\n\n\n\n\n\nTwo Ego Networks with Nodes Colored by Homeworld\n\n\n\nAnd here are the corresponding \\(IQV\\) scores for everyone in the network:\n\n   round(sapply(V(g)$name, blau, w = g, \n                a = \"homeworld\", norm = TRUE), 3)\n\n          R2-D2   CAPTAIN TYPHO         EMPEROR SENATOR ASK AAK    ORN FREE TAA \n          0.889           1.000           0.951           0.969           0.889 \n     MACE WINDU            YODA    KI-ADI-MUNDI     BAIL ORGANA         JAR JAR \n          0.957           0.960           1.000           0.969           0.936 \n        OBI-WAN          ANAKIN           PADME            SOLA           JOBAL \n          0.953           0.924           0.910           0.937           0.937 \n          RUWEE         TAUN WE         LAMA SU       BOBA FETT      JANGO FETT \n          0.937           0.937           1.000           1.000           0.937 \n          C-3PO            OWEN            BERU          CLIEGG     COUNT DOOKU \n          0.889           0.640           0.640           0.640           0.983 \n        SUN RIT          POGGLE     NUTE GUNRAY \n          1.000           1.000           1.000 \n\n\nAs we noted an ego network with maximum diversity \\(IQV = 1.0\\) is one where every alter is in a different category. Here are two examples:\n\n\n\n\n\n\n\n\n\n\n\nTwo Ego Networks with Maximum Homeworld Diversity.\n\n\n\n\nEgo Network Homophily\nDiversity measures the extent to which ego’s connect to alters who are the same or different from one another. We may also want to get a sense of how homophilous an ego network is, namely, the extent to which ego connects to alters that are the same or different from them.\nFor instance, a person can have an ego network composed of 100% alters who are different from them on a given attribute (maximum “heterophily”) but those alters could be 100% homogeneous—e.g., all come from the same planet—and thus ego will have the minimum Blau diversity score (\\(H = 0\\)).\nTo measure homophily in the ego network we use the EI homophily index. This is given by:\n\\[\nEI = \\frac{E-I}{E+I}\n\\]\nWhere \\(E\\) is the number of “external” ties (alter different from ego on attribute), and \\(I\\) is the number of “internal” ties (alter same as ego on attribute).\nThe \\(EI\\) index ranges from a minimum of \\(EI = -1\\), indicating maximum homophily, to a maximum of \\(EI = 1\\), indicating maximum heterophily. An EI index value of zero indicates no preference for internal over external ties.\nSo let’s write a function that does what we want to calculate EI:\n\n   EI <- function(n, w, a) {\n      x <- subgraph(w, neighbors(w, n))\n      E <- sum(vertex_attr(w, a, n) != vertex_attr(x, a)) #n. external ties\n      I <- sum(vertex_attr(w, a, n) == vertex_attr(x, a))  #n. internal ties\n      res = (E - I)/(E + I) #EI Index\n      return(res)\n      }\n\nLet’s look at the attribute sex:\n\n   V(g)$sex\n\n [1] \"none\"   \"male\"   \"male\"   NA       NA       \"male\"   \"male\"   \"male\"  \n [9] \"male\"   \"male\"   \"male\"   \"male\"   \"female\" NA       NA       NA      \n[17] \"female\" \"male\"   \"male\"   \"male\"   \"none\"   \"male\"   \"female\" \"male\"  \n[25] \"male\"   NA       \"male\"   \"male\"  \n\n\nGetting rid of the NA values:\n\n   V(g)$sex[is.na(V(g)$sex)] <- \"Other\"\n   V(g)$sex\n\n [1] \"none\"   \"male\"   \"male\"   \"Other\"  \"Other\"  \"male\"   \"male\"   \"male\"  \n [9] \"male\"   \"male\"   \"male\"   \"male\"   \"female\" \"Other\"  \"Other\"  \"Other\" \n[17] \"female\" \"male\"   \"male\"   \"male\"   \"none\"   \"male\"   \"female\" \"male\"  \n[25] \"male\"   \"Other\"  \"male\"   \"male\"  \n\n\nAnd calculating the homophily index on gender for everyone:\n\n   EI <- sapply(V(g)$name, EI, w = g, a = \"sex\")\n   round(EI, 2)\n\n          R2-D2   CAPTAIN TYPHO         EMPEROR SENATOR ASK AAK    ORN FREE TAA \n           0.33           -1.00           -0.56            0.71            0.33 \n     MACE WINDU            YODA    KI-ADI-MUNDI     BAIL ORGANA         JAR JAR \n          -0.60           -0.60           -1.00           -0.71           -0.40 \n        OBI-WAN          ANAKIN           PADME            SOLA           JOBAL \n          -0.53           -0.11            0.88            0.00            0.00 \n          RUWEE         TAUN WE         LAMA SU       BOBA FETT      JANGO FETT \n           0.00            1.00            0.00           -0.33           -0.50 \n          C-3PO            OWEN            BERU          CLIEGG     COUNT DOOKU \n           0.67            0.20            0.60            0.20           -0.60 \n        SUN RIT          POGGLE     NUTE GUNRAY \n           1.00           -0.33           -0.33 \n\n\nAs we can see, the Emperor, Mace Windu, Obi-Wan and other such characters have a homophilous “bro” network. Padme, on the other hand, has a heterophilous network with respect to gender.\nLet’s see a side-by-side comparison:\n\n\n\n\n\n\nH = -0.60\n\n\n\n\n\n\n\nH = 0.88\n\n\n\n\n\n\nTwo Ego Networks with Nodes Colored by Gender\n\n\n\nAs we can see, Mace Windu is mostly surrounded by other men (like him) but Padme’s network includes only one other woman, and the rest are composed of people with a different gender presentation than her (or have no discernible gender like the droids).\nLike the clustering coefficient, we can compute a graph level index of homophily on a given attribute. This is given by the average EI index of nodes in the graph for that attribute.\nIn the case of gender in Attack of the Clones:\n\n   EI.gender = mean(EI)\n   round(EI.gender, 2)\n\n[1] -0.06\n\n\nWhich shows a slight preference for same-gender ties in the network.\n\n\nNormalized EI\nSometimes we may want to take into account that the group sizes of different categories of people is unequal in the network. For instance, Star Wars is full characters gendered as men, which means that any homophily index will penalize men as being more homophilous simply because there are more men around to form ties with.\nEverett and Borgatti (2012, 564–65) propose approach to normalizing the EI index to account for unequal group sizes, yielding the \\(NEI\\). So instead of computing EI they suggest calculating:\n\\[\nE^* = \\frac{E}{N-Ns}\n\\]\n\\[\nI^* = \\frac{I}{Ns}\n\\]\n\\[\nNEI = \\frac{E^*-I^*}{E^*+I^*}\n\\]\nWith \\(N_s\\) being the number of “similar” nodes to ego in the whole graph (or external population) and \\(N\\) being the total number of nodes (or persons in the population). As you can see, the \\(NEI\\) weights both the number of external and \\(E\\) the number of internal ties \\(I\\) by their maximum possible values in the network.\nHere’s a function that does that:\n\n   NEI <- function(n, w, a) {\n      N <- vcount(w) \n      Ns <- sum(vertex_attr(w, a) == vertex_attr(w, a, n))\n      x <- subgraph(w, neighbors(w, n))\n      E <- sum(vertex_attr(w, a, n) != vertex_attr(x, a)) \n      I <- sum(vertex_attr(w, a, n) == vertex_attr(x, a)) \n      E <- E/(N - Ns) #normalized E\n      I <- I/Ns #normalized I\n      res = (E - I)/(E + I)\n      return(res)\n   }\n\nLet’s re-check Mace Windu’s and Padme’s EI index using the normalized version:\n\n   round(NEI(\"MACE WINDU\", g, \"sex\"), 2)\n\n[1] -0.44\n\n   round(NEI(\"PADME\", g, \"sex\"), 2)\n\n[1] 0.32\n\n\nAs we can see the NEI scores are less extreme than the unnormalized ones, once we take into account that the majority of characters in the film are men.\nHere are the NEI scores with respect to gender for everyone:\n\n   NEI <- sapply(V(g)$name, NEI, w = g, a = \"sex\")\n   round(NEI, 2)\n\n          R2-D2   CAPTAIN TYPHO         EMPEROR SENATOR ASK AAK    ORN FREE TAA \n          -0.73           -1.00           -0.39            0.24           -0.29 \n     MACE WINDU            YODA    KI-ADI-MUNDI     BAIL ORGANA         JAR JAR \n          -0.44           -0.44           -1.00           -0.59           -0.20 \n        OBI-WAN          ANAKIN           PADME            SOLA           JOBAL \n          -0.36            0.11            0.32           -0.57           -0.57 \n          RUWEE         TAUN WE         LAMA SU       BOBA FETT      JANGO FETT \n          -0.57            1.00            0.21           -0.13           -0.32 \n          C-3PO            OWEN            BERU          CLIEGG     COUNT DOOKU \n          -0.44            0.40           -0.35            0.40           -0.44 \n        SUN RIT          POGGLE     NUTE GUNRAY \n           1.00           -0.13           -0.13 \n\n\nInterestingly, while most people’s scores are attenuated towards zero in the normalized scale, R2-D2’s becomes more extreme going from weakly positive (demonstrating “gender” heterophily) to extreme negative (showing same “gender” preference).\nLet’s see what’s going on:\n\n\n\n\n\nR2-D2’s Ego Network with Nodes Colored by Gender\n\n\n\n\nHere we can see that the reason why R2-D2 ends up being high in homophily in the NEI despite containing a network with just three nodes and only a single “same-gender” (i.e., none) tie, is that he is connected to C3PO who is the only other character (a droid) whose gender is also assigned to “none.”\nAnd here’s the graph’s overall NEI:\n\n   NEI.gender = mean(NEI)\n   round(NEI.gender, 2)\n\n[1] -0.19\n\n\nWhich shows that our previous unnormalized average under-estimated homophily in this network. Instead, there is a moderately strong tendency for characters to co-appear with others of the same gender classification once the imbalance favoring men is accounted for.\n\n\n\nOther Ways of Accounting for Imbalanced Group Sizes in Homophily Metrics\nAs may already be evident, when constructing a homophily measure that takes into account the population (or local network) proportions of various types of alters, there are four pieces of information that we have to take into account:\n\nNumber of alters linked to ego of the same category as ego.\nNumber of alters linked to ego of a different category from ego.\nNumber of alters not linked to ego of the same category as ego.\nNumber of alters not linked to ego of a different category from ego.\n\nWhich yields a classic 2 by 2 table.\nHere’s a function that produces such a table for each ego for a given attribute:\n\n   abcd <- function(n, w, a) {\n      x <- subgraph(w, neighbors(w, n))\n      same <- vertex_attr(w, a, n) == vertex_attr(w, a)\n      same <- same[!(same %in% n)] #deleting ego node from vector\n      tied <- as.vector(V(w)$name) %in% names(neighbors(w, n))\n      tab <- table(tied, same)\n      tab <- tab[2:1, 2:1] #reversing row and column order of table\n      return(tab)\n   }\n\nSo for Count Dooku with respect to gender, this 2 X 2 table looks like:\n\n   abcd(\"COUNT DOOKU\", g, \"sex\")\n\n       same\ntied    TRUE FALSE\n  TRUE     8     2\n  FALSE    9     9\n\n\nSo here we can see that Dooku is linked to eight others of the same gender, but there are nine other men he’s not linked to. In the same way, he has two different-gender ties, but there are nine others of a different gender he’s not tied to.\nAs Everett and Borgatti (2012) note, we can label the cells of the 2 X 2 EI table with the letters from the above list to highlight each piece of information in each cell:\n\n\n       same\ntied    TRUE FALSE\n  TRUE  a    b    \n  FALSE c    d    \n\n\nUsing this nomenclature, the EI index is given by:\n\\[\nEI = \\frac{a-b}{a+b}\n\\]\nIt is clear that the EI only uses information only from the first row (alters tied to ego) and is thus sensitive to group size imbalances because it ignores the other pieces of information (\\(c\\) and \\(d\\)).\nThe NEI, on the other hand, using the same 2 X 2 table cell coding, is given by:\n\\[\nNEI =  \\frac{\\left(\\frac{a}{a+c}\\right)-\\left(\\frac{b}{b+d}\\right)}{\\left(\\frac{a}{a+c}\\right)+\\left(\\frac{b}{b+d}\\right)}\n\\]\nWhich makes clear that NEI uses information from all four cells therefore, it is insensitive to group size imbalances.\nOther measures of homophily could thus be constructed from the entries in the 2 X 2 table, that, like the NEI, are not sensitive to group sizes because they use all four pieces of information.\n\nThe Point Biserial Correlation\nOne such measure Everett and Borgatti (2012, 565) recommend is the point biserial correlation coefficient (\\(r^{pb}\\)), which is given, using the cell labels in the table above, by:\n\\[\nr^{pb} = \\frac{ad-bc}{\\sqrt{(a+c)(b+d)(a+b)(c+d))}}\n\\]\nFrom the formula it is clear that \\(r^{pb}\\) is positive whenever ego is connected to similar alters and disconnected from non-similar ones (the numbers in the main diagonals of the 2 X 2 table, \\(a\\) and \\(d\\), are big). In this way, the \\(r^{pb}\\) works just like a correlation coefficient. A value 1.0 indicates maximum preference for same category alters, and a value closer to -1.0 indicates preference to connect with people different from ego.\nA function that computes \\(r^{pb}\\) from the output of abcd above is:\n\n   pb.corr <- function(x) {\n      a <- x[1,1]\n      b <- x[1,2]\n      c <- x[2,1]\n      d <- x[2,2]\n   num <- (a*d - b*c)\n   den <- sqrt((a+c)*(b+d)*(a+b)*(c+d))\n   return(num/den)\n   }\n\nFor Dooku, \\(r^{pb}\\) is:\n\n   round(pb.corr(abcd(\"COUNT DOOKU\", g, \"sex\")), 2)\n\n[1] 0.29\n\n\nWhich shows a positive tendency for same gender ties, net of the imbalance between men and other-gendered characters in the film.\nTo calculate \\(r^{pb}\\) for the whole network, first we need to create a list containing the corresponding 2 X 2 EI tables for each node for the gender attribute:\n\n   abcd.gender <- lapply(V(g)$name, abcd, w = g, a = \"sex\")\n   names(abcd.gender) <- V(g)$name\n   head(abcd.gender)\n\n$`R2-D2`\n       same\ntied    TRUE FALSE\n  TRUE     1     2\n  FALSE    1    24\n\n$`CAPTAIN TYPHO`\n       same\ntied    TRUE FALSE\n  TRUE     3     0\n  FALSE   14    11\n\n$EMPEROR\n       same\ntied    TRUE FALSE\n  TRUE     7     2\n  FALSE   10     9\n\n$`SENATOR ASK AAK`\n       same\ntied    TRUE FALSE\n  TRUE     1     6\n  FALSE    5    16\n\n$`ORN FREE TAA`\n       same\ntied    TRUE FALSE\n  TRUE     1     2\n  FALSE    5    20\n\n$`MACE WINDU`\n       same\ntied    TRUE FALSE\n  TRUE     8     2\n  FALSE    9     9\n\n\nAnd then sapply the function pb.corr to each element of this list:\n\n   PBC <- sapply(abcd.gender, pb.corr)\n   round(PBC, 2)\n\n          R2-D2   CAPTAIN TYPHO         EMPEROR SENATOR ASK AAK    ORN FREE TAA \n           0.35            0.28            0.24           -0.10            0.10 \n     MACE WINDU            YODA    KI-ADI-MUNDI     BAIL ORGANA         JAR JAR \n           0.29            0.29            0.33            0.30            0.14 \n        OBI-WAN          ANAKIN           PADME            SOLA           JOBAL \n           0.40           -0.14           -0.19            0.28            0.28 \n          RUWEE         TAUN WE         LAMA SU       BOBA FETT      JANGO FETT \n           0.28           -0.14           -0.06            0.04            0.12 \n          C-3PO            OWEN            BERU          CLIEGG     COUNT DOOKU \n           0.19           -0.20            0.14           -0.20            0.29 \n        SUN RIT          POGGLE     NUTE GUNRAY \n          -0.27            0.06            0.06 \n\n\nWhich shows that, after accounting for group sizes, most characters display slight to moderate preferences for same-gender ties, with the exception of Anakin, Padme, Taun We, Owen, Clegg, and Sun Rit.\nIf we wanted to see what proportion of nodes have a same-gender preference we just type:\n\n   round(sum(PBC > 0)/length(PBC), 2)\n\n[1] 0.71\n\n\nWhich suggest that about 70% of the characters display a tendency to appear with same gender others (accounting for differences in the sizes of different gender groups).\nAnd at the network level, the point-biserial correlation homophily measure for gender is:\n\n   round(mean(PBC), 4)\n\n[1] 0.1139\n\n\nWhich also suggests a slight preference for same gender ties in this network.\n\n\nYule’s Q\nThe point biserial correlation is a good measure of homiphily but comes at the expense of the simplicity of the EI index. If you favor a simpler approach that also makes us of all of the information in the 2 X 2 table, Yule’s \\(Q\\) is a good option (Borgatti et al. 2024, 159):\n\\[\nQ = \\frac{ad-bc}{ad+bc}\n\\]\nLike \\(r^{pb}\\), \\(Q\\) works like a correlation coefficient. It is at is maximum value of \\(Q = 1\\) when ego has no connections to different-category others (\\(b=0\\))—maximum homophily—and it is at a minimum of \\(Q = -1\\) when ego has no connections to same-category others (\\(a=0\\))—maximum heterophily.\nA function to calculate \\(Q\\) from the abcd function output is as follows:\n\n   y.Q <- function(x) {\n      a <- x[1,1]\n      b <- x[1,2]\n      c <- x[2,1]\n      d <- x[2,2]\n   num <- a*d - b*c\n   den <- a*d + b*c\n   return(num/den)    \n   }\n\nAnd here are the values for the Attack of the Clones network:\n\n   Q <- sapply(abcd.gender, y.Q)\n   round(Q, 2)\n\n          R2-D2   CAPTAIN TYPHO         EMPEROR SENATOR ASK AAK    ORN FREE TAA \n           0.85            1.00            0.52           -0.30            0.33 \n     MACE WINDU            YODA    KI-ADI-MUNDI     BAIL ORGANA         JAR JAR \n           0.60            0.60            1.00            0.69            0.30 \n        OBI-WAN          ANAKIN           PADME            SOLA           JOBAL \n           0.70           -0.30           -0.56            0.67            0.67 \n          RUWEE         TAUN WE         LAMA SU       BOBA FETT      JANGO FETT \n           0.67           -1.00           -0.23            0.14            0.36 \n          C-3PO            OWEN            BERU          CLIEGG     COUNT DOOKU \n           0.62           -0.48            0.45           -0.48            0.60 \n        SUN RIT          POGGLE     NUTE GUNRAY \n          -1.00            0.16            0.16 \n\n\nLet’s compare four egos with extreme values on \\(Q\\):\n\n\n\n\n\n\nQ = 1\n\n\n\n\n\n\n\nQ = -1\n\n\n\n\n\n\n\n\n\nQ = -1\n\n\n\n\n\n\n\nQ = 1\n\n\n\n\n\n\nTwo Ego Networks with Maximum and Minimum Q Values\n\n\n\nAs we can see, Ki-Adi-Mundi and Captain Typhoo have an ego network that combine maximum homophily and homogeneity with respect to gender (both ego networks are composed of all men)–and therefore both receive the minimum score of \\(Q=-1\\).\nTaun We, on the other hand, has an ego network that combines maximum heterophily with maximum homogeneity of alters with respect to the same attribute (she has an all-men ego network), thus receiving the maximum \\(Q=-1\\). Sun Rit has some heterogeneity in their ego network, but because all alters are of a different gender category from them, they also receive a minimum score of \\(Q=-1\\).\nThe average value of \\(Q\\) for the whole network is:\n\n   round(mean(Q), 2)\n\n[1] 0.24\n\n\nWhich also suggests a slight same-gender preference in the network after accounting for imbalanced group sizes."
  },
  {
    "objectID": "eigen.html",
    "href": "eigen.html",
    "title": "Matrix Eigendecomposition",
    "section": "",
    "text": "In this lecture, we review the idea of an eigendecomposition of a square matrix. Let’s say we have the following matrix \\(\\mathbf{B}\\) of dimensions \\(3 \\times 3\\):\n\n   set.seed(567)\n   B <- matrix(round(runif(9), 2), nrow = 3, ncol = 3)\n   B\n\n     [,1] [,2] [,3]\n[1,] 0.74 0.49 0.07\n[2,] 0.88 0.26 0.51\n[3,] 0.63 0.24 0.59\n\n\nWe use the R function runif (which stands for “random uniform”) to populate the matrix with random numbers in the zero to one interval.\nMost matrices like this can be decomposed into two other matrices \\(\\mathbf{U}\\) and \\(\\mathbf{\\lambda}\\), such that the following matrix multiplication equation is true:\n\\[\n\\mathbf{B} = \\mathbf{U}\\mathbf{\\lambda}\\mathbf{U}^{-1}\n\\]\nBoth \\(\\mathbf{U}\\) and \\(\\mathbf{\\lambda}\\) are of the same dimensions as the original, with \\(\\mathbf{U}\\) having numbers in each cell and \\(\\mathbf{\\lambda}\\) being a matrix with values along the diagonals and zeros everywhere else.\nThe column values of \\(\\mathbf{U}\\) are called the eigenvectors of \\(\\mathbf{B}\\) and the diagonal values of \\(\\mathbf{\\lambda}\\) are called the eigenvalues of \\(\\mathbf{B}\\).\nIn R you can find the values that yield the eigendecomposition of any square matrix (if one exists) using the function eigen.\nSo in our case this would be:\n\n   eig.res <- eigen(B)\n   eig.res\n\neigen() decomposition\n$values\n[1]  1.4256541  0.3195604 -0.1552145\n\n$vectors\n           [,1]       [,2]       [,3]\n[1,] -0.5148954 -0.4669390  0.4838633\n[2,] -0.6388257  0.2808667 -0.8653808\n[3,] -0.5716507  0.8384997 -0.1303551\n\n\nThe function eigen returns a list with two components, one called values are the diagonal values of \\(\\mathbf{\\lambda}\\), and the other one called vectors is the eigenvector matrix \\(\\mathbf{U}\\).\nWe can check that these two elements can help us reconstruct the original matrix as follows:\n\n   lambda <- diag(eig.res$values)\n   U <- eig.res$vectors\n   B.rec <- U %*% lambda %*% solve(U)\n   B.rec\n\n     [,1] [,2] [,3]\n[1,] 0.74 0.49 0.07\n[2,] 0.88 0.26 0.51\n[3,] 0.63 0.24 0.59\n\n\nWhich are indeed the original values of \\(\\mathbf{B}\\)!"
  },
  {
    "objectID": "eigen.html#the-symmetric-matrix-case",
    "href": "eigen.html#the-symmetric-matrix-case",
    "title": "Matrix Eigendecomposition",
    "section": "The Symmetric Matrix Case",
    "text": "The Symmetric Matrix Case\nNow imagine that the matrix \\(\\mathbf{B}\\) is symmetric:\n\n   B[upper.tri(B)] <- B[lower.tri(B)]\n   B\n\n     [,1] [,2] [,3]\n[1,] 0.74 0.88 0.63\n[2,] 0.88 0.26 0.24\n[3,] 0.63 0.24 0.59\n\n\nAnd let’s do the eigendecomposition of this matrix:\n\n   eig.res <- eigen(B)\n   eig.res\n\neigen() decomposition\n$values\n[1]  1.7709909  0.2757779 -0.4567688\n\n$vectors\n           [,1]       [,2]       [,3]\n[1,] -0.7200185 -0.2389854  0.6515054\n[2,] -0.4963684 -0.4787344 -0.7241766\n[3,] -0.4849657  0.8448073 -0.2260728\n\n\nThe interesting thing here is that now the reconstruction equation boils down to:\n\\[\n\\mathbf{B} = \\mathbf{U}\\mathbf{\\lambda}\\mathbf{U}^T\n\\]\nNote that now we just need to post-multiply \\(\\mathbf{U}\\mathbf{\\lambda}\\) by the transpose of \\(\\mathbf{U}\\) rather than the inverse, which is a much simpler matrix operation.\nWe can check that this is true as follows:\n\n   lambda <- diag(eig.res$values)\n   U <- eig.res$vectors\n   B.rec <- U %*% lambda %*% t(U)\n   B.rec\n\n     [,1] [,2] [,3]\n[1,] 0.74 0.88 0.63\n[2,] 0.88 0.26 0.24\n[3,] 0.63 0.24 0.59\n\n\nWhich are indeed the original values of the symmetric version of \\(\\mathbf{B}\\)!\nNow, the idea is that we can perform the asymmetric or symmetric eigendecomposition with any matrix, including a network adjacency matrix or a proximity matrix derived from it.\nIn fact, we have already done a partial version of the matrix eigendecomposition many times before, because the reflective status game is a way to compute the first column (leading eigenvector) of the \\(\\mathbf{U}\\) matrix for any proximity or adjacency matrix you feed into it."
  },
  {
    "objectID": "eigen.html#low-rank-approximation-of-the-original-matrix",
    "href": "eigen.html#low-rank-approximation-of-the-original-matrix",
    "title": "Matrix Eigendecomposition",
    "section": "Low Rank Approximation of the Original Matrix",
    "text": "Low Rank Approximation of the Original Matrix\nThe more important thing is that, once you have the eigendecomposition of a matrix, and the full set of eigenvectors stored in \\(\\mathbf{U}\\), the first few columns of \\(\\mathbf{U}\\), gives us the best low dimensional approximation of the original matrix.\nFor instance, in the above case, the one-dimensional (also called “rank one”) approximation of the original matrix is given by:\n\\[\n\\mathbf{B}_{1-dim} = u_1\\lambda_1u_1^T\n\\]\nWhere \\(u\\) is just the first column (eigenvector) of \\(\\mathbf{U}\\), and \\(\\lambda_1\\) is just the first eigenvalue.\nIn R we can do this approximation as follows:\n\n   u.1 <- as.matrix(U[, 1]) #column vector\n   B.1dim <- u.1 %*% lambda[1, 1] %*% t(u.1)\n   round(B.1dim, 2)\n\n     [,1] [,2] [,3]\n[1,] 0.92 0.63 0.62\n[2,] 0.63 0.44 0.43\n[3,] 0.62 0.43 0.42\n\n\nWhich are not quite the same as the original values of \\(\\mathbf{B}\\), but they are not wildly far off either.\nIf we wanted to be more accurate, however, we would use a two-dimensional approximation (rank two) and thus use more of the information:\n\\[\n\\mathbf{B}_{2-dim} = u_1\\lambda_1u_1^T + u_2\\lambda_2u_2^T\n\\]\nIn R:\n\n   u.2 <- as.matrix(U[, 2])\n   B.2dim <- u.1 %*% lambda[1, 1] %*% t(u.1) + u.2 %*% lambda[2, 2] %*% t(u.2)\n   round(B.2dim, 2)\n\n     [,1] [,2] [,3]\n[1,] 0.93 0.66 0.56\n[2,] 0.66 0.50 0.31\n[3,] 0.56 0.31 0.61\n\n\nWhich are not the same as the original, but are now a bit closer!\nOf course, if we wanted to reconstruct the original matrix, all we have to do is add the product of the eigenvector, eigenvalue, and transpose of the eigenvector across all three dimensions of the matrix:\n\n   u.3 <- as.matrix(U[, 3])\n   B.3dim <- \n      u.1 %*% lambda[1, 1] %*% t(u.1) + \n      u.2 %*% lambda[2, 2] %*% t(u.2) + \n      u.3 %*% lambda[3, 3] %*% t(u.3)\n   round(B.3dim, 2)\n\n     [,1] [,2] [,3]\n[1,] 0.74 0.88 0.63\n[2,] 0.88 0.26 0.24\n[3,] 0.63 0.24 0.59\n\n\nWhich reconstructs the original values.\nSo in general, if you have a symmetric square matrix \\(\\mathbf{B}\\) of dimensions \\(k \\times k\\), and you obtain an eigenvalue decomposition of \\(\\mathbf{B}\\) with eigenvectors stored in the columns of \\(\\mathbf{U}\\) and eigenvalues in \\(\\lambda\\), then the rank-\\(p\\) approximation of the original matrix is given by:\n\\[\n\\mathbf{B}_{rank-p} = \\sum_{m = 1}^p u_{m}\\lambda_mu_m^T\n\\]\nWhen \\(p = k\\), the equation above gives you the original matrix back. When \\(p<k\\) you get the best guess as to what the original was, given \\(p\\) dimensions."
  },
  {
    "objectID": "ergm1.html",
    "href": "ergm1.html",
    "title": "Exponential Random Graphs Models I",
    "section": "",
    "text": "In this previous handout we saw how to do some basic, single-hypothesis testing against plausible null models. This approach can be thought of as conditioning on some set of graph level properties and seeing if something we compute in the graph obtains “net” of these conditioning (e.g., the observed degree distribution).\nConditioning, of course, can be thought of as the analogue of “controlling for” something in a standard linear regression model. This means that it is possible to extend the framework we explored to a setting in which we conditioning on multiple graph properties to see if what we measure obtains “net” of all these properties at the same time, something like a parametric version of the QAP regression approach."
  },
  {
    "objectID": "ergm1.html#exponential-random-graph-models",
    "href": "ergm1.html#exponential-random-graph-models",
    "title": "Exponential Random Graphs Models I",
    "section": "Exponential Random Graph Models",
    "text": "Exponential Random Graph Models\nThe framework of Exponential Random Graph Models (ERGMs) is useful for these purposes. There are many ways to think of ERGMs, one of them is as a network regression in which we are modeling the probability of observing each tie as a function of a bunch of tie level variables. Another way of thinking about it is as specifying a dependence structure that tell us how probability of observing a given tie depends on other things going on in the “neighborhood” of that tie, like whether one of the nodes incident to that edge is also incident to other edges (Pattison and Robins 2002).\nFormally, this means that an Exponential Random Graph Model is a probabilistic (auto)“regression” model in which the network itself, considered as a random variable (\\(\\mathbf{Y}\\)) is the main outcome and the specified patterns of local dependence are the main predictors:\n\\[\n    Pr(\\mathbf{Y} = \\mathbf{y}) = \\frac{exp\\left[\\theta^T S(\\mathbf{y})\\right] h(\\mathbf{y})}{\\kappa(\\theta)}\n\\]\nThis means that the probability of observing a particular realization of the network (\\(Pr(\\mathbf{Y} = \\mathbf{y})\\)) is a function of a given set of local structural configurations defined on the same network (\\(S(\\mathbf{y})\\)) with the parameter \\(\\theta\\) specifying the form of the dependence (e.g., positive or negative) of that local structural factor.1\nNote that if the we take the natural logarithm of the numerator above it leads to the usual additive linear model expression:\n\\[\nlog(exp\\left[\\theta^T S(\\mathbf{y})\\right]) = \\theta_1s_1(y) + \\theta_2s_2(y) + \\theta_3s_3(y) + \\ldots \\theta_ps_p(y)\n\\]\nWhere \\(p\\) is the number of terms included in the model. Each \\(S_p\\) is thus a network statistic specifying counts of specific configurations (e.g., dyads or triads of a certain type, or links beginning at a particular type of node). When \\(\\theta\\) is positive, the existence of a given configuration enhances the probability of observing a link contained in it, while when it is negative \\(\\theta\\) is positive, the same configuration depresses the respective probability.\nThe whole network, therefore, is seen as being generated by the entire ensemble of local structural mechanisms operating in tandem. This means that each ergm also specifies a probability distribution of graphs that are generated by the model’s parameters."
  },
  {
    "objectID": "ergm1.html#fitting-ergms",
    "href": "ergm1.html#fitting-ergms",
    "title": "Exponential Random Graphs Models I",
    "section": "Fitting ERGMs",
    "text": "Fitting ERGMs\nThis is all very abstract, so let’s see how it works in practice. First, we need a package to estimate ERGMs. Thankfully, there is one, called (predictably) ergm (which is dependent on the package network):\n\n   library(networkdata)\n   library(igraph)\n   #install.packages(\"network\")\n   #install.packages(\"ergm\")\n   #install.packages(\"intergraph\")\n   library(ergm)\n   library(intergraph)\n\nLet’s bring back the friendship network from Krackhardt’s 21 high tech managers:\n\n   g <- ht_friends\n   g <- as_undirected(g, mode = \"collapse\")\n   n <- asNetwork(g)\n\nNote that we use the function asNetwork from the intergraph package to coerce the igraph object into something that the package ergm can work with (namely, a network object).\nAnd, now, let’s fit an ergm!\n\n   m1 <- ergm(n ~ edges)\n   summary(m1)\n\nCall:\nergm(formula = n ~ edges)\n\nMaximum Likelihood Results:\n\n      Estimate Std. Error MCMC % z value Pr(>|z|)    \nedges  -0.5057     0.1424      0   -3.55 0.000385 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 291.1  on 210  degrees of freedom\n Residual Deviance: 278.1  on 209  degrees of freedom\n \nAIC: 280.1  BIC: 283.5  (Smaller is better. MC Std. Err. = 0)\n\n\nThe typical structure of an ergm call is modeled after the linear regression models R functions like lm or glm, with dependent variable followed by squiggly symbol followed by an additive list of independent variables of the form y ~ x.\nThe output which we can check out by using the function summary on the resulting object, is also pretty similar to the typical outputs of linear model functions in R. We have a table with the independent variable edges, followed by a coefficient estimate, and some kind of p-value against the null hypothesis that that estimate is zero. Which in this case we can reject pretty handily (\\(Z = - 3.55\\), \\(p < 0.01\\)).\nWhat is edges? Well as we said before, the independent variables in ergm are just counts of stuff that’s going on in the network, to predict…the network!\nIn this case, edges fits the number of edges in the network, which means that it fits anything that is a function of this number, like the density (or the average degree).\nIn fact, the coefficient estimate of the edges parameters without any other network covariates is the density.\nWe can check this as follows, let’s transform the coefficient estimate from the logit to the probability scale:\n\n   m1$coefficients\n\n     edges \n-0.5057495 \n\n   exp(m1$coefficients)/(1 + exp(m1$coefficients))\n\n    edges \n0.3761905 \n\n\nThis probability estimate is the same as:\n\n   edge_density(g)\n\n[1] 0.3761905\n\n\nThat is, the expected probability of an edge existing between two randomly chosen nodes in the network, which is the same as the observed density.\nIf you like regression analogies, fitting an ergm including only the edges parameter and nothing else is equivalent to fitting a linear regression model with only an intercept (which returns the expected value—a.k.a., the mean—of the dependent variable).\nAs we noted, an ergm model is a model of the network that conditions on the things we put on the right-hand side of the equation, in this case, we conditioned on the density (number of edges), which means that this is an Erdos-Renyi random graph model for the observed network, like the ones we simulated using edge-swapping.\nThis also means that we can simulate a bunch of networks that condition on the fit of a given ergm. For this we use the function simulate:\n\n   set.seed(123)\n   sim <- simulate(m1, nsim = 100) #generates list of network objects\n   sim <- lapply(sim, asIgraph) #converts to list of igraph objects\n\n\n\n\n\n\n\nOriginal Friendship Network.\n\n\n\n\n\n\n\nOne ergm simulation.\n\n\n\n\n\n\n\n\n\nAnother ergm simulation.\n\n\n\n\n\n\n\nYet another ergm simulation.\n\n\n\n\n\nNote that the simulated networks are a graph ensemble of networks drawn from a probability distribution in which the expected (mean) density is equivalent to the observed density.\nWe can check by looking at the densities of the separate networks:\n\n   dens <- sapply(sim, edge_density)\n   round(dens, 2)\n\n  [1] 0.35 0.38 0.35 0.38 0.34 0.36 0.33 0.37 0.35 0.32 0.44 0.36 0.40 0.42 0.37\n [16] 0.39 0.39 0.46 0.37 0.36 0.33 0.37 0.40 0.36 0.40 0.42 0.40 0.39 0.36 0.35\n [31] 0.36 0.37 0.38 0.45 0.36 0.41 0.34 0.39 0.36 0.38 0.38 0.35 0.34 0.38 0.36\n [46] 0.38 0.34 0.43 0.38 0.40 0.37 0.43 0.34 0.37 0.34 0.39 0.40 0.40 0.39 0.49\n [61] 0.32 0.38 0.41 0.43 0.40 0.40 0.33 0.32 0.38 0.37 0.35 0.41 0.44 0.42 0.39\n [76] 0.38 0.38 0.40 0.38 0.38 0.36 0.37 0.38 0.35 0.37 0.37 0.40 0.43 0.42 0.43\n [91] 0.31 0.37 0.38 0.35 0.39 0.34 0.34 0.42 0.35 0.35\n\n\nSome of the values are larger and some of them are lower, but they all revolve around the estimate:\n\n\n\n\n\nIf we take larger and larger graph ensembles the distribution of densities would become bell-shaped and the average of the ensemble would converge around the density of the observed network.\nSo a useful way to think about ergms is as models that fix expected graph statistics in a given ensemble. The values of the expectations are provided by the coefficients obtained in the model for that statistic, and the range of possible variation around those expectations are given by the standard error of the estimate."
  },
  {
    "objectID": "ergm1.html#fitting-a-real-ergm",
    "href": "ergm1.html#fitting-a-real-ergm",
    "title": "Exponential Random Graphs Models I",
    "section": "Fitting a Real ERGM",
    "text": "Fitting a Real ERGM\nOf course, nobody uses the ergm package to fit single parameter Erdo-Renyi random graph models. The point is to test multivariate hypotheses of the type: Is the value of a given network effect (e.g., homophily) I observe larger or smaller than we would expect in a network with these other characteristics?\nSo let’s recreate our original analysis of homophily (assortativity) by managerial level from our previous analysis:\n\n   m2 <- ergm(n ~ edges + nodematch(\"level\"))\n   summary(m2)\n\nCall:\nergm(formula = n ~ edges + nodematch(\"level\"))\n\nMaximum Likelihood Results:\n\n                Estimate Std. Error MCMC % z value Pr(>|z|)    \nedges            -1.0361     0.2481      0  -4.175  < 1e-04 ***\nnodematch.level   0.8450     0.3060      0   2.762  0.00575 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 291.1  on 210  degrees of freedom\n Residual Deviance: 270.1  on 208  degrees of freedom\n \nAIC: 274.1  BIC: 280.8  (Smaller is better. MC Std. Err. = 0)\n\n\nHere we fit a nodematch term with the value of the node-characteristic “level.” This term fixes the expected number of dyads that match on this attribute to equal that observed in the network. That is, it adds a “dummy” (change score) indicator if a dyad matches on this attribute.\nThe results tell us that, net of density, there is higher likelihood of observing ties that match on this attribute (\\(\\theta = 0.86, p <0.01\\)).\nIf we wanted to put a percentage, on this estimate, we could exponentiate:\n\n   exp(m2$coefficients[2])\n\nnodematch.level \n       2.328063 \n\n\nWhich tells us that there is more than 2 to 1 odds of observing connected dyads that match on level than not."
  },
  {
    "objectID": "ergm1.html#activity-differences-based-on-a-categorical-factor",
    "href": "ergm1.html#activity-differences-based-on-a-categorical-factor",
    "title": "Exponential Random Graphs Models I",
    "section": "Activity Differences Based on a Categorical Factor",
    "text": "Activity Differences Based on a Categorical Factor\nAs we noted before, it could be that people with some values of the level variable are just more likely to form ties (e.g., the mean degree of nodes varies across the value of levels), so before we conclude that there is a lot of homophily going on, we may want to condition on the expected probability of a node with a given value of the level variable to show up at either end of an edge. We can do that like this:\n\n   m3 <- ergm(n ~ edges + nodefactor(\"level\", base = 2) + nodematch(\"level\"))\n   summary(m3)\n\nCall:\nergm(formula = n ~ edges + nodefactor(\"level\", base = 2) + nodematch(\"level\"))\n\nMaximum Likelihood Results:\n\n                   Estimate Std. Error MCMC % z value Pr(>|z|)  \nedges               -0.7035     0.4879      0  -1.442   0.1493  \nnodefactor.level.1  -0.9055     0.6891      0  -1.314   0.1888  \nnodefactor.level.3  -0.1587     0.4039      0  -0.393   0.6944  \nnodematch.level      0.8147     0.4902      0   1.662   0.0965 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 291.1  on 210  degrees of freedom\n Residual Deviance: 268.1  on 206  degrees of freedom\n \nAIC: 276.1  BIC: 289.5  (Smaller is better. MC Std. Err. = 0)\n\n\nThe ergm term nodefactor uses dummy coding for the levels of a given categorical variable and fits \\(k-1\\) terms for mean differences in degree for each category with a category serving as the (omitted) comparison. In this case, we indicate that using the base argument, which set the second level category as the reference.\nAs we can see, after adjusting for mean differences in degree across levels the assortativity estimate for the nodematch term is no longer significant at conventional levels \\(p = 0.10\\), indicating that heterogeneity in connectivity by nodes at different levels partially accounts for this effect (although model fit estimate prefer the more parsimonious model without nodefactor level effects)."
  },
  {
    "objectID": "ergm1.html#considering-multiple-homophily-effects",
    "href": "ergm1.html#considering-multiple-homophily-effects",
    "title": "Exponential Random Graphs Models I",
    "section": "Considering Multiple Homophily Effects",
    "text": "Considering Multiple Homophily Effects\nOf course, what makes ergms such a flexible modeling system is that we can fit multiple homophily terms at once. In fact, the list of “covariates” can be as long as you want, with the caveat that the more stuff you condition on, the more restricted (and perhaps nonsensical) is the space of graphs that you are presuming your actual data came from.\nRegardless, here’s a model with multiple homophily effects, one for levels and the other for “Department”:\n\n   m4 <- ergm(n ~ edges +  nodematch(\"level\") + nodematch(\"dept\"))\n   summary(m4)\n\nCall:\nergm(formula = n ~ edges + nodematch(\"level\") + nodematch(\"dept\"))\n\nMaximum Likelihood Results:\n\n                Estimate Std. Error MCMC % z value Pr(>|z|)    \nedges            -1.2682     0.2679      0  -4.734  < 1e-04 ***\nnodematch.level   0.8217     0.3127      0   2.628  0.00859 ** \nnodematch.dept    1.0345     0.3458      0   2.991  0.00278 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 291.1  on 210  degrees of freedom\n Residual Deviance: 261.0  on 207  degrees of freedom\n \nAIC: 267  BIC: 277.1  (Smaller is better. MC Std. Err. = 0)\n\n\nWhich reveals strong homophily effects for each node-level factor “net” of the other one, suggesting that our graph is likely to have come from a world in which nodes form ties selectively based on both these criteria."
  },
  {
    "objectID": "ergm1.html#homophily-on-a-continuous-attribute",
    "href": "ergm1.html#homophily-on-a-continuous-attribute",
    "title": "Exponential Random Graphs Models I",
    "section": "Homophily on a Continuous Attribute",
    "text": "Homophily on a Continuous Attribute\nSome node attributes like “age” and “tenure” in the Krackhardt’s High-Tech Managers data are continuous. Obviously, we couldn’t condition on matching on each level of a continuous attribute.\nIn the ergm framework homophily on a continuous attribute is handled using the absdiff term, which computes the absolute difference in the value of the attribute between the two nodes incident to the edge:\n\n   m5 <- ergm(n ~ edges \n              +  nodematch(\"level\") + nodematch(\"dept\")\n              + absdiff(\"age\")\n              )\n   summary(m5)\n\nCall:\nergm(formula = n ~ edges + nodematch(\"level\") + nodematch(\"dept\") + \n    absdiff(\"age\"))\n\nMaximum Likelihood Results:\n\n                Estimate Std. Error MCMC % z value Pr(>|z|)   \nedges           -0.89316    0.31550      0  -2.831  0.00464 **\nnodematch.level  0.85287    0.31692      0   2.691  0.00712 **\nnodematch.dept   1.06643    0.35049      0   3.043  0.00234 **\nabsdiff.age     -0.03972    0.01850      0  -2.147  0.03180 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 291.1  on 210  degrees of freedom\n Residual Deviance: 256.2  on 206  degrees of freedom\n \nAIC: 264.2  BIC: 277.6  (Smaller is better. MC Std. Err. = 0)\n\n\nWhich tells us that there is indeed age homophily in this network \\(\\theta_{agediff} = -0.04, p < 0.05\\). Note that here a negative effect indicates a smaller age difference in the tie, which indicates homophily. A positive effect in the absdiff term would indicate heterophily on the continuous attribute."
  },
  {
    "objectID": "ergm1.html#activity-differences-based-on-a-continuous-factor",
    "href": "ergm1.html#activity-differences-based-on-a-continuous-factor",
    "title": "Exponential Random Graphs Models I",
    "section": "Activity Differences Based on a Continuous Factor",
    "text": "Activity Differences Based on a Continuous Factor\nOf course, just like before, we may want to check that we are not mistaking homophily, with the tendency of people in certain age groups to form more ties in general (e.g., an age/degree correlation in the network).\nTo fit the analogue of nodefactor for continuous attributes we use nodecov, which conditions on the sum of the values a given attribute for the two nodes incident to the edge:\n\n   m6 <- ergm(n ~ edges \n              +  nodecov(\"age\") + nodematch(\"level\") \n              + nodematch(\"dept\") + absdiff(\"age\")\n              )\n   summary(m6)\n\nCall:\nergm(formula = n ~ edges + nodecov(\"age\") + nodematch(\"level\") + \n    nodematch(\"dept\") + absdiff(\"age\"))\n\nMaximum Likelihood Results:\n\n                Estimate Std. Error MCMC % z value Pr(>|z|)   \nedges           -0.10503    1.11891      0  -0.094  0.92521   \nnodecov.age     -0.01104    0.01511      0  -0.731  0.46496   \nnodematch.level  0.81527    0.32107      0   2.539  0.01111 * \nnodematch.dept   1.07818    0.35134      0   3.069  0.00215 **\nabsdiff.age     -0.02951    0.02312      0  -1.276  0.20181   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 291.1  on 210  degrees of freedom\n Residual Deviance: 255.6  on 205  degrees of freedom\n \nAIC: 265.6  BIC: 282.4  (Smaller is better. MC Std. Err. = 0)\n\n\nLike before, after adjusting for the age/degree correlation, there does not seem to be a statistically significant age homophily effect (\\(p = 0.20\\)).\nFinally, here’s a model including homophily effects for all four node attributes (two categorical and two continuous) in the Krackhardt High-Tech Managers data:\n\n   m7 <- ergm(n ~ edges \n              +  nodecov(\"age\") + nodecov(\"tenure\")\n              + nodematch(\"level\") + nodematch(\"dept\")  \n              + absdiff(\"age\") + absdiff(\"tenure\")\n              )\n   summary(m7)\n\nCall:\nergm(formula = n ~ edges + nodecov(\"age\") + nodecov(\"tenure\") + \n    nodematch(\"level\") + nodematch(\"dept\") + absdiff(\"age\") + \n    absdiff(\"tenure\"))\n\nMaximum Likelihood Results:\n\n                Estimate Std. Error MCMC % z value Pr(>|z|)    \nedges            0.39296    1.21536      0   0.323 0.746444    \nnodecov.age     -0.04671    0.01858      0  -2.514 0.011954 *  \nnodecov.tenure   0.07830    0.02116      0   3.701 0.000215 ***\nnodematch.level  1.25596    0.35743      0   3.514 0.000442 ***\nnodematch.dept   1.63590    0.41072      0   3.983  < 1e-04 ***\nabsdiff.age     -0.03375    0.02681      0  -1.259 0.208042    \nabsdiff.tenure   0.01060    0.02936      0   0.361 0.718209    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 291.1  on 210  degrees of freedom\n Residual Deviance: 236.4  on 203  degrees of freedom\n \nAIC: 250.4  BIC: 273.9  (Smaller is better. MC Std. Err. = 0)\n\n\nNote that this model also conditions on the degree correlation between tenure at the firm and degree. As we can see, the categorical homophily effects due to level and department remain strong, however, there is no evidence for homophily effects based on age and tenure.\nInstead, the continuous attributes are correlated with degree, with ties featuring younger people being more likely to be observed (negative age effect on the nodecov term) and ties featuring people with longer tenure (positive tenure effect on the nodecov term) also being more likely to be observed. It makes sense to condition on both age and tenure at the same time, since these are strongly correlated at the individual level (\\(r=\\) 0.48)"
  },
  {
    "objectID": "ergm1.html#heterogeneous-homophily-effects",
    "href": "ergm1.html#heterogeneous-homophily-effects",
    "title": "Exponential Random Graphs Models I",
    "section": "Heterogeneous Homophily Effects",
    "text": "Heterogeneous Homophily Effects\nNote that nodematch fits a single term for the homophily on a categorical attribute. For an attribute with just two categories this may not be a problem, since we are just comparing “same” connected dyads to “different” connected dyads.\nBut consider an attribute like “department” in these data, which has four levels. Fitting a single term means we are constraining the homophily effect to be the same regardless of the levels at which the nodes match. So the effect for 4-4 nodes is the same as the effect for 3-3 nodes, which is the same as the effect for 2-2 nodes and so forth.\nWe can of course relax this homogeneity restriction and specify separate nodematch effects by levels of the categorical attribute:\n\n   m8 <- ergm(n ~ edges \n              + nodecov(\"age\") + nodecov(\"tenure\")\n              + nodematch(\"level\", levels = c(2, 3)) \n              + nodematch(\"dept\", diff = TRUE, levels = c(2, 3, 4))  \n              + absdiff(\"age\") + absdiff(\"tenure\") \n              )\n   summary(m8)\n\nCall:\nergm(formula = n ~ edges + nodecov(\"age\") + nodecov(\"tenure\") + \n    nodematch(\"level\", levels = c(2, 3)) + nodematch(\"dept\", \n    diff = TRUE, levels = c(2, 3, 4)) + absdiff(\"age\") + absdiff(\"tenure\"))\n\nMaximum Likelihood Results:\n\n                 Estimate Std. Error MCMC % z value Pr(>|z|)    \nedges             1.03210    1.26058      0   0.819 0.412928    \nnodecov.age      -0.05374    0.02055      0  -2.616 0.008909 ** \nnodecov.tenure    0.07958    0.02327      0   3.420 0.000627 ***\nnodematch.level   1.15303    0.34606      0   3.332 0.000863 ***\nnodematch.dept.1  1.26854    0.74658      0   1.699 0.089294 .  \nnodematch.dept.2  1.55826    0.56209      0   2.772 0.005566 ** \nnodematch.dept.3 -0.34708    1.33313      0  -0.260 0.794592    \nabsdiff.age      -0.03089    0.02671      0  -1.157 0.247476    \nabsdiff.tenure    0.01397    0.02941      0   0.475 0.634642    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 291.1  on 210  degrees of freedom\n Residual Deviance: 242.7  on 201  degrees of freedom\n \nAIC: 260.7  BIC: 290.8  (Smaller is better. MC Std. Err. = 0)\n\n\nHere we are using connected dyads that match at department = 1 in the data (there’s one node that is coded at zero on this variable, so there cannot be dyads that match at this value) as the reference by specifying the levels argument. We specify we want heterogeneous homophily effects by department by setting the diff argument inside the nodematch term to TRUE (it is FALSE by default).\nAs we can see, we can observe heterogeneous homophily effects on the department attribute in these data. Connected dyads matching on department at levels 2 and 3 are responsible for most of the positive homophily effects. For dyads that match at level 4 of department, there is no homophily (\\(\\theta_{dept4-4} = - 0.35, p = 0.80\\))."
  },
  {
    "objectID": "ergm1.html#looking-at-combinations-of-ties-featuring-an-attribute",
    "href": "ergm1.html#looking-at-combinations-of-ties-featuring-an-attribute",
    "title": "Exponential Random Graphs Models I",
    "section": "Looking at combinations of ties featuring an attribute",
    "text": "Looking at combinations of ties featuring an attribute\nIn the preceding we looked at terms that condition on homophily or the probability that a given edge in a network drawn at random would feature nodes that match on a given attribute. Sometimes, we may be interested in looking at terms that condition on both matching and not-matching. For this task, we use the ergm term nodemix.\nLet’s look at an example. But first let’s switch to a different data set, this time the friendship nomination network from Lazega’s Law Firm data:\n\n   g <- law_friends\n   g <- as_undirected(g, mode = \"collapse\")\n   n <- asNetwork(g)\n\nAnd now let’s fit an ergm that conditions on homophily and activity effects for four individual level covariates: gender, partner status, age, seniority.\n\n   m9 <- ergm(n ~ edges + \n                  + nodecov(\"age\") + nodecov(\"seniority\")\n                  + nodefactor(\"status\") + nodefactor(\"gender\")\n                  + nodematch(\"status\") + nodematch(\"gender\")\n                  + absdiff(\"age\") + absdiff(\"seniority\")\n              )\n   summary(m9)\n\nCall:\nergm(formula = n ~ edges + +nodecov(\"age\") + nodecov(\"seniority\") + \n    nodefactor(\"status\") + nodefactor(\"gender\") + nodematch(\"status\") + \n    nodematch(\"gender\") + absdiff(\"age\") + absdiff(\"seniority\"))\n\nMaximum Likelihood Results:\n\n                     Estimate Std. Error MCMC % z value Pr(>|z|)    \nedges               -0.851155   0.524376      0  -1.623 0.104551    \nnodecov.age         -0.008933   0.007078      0  -1.262 0.206900    \nnodecov.seniority    0.017056   0.008958      0   1.904 0.056905 .  \nnodefactor.status.2 -0.406772   0.118871      0  -3.422 0.000622 ***\nnodefactor.gender.2  0.193677   0.115764      0   1.673 0.094320 .  \nnodematch.status     0.799008   0.147556      0   5.415  < 1e-04 ***\nnodematch.gender     0.389504   0.149017      0   2.614 0.008953 ** \nabsdiff.age         -0.030883   0.010068      0  -3.068 0.002158 ** \nabsdiff.seniority   -0.067122   0.012294      0  -5.460  < 1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 3445  on 2485  degrees of freedom\n Residual Deviance: 1928  on 2476  degrees of freedom\n \nAIC: 1946  BIC: 1999  (Smaller is better. MC Std. Err. = 0)\n\n\nWe can see that there are not statistically significant activity effects based on attribute, except for the one premised on status in the firm (although both the positive seniority and gender effects are significant at the \\(p <0.10\\) level. We can also observe predicted homophily effects based on all four characteristics. This looks like a well-fitting useful baseline model.\nLet’s now add additional activity and homophily effects law_school. This variable is coded into three categories: (1) Harvard/Yale, (2) Uconn, (3) “Other”:\n\n   m10 <- ergm(n ~ edges +\n                  + nodecov(\"age\") + nodecov(\"seniority\")\n                  + nodefactor(\"status\") + nodefactor(\"gender\")\n                  + nodefactor(\"law_school\", base = 3)\n                  + nodematch(\"status\") + nodematch(\"gender\")\n                  + absdiff(\"age\") + absdiff(\"seniority\")\n                  + nodematch(\"law_school\")\n              )\n   summary(m10)\n\nCall:\nergm(formula = n ~ edges + +nodecov(\"age\") + nodecov(\"seniority\") + \n    nodefactor(\"status\") + nodefactor(\"gender\") + nodefactor(\"law_school\", \n    base = 3) + nodematch(\"status\") + nodematch(\"gender\") + absdiff(\"age\") + \n    absdiff(\"seniority\") + nodematch(\"law_school\"))\n\nMaximum Likelihood Results:\n\n                         Estimate Std. Error MCMC % z value Pr(>|z|)    \nedges                   -1.275735   0.550528      0  -2.317 0.020488 *  \nnodecov.age             -0.006860   0.007370      0  -0.931 0.351919    \nnodecov.seniority        0.015054   0.009091      0   1.656 0.097734 .  \nnodefactor.status.2     -0.429909   0.120257      0  -3.575 0.000350 ***\nnodefactor.gender.2      0.134855   0.118160      0   1.141 0.253747    \nnodefactor.law_school.1  0.091510   0.128525      0   0.712 0.476464    \nnodefactor.law_school.2  0.319710   0.094629      0   3.379 0.000729 ***\nnodematch.status         0.802762   0.148891      0   5.392  < 1e-04 ***\nnodematch.gender         0.383468   0.149576      0   2.564 0.010356 *  \nabsdiff.age             -0.030985   0.010123      0  -3.061 0.002207 ** \nabsdiff.seniority       -0.066486   0.012437      0  -5.346  < 1e-04 ***\nnodematch.law_school     0.109033   0.122245      0   0.892 0.372436    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 3445  on 2485  degrees of freedom\n Residual Deviance: 1915  on 2473  degrees of freedom\n \nAIC: 1939  BIC: 2009  (Smaller is better. MC Std. Err. = 0)\n\n\nThis model shows that Uconn graduates are more popular in the friendship network (they form more ties), compared to graduates from “other” schools, but that there doesn’t seem to be homophily based on this factor.\nAs we noted, sometimes we may suspect that homophily happens via a combination of attributes (e.g., gender and status). We can specify such effects using the nodemix term as follows:\n\n   m11 <- ergm(n ~ edges +\n                  + nodecov(\"age\") + nodecov(\"seniority\")\n                  + nodefactor(\"status\") + nodefactor(\"gender\")\n                  + nodefactor(\"law_school\", base = 3)\n                  + absdiff(\"age\") + absdiff(\"seniority\")\n                  + nodematch(\"law_school\")\n                  + nodemix(c(\"gender\", \"status\"), levels = TRUE, levels2 = c(1:6))\n              )\n   summary(m11)\n\nCall:\nergm(formula = n ~ edges + +nodecov(\"age\") + nodecov(\"seniority\") + \n    nodefactor(\"status\") + nodefactor(\"gender\") + nodefactor(\"law_school\", \n    base = 3) + absdiff(\"age\") + absdiff(\"seniority\") + nodematch(\"law_school\") + \n    nodemix(c(\"gender\", \"status\"), levels = TRUE, levels2 = c(1:6)))\n\nMaximum Likelihood Results:\n\n                           Estimate Std. Error MCMC % z value Pr(>|z|)    \nedges                     -2.225120   0.715918      0  -3.108 0.001883 ** \nnodecov.age               -0.006030   0.007508      0  -0.803 0.421886    \nnodecov.seniority          0.017111   0.009211      0   1.858 0.063210 .  \nnodefactor.status.2        0.132407   0.239020      0   0.554 0.579607    \nnodefactor.gender.2        0.554151   0.238303      0   2.325 0.020050 *  \nnodefactor.law_school.1    0.080613   0.128892      0   0.625 0.531689    \nnodefactor.law_school.2    0.326575   0.095087      0   3.434 0.000594 ***\nabsdiff.age               -0.029508   0.010222      0  -2.887 0.003891 ** \nabsdiff.seniority         -0.069504   0.012543      0  -5.541  < 1e-04 ***\nnodematch.law_school       0.110916   0.122475      0   0.906 0.365138    \nmix.gender.status.1.1.1.1  1.940435   0.472391      0   4.108  < 1e-04 ***\nmix.gender.status.1.1.1.2  0.645250   0.358610      0   1.799 0.071970 .  \nmix.gender.status.1.2.1.2  1.134592   0.380036      0   2.985 0.002831 ** \nmix.gender.status.1.1.2.1  1.462671   0.443169      0   3.300 0.000965 ***\nmix.gender.status.1.2.2.1 -0.869626   0.622416      0  -1.397 0.162360    \nmix.gender.status.2.1.2.1  2.127482   1.314879      0   1.618 0.105661    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 3445  on 2485  degrees of freedom\n Residual Deviance: 1908  on 2469  degrees of freedom\n \nAIC: 1940  BIC: 2033  (Smaller is better. MC Std. Err. = 0)\n\n\nHere we use low status homogeneous dyads (regardless of gender mix) as the comparison category. The results show that compared to those dyads, high-status homogeneous, all men dyads are more prevalent, as are gender-mixed dyads where the man is higher status. We also find a preponderance of all-men, status heterogeneous dyads."
  },
  {
    "objectID": "ergm1.html#checking-for-model-fit",
    "href": "ergm1.html#checking-for-model-fit",
    "title": "Exponential Random Graphs Models I",
    "section": "Checking for Model Fit",
    "text": "Checking for Model Fit\nIn ergms we can always check for model fit using standard statistics they share with general linear models, like the model deviance, BIC, AIC and so forth. These are particularly useful when trying to decide between nested models.\nHowever, we can also use the fact that each ergm specifies a probability distribution across a graph ensemble consistent with the estimated parameters to see whether the specification we decided on actually produces graphs with global (network level) characteristics that fall within a plausible range.\nThat is, even though network level factors are not directly specified in the model, a good model should be able to reproduce them via the local tie generating processes we specified.\nWhen an ergm specification does not do this, and instead produces graphs within a narrow or weird range of values—e.g., for basic network statistics like the degree distribution, the geodesic distance distribution, or basic counts of key motifs like triangles and number of shared partners (a technical issue known as “degeneracy” in the ergm framework)—then that is a signal we need to change something in our specification.\nIn the ergm package we can check for goodness of fit based on the ability to match global network characteristics using the gof function. The function takes a ergm model object as input along with an optional argument called GOF specifying what simulated global network property across the implied graph ensemble we want to compare to our observed values. The main ones include the degree distribution, the distribution of edge-wise shared partners (common neighbors of connected dyads), and the geodesic distance distribution (number of dyads separated by shortest parths of a given length).\nLet’s use m8 above which was the attribute-driven model with heterogeneous homophily effects for department we fit to the Krackhardt High Tech Managers data. Let’s say, we wanted to see whether our model was successful in reproducing the distribution of activity (degree) across nodes. In that case, we would type:\n\n   set.seed(123)\n   gof.m8 <- gof(m8, GOF = ~ degree)\n\nHere we asked gof to create a graph ensemble of one hundred graphs (the default) and look at the degree distribution in each, recording how many times a node with a given degree is observed in each graph.\nTo see the result we can check out the following object:\n\n   gof.m8$pval.deg\n\n         obs min mean max MC p-value\ndegree0    0   0 0.08   1       1.00\ndegree1    0   0 0.36   3       1.00\ndegree2    1   0 0.45   2       0.78\ndegree3    1   0 1.03   4       1.00\ndegree4    1   0 1.32   5       1.00\ndegree5    3   0 1.93   5       0.68\ndegree6    4   0 2.37   8       0.44\ndegree7    2   0 2.68   7       1.00\ndegree8    2   0 2.71   8       1.00\ndegree9    2   0 2.66   8       1.00\ndegree10   3   0 1.97   6       0.56\ndegree11   0   0 1.60   5       0.46\ndegree12   0   0 1.05   4       0.64\ndegree13   0   0 0.48   3       1.00\ndegree14   1   0 0.18   1       0.36\ndegree15   0   0 0.09   2       1.00\ndegree16   0   0 0.03   1       1.00\ndegree17   0   0 0.01   1       1.00\ndegree18   1   0 0.00   0       0.00\ndegree19   0   0 0.00   0       1.00\ndegree20   0   0 0.00   0       1.00\n\n\nThe way to read the table is as follows: Under the column “obs” is the values of degree in the data (observed), then the other columns contain the range (min and max) and the mean of the each value of degree in the graph ensemble implied by the model.\nThe p-value is a test that the observed value is significantly different from the mean of the graph ensemble. Note that in a good fitting model, the mean of the ensemble implied by the model so we want \\(p > 0.05\\) for each row in the table as this indicates a good fitting model. We can see that this is indeed the case for every value except the last outlier one (\\(k = 18\\)), the simulations show this never happening (max = 0), so it is outside of the range of the observed value of one (\\(p = 0.00\\)).\nOverall, however, the model does a good job of reproducing a distribution of degrees in the ensemble consistent with the observed data. If we wanted to see it in a plot, we would just type:\n\n   plot(gof.m8)\n\n\n\n\n\n\n\nIn this box plot, the ensemble mean is the blue diamond, the gray box is the interquartile (25th to 75th percentile) range, the black horizontal line in the gray box is the ensemble median, and the black connected line plot is the observed values.\nWhat we would like to see in a well-fitting model is a black line that goes through the gray box and is not outside them, which is generally the case here for most of the observed range except for the very large values."
  },
  {
    "objectID": "ergm1.html#conditioning-on-degree-and-shared-partners",
    "href": "ergm1.html#conditioning-on-degree-and-shared-partners",
    "title": "Exponential Random Graphs Models I",
    "section": "Conditioning on Degree and Shared Partners",
    "text": "Conditioning on Degree and Shared Partners\nNote that all the models estimated earlier only adjust for the edges parameter as a structural effect. In this sense, they are using the Erdos-Renyi model as the structural baseline.\nAs we saw with the edge swapping models, it is probably better to use more sophisticated null models than the Erdos-Renyi one. For instance, models that condition on degree heterogeneity in some way, or that also account for transitivity (e.g., triangle) effects (e.g., the tendency of dyads sharing common partners to be connected).\nIn an ergm context, we could condition on the degree distribution by adding a separate parameter for each observed degree, or adding a triangle term. These models are computationally intensive, and often fail to converge (for obscure technical reasons).\nA better approach is to fit both degree heterogeneity and transitivity effect using a family of geometrically weighted decay parameter terms. The idea is to fit the distribution of a given quantity (e.g., degree, number of shared partners per dyad) by fitting a curve with a term that decays as either the degree or the number of shared partners increases, thus giving more weight to lower degrees and the first or second shared partner as compared to the third, fourth, fifth, and so forth.\nThese gw terms are less computationally intensive and use a single parameter to fit both the degree distribution—using the gwdegree term—and the (dyad or edge)wise shared partner distribution—using the gwdsp and gwesp terms (respectively).\nLet’s see how the gw ergm terms work, going back to Krackhardt’s High Tech Managers data. Here’s a model looking at activity and homophily effects conditioning on the degree distribution:\n\n   g <- ht_friends\n   g <- as_undirected(g, mode = \"collapse\")\n   n <- asNetwork(g)\n   m12 <- ergm(n ~ edges \n              + nodecov(\"age\") + nodecov(\"tenure\")\n              + nodematch(\"level\", levels = c(2, 3)) \n              + nodematch(\"dept\", diff = TRUE, levels = c(2, 3, 4))  \n              + absdiff(\"age\") + absdiff(\"tenure\") \n              + gwdegree(decay = 0.25, fixed = TRUE)\n              )\n   summary(m12)\n\nCall:\nergm(formula = n ~ edges + nodecov(\"age\") + nodecov(\"tenure\") + \n    nodematch(\"level\", levels = c(2, 3)) + nodematch(\"dept\", \n    diff = TRUE, levels = c(2, 3, 4)) + absdiff(\"age\") + absdiff(\"tenure\") + \n    gwdegree(decay = 0.25, fixed = TRUE))\n\nMonte Carlo Maximum Likelihood Results:\n\n                 Estimate Std. Error MCMC % z value Pr(>|z|)   \nedges             1.56101    1.53828      0   1.015  0.31021   \nnodecov.age      -0.06362    0.02524      0  -2.520  0.01173 * \nnodecov.tenure    0.08518    0.02593      0   3.285  0.00102 **\nnodematch.level   1.15615    0.36159      0   3.197  0.00139 **\nnodematch.dept.1  1.39805    0.78609      0   1.778  0.07532 . \nnodematch.dept.2  1.62179    0.58450      0   2.775  0.00553 **\nnodematch.dept.3 -0.50239    1.36220      0  -0.369  0.71227   \nabsdiff.age      -0.03298    0.02463      0  -1.339  0.18058   \nabsdiff.tenure    0.01995    0.03107      0   0.642  0.52079   \ngwdeg.fixed.0.25  7.55080   15.28561      0   0.494  0.62132   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 291.1  on 210  degrees of freedom\n Residual Deviance: 242.1  on 200  degrees of freedom\n \nAIC: 262.1  BIC: 295.6  (Smaller is better. MC Std. Err. = 0.08229)\n\n\nNote that our homophily estimates are still significant, even after accounting for heterogeneity in the degree distribution, the estimate of corresponding to the gwdegree term is not significant, suggesting that our activity and homophily attributes do a good job of fitting the empirical degree distribution (which we knew form our goodness of fit tests).\nNow, here’s a model that conditions on the tendency for dyads who share common partners to themselves be connected:\n\n   g <- ht_friends\n   g <- as_undirected(g, mode = \"collapse\")\n   n <- asNetwork(g)\n   m12 <- ergm(n ~ edges \n              + nodecov(\"age\") + nodecov(\"tenure\")\n              + nodematch(\"level\", levels = c(2, 3)) \n              + nodematch(\"dept\", diff = TRUE, levels = c(2, 3, 4))  \n              + absdiff(\"age\") + absdiff(\"tenure\") \n              + gwdsp(decay = 0.25, fixed = TRUE)\n              )\n   summary(m12)\n\nCall:\nergm(formula = n ~ edges + nodecov(\"age\") + nodecov(\"tenure\") + \n    nodematch(\"level\", levels = c(2, 3)) + nodematch(\"dept\", \n    diff = TRUE, levels = c(2, 3, 4)) + absdiff(\"age\") + absdiff(\"tenure\") + \n    gwdsp(decay = 0.25, fixed = TRUE))\n\nMonte Carlo Maximum Likelihood Results:\n\n                 Estimate Std. Error MCMC % z value Pr(>|z|)   \nedges             1.16726    1.40838      0   0.829  0.40722   \nnodecov.age      -0.06852    0.02288      0  -2.995  0.00274 **\nnodecov.tenure    0.08571    0.02619      0   3.273  0.00107 **\nnodematch.level   1.26334    0.40822      0   3.095  0.00197 **\nnodematch.dept.1  1.40475    0.76587      0   1.834  0.06663 . \nnodematch.dept.2  1.53716    0.58401      0   2.632  0.00849 **\nnodematch.dept.3 -0.67235    1.36362      0  -0.493  0.62197   \nabsdiff.age      -0.03087    0.02743      0  -1.125  0.26039   \nabsdiff.tenure    0.01625    0.03146      0   0.516  0.60552   \ngwdsp.fixed.0.25  0.55992    0.23784      0   2.354  0.01856 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 291.1  on 210  degrees of freedom\n Residual Deviance: 235.4  on 200  degrees of freedom\n \nAIC: 255.4  BIC: 288.9  (Smaller is better. MC Std. Err. = 0.1518)\n\n\nIn this model, we find a statistically significant \\(\\theta\\) estimate for the geometrically weighted dyadwise shared partners effect (with decay parameter \\(\\gamma = 0.25\\)), suggesting that this network is likely drawn from a distribution of graphs where pairs of nodes mutually connected to a third are also more likely to be connected themselves (\\(p < 0.05\\)).\nHowever, even after adjusting for this structural tendency our homophily effects are still statistically significant, suggesting that they are not the spurious byproduct of transitivity in friendship ties.\nFinally, we can also adjust for the tendency of connected dyads to share multiple friends (e.g., connected dyads being embedded on one or more triangles), also known as the Simmelian Tie effect, also known as the geometrically weighted edgewise shared partners effect:\n\n   g <- ht_friends\n   g <- as_undirected(g, mode = \"collapse\")\n   n <- asNetwork(g)\n   m13 <- ergm(n ~ edges \n              + nodecov(\"age\") + nodecov(\"tenure\")\n              + nodematch(\"level\", levels = c(2, 3)) \n              + nodematch(\"dept\", diff = TRUE, levels = c(2, 3, 4))  \n              + absdiff(\"age\") + absdiff(\"tenure\") \n              + gwdsp(decay = 0.25, fixed = TRUE)\n              + gwesp(decay = 0.25, fixed = TRUE)\n              )\n   summary(m13)\n\nCall:\nergm(formula = n ~ edges + nodecov(\"age\") + nodecov(\"tenure\") + \n    nodematch(\"level\", levels = c(2, 3)) + nodematch(\"dept\", \n    diff = TRUE, levels = c(2, 3, 4)) + absdiff(\"age\") + absdiff(\"tenure\") + \n    gwdsp(decay = 0.25, fixed = TRUE) + gwesp(decay = 0.25, fixed = TRUE))\n\nMonte Carlo Maximum Likelihood Results:\n\n                 Estimate Std. Error MCMC % z value Pr(>|z|)   \nedges            -0.75620    1.99545      0  -0.379  0.70471   \nnodecov.age      -0.06005    0.02130      0  -2.820  0.00480 **\nnodecov.tenure    0.07811    0.02526      0   3.092  0.00199 **\nnodematch.level   1.20285    0.39791      0   3.023  0.00250 **\nnodematch.dept.1  1.32797    0.78746      0   1.686  0.09172 . \nnodematch.dept.2  1.39367    0.55827      0   2.496  0.01255 * \nnodematch.dept.3 -0.71792    1.32015      0  -0.544  0.58657   \nabsdiff.age      -0.03118    0.02791      0  -1.117  0.26396   \nabsdiff.tenure    0.01854    0.03028      0   0.612  0.54038   \ngwdsp.fixed.0.25  0.40011    0.23478      0   1.704  0.08835 . \ngwesp.fixed.0.25  1.14136    1.06525      0   1.071  0.28397   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 291.1  on 210  degrees of freedom\n Residual Deviance: 233.1  on 199  degrees of freedom\n \nAIC: 255.1  BIC: 292  (Smaller is better. MC Std. Err. = 0.2465)\n\n\nWhich shows that our results are also robust to this structural tendency, although the transitivity effect is weakened a bit (\\(p < 0.10\\)) when adjusting for statistical prevalence of Simmelian ties."
  },
  {
    "objectID": "ergm2.html",
    "href": "ergm2.html",
    "title": "Exponential Random Graphs Models II",
    "section": "",
    "text": "In the previous handout we saw how to fit exponential random graph models (ergms) for simple undirected networks. As noted, these models are useful for multivariate null hypothesis testing using network data, answering questions of the form: Is this network statistic larger or smaller than I would expect by chance, after conditioning (e.g., “controling for”) these other network statistics?\nThe ergm modeling framework is pretty flexible, allowing you to test for the statistical significance of any network statistic you can imagine while holding constant a bunch of other ones.\nHere we will see how to extend the ergm framework to other types of directed, two-mode, and temporal networks."
  },
  {
    "objectID": "ergm2.html#ergms-in-directed-networks",
    "href": "ergm2.html#ergms-in-directed-networks",
    "title": "Exponential Random Graphs Models II",
    "section": "ERGMs in Directed Networks",
    "text": "ERGMs in Directed Networks\nThe extension to the directed case is the most straightforward.\nThe key network statistic that we need to think about in the directed case is reciprocity or mutuality, namely, the fact that in the directed case, it is possible that I send a tie to you but you don’t send it back. In this advice network case this would happen if I seek advice from you, but you don’t seek advice from me.\nIn an ergm, we use the mutual term to condition on the number of dyads featuring arcs going on both directions (e.g., from \\(i\\) to \\(j\\) and from \\(j\\) to \\(i\\)).\nLet’s see an example using Krackhardt’s High-Tech Managers:\n\n   library(networkdata)\n   library(igraph)\n   library(ergm)\n   library(intergraph)\n   set.seed(123)\n   g1 <- ht_friends\n   n1 <- asNetwork(g1)\n   g2 <- ht_advice\n   n2 <- asNetwork(g2)\n   m1 <- ergm(n1 ~ edges + mutual)\n   summary(m1)\n\nCall:\nergm(formula = n1 ~ edges + mutual)\n\nMonte Carlo Maximum Likelihood Results:\n\n       Estimate Std. Error MCMC % z value Pr(>|z|)    \nedges   -1.5416     0.1553      0  -9.929  < 1e-04 ***\nmutual   1.3320     0.3607      0   3.692 0.000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 582.2  on 420  degrees of freedom\n Residual Deviance: 451.6  on 418  degrees of freedom\n \nAIC: 455.6  BIC: 463.7  (Smaller is better. MC Std. Err. = 0.3209)\n\n   m2 <- ergm(n2 ~ edges + mutual)\n   summary(m2)\n\nCall:\nergm(formula = n2 ~ edges + mutual)\n\nMonte Carlo Maximum Likelihood Results:\n\n       Estimate Std. Error MCMC % z value Pr(>|z|)  \nedges   -0.2752     0.1629      0  -1.690   0.0911 .\nmutual   0.1693     0.2939      0   0.576   0.5647  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 582.2  on 420  degrees of freedom\n Residual Deviance: 578.2  on 418  degrees of freedom\n \nAIC: 582.2  BIC: 590.2  (Smaller is better. MC Std. Err. = 0.05879)\n\n\nHere we see strong evidence for mutuality effects in friendship nominations (\\(\\theta_m = 1.33, p < 0.01\\)), consistent with the social meaning of the relation, but not in advice relations (\\(\\theta_m = 0.17, p = 0.57\\)), suggesting that these ties are more hierarchical).\nNext we may also want to fit models that condition on the different types of dependence between pairs of directed ties that are centered on the same node. There are three flavors of these types of dependencies:\n\nNodes that send a tie are more likely to send another tie. This is a rich get richer effect for outdegree.\nNodes that receive a tie are more likely to receive another tie. This is a rich get richer effect for indegree.\nNodes that send/receive a tie are more like to receive/send a tie. This fits the correlation between in and outdegree at the node level. If positive, nodes that send ties also receive ties.\n\nThe respective ergm terms for these three effects are ostar(2), istar(2) and twopath.\nLet’s test for a twopath effect in the friendship and advice networks:\n\n   set.seed(123)\n   m3 <- ergm(n1 ~ edges + mutual + twopath)\n   summary(m3)\n\nCall:\nergm(formula = n1 ~ edges + mutual + twopath)\n\nMonte Carlo Maximum Likelihood Results:\n\n        Estimate Std. Error MCMC % z value Pr(>|z|)    \nedges   -1.69215    0.52987      0  -3.194  0.00141 ** \nmutual   1.33011    0.34058      0   3.905  < 1e-04 ***\ntwopath  0.01649    0.05477      0   0.301  0.76330    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 582.2  on 420  degrees of freedom\n Residual Deviance: 450.1  on 417  degrees of freedom\n \nAIC: 456.1  BIC: 468.2  (Smaller is better. MC Std. Err. = 0.4178)\n\n   m4 <- ergm(n2 ~ edges + mutual + twopath)\n   summary(m4)\n\nCall:\nergm(formula = n2 ~ edges + mutual + twopath)\n\nMonte Carlo Maximum Likelihood Results:\n\n        Estimate Std. Error MCMC % z value Pr(>|z|)    \nedges    3.11004    0.51671      0   6.019   <1e-04 ***\nmutual   0.48283    0.30122      0   1.603    0.109    \ntwopath -0.20636    0.03315      0  -6.225   <1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 582.2  on 420  degrees of freedom\n Residual Deviance: 542.9  on 417  degrees of freedom\n \nAIC: 548.9  BIC: 561  (Smaller is better. MC Std. Err. = 0.7573)\n\n\nWe can see that in the friendship network, there is no two path effect (\\(p = 0.76\\)); that is, people who send friendship nominations are not more or less likely to receive them.\nHowever, in the advice network, we see a significant and negative two path effect (\\(\\theta_{2p} = -0.21\\)); people who seek advice are less likely to be sought as advisors from others, which makes sense.\nNow let’s look at rich get richer effects by indegree: Are people who are nominated as friends byy one person more likely to be nominated as friends by others? Are people who are sought after for advice by one person more likely to be sought after for advice by others?\n\n   set.seed(123)\n   m5 <- ergm(n1 ~ edges + mutual + istar(2))\n   summary(m5)\n\nCall:\nergm(formula = n1 ~ edges + mutual + istar(2))\n\nMonte Carlo Maximum Likelihood Results:\n\n       Estimate Std. Error MCMC % z value Pr(>|z|)    \nedges  -1.84755    0.37451      0  -4.933  < 1e-04 ***\nmutual  1.34842    0.35668      0   3.780 0.000157 ***\nistar2  0.06630    0.07263      0   0.913 0.361348    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 582.2  on 420  degrees of freedom\n Residual Deviance: 450.0  on 417  degrees of freedom\n \nAIC: 456  BIC: 468.2  (Smaller is better. MC Std. Err. = 0.3208)\n\n   m6 <- ergm(n2 ~ edges + mutual + twopath + istar(2))\n   summary(m6)\n\nCall:\nergm(formula = n2 ~ edges + mutual + twopath + istar(2))\n\nMonte Carlo Maximum Likelihood Results:\n\n        Estimate Std. Error MCMC % z value Pr(>|z|)    \nedges    0.85773    0.78088      0   1.098   0.2720    \nmutual   0.77351    0.31932      0   2.422   0.0154 *  \ntwopath -0.15003    0.03577      0  -4.194   <1e-04 ***\nistar2   0.13288    0.03318      0   4.005   <1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 582.2  on 420  degrees of freedom\n Residual Deviance: 534.1  on 416  degrees of freedom\n \nAIC: 542.1  BIC: 558.3  (Smaller is better. MC Std. Err. = 0.7616)\n\n\nWe can that friendship network displays not cumulative advantage by indegree, but that they advice network does (\\(\\theta_{is} = 0.13\\)), consistent with the idea that the latter is more unequal and hierarchical. Note also that once we adjust for the two path and the rich get richer indegree effect, we can see a strong mutuality effect in advice relations (\\(\\theta_{m} = 0.77\\)).\nNow, let’s adjust for the tendency of people who send one tie, to send other ties:\n\n   set.seed(123)\n   m7 <- ergm(n1 ~ edges + mutual + ostar(2), \n              estimate = \"MPLE\",\n              control = control.ergm(MPLE.covariance.method = \"Godambe\"))\n   summary(m7)\n\nCall:\nergm(formula = n1 ~ edges + mutual + ostar(2), estimate = \"MPLE\", \n    control = control.ergm(MPLE.covariance.method = \"Godambe\"))\n\nMaximum Pseudolikelihood Results:\n\n       Estimate Std. Error MCMC % z value Pr(>|z|)    \nedges  -3.06823    0.35873      0  -8.553   <1e-04 ***\nmutual  1.70160    0.32521      0   5.232   <1e-04 ***\nostar2  0.26496    0.05675      0   4.669   <1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 582.2  on 420  degrees of freedom\n Residual Deviance: 356.9  on 417  degrees of freedom\n \nAIC: 362.9  BIC: 375.1  (Smaller is better. MC Std. Err. = 0)\n\n   m8 <- ergm(n2 ~ edges + mutual + twopath  \n                 + istar(2) + ostar(2),  \n              estimate = \"MPLE\",\n              control = control.ergm(MPLE.covariance.method = \"Godambe\"))\n   summary(m8)\n\nCall:\nergm(formula = n2 ~ edges + mutual + twopath + istar(2) + ostar(2), \n    estimate = \"MPLE\", control = control.ergm(MPLE.covariance.method = \"Godambe\"))\n\nMaximum Pseudolikelihood Results:\n\n        Estimate Std. Error MCMC % z value Pr(>|z|)    \nedges   -4.38711    1.45877      0  -3.007  0.00263 ** \nmutual   1.70972    0.07481      0  22.855  < 1e-04 ***\ntwopath -0.09056    0.03393      0  -2.669  0.00760 ** \nistar2   0.27931    0.06355      0   4.395  < 1e-04 ***\nostar2   0.29084    0.04304      0   6.757  < 1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 582.2  on 420  degrees of freedom\n Residual Deviance: 389.0  on 415  degrees of freedom\n \nAIC: 399  BIC: 419.2  (Smaller is better. MC Std. Err. = 0)\n\n\nWe can now see that in contrast to the previous null results, there is outdegree dependency between nominations sent by the same node in the friendship network: People who nominate one person as a friend tend to nominate others (\\(\\theta_{os} = 0.27, p < 0.01\\)).\nIn the advice network, we can see that all tendencies are operative, as the network displays all forms of degree dependence at the node level, along with mutuality.\nWe can also incorporate homophily based non attributes into our analysis of reciprocity. That is, we can test hypotheses of the form: Is mutuality weaker or stronger for dyads that match on a certain characteristic?\nFor instance, we may suspect that friendship ties among high-tech managers are influenced by formal organizational position, so that there is more mutuality within levels (e.g., mid-manager to mid-manager) than across levels (mid to lower-level). This is what Lusher, Koskinen, and Robins (2013, 18) call “reciprocated homophily.”\nWe can test this hypothesis as using the same argument in the mutual ergm term:\n\n   set.seed(123)\n   m9 <- ergm(n1 ~ edges \n              + mutual(same = \"level\", diff = TRUE, levels = 2)\n              + ostar(2), \n              estimate = \"MPLE\",\n              control = control.ergm(MPLE.covariance.method = \"Godambe\"))\n   summary(m9)\n\nCall:\nergm(formula = n1 ~ edges + mutual(same = \"level\", diff = TRUE, \n    levels = 2) + ostar(2), estimate = \"MPLE\", control = control.ergm(MPLE.covariance.method = \"Godambe\"))\n\nMaximum Pseudolikelihood Results:\n\n                    Estimate Std. Error MCMC % z value Pr(>|z|)    \nedges               -2.53164    0.18751      0 -13.501   <1e-04 ***\nmutual.same.level.2  3.53634    0.58476      0   6.047   <1e-04 ***\nostar2               0.25071    0.02919      0   8.589   <1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 582.2  on 420  degrees of freedom\n Residual Deviance: 380.1  on 417  degrees of freedom\n \nAIC: 386.1  BIC: 398.2  (Smaller is better. MC Std. Err. = 0)\n\n   m10 <- ergm(n2 ~ edges \n              + mutual(same = \"level\", diff = TRUE, levels = 2)\n              + twopath  + istar(2) + ostar(2), \n              estimate = \"MPLE\",\n              control = control.ergm(MPLE.covariance.method = \"Godambe\"))\n   summary(m10)\n\nCall:\nergm(formula = n2 ~ edges + mutual(same = \"level\", diff = TRUE, \n    levels = 2) + twopath + istar(2) + ostar(2), estimate = \"MPLE\", \n    control = control.ergm(MPLE.covariance.method = \"Godambe\"))\n\nMaximum Pseudolikelihood Results:\n\n                    Estimate Std. Error MCMC % z value Pr(>|z|)    \nedges               -4.13775    1.36925      0  -3.022  0.00251 ** \nmutual.same.level.2  1.56828    0.35108      0   4.467  < 1e-04 ***\ntwopath             -0.02600    0.03249      0  -0.800  0.42345    \nistar2               0.23461    0.05989      0   3.918  < 1e-04 ***\nostar2               0.26504    0.04004      0   6.619  < 1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 582.2  on 420  degrees of freedom\n Residual Deviance: 416.7  on 415  degrees of freedom\n \nAIC: 426.7  BIC: 446.9  (Smaller is better. MC Std. Err. = 0)\n\n\nRecall than in this network there are three managerial levels but only one top manager, so we here, we compare the middle level to the rest. And indeed, we find that compared to lower level managers, tendencies toward mutuality in friendship choices are stronger among mid-level managers. Note also that once we adjust for this attribute-specific mutuality effect, the two path effect in the advice network is no longer significant.\nWe can also test hypotheses of the form: Are nodes with certain characteristics more likely to be involved in mutual dyads regardless of the characteristics of the node at the other end of the tie? That is, this tests for differential tendencies toward mutuality with other people in the network based on an actor attribute.\nWe can do this using the by argument in the mutual term, this time using department as the relevant attribute:\n\n   set.seed(123)\n   m11 <- ergm(n1 ~ edges \n              + mutual(by = \"dept\", levels = 2:4)\n              + ostar(2), \n              estimate = \"MPLE\",\n              control = control.ergm(MPLE.covariance.method = \"Godambe\"))\n   summary(m11)\n\nCall:\nergm(formula = n1 ~ edges + mutual(by = \"dept\", levels = 2:4) + \n    ostar(2), estimate = \"MPLE\", control = control.ergm(MPLE.covariance.method = \"Godambe\"))\n\nMaximum Pseudolikelihood Results:\n\n                 Estimate Std. Error MCMC % z value Pr(>|z|)    \nedges            -2.76686    0.27732      0  -9.977   <1e-04 ***\nmutual.by.dept.1  1.08797    0.25097      0   4.335   <1e-04 ***\nmutual.by.dept.2  0.54380    0.30474      0   1.784   0.0743 .  \nmutual.by.dept.3  0.33191    0.58650      0   0.566   0.5715    \nostar2            0.24691    0.04965      0   4.972   <1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 582.2  on 420  degrees of freedom\n Residual Deviance: 371.6  on 415  degrees of freedom\n \nAIC: 381.6  BIC: 401.8  (Smaller is better. MC Std. Err. = 0)\n\n   m12 <- ergm(n2 ~ edges \n              + mutual(by = \"dept\", levels = 2:4)\n              + istar(2) + ostar(2), \n              estimate = \"MPLE\",\n              control = control.ergm(MPLE.covariance.method = \"Godambe\"))\n   summary(m12)\n\nCall:\nergm(formula = n2 ~ edges + mutual(by = \"dept\", levels = 2:4) + \n    istar(2) + ostar(2), estimate = \"MPLE\", control = control.ergm(MPLE.covariance.method = \"Godambe\"))\n\nMaximum Pseudolikelihood Results:\n\n                 Estimate Std. Error MCMC % z value Pr(>|z|)    \nedges            -5.26275    0.57206      0  -9.200   <1e-04 ***\nmutual.by.dept.1  1.03995    0.06568      0  15.834   <1e-04 ***\nmutual.by.dept.2 -0.06611    0.09533      0  -0.694    0.488    \nmutual.by.dept.3  1.11460    0.03769      0  29.571   <1e-04 ***\nistar2            0.25074    0.03531      0   7.101   <1e-04 ***\nostar2            0.29014    0.02182      0  13.296   <1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 582.2  on 420  degrees of freedom\n Residual Deviance: 397.1  on 414  degrees of freedom\n \nAIC: 409.1  BIC: 433.3  (Smaller is better. MC Std. Err. = 0)\n\n\nThe results show that, in the friendship network, nodes in Dept 2 tend to have more mutual ties compared to nodes in Dept 1 (the reference), in the advice network, both nodes in Dept 2 and and Dept 4 tend to have more mutual ties, while nodes in Dept 3 are not statistically different from nodes in Dept 1."
  },
  {
    "objectID": "hits.html",
    "href": "hits.html",
    "title": "Hubs and Authorities",
    "section": "",
    "text": "Recall from previous discussions that everything doubles (sometimes quadruples like degree correlations) in directed graphs. The same goes for status as reflected in a distributive model through the network.\nConsider two ways of showing your status in a system governed by directed relations (like advice). You can be highly sought after by others (be an “authority”), or you can be discerning in who you seek advice from, preferably seeking out people who are also sought after (e.g., be a “hub” pointing to high-quality others).\nThese two forms of status are mutually defining (Bonacich and Lloyd 2001). The top authorities are those who are sought after by the top hubs, and the top hubs are the ones who seek the top authorities!\nSo this leads to a doubling of the Bonacich prestige status accounting equation:\n\\[  \n   x^h_i = \\sum_j a_{ij} x^a_j\n\\]\n\\[\n   x^a_i = \\sum_i a_{ij} x^h_i\n\\]\nWhich says that the hub score \\(x^h\\) of a node is the sum of the authority scores \\(x^a\\) of the nodes they point to (sum over \\(j\\); the outdegree), and the authority score of a node is the sum of the hub scores of the nodes that point to it (sum over \\(i\\); the indegree).\nSo we need to make our status game a bit more complicated (but not too much) to account for this duality:\nEverything is like our previous status1 function except now we are keeping track of two mutually defining scores a and h. We first initialize the authority scores by setting them to the value of \\(1/n\\) (where \\(n\\) is the number of nodes or the number of rows in the adjacency matrix) in line 2. We then initialize the \\(\\delta\\) difference and \\(k\\) counter in lines 3-4. The while loop in lines 5-13 then updates the hub scores (to be the sum of the authority scores of each out-neighbor) in line 7 normalize them in line 8 and update the new authority scores to be the sum (across each in-neighbor) of these new hub scores, which are then themselves normalized in line 10.\nSo at each step \\(t\\), the authority and hub scores are calculated like this:\n\\[  \n   x^h_i(t) = \\sum_j a_{ij} x^a_j(t-1)\n\\]\n\\[\n   x^a_i(t) = \\sum_j a^T_{ij} x^h_j(t)\n\\]\nWhere \\(a^T_{ij}\\) is the corresponding entry in the transpose of the adjacency matrix (t(w) in line 9 of the above function).\nAs you may have guessed this is just an implementation of the “HITS” algorithm developed by Kleinberg (1999).1\nThe results for the Krackhardt advice network are:\nWhich are equivalent to using the igraph function hits_scores:\nNote that the just like the status2 function, the igraph function hits_scores returns the two sets of scores as elements of a list, so we need to access them using the $ operator on the object that we store the results in (in this case ha). We also set the scale argument to TRUE so that the scores are normalized by the maximum."
  },
  {
    "objectID": "hits.html#hubs-authorities-and-eigenvectors",
    "href": "hits.html#hubs-authorities-and-eigenvectors",
    "title": "Hubs and Authorities",
    "section": "Hubs, Authorities and Eigenvectors",
    "text": "Hubs, Authorities and Eigenvectors\nRecall that both the eigenvector and PageRank status scores computed via the network status distribution game routine ended up being equivalent to the leading eigenvector of a network proximity matrix (the adjacency matrix \\(\\mathbf{A}\\) and the probability matrix \\(\\mathbf{P}\\) respectively). It would be surprising if the same wasn’t true of the hub and authority status scores.\nLet’s find out which ones!\nConsider the matrices:\n\\[\n\\mathbf{M}_h = \\mathbf{A}\\mathbf{A}^T\n\\]\n\\[\n\\mathbf{M}_a = \\mathbf{A}^T\\mathbf{A}\n\\]\nLet’s see what they look like in the Krackhardt manager’s network:\n\n   M.h = A %*% t(A)\n   M.a = t(A) %*% A\n   M.h[1:10, 1:10]\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n [1,]    6    1    5    5    5    1    3    4    5     5\n [2,]    1    3    3    2    3    1    2    3    3     0\n [3,]    5    3   15   11   12    1    8    8   12     8\n [4,]    5    2   11   12   11    1    7    6   11     8\n [5,]    5    3   12   11   15    1    7    7   12    10\n [6,]    1    1    1    1    1    1    1    1    1     0\n [7,]    3    2    8    7    7    1    8    5    8     4\n [8,]    4    3    8    6    7    1    5    8    7     4\n [9,]    5    3   12   11   12    1    8    7   13     7\n[10,]    5    0    8    8   10    0    4    4    7    14\n\n   M.a[1:10, 1:10]\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n [1,]   13   13    4    5    5    6    8    8    4     8\n [2,]   13   18    5    8    5    9   11   10    4     9\n [3,]    4    5    5    4    4    2    4    4    2     3\n [4,]    5    8    4    8    3    4    6    6    3     4\n [5,]    5    5    4    3    5    1    3    3    3     3\n [6,]    6    9    2    4    1   10    7    7    2     6\n [7,]    8   11    4    6    3    7   13    6    3     7\n [8,]    8   10    4    6    3    7    6   10    3     6\n [9,]    4    4    2    3    3    2    3    3    4     3\n[10,]    8    9    3    4    3    6    7    6    3     9\n\n\nWhat’s in these matrices? Well let’s look at \\(\\mathbf{M}_h\\). The diagonals will look familiar because they happen to be the outdegree of each node:\n\n   degree(g, mode = \"out\")[1:10]\n\n [1]  6  3 15 12 15  1  8  8 13 14\n\n\nYou may have guessed that the diagonals of matrix \\(\\mathbf{M}_a\\) contain the indegrees:\n\n   degree(g, mode = \"in\")[1:10]\n\n [1] 13 18  5  8  5 10 13 10  4  9\n\n\nWhich means that the off-diagonals cells of each matrix \\(m_{ij}\\) and \\(n_{ij}\\), contain the common out-neighbors and common in-neighbors shared by nodes \\(i\\) and \\(j\\) in the graph, respectively.\n\n\n\n\n\nFigure 1: Subgraph from Krackhardt’s Managers Network.\n\n\n\n\nIn information science, \\(\\mathbf{M}_h\\) and \\(\\mathbf{M}_a\\) matrices have special interpretations. Consider the subgraph shown in Figure 1, which contains nodes 2 and 11 from the Krackhardt advice network and their respective neighbors:\nIf these nodes where papers, then we would say that both 2 and 11 point to a common third node 1. In an information network, the papers that other papers point to are their common references. Therefore the number of common out-neighbors of two nodes is is called the bibliographic coupling score between the two papers. In the same way, we can see that 2 and 11 are pointed to by a common third neighbor 21. The number of common in-neighbors between two-papers is called their co-citation score.\nBoth the bibliographic coupling and the co-citation scores get at two ways that nodes can be similar in a directed graph. In the social context of advice seeking, for instance, two people can be similar if they seek advice from the same others, or two people can be similar if they are sought after for advice by the same others.\nThe \\(\\mathbf{M}_h\\) and \\(\\mathbf{M}_a\\) matrices, therefore are two (unweighted) similarity matrices between the nodes in a directed graph. As you may also be suspecting, the hub and authorities scores are the leading eigenvectors of the \\(\\mathbf{M}_h\\) and \\(\\mathbf{M}_a\\) matrices (Kleinberg 1999):\n\n   a <- eigen(M.a)$vector[,1] * -1\n   h <- eigen(M.h)$vector[,1] * -1\n   round(a/max(a), 3)\n\n [1] 0.782 1.000 0.356 0.496 0.330 0.644 0.684 0.711 0.290 0.615 0.769 0.498\n[13] 0.323 0.677 0.267 0.570 0.645 0.871 0.323 0.589 0.776\n\n   round(h/max(h), 3)\n\n [1] 0.370 0.176 0.841 0.709 0.835 0.065 0.492 0.490 0.773 0.672 0.206 0.122\n[13] 0.331 0.279 1.000 0.274 0.313 0.800 0.581 0.687 0.600\n\n\nGiven the connection to the HITS dual status ranking, sometimes the \\(\\mathbf{M}_h\\) is called the hub matrix and the \\(\\mathbf{M}_a\\) is called the authority matrix (Ding et al. 2002).\nNote that this also means we could have obtained the hub and authority scores using our old status1 function, but we would have had to play the game twice, once for the matrix \\(\\mathbf{M}_a\\) and the other one for the matrix \\(\\mathbf{M}_h\\):\n\n   status1 <- function(w) {\n      x <- rep(1, nrow(w)) #initial status vector set to all ones of length equal to the number of nodes\n      d <- 1 #initial delta\n      k <- 0 #initializing counter\n      while (d > 1e-10) {\n          o.x <- x #old status scores\n          x <- w %*% o.x #new scores a function of old scores and adjacency matrix\n          x <- x/norm(x, type = \"E\") #normalizing new status scores\n          d <- abs(sum(abs(x) - abs(o.x))) #delta between new and old scores\n          k <- k + 1 #incrementing while counter\n      }\n   return(as.vector(x))\n   }\n\n   a <- status1(M.a)\n   round(a/max(a), 3)\n\n [1] 0.782 1.000 0.356 0.496 0.330 0.644 0.684 0.711 0.290 0.615 0.769 0.498\n[13] 0.323 0.677 0.267 0.570 0.645 0.871 0.323 0.589 0.776\n\n   h <- status1(M.h)\n   round(h/max(h), 3)\n\n [1] 0.370 0.176 0.841 0.709 0.835 0.065 0.492 0.490 0.773 0.672 0.206 0.122\n[13] 0.331 0.279 1.000 0.274 0.313 0.800 0.581 0.687 0.600\n\n\nThis link once again demonstrates the equivalence between the eigenvectors of the hub and authority matrices, as similarity matrices between nodes in the network, and our prismatic status distribution game!"
  },
  {
    "objectID": "hits.html#combining-pagerank-and-hits-salsa",
    "href": "hits.html#combining-pagerank-and-hits-salsa",
    "title": "Hubs and Authorities",
    "section": "Combining PageRank and HITS: SALSA",
    "text": "Combining PageRank and HITS: SALSA\nLempel and Moran (2001) show that we can combine the logic of PageRank and HITS. Their basic idea is to use the same mutually reinforcing approach as in HITS but with degree-normalized (stochastic) versions of the adjacency matrix (like in PageRank).2\nLet’s see how it works.\nRecall that PageRank works on the \\(\\mathbf{P}\\) matrix, which is defined like this:\n\\[\n\\mathbf{P}_{a} = \\mathbf{D}_{out}^{-1}\\mathbf{A}\n\\]\nIn R we compute it like this:\n\n   D.o <- diag(1/rowSums(A))\n   P.a <- D.o %*% A\n\nThis matrix is row-stochastic, because each row is divided by the row total (the outdegrees of each node), meaning its rows sum to one, like we saw before:\n\n   rowSums(P.a)\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n\nIt is also possible to compute the indegree normalized version of the \\(\\mathbf{P}\\) matrix, defined like this:\n\\[\n\\mathbf{P}_{h} = \\mathbf{D}_{in}^{-1} \\mathbf{A}^T\n\\]\nWhere \\(\\mathbf{D}_{in}^{-1}\\) is a matrix containing the inverse of the indegrees along the diagonals (and zeroes elsewhere) and \\(\\mathbf{A}^T\\) is the transpose of the adjacency matrix. Each non-zero entry of is thus equal to one divided by that row node’s indegree.\nIn R we compute it like this:\n\n   D.i <- diag(1/colSums(A))\n   P.h <- D.i %*% t(A)\n\nLike the \\(\\mathbf{P}_{a}\\) matrix, the \\(\\mathbf{P}_{a}\\) matrix is row-stochastic, meaning its rows sum to 1.0:\n\n   rowSums(P.h)\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n\nTo get the SALSA version of the hub and authority scores, we can just play our status game over newly defined versions of the hub and authority matrices (Langville and Meyer 2005, 156).\nThe SALSA hub matrix is defined like this:\n\\[\n\\mathbf{Q}_h = \\mathbf{P}_a\\mathbf{P}_h\n\\]\nAnd the SALSA authority matrix like this:\n\\[\n\\mathbf{Q}_a = \\mathbf{P}_h\\mathbf{P}_a\n\\]\nWhich in R looks like:\n\n   Q.h <- P.a %*% P.h\n   Q.a <- P.h %*% P.a\n\nEach of these matrices are row stochastic:\n\n   rowSums(Q.h)\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n   rowSums(Q.a)\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n\nWhich means that inequalities will be defined according to differences in the in-degrees of each node just like PageRank.\nAnd now to obtain our SALSA hub and authority scores, we simply play our status1 game on (the transpose of) these matrices, just like we did for PageRank:\n\n   salsa.h <- status1(t(Q.h)) \n   round(salsa.h/sum(salsa.h), 2)\n\n [1] 0.03 0.02 0.08 0.06 0.08 0.01 0.04 0.04 0.07 0.07 0.02 0.01 0.03 0.02 0.11\n[16] 0.02 0.03 0.09 0.06 0.06 0.06\n\n   salsa.a <- status1(t(Q.a)) \n   round(salsa.a/sum(salsa.a), 2)\n\n [1] 0.07 0.09 0.03 0.04 0.03 0.05 0.07 0.05 0.02 0.05 0.06 0.04 0.02 0.05 0.02\n[16] 0.04 0.05 0.08 0.02 0.04 0.08\n\n\nWhat are these numbers? Well, it turns out that they are equivalent to the out and indegrees of each node, divided by the total number of edges in the network (Fouss, Renders, and Saerens 2004, 451).\nSo the SALSA hub and authority scores can also be obtained like this, without having to play our status game over any matrix:\n\n   round(rowSums(A)/sum(A), 2)\n\n [1] 0.03 0.02 0.08 0.06 0.08 0.01 0.04 0.04 0.07 0.07 0.02 0.01 0.03 0.02 0.11\n[16] 0.02 0.03 0.09 0.06 0.06 0.06\n\n   round(colSums(A)/sum(A), 2)\n\n [1] 0.07 0.09 0.03 0.04 0.03 0.05 0.07 0.05 0.02 0.05 0.06 0.04 0.02 0.05 0.02\n[16] 0.04 0.05 0.08 0.02 0.04 0.08\n\n\nNote that for the SALSA scores we use a different normalization compared to before. Instead of dividing by the maximum, we divide by the sum, so that the vector of SALSA hub and authority scores sum to 1.0.\nThe reason for this is that, as Fouss, Renders, and Saerens (2004) explain, these numbers have a straightforward interpretation in terms of the probability that a random walker in the network will find itself in that particular node, when the walker goes from hub -> authority -> hub -> authority (e.g., never going from hub to hub or authority to authority) using the entries in the P.a and P.h matrices to determine the probability of jumping from hub \\(i\\) to authority \\(j\\) and vice versa. Thus, the higher the probability the more “central” the specific hub or authority is, just like the random walk interpretation of the PageRank scores."
  },
  {
    "objectID": "hits.html#hits-versus-principal-components-analysis",
    "href": "hits.html#hits-versus-principal-components-analysis",
    "title": "Hubs and Authorities",
    "section": "HITS versus Principal Components Analysis",
    "text": "HITS versus Principal Components Analysis\nSaerens and Fouss (2005) argue that there is an intimate relationship between the HITS dual ranking scores and one of the most widely used multivariate analysis techniques, Principal Components Analysis (PCA).\nIn fact, they argue that HITS is just PCA on an uncentered data matrix (which in the this case is the network’s adjacency matrix). Technically we could also just argue that PCA is just HITS on a centered adjacency matrix, as we will see in just a bit.\nLet’s see how this works. First, let’s create a centered version of the network adjacency matrix. To center a typical data matrix (e.g., of individuals in the rows by variables in the columns) we subtract the column mean (the average score of all individuals on that variable) from each individual’s score.\nSo to get the centered network adjacency matrix, we first need to compute the corresponding column means of the matrix. Note that this will be equivalent to each individual’s indegree (the sum of the columns) divided by the total number of nodes in the network (\\(k^{in}/n\\)), which is kind of a normalized degree centrality score, except the usual normalization is to divide by \\(n-1\\) (so as to have a maximum of 1.0) as we saw in our discussion of centrality.\nFirst, let’s compute the column means vector, using the native R function colMeans which takes a matrix as input and returns a vector of column means:\n\n   cm <- colMeans(A)\n   round(cm, 2)\n\n [1] 0.62 0.86 0.24 0.38 0.24 0.48 0.62 0.48 0.19 0.43 0.52 0.33 0.19 0.48 0.19\n[16] 0.38 0.43 0.71 0.19 0.38 0.71\n\n\nNow that we have this vector of column means, all we need to do is subtract it from each column of the adjacency matrix, to create the centered adjacency matrix:\n\n   A.c <- t(t(A) - cm)\n\nNote that to subtract the column mean vector from each column of the adjacency matrix we first transpose it, do the subtraction, and “untranspose” back to the original (by taking the transpose of the transpose). We can use the colMeans function to check that the column means of the centered adjacency matrix are indeed zero:\n\n   round(colMeans(A.c), 2)\n\n [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n\n\nNow all we need to do is play our HITS status game on the centered adjacency matrix:\n\n   hits.res <- status2(A.c)\n\nWe then use the function below to normalize each status score to be within the minus one to plus one interval and have mean zero:\n\n   norm.v <- function(x) {\n      x <- x - min(x)\n      x <- x/max(x)\n      x <- x - mean(x)\n      return(x)\n   }\n\nThe first thing the function does (line 2) is subtract the minimum value from the vector (which becomes zero), in line 3 we divided by the maximum (which becomes one), and in line 4 we subtract the mean from each value.\nThe following code applies the normalization to the results from our status game on the centered adjacency matrix:\n\n   h2 <- round(norm.v(hits.res$h), 3)\n   a2 <- round(norm.v(hits.res$a), 3)\n   names(h2) <- 1:21\n   names(a2) <- 1:21\n\nAnd the resulting normalized hub and authority scores for each node in the Krackhardt managers advice network are:\n\n   round(h2, 3)\n\n     1      2      3      4      5      6      7      8      9     10     11 \n-0.162 -0.374  0.318  0.241  0.364 -0.429 -0.027 -0.117  0.239  0.334 -0.335 \n    12     13     14     15     16     17     18     19     20     21 \n-0.421 -0.160 -0.339  0.571 -0.240 -0.295  0.385  0.105  0.206  0.134 \n\n   round(a2, 3)\n\n     1      2      3      4      5      6      7      8      9     10     11 \n 0.052 -0.155 -0.031 -0.177 -0.150 -0.020 -0.555  0.415 -0.199  0.106  0.411 \n    12     13     14     15     16     17     18     19     20     21 \n 0.031  0.020  0.215 -0.233  0.232  0.314 -0.056  0.020  0.343 -0.585 \n\n\nSaerens and Fouss (2005) show that these are the same scores we would obtain from a simple PCA of the regular old adjacency matrix. To see this, let’s do the PCA analysis using the PCA function from the package FactorMineR.3\n\n   library(FactoMineR)\n   pca.res <- PCA(A, graph = FALSE, scale.unit = FALSE)\n\nNote that we set the argument scale.unit to FALSE so that the PCA is conducted on the centered adjacency matrix and not a standardized (to unit variance) version of it.\nThe PCA function stores the corresponding scores for the rows and columns of the matrix in these objects:\n\n   pca.h <- pca.res$ind$coord[, 1]\n   pca.a <- pca.res$var$coord[, 1]\n   names(pca.a) <- 1:21\n\nAnd now, for the big reveal:\n\n   round(norm.v(pca.h), 3)\n\n     1      2      3      4      5      6      7      8      9     10     11 \n-0.162 -0.374  0.318  0.241  0.364 -0.429 -0.027 -0.117  0.239  0.334 -0.335 \n    12     13     14     15     16     17     18     19     20     21 \n-0.421 -0.160 -0.339  0.571 -0.240 -0.295  0.385  0.105  0.206  0.134 \n\n   round(norm.v(pca.a), 3)\n\n     1      2      3      4      5      6      7      8      9     10     11 \n 0.052 -0.155 -0.031 -0.177 -0.150 -0.020 -0.555  0.415 -0.199  0.106  0.411 \n    12     13     14     15     16     17     18     19     20     21 \n 0.031  0.020  0.215 -0.233  0.232  0.314 -0.056  0.020  0.343 -0.585 \n\n\nWhich are indeed the same scores we obtained earlier when we played our status game on the centered adjacency matrix!\nWe will see one way to interpret these scores in the next section."
  },
  {
    "objectID": "hits.html#hits-and-salsa-versus-correspondence-analysis",
    "href": "hits.html#hits-and-salsa-versus-correspondence-analysis",
    "title": "Hubs and Authorities",
    "section": "HITS and SALSA versus Correspondence Analysis",
    "text": "HITS and SALSA versus Correspondence Analysis\nFouss, Renders, and Saerens (2004) also argue that there is a close link between a method to analyze two-way tables called correspondence analysis, and both Lempel and Moran’s SALSA and Kleinberg’s HITS algorithms.\nThey first ask: What if we play our status distribution game not on the transpose of the SALSA hub and authority matrices like we just did but just on the regular matrices without transposition?\nHere’s what happens:\n\n   round(status1(Q.h), 3)\n\n [1] 0.218 0.218 0.218 0.218 0.218 0.218 0.218 0.218 0.218 0.218 0.218 0.218\n[13] 0.218 0.218 0.218 0.218 0.218 0.218 0.218 0.218 0.218\n\n   round(status1(Q.a), 3)\n\n [1] 0.218 0.218 0.218 0.218 0.218 0.218 0.218 0.218 0.218 0.218 0.218 0.218\n[13] 0.218 0.218 0.218 0.218 0.218 0.218 0.218 0.218 0.218\n\n\nOK, so that’s weird. All that we get is a vector with the same number repeated twenty one times (in this case, the number of nodes in the graph). What’s going on?\nRecall from the previous that the status game computes the leading eigenvector of the matrix we play the game on, and spits that vector out as our status scores for that matrix. The leading eigenvector is that associated with the largest eigenvalue (if the matrix contains one).\nSo all that this is telling us is that the first eigenvector of the un-transposed versions of the SALSA hub and authority matrices is pretty useless because it assigns everyone the same status score.\nBut Fouss, Renders, and Saerens (2004) note, like we did at the beginning, that a matrix has many eigenvector/eigenvalue pairs and that perhaps the second leading eigenvector is not that useless; this is the eigenvector associated with the second largest eigenvalue.\nHow do we get that vector? Well, as always, there is a mathematical workaround. The trick is to create a new matrix that removes the influence of that first (useless) eigenvector and then play our status game on that matrix.\nTo do that, let’s create a matrix that is equal to the original useless eigenvector times its own transpose. In R this goes like this:\n\n   v1 <- status1(Q.h)\n   D <- v1 %*% t(v1)\n\nWhat’s in this matrix? Let’s see the first ten rows and columns:\n\n   round(D[1:10, 1:10], 3)\n\n       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10]\n [1,] 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048\n [2,] 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048\n [3,] 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048\n [4,] 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048\n [5,] 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048\n [6,] 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048\n [7,] 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048\n [8,] 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048\n [9,] 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048\n[10,] 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048\n\n\nSo it’s just a matrix of the same dimension as the SALSA hub matrix with the same number over and over. In fact that number is equal to:\n\n   round(0.2182179^2, 3)\n\n[1] 0.048\n\n\nWhich is just the useless constant status score multiplied by itself (squared).\nNow, we create new SALSA hub and authority matrices, which are equal to the original minus the constant D matrix above:\n\n   Q.h2 <- Q.h - D\n   Q.a2 <- Q.a - D\n\nAnd now we play our status game on these matrices:\n\n   h3 <- status1(Q.h2)\n   a3 <- status1(Q.a2)\n   names(h3) <- 1:21\n   names(a3) <- 1:21\n\nWe then use the same function we used for the PCA scores to normalize each status score to be within the minus one to plus one interval and have mean zero:\n\n   h3 <- norm.v(h3)\n   a3 <- norm.v(a3)\n\nAnd the resulting scores are:\n\n   round(h3, 3)\n\n     1      2      3      4      5      6      7      8      9     10     11 \n 0.007 -0.436  0.074  0.074  0.138 -0.626 -0.019 -0.098  0.023  0.374 -0.063 \n    12     13     14     15     16     17     18     19     20     21 \n-0.530  0.334 -0.243  0.212  0.119 -0.147  0.292  0.290  0.107  0.115 \n\n   round(a3, 3)\n\n     1      2      3      4      5      6      7      8      9     10     11 \n-0.008 -0.146  0.299 -0.106  0.414 -0.329 -0.455 -0.005  0.223 -0.049 -0.027 \n    12     13     14     15     16     17     18     19     20     21 \n-0.158  0.291  0.035  0.323  0.026 -0.057 -0.114  0.291  0.139 -0.586 \n\n\nNow, these scores don’t seem useless. They are different across each node; and like the PCA scores we obtained earlier, some are positive and some are negative.\nFouss, Renders, and Saerens (2004) show that these are the same scores you would obtain from a correspondence analysis (CA) of the original affiliation matrix.\nLet’s check that out in our case. The same package we used to compute the PCA of the adjacency matrix (FactoMineR) can be used to compute the correspondence analysis of any matrix in R, including a network adjacency matrix, using the function CA:\n\n   #library(FactoMineR)\n   ca.res <- CA(A, graph = FALSE)\n   ca.h <- ca.res$row$coord[, 1]\n   ca.a <- ca.res$col$coord[, 1]\n   names(ca.a) <- 1:21\n\nIn line 2 we store the CA results in the object ca.res. We then grab the CA scores associated with the columns of the adjacency matrix and put them in the object ca.h in line 3 and the scores associated with the rows of the adjacency matrix and put them in the object ca.a inline 4.\nNow for the big reveal:\n\n   round(norm.v(ca.h), 3)\n\n     1      2      3      4      5      6      7      8      9     10     11 \n 0.007 -0.436  0.074  0.074  0.138 -0.626 -0.019 -0.098  0.023  0.374 -0.063 \n    12     13     14     15     16     17     18     19     20     21 \n-0.530  0.334 -0.243  0.212  0.119 -0.147  0.292  0.290  0.107  0.115 \n\n   round(norm.v(ca.a), 3)\n\n     1      2      3      4      5      6      7      8      9     10     11 \n-0.008 -0.146  0.299 -0.106  0.414 -0.329 -0.455 -0.005  0.223 -0.049 -0.027 \n    12     13     14     15     16     17     18     19     20     21 \n-0.158  0.291  0.035  0.323  0.026 -0.057 -0.114  0.291  0.139 -0.586 \n\n\nWhich shows that indeed the CA scores are the same ones as we obtain from playing our status game on the corrected versions of the un-transposed SALSA hub and authorities matrices!\nBut what are the CA (and PCA) scores supposed to capture? Let’s look at them side-by-side next to the SALSA hub and authority scores, as shown in Table 1 which has nodes rank ordered by their SALSA hub score.\n\n\n\n\nTable 1:  Hubs and Authorties Scores for Krackhardt’s managers advice network \n \n  \n      \n    salsa.h \n    salsa.a \n    pca.hub \n    pca.auth \n    ca.hub \n    ca.auth \n  \n \n\n  \n    15 \n    0.416 \n    0.088 \n    0.571 \n    -0.233 \n    0.212 \n    0.323 \n  \n  \n    18 \n    0.353 \n    0.331 \n    0.385 \n    -0.056 \n    0.292 \n    -0.114 \n  \n  \n    3 \n    0.312 \n    0.110 \n    0.318 \n    -0.031 \n    0.074 \n    0.299 \n  \n  \n    5 \n    0.312 \n    0.110 \n    0.364 \n    -0.150 \n    0.138 \n    0.414 \n  \n  \n    10 \n    0.291 \n    0.199 \n    0.334 \n    0.106 \n    0.374 \n    -0.049 \n  \n  \n    9 \n    0.270 \n    0.088 \n    0.239 \n    -0.199 \n    0.023 \n    0.223 \n  \n  \n    4 \n    0.249 \n    0.177 \n    0.241 \n    -0.177 \n    0.074 \n    -0.106 \n  \n  \n    20 \n    0.249 \n    0.177 \n    0.206 \n    0.343 \n    0.107 \n    0.139 \n  \n  \n    19 \n    0.229 \n    0.088 \n    0.105 \n    0.020 \n    0.290 \n    0.291 \n  \n  \n    21 \n    0.229 \n    0.331 \n    0.134 \n    -0.585 \n    0.115 \n    -0.586 \n  \n  \n    7 \n    0.166 \n    0.287 \n    -0.027 \n    -0.555 \n    -0.019 \n    -0.455 \n  \n  \n    8 \n    0.166 \n    0.221 \n    -0.117 \n    0.415 \n    -0.098 \n    -0.005 \n  \n  \n    1 \n    0.125 \n    0.287 \n    -0.162 \n    0.052 \n    0.007 \n    -0.008 \n  \n  \n    13 \n    0.125 \n    0.088 \n    -0.160 \n    0.020 \n    0.334 \n    0.291 \n  \n  \n    17 \n    0.104 \n    0.199 \n    -0.295 \n    0.314 \n    -0.147 \n    -0.057 \n  \n  \n    14 \n    0.083 \n    0.221 \n    -0.339 \n    0.215 \n    -0.243 \n    0.035 \n  \n  \n    16 \n    0.083 \n    0.177 \n    -0.240 \n    0.232 \n    0.119 \n    0.026 \n  \n  \n    2 \n    0.062 \n    0.398 \n    -0.374 \n    -0.155 \n    -0.436 \n    -0.146 \n  \n  \n    11 \n    0.062 \n    0.243 \n    -0.335 \n    0.411 \n    -0.063 \n    -0.027 \n  \n  \n    12 \n    0.042 \n    0.155 \n    -0.421 \n    0.031 \n    -0.530 \n    -0.158 \n  \n  \n    6 \n    0.021 \n    0.221 \n    -0.429 \n    -0.020 \n    -0.626 \n    -0.329 \n  \n\n\n\n\n\n\nUsing information in Table 1, Figure 2 (a) shows a point and line network plot but using the CA hub and authority scores to embed the nodes in a common two-dimensional space. In the plot nodes are colored by the difference between their SALSA hub and authority scores such that nodes with positive scores (hub score larger than their authority scores) appear in blue and nodes with negative scores (authority scores larger than their hub scores) appear in red.\n\n\n\n\n\n\n\n(a) CA Coordinates\n\n\n\n\n\n\n\n(b) PCA Coordinates\n\n\n\n\nFigure 2: Krackhardt’s managers advice network (top hubs in blue and top authorities in red)\n\n\nWe can see that the CA hub score places the “hubbiest” of hubs (blue nodes) in the lower-right quadrant of the diagram, with positive CA hub scores and negative CA authority scores (this last multiplied by minus one from the ones shown above). Note from Table 1, that the nodes in this region (e.g. {15, 5, 3, 19, 9, 13}) all have large SALSA hub scores and very low SALSA authority scores.\nIn the same way, the most authoritative of authorities (red nodes) appear in the upper-left side, with negative CA hub scores and positive CA authority scores. Note that nodes in this region have very large SALSA authority scores and very low SALSA hub scores.\nFinally, note that nodes in the upper-right quadrant of the diagram (e.g. {21, 7, 4, 18, 10}) are “ambidextrous” nodes, showcasing relatively large scores as both hubs and authorities according to SALSA.\nThus what the CA scores seem to be doing is separating out the purest examples of hubs and authorities in the network from those who play both roles.\nNote that as shown in Figure 2 (b), we get an even stronger separation between hubs and authorities when using the PCA scores to place nodes in a common space, with all the authorities (except nodes 7 and 21) to the upper left, and the hubs on the right-hand side of the plot. So the PCA scores reflect the same distinction between hub and authority status as the CA scores."
  },
  {
    "objectID": "hits.html#a-hybrid-pagerankhits-approach",
    "href": "hits.html#a-hybrid-pagerankhits-approach",
    "title": "Hubs and Authorities",
    "section": "A Hybrid PageRank/HITS Approach",
    "text": "A Hybrid PageRank/HITS Approach\nBorodin et al. (2005) argue that perhaps a better approach to combining PageRank and HITS is to normalize only one of the scores by degree while leaving the other score alone.\n\nPicky Hubs\nFor instance, in some settings, it might make more sense to assign more authority to nodes that are pointed to by picky hubs (e.g., people who seek advice from a few select others), and discount the authority scores of nodes that are pointed to by indiscriminate hubs (people who seek advice from everyone).\nWe can do this by changing the way we compute the hub score of each node. Instead of just summing the authority scores of all the nodes they point to (like in regular HITS) we instead take the average of the authority scores of all nodes they point to. We then feed this average back to the authority score calculation.\nThis entails slightly modifying the HITS status game as follows:\n\n   status3 <- function(w) {\n     a <- rep(1, nrow(w))  #initializing authority scores\n     d.o <- rowSums(w) #outdegree of each node\n     d <- 1 #initializing delta\n     k <- 0 #initializing counter\n     while (d >= 1e-10) {\n         o.a <- a #old authority scores\n         h <- (w %*% o.a) * 1/d.o #new hub scores a function of previous authority scores of their out-neighbors\n         h <- h/norm(h, type = \"E\")\n         a <- t(w) %*% h #new authority scores a function of current hub scores of their in-neighbors\n         a <- a/norm(a, type = \"E\")\n         d <- abs(sum(abs(o.a) - abs(a))) #delta between old and new authority scores\n         k <- k + 1\n         }\n   return(list(h = as.vector(h), a = as.vector(a), k = k))\n   }\n\nNote that the only modification is the addition of the multiplication by the inverse of the outdegrees in line 8, which divides the standard HITS hub score by the outdegree of each hub.\nThe resulting hubs and authorities scores are:\n\n   hits.res2 <- status3(A)\n   round(hits.res2$a/max(hits.res2$a), 3)\n\n [1] 0.693 1.000 0.221 0.414 0.223 0.538 0.759 0.494 0.187 0.470 0.552 0.361\n[13] 0.174 0.498 0.180 0.395 0.451 0.817 0.174 0.375 0.893\n\n   round(hits.res2$h/max(hits.res2$h), 3)\n\n [1] 0.749 0.818 0.635 0.657 0.619 1.000 0.716 0.762 0.683 0.493 0.916 0.925\n[13] 0.639 0.972 0.543 0.835 0.842 0.508 0.590 0.642 0.604\n\n\nThis is an implementation of the “HubAvg” algorithm described by Borodin et al. (2005, 238–39).\n\n\nExclusive Authorities\nIn the same way, depending on the application, it might make more sense to assign a larger hub score to hubs that point to exclusive authorities (authorities that are sought after by a few select others) and discount the “hubness” of hubs that point to popular authorities (those who are sought after by everyone).\nWe can implement this approach—let’s call it the “AuthAvg” algorithm—with a slight modification of the status2 function similar to the one we used to create status3:\n\n   status4 <- function(w) {\n     a <- rep(1, nrow(w))  #initializing authority scores\n     d.i <- colSums(w) #indegree of each node\n     d <- 1 #initializing delta\n     k <- 0 #initializing counter\n     while (d >= 1e-10) {\n         o.a <- a #old authority scores\n         h <- w %*% o.a #new hub scores a function of previous authority scores of their out-neighbors\n         h <- h/norm(h, type = \"E\")\n         a <- (t(w) %*% h) * 1/d.i #new authority scores a function of current hub scores of their in-neighbors\n         a <- a/norm(a, type = \"E\")\n         d <- abs(sum(abs(o.a) - abs(a))) #delta between old and new authority scores\n         k <- k + 1\n         }\n   return(list(h = as.vector(h), a = as.vector(a), k = k))\n   }\n\nAnd the resulting hubs and authorities scores are:\n\n   hits.res3 <- status4(A)\n   round(hits.res3$a/max(hits.res3$a), 3)\n\n [1] 0.651 0.579 0.877 0.694 0.819 0.658 0.548 0.784 0.855 0.733 0.761 0.736\n[13] 1.000 0.738 0.808 0.797 0.768 0.600 1.000 0.847 0.524\n\n   round(hits.res3$h/max(hits.res3$h), 3)\n\n [1] 0.266 0.116 0.700 0.564 0.734 0.035 0.358 0.341 0.593 0.734 0.119 0.072\n[13] 0.283 0.150 1.000 0.171 0.200 0.869 0.532 0.561 0.523"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Social Networks Sequence",
    "section": "",
    "text": "Website containing syllabi, reading schedules, lecture notes, R code, and other instructional materials for the courses in the Social Network Analysis computational sequence (208A & 208B) at UCLA Sociology."
  },
  {
    "objectID": "loading.html",
    "href": "loading.html",
    "title": "Loading Network Data from a File",
    "section": "",
    "text": "When get network data from an archival source, and it will be in the form of a matrix or an edge list, typically in some kind of comma separated value (csv) format. Here will show how to input that into R to create an igraph network object from an outside file.\nFirst we will write the Pulp Fiction data into an edge list and save it to your local folder as a csv file.\nFirst, let’s load the data and transform it into an edgelist:\n\n   library(igraph)\n   library(networkdata)\n   g <- movie_559 #pulp fiction data\n   #install.packages(here)\n   library(here)\n   g.el <- as_edgelist(g) #transforming graph to edgelist\n   names(g.el) <- c(\"name1\", \"name2\")\n\nNow, let’s write it into disk:\n\n   write.csv(g.el, here(\"pulp.csv\"))\n\nThe write.csv function just saves an R object into a .csv file. Here the R object is “g.el” and we asked it to save just the columns which contain the name of each character. This represents the adjacency relations in the network as an edge list. We use the package here to keep track of our working directory. See here (pun intended) for details.\nNow suppose that’s the network we want to work with and it’s saved in our hard drive. To load it, we just type:\n\n   g.el <- read.csv(here(\"pulp.csv\"), \n                    col.names = c(\"name1\", \"name2\"))\n   head(g.el)\n\n  name1     name2\n1 BRETT MARSELLUS\n2 BRETT    MARVIN\n3 BRETT     ROGER\n4 BRETT   VINCENT\n5 BUDDY       MIA\n6 BUDDY   VINCENT\n\n\nWhich gives us the edge list we want now saved into an R object of class data.frame. So all we need is to convert that into an igraph object. To do that we use one of the many graph_from... functions in the igraph package. In this case, we want graph_from_edgelist because our network is stored as an edge list:\n\n   g.el <- as.matrix(g.el)\n   g <- graph_from_edgelist(g.el, directed = FALSE)\n   V(g)\n\n+ 38/38 vertices, named, from acb163f:\n [1] BRETT           MARSELLUS       MARVIN          ROGER          \n [5] VINCENT         BUDDY           MIA             BUTCH          \n [9] CAPT KOONS      ESMARELDA       GAWKER #2       JULES          \n[13] PEDESTRIAN      SPORTSCASTER #1 ENGLISH DAVE    FABIENNE       \n[17] FOURTH MAN      HONEY BUNNY     MANAGER         JIMMIE         \n[21] JODY            PATRON          PUMPKIN         RAQUEL         \n[25] WINSTON         LANCE           MAYNARD         THE GIMP       \n[29] ZED             ED SULLIVAN     MOTHER          WOMAN          \n[33] PREACHER        SPORTSCASTER #2 THE WOLF        WAITRESS       \n[37] YOUNG MAN       YOUNG WOMAN    \n\n   E(g)\n\n+ 102/102 edges from acb163f (vertex names):\n [1] BRETT      --MARSELLUS       BRETT      --MARVIN         \n [3] BRETT      --ROGER           BRETT      --VINCENT        \n [5] BUDDY      --MIA             VINCENT    --BUDDY          \n [7] BRETT      --BUTCH           BUTCH      --CAPT KOONS     \n [9] BUTCH      --ESMARELDA       BUTCH      --GAWKER #2      \n[11] BUTCH      --JULES           MARSELLUS  --BUTCH          \n[13] BUTCH      --PEDESTRIAN      BUTCH      --SPORTSCASTER #1\n[15] BUTCH      --ENGLISH DAVE    BRETT      --FABIENNE       \n[17] BUTCH      --FABIENNE        JULES      --FABIENNE       \n[19] JULES      --FOURTH MAN      VINCENT    --FOURTH MAN     \n+ ... omitted several edges\n\n\nWhich gives us back the original igraph object. Note that we specified that the graph is undirected by setting the option directed to false."
  },
  {
    "objectID": "louvain.html",
    "href": "louvain.html",
    "title": "An Agglomertive Approach to Community Detection",
    "section": "",
    "text": "library(networkdata)\n   library(igraph)\n\n\n    louvain <- function(x)  {\n      iter <- 0\n      n.moves <- 1\n      Q <- modularity_matrix(x, directed = FALSE)\n      v.names <- V(x)$name\n      while(n.moves != 0) {\n        n.moves <- 0\n        if (iter == 0) {\n          I <- diag(1, vcount(x), vcount(x)) #identity matrix\n          U <- I\n          l <- 1:vcount(x)\n          names(l) <- 1:vcount(x)\n          A <- as.matrix(as_adjacency_matrix(x))\n          vol <- sum(A)\n          }\n        node.list <- 1:vcount(x)\n        sampled.nodes <- 0\n        m <- 1\n        while (length(sampled.nodes) < 5*vcount(x)) {\n          i <- sample(node.list, 1, replace = TRUE)\n          sampled.nodes[m] <- i\n          #sampled.nodes <- sort(unique(sampled.nodes))\n          #print(sampled.nodes)\n          u.k <- U[, l[i]] #i's group\n          N <- unique(as.numeric(neighbors(x, i))) #i's neighbors\n          adj.clus <- l[N] #adjacent clusters\n          e.i <- I[, i]\n          Q.delta <- 0\n          k <- 1\n          for (j in N) { #populating Q.delta vector\n            u.l <- U[, adj.clus[k]] #j's group\n            Q.new <- (t(u.k - e.i) %*% Q %*% (u.k - e.i) + \n                      t(u.l + e.i) %*% Q %*% (u.l + e.i))\n            Q.old <- (t(u.k) %*% Q %*% u.k) + (t(u.l) %*% Q %*% u.l)\n            Q.delta[k] <- (Q.new - Q.old)/vol\n            k <- k + 1\n            } #end j for loop\n          max.Q <- max(Q.delta)\n          max.Q.pos <- which(Q.delta == max.Q)\n          if (max.Q > 0 & length(max.Q.pos) == 1) {\n            j <- N[max.Q.pos]\n            U[i, l[i]] <- 0\n            U[i, l[j]] <- 1\n            n.moves <- n.moves + 1\n            } #end if\n          if (max.Q > 0 & length(max.Q.pos) > 1) {\n            j <- N[sample(max.Q.pos, 1)]\n            U[i, l[i]] <- 0\n            U[i, l[j]] <- 1\n            n.moves <- n.moves + 1\n            } #end if\n          m <- m + 1\n          } # end main while loop\n        U <- U[, colSums(U)> 0] #eliminating zero columns\n        for (k in 1:ncol(U)) {\n          l[which(U[, k] == 1)] <- k #re-assigning node labels\n          }\n        #print(l)\n        print(n.moves)\n        A.new <- t(U) %*% A %*% U #coarsening adjacency matrix\n        x <- graph_from_adjacency_matrix(A.new, mode = \"undirected\")\n        iter <- iter + 1\n        } #end while\n      rownames(U) <- v.names\n      return(list(U, l))\n      } #end function\n  g <- movie_651\n  louvain(g)\n\n[1] 83\n[1] 7\n[1] 9\n[1] 0\n\n\n[[1]]\n                    [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]\nBEACH                  0    0    0    0    0    0    0    0    1\nBONES                  0    0    0    0    1    0    0    0    0\nCADET                  0    0    0    0    0    0    0    0    1\nCAROL                  0    1    1    0    0    0    0    0    0\nCHEKOV                 0    0    1    1    0    0    0    0    1\nCOMPUTER VOICE         0    0    0    0    0    0    1    0    0\nDAVID                  0    0    1    0    0    0    0    0    0\nHELMSMAN               0    0    0    1    0    0    0    0    1\nHURRY                  0    0    0    0    0    0    1    0    0\nINTERCOM VOICE         0    0    0    0    1    0    1    0    0\nJEDDA                  0    0    1    0    0    0    0    0    0\nJOACHIM                0    0    0    1    0    0    0    0    0\nKHAN                   0    0    0    1    0    0    0    0    0\nKIRK                   0    0    0    0    0    0    1    0    0\nKYLE                   1    0    0    0    0    0    0    0    0\nMADISON                0    0    1    0    0    0    0    0    0\nMAIN TITLE SEQUENCE    0    0    0    0    0    1    0    0    0\nPRESTON                0    0    0    0    1    0    0    0    0\nSAAVIK                 0    0    0    0    0    0    1    0    0\nSCOTTY                 0    0    0    0    1    0    0    0    0\nSPOCK                  0    0    0    0    0    0    1    0    0\nSTATIC                 0    0    0    0    0    0    1    0    0\nSULU                   0    0    0    0    0    0    1    0    0\nTERRELL                0    0    0    0    0    0    0    0    1\nUHURA                  0    0    0    0    0    0    1    1    0\n\n[[2]]\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 \n 9  5  9  3  9  7  3  9  7  7  3  4  4  7  1  3  6  5  7  5  7  7  7  9  8"
  },
  {
    "objectID": "network-lists.html",
    "href": "network-lists.html",
    "title": "Handling Graph Objects in Lists",
    "section": "",
    "text": "Sometimes network data comes pre-stored as an R list. This is typical if you have a network with multiple kinds of ties recorded on the same set of actors (and thus multiple networks), or longitudinal network data, where we collect multiple “snapshots” of the same system (containing the same or more typically a different set of actors per time slice).\nThe networkdata package contains one such data set called atp. It’s a network of Tennis players who played in grand slam or official matches of the Association of Tennis Professionals (hence ATP) covering the years 1968-2021 (Radicchi 2011).\nIn the directed graph representing each network, a tie goes from the loser to the winner of each match. Accordingly, it can be interpreted as a directed “deference” network (it would be a dominance network if it was the other way around), where actor i “defers” to actor j by getting their ass kicked by them.\nLet’s see how this list of networks works:\n\n   library(networkdata)\n   library(igraph)\n   g <- atp\n   head(g)\n\n[[1]]\nIGRAPH 08a202a DNW- 497 1213 -- ATP Season 1968\n+ attr: name (g/c), name (v/c), age (v/n), hand (v/c), country (v/c),\n| surface (e/c), weight (e/n)\n+ edges from 08a202a (vertex names):\n [1] U Unknown    ->Jose Mandarino     Alfredo Acuna->Alan Fox          \n [3] Andres Gimeno->Ken Rosewall       Andres Gimeno->Raymond Moore     \n [5] Juan Gisbert ->Tom Okker          Juan Gisbert ->Zeljko Franulovic \n [7] Onny Parun   ->Jan Kodes          Peter Curtis ->Tom Okker         \n [9] Premjit Lall ->Clark Graebner     Rod Laver    ->Ken Rosewall      \n[11] Thomas Lejus ->Nicola Pietrangeli Tom Okker    ->Arthur Ashe       \n[13] U Unknown    ->Jaidip Mukherjea   U Unknown    ->Jose Luis Arillla \n+ ... omitted several edges\n\n[[2]]\nIGRAPH c895c57 DNW- 446 1418 -- ATP Season 1969\n+ attr: name (g/c), name (v/c), age (v/n), hand (v/c), country (v/c),\n| surface (e/c), weight (e/n)\n+ edges from c895c57 (vertex names):\n [1] U Unknown           ->U Unknown        \n [2] Alejandro Olmedo    ->Ron Holmberg     \n [3] Arthur Ashe         ->Rod Laver        \n [4] Bob Carmichael      ->Jim Osborne      \n [5] Cliff Richey        ->Zeljko Franulovic\n [6] Francois Jauffret   ->Martin Mulligan  \n [7] Fred Stolle         ->John Newcombe    \n+ ... omitted several edges\n\n[[3]]\nIGRAPH d6709af DNW- 451 1650 -- ATP Season 1970\n+ attr: name (g/c), name (v/c), age (v/n), hand (v/c), country (v/c),\n| surface (e/c), weight (e/n)\n+ edges from d6709af (vertex names):\n [1] Mark Cox            ->Jan Kodes        \n [2] Charlie Pasarell    ->Stan Smith       \n [3] Cliff Richey        ->Arthur Ashe      \n [4] Francois Jauffret   ->Manuel Orantes   \n [5] Georges Goven       ->Jan Kodes        \n [6] Harald Elschenbroich->Zeljko Franulovic\n [7] Ilie Nastase        ->Zeljko Franulovic\n+ ... omitted several edges\n\n[[4]]\nIGRAPH a73a020 DNW- 459 2580 -- ATP Season 1971\n+ attr: name (g/c), name (v/c), age (v/n), hand (v/c), country (v/c),\n| surface (e/c), weight (e/n)\n+ edges from a73a020 (vertex names):\n [1] Andres Gimeno    ->Ken Rosewall   Arthur Ashe      ->Rod Laver     \n [3] Charlie Pasarell ->Cliff Drysdale Frank Froehling  ->Clark Graebner\n [5] Joaquin Loyo Mayo->Thomaz Koch    John Alexander   ->John Newcombe \n [7] John Newcombe    ->Marty Riessen  Nikola Pilic     ->Cliff Drysdale\n [9] Owen Davidson    ->Cliff Drysdale Robert Maud      ->Cliff Drysdale\n[11] Roger Taylor     ->Marty Riessen  Roy Emerson      ->Rod Laver     \n[13] Tom Okker        ->John Newcombe  Allan Stone      ->Bob Carmichael\n+ ... omitted several edges\n\n[[5]]\nIGRAPH 761ed05 DNW- 504 2767 -- ATP Season 1972\n+ attr: name (g/c), name (v/c), age (v/n), hand (v/c), country (v/c),\n| surface (e/c), weight (e/n)\n+ edges from 761ed05 (vertex names):\n [1] Jean Loup Rouyer->Stan Smith     Marty Riessen   ->Cliff Drysdale\n [3] Roy Emerson     ->Arthur Ashe    Roy Emerson     ->Arthur Ashe   \n [5] Tom Leonard     ->John Newcombe  Tom Okker       ->Arthur Ashe   \n [7] Tom Okker       ->Rod Laver      Adriano Panatta ->Andres Gimeno \n [9] Adriano Panatta ->Ilie Nastase   Allan Stone     ->John Alexander\n[11] Allan Stone     ->Marty Riessen  Andres Gimeno   ->Jan Kodes     \n[13] Andres Gimeno   ->Stan Smith     Andrew Pattison ->Ilie Nastase  \n+ ... omitted several edges\n\n[[6]]\nIGRAPH 92ff576 DNW- 592 3653 -- ATP Season 1973\n+ attr: name (g/c), name (v/c), age (v/n), hand (v/c), country (v/c),\n| surface (e/c), weight (e/n)\n+ edges from 92ff576 (vertex names):\n [1] Harold Solomon    ->Stan Smith       John Alexander    ->Stan Smith      \n [3] Patrice Dominguez ->Paolo Bertolucci Paul Gerken       ->Jimmy Connors   \n [5] Roy Emerson       ->Rod Laver        Adriano Panatta   ->Ilie Nastase    \n [7] Bjorn Borg        ->Adriano Panatta  Brian Gottfried   ->Cliff Richey    \n [9] Charlie Pasarell  ->John Alexander   Cliff Richey      ->Stan Smith      \n[11] Corrado Barazzutti->Bjorn Borg       Francois Jauffret ->Ilie Nastase    \n[13] Georges Goven     ->Manuel Orantes   Manuel Orantes    ->Ilie Nastase    \n+ ... omitted several edges\n\n\nWe create a graph object and then examine its contents, which we can see is a set of graph objects. In unnamed R lists each of the objects inside is indexed by a number in double brackets. So [[6]] just means the sixth network in the list object (corresponding to the year 1973).\nNow let’s say we wanted to compute a network statistic like density. One way to proceed would be:\n\n   edge_density(g)\n\nError in `ensure_igraph()`:\n! Must provide a graph object (provided wrong object type).\n\n\nWhich gives us a weird error about the wrong object type. The reason is that edge_density expects an igraph graph object as input, but g is not a graph object it is a list of such objects. For it to work you have to reference a particular element inside the list not the whole list.\nTo do that, we use the double bracket notation:\n\n   edge_density(g[[6]])\n\n[1] 0.01044096\n\n\nWhich gives us the density for the 1973 network.\n\nLooping Through Lists\nBut what if we wanted a table of network statistics for all the years or some subset of years? Of course, we could just type a million versions of the edge_density command or whatever, but that would be tedious. We could also write a for loop or something like that (less tedious). Even less tedious is to use the many apply functions in R that are designed to work with lists, which is a subject onto itself in R programming.\nBut here we can just use the simple version. Let’s say we wanted a vector of densities (or any other whole network statistic) for the whole 54 years. In that case, our friend sapply can do the job:\n\n   sapply(g, edge_density)\n\n [1] 0.004920653 0.007144657 0.008130081 0.012272740 0.010914671 0.010440961\n [7] 0.010567864 0.013315132 0.012088214 0.014019237 0.014135328 0.011649909\n[13] 0.011172821 0.011261426 0.012703925 0.012177336 0.012648755 0.012445937\n[19] 0.012034362 0.012351377 0.010174271 0.009772014 0.019526953 0.012236462\n[25] 0.014050245 0.015054181 0.013872832 0.014727924 0.014329906 0.013935502\n[31] 0.013962809 0.013870042 0.013665097 0.013818887 0.012551113 0.011571679\n[37] 0.012329090 0.012923683 0.011402945 0.012677988 0.012256963 0.013512884\n[43] 0.012543025 0.013661748 0.013786518 0.013679697 0.015052857 0.015075622\n[49] 0.015081206 0.014346468 0.015764351 0.020169225 0.011889114 0.016935400\n\n\nsapply is kind of a “meta” function that takes two inputs: A list, and the name of a function (which could be native, a package, or user defined); sapply then “applies” that function to each element inside the list. Here we asked R to apply the function edge_density to each element of the list of networks g and it obliged, creating a vector of length 54 containing the info.\nWe could use any igraph function, like number of nodes in the graph:\n\n   sapply(g, vcount)\n\n [1] 497 446 451 459 504 592 595 535 553 524 509 572 582 573 554 532 495 513 510\n[20] 523 596 597 405 542 509 498 520 496 502 499 497 480 486 479 497 517 505 492\n[39] 524 488 493 464 482 459 457 453 428 430 431 438 419 364 345 393\n\n\nWe could also select subset of elements inside the list. For instance this counts the number of nodes for the first five years:\n\n   sapply(g[1:5], vcount)\n\n[1] 497 446 451 459 504\n\n\nOr for years 2, 6, 8, and 12:\n\n   sapply(g[c(2, 6, 8, 12)], vcount)\n\n[1] 446 592 535 572\n\n\nNote the single bracket notation here to refer to subsets of elements in the list. Inside the brackets we could put any arbitrary vector, as long as the numbers in the vector do no exceed the length of the list.\nOf course, sometimes the functions we apply to elements of the list don’t return single numbers but vectors or other igraph objects. In that case it would be better to use lapply which is just like sapply but returns another list with the set of answers inside it.\nFor instance, let’s say we wanted the top five players for each year. In this deference network, a “top” player is one who beats many others, which means they have high indegree (lots of losers pointing at them).\nFirst we create a custom function to compute the indegree and return an ordered named vector of top 5 players:\n\n   top5 <- function(x) {\n      library(igraph)\n      t <- degree(x, mode = \"in\")\n      t <- sort(t, decreasing = TRUE)[1:5]\n      return(t)\n   }\n\nNow, we can just feed that function to lapply:\n\n   top.list <- lapply(g, top5)\n   head(top.list)\n\n[[1]]\n   Arthur Ashe      Rod Laver Clark Graebner   Ken Rosewall      Tom Okker \n            33             27             25             23             22 \n\n[[2]]\nJohn Newcombe     Tom Okker     Rod Laver    Tony Roche   Arthur Ashe \n           45            41            40            40            33 \n\n[[3]]\n      Arthur Ashe      Cliff Richey         Rod Laver        Stan Smith \n               51                49                48                45 \nZeljko Franulovic \n               42 \n\n[[4]]\n     Ilie Nastase         Tom Okker     Marty Riessen        Stan Smith \n               69                63                61                61 \nZeljko Franulovic \n               60 \n\n[[5]]\n  Ilie Nastase     Stan Smith Manuel Orantes  Jimmy Connors    Arthur Ashe \n            99             72             68             65             55 \n\n[[6]]\n Ilie Nastase     Tom Okker Jimmy Connors   Arthur Ashe    Stan Smith \n           96            81            68            63            63 \n\n\nWhich is a list of named vectors containing the number of victories of the top five players each year.\nBecause the object top.list is just a list, we can subset it just like before. Let’s say we wanted to see the top players for more recent years:\n\n   top.list[49:54]\n\n[[1]]\n   Andy Murray  Dominic Thiem  Kei Nishikori Novak Djokovic   David Goffin \n            63             55             53             50             47 \n\n[[2]]\n         Rafael Nadal          David Goffin      Alexander Zverev \n                   58                    55                    52 \nRoberto Bautista Agut         Dominic Thiem \n                   45                    43 \n\n[[3]]\n   Dominic Thiem Alexander Zverev   Novak Djokovic    Fabio Fognini \n              51               50               46               45 \n   Roger Federer \n              44 \n\n[[4]]\n   Daniil Medvedev     Novak Djokovic       Rafael Nadal Stefanos Tsitsipas \n                55                 52                 52                 49 \n     Roger Federer \n                47 \n\n[[5]]\n     Andrey Rublev     Novak Djokovic Stefanos Tsitsipas       Rafael Nadal \n                40                 36                 27                 26 \n   Daniil Medvedev \n                24 \n\n[[6]]\n   Daniil Medvedev Stefanos Tsitsipas        Casper Ruud   Alexander Zverev \n                54                 52                 52                 51 \n    Novak Djokovic \n                49 \n\n\nA series of names which make sense to you if you follow Tennis.\n\n\nNaming Lists\nFinally, sometimes it useful to name the elements of a list. In this case, for instance, having the year number would be easier to remember what’s what. For this, you can use the names command, which works via standard R assignment:\n\n   names(g) <- c(1968:2021)\n   head(g)\n\n$`1968`\nIGRAPH 08a202a DNW- 497 1213 -- ATP Season 1968\n+ attr: name (g/c), name (v/c), age (v/n), hand (v/c), country (v/c),\n| surface (e/c), weight (e/n)\n+ edges from 08a202a (vertex names):\n [1] U Unknown    ->Jose Mandarino     Alfredo Acuna->Alan Fox          \n [3] Andres Gimeno->Ken Rosewall       Andres Gimeno->Raymond Moore     \n [5] Juan Gisbert ->Tom Okker          Juan Gisbert ->Zeljko Franulovic \n [7] Onny Parun   ->Jan Kodes          Peter Curtis ->Tom Okker         \n [9] Premjit Lall ->Clark Graebner     Rod Laver    ->Ken Rosewall      \n[11] Thomas Lejus ->Nicola Pietrangeli Tom Okker    ->Arthur Ashe       \n[13] U Unknown    ->Jaidip Mukherjea   U Unknown    ->Jose Luis Arillla \n+ ... omitted several edges\n\n$`1969`\nIGRAPH c895c57 DNW- 446 1418 -- ATP Season 1969\n+ attr: name (g/c), name (v/c), age (v/n), hand (v/c), country (v/c),\n| surface (e/c), weight (e/n)\n+ edges from c895c57 (vertex names):\n [1] U Unknown           ->U Unknown        \n [2] Alejandro Olmedo    ->Ron Holmberg     \n [3] Arthur Ashe         ->Rod Laver        \n [4] Bob Carmichael      ->Jim Osborne      \n [5] Cliff Richey        ->Zeljko Franulovic\n [6] Francois Jauffret   ->Martin Mulligan  \n [7] Fred Stolle         ->John Newcombe    \n+ ... omitted several edges\n\n$`1970`\nIGRAPH d6709af DNW- 451 1650 -- ATP Season 1970\n+ attr: name (g/c), name (v/c), age (v/n), hand (v/c), country (v/c),\n| surface (e/c), weight (e/n)\n+ edges from d6709af (vertex names):\n [1] Mark Cox            ->Jan Kodes        \n [2] Charlie Pasarell    ->Stan Smith       \n [3] Cliff Richey        ->Arthur Ashe      \n [4] Francois Jauffret   ->Manuel Orantes   \n [5] Georges Goven       ->Jan Kodes        \n [6] Harald Elschenbroich->Zeljko Franulovic\n [7] Ilie Nastase        ->Zeljko Franulovic\n+ ... omitted several edges\n\n$`1971`\nIGRAPH a73a020 DNW- 459 2580 -- ATP Season 1971\n+ attr: name (g/c), name (v/c), age (v/n), hand (v/c), country (v/c),\n| surface (e/c), weight (e/n)\n+ edges from a73a020 (vertex names):\n [1] Andres Gimeno    ->Ken Rosewall   Arthur Ashe      ->Rod Laver     \n [3] Charlie Pasarell ->Cliff Drysdale Frank Froehling  ->Clark Graebner\n [5] Joaquin Loyo Mayo->Thomaz Koch    John Alexander   ->John Newcombe \n [7] John Newcombe    ->Marty Riessen  Nikola Pilic     ->Cliff Drysdale\n [9] Owen Davidson    ->Cliff Drysdale Robert Maud      ->Cliff Drysdale\n[11] Roger Taylor     ->Marty Riessen  Roy Emerson      ->Rod Laver     \n[13] Tom Okker        ->John Newcombe  Allan Stone      ->Bob Carmichael\n+ ... omitted several edges\n\n$`1972`\nIGRAPH 761ed05 DNW- 504 2767 -- ATP Season 1972\n+ attr: name (g/c), name (v/c), age (v/n), hand (v/c), country (v/c),\n| surface (e/c), weight (e/n)\n+ edges from 761ed05 (vertex names):\n [1] Jean Loup Rouyer->Stan Smith     Marty Riessen   ->Cliff Drysdale\n [3] Roy Emerson     ->Arthur Ashe    Roy Emerson     ->Arthur Ashe   \n [5] Tom Leonard     ->John Newcombe  Tom Okker       ->Arthur Ashe   \n [7] Tom Okker       ->Rod Laver      Adriano Panatta ->Andres Gimeno \n [9] Adriano Panatta ->Ilie Nastase   Allan Stone     ->John Alexander\n[11] Allan Stone     ->Marty Riessen  Andres Gimeno   ->Jan Kodes     \n[13] Andres Gimeno   ->Stan Smith     Andrew Pattison ->Ilie Nastase  \n+ ... omitted several edges\n\n$`1973`\nIGRAPH 92ff576 DNW- 592 3653 -- ATP Season 1973\n+ attr: name (g/c), name (v/c), age (v/n), hand (v/c), country (v/c),\n| surface (e/c), weight (e/n)\n+ edges from 92ff576 (vertex names):\n [1] Harold Solomon    ->Stan Smith       John Alexander    ->Stan Smith      \n [3] Patrice Dominguez ->Paolo Bertolucci Paul Gerken       ->Jimmy Connors   \n [5] Roy Emerson       ->Rod Laver        Adriano Panatta   ->Ilie Nastase    \n [7] Bjorn Borg        ->Adriano Panatta  Brian Gottfried   ->Cliff Richey    \n [9] Charlie Pasarell  ->John Alexander   Cliff Richey      ->Stan Smith      \n[11] Corrado Barazzutti->Bjorn Borg       Francois Jauffret ->Ilie Nastase    \n[13] Georges Goven     ->Manuel Orantes   Manuel Orantes    ->Ilie Nastase    \n+ ... omitted several edges\n\n\nNow instead of the useless one, two, three, etc. names, we have the actual year numbers as the names of the elements on each list.\nSo if we wanted to know the top five players for 1988 we could just type:\n\n   top5(g[[\"1988\"]])\n\n   Stefan Edberg     Andre Agassi     Boris Becker    Mats Wilander \n              63               59               52               49 \nAaron Krickstein \n              48 \n\n\nNote the double bracket notation and the fact that the name of the list is a character not a number (hence the scare quotes).\nIf we don’t want to remember the bracket business, we could also use the $ operator to refer to particular list elements:\n\n   top5(g$\"1988\")\n\n   Stefan Edberg     Andre Agassi     Boris Becker    Mats Wilander \n              63               59               52               49 \nAaron Krickstein \n              48 \n\n\nOf course, we can also use the names to subset the list. Let’s say we wanted the top five players for 1970, 1980, 1990, 2000, 2010, and 2020.\nAll we have to do is type:\n\n   decades <- c(\"1970\", \"1980\", \"1990\", \"2000\", \"2010\", \"2020\")\n   lapply(g[decades], top5)\n\n$`1970`\n      Arthur Ashe      Cliff Richey         Rod Laver        Stan Smith \n               51                49                48                45 \nZeljko Franulovic \n               42 \n\n$`1980`\n     Ivan Lendl    John Mcenroe Brian Gottfried      Bjorn Borg Eliot Teltscher \n             97              76              63              62              62 \n\n$`1990`\n  Boris Becker  Stefan Edberg     Ivan Lendl   Pete Sampras Emilio Sanchez \n            62             57             50             47             44 \n\n$`2000`\nYevgeny Kafelnikov        Marat Safin    Gustavo Kuerten      Magnus Norman \n                63                 61                 59                 58 \n    Lleyton Hewitt \n                53 \n\n$`2010`\n   Rafael Nadal   Roger Federer    David Ferrer Robin Soderling   Jurgen Melzer \n             63              54              53              53              51 \n\n$`2020`\n     Andrey Rublev     Novak Djokovic Stefanos Tsitsipas       Rafael Nadal \n                40                 36                 27                 26 \n   Daniil Medvedev \n                24 \n\n\nNote that we are back to the single bracket notation.\nWith a bit of practice, lists will become your friends!\n\n\n\n\n\nReferences\n\nRadicchi, Filippo. 2011. “Who Is the Best Player Ever? A Complex Network Analysis of the History of Professional Tennis.” PloS One 6 (2): e17249."
  },
  {
    "objectID": "pagerank.html",
    "href": "pagerank.html",
    "title": "PageRank Prestige Scoring",
    "section": "",
    "text": "The model of status distribution implied by the Eigenvector Centrality approach implies that each actor distributes the same amount of status independently of the number of connections they have. Status just replicates indefinitely. Thus, a node with a 100 friends has 100 status units to distribute to each of them and a node with a 10 friends has 10 units.\nThis is why the eigenvector idea rewards nodes who are connected to popular others more. Even though everyone begins with a single unit of status, well-connected nodes by degree end up having more of it to distribute."
  },
  {
    "objectID": "pagerank.html#a-degree-normalized-model-of-status",
    "href": "pagerank.html#a-degree-normalized-model-of-status",
    "title": "PageRank Prestige Scoring",
    "section": "A Degree-Normalized Model of Status",
    "text": "A Degree-Normalized Model of Status\nBut what if status propagated in the network proportionately to the number of connections one had? For instance, if someone has 100 friends and they only had so much time or energy, they would only have a fraction of status to distribute to others than a person with 10 friends.\nIn that case, the node with a hundred friends would only have 1/100 of status units to distribute to each of their connections while the node with 10 friends would have 1/10 units. Under this formulation, being connected to discerning others, that is people who only connect to a few, is better than being connected to others who connect to everyone else indiscriminately.\nHow would we implement this model? First, let’s create a variation of the undirected friendship nomination adjacency matrix called the \\(\\mathbf{P}\\) matrix. It is defined like this:\n\\[\n\\mathbf{P} = \\mathbf{D}_{out}^{-1}\\mathbf{A}\n\\]\nWhere \\(\\mathbf{A}\\) is our old friend the adjacency matrix, and \\(\\mathbf{D}_{out}^{-1}\\) is a matrix containing the inverse of each node outdegree along the diagonals and zeroes in every other cell.\nIn R we can create the \\(\\mathbf{D}_{out}^{-1}\\) matrix using the native diag function like this (using an undirected version of the Krackhardt high-tech managers friendship network):\n\n   library(networkdata)\n   library(igraph)\n   g <- as_undirected(ht_friends, mode = \"collapse\")\n   A <- as.matrix(as_adjacency_matrix(g))\n   D.o <- diag(1/rowSums(A))\n\nRecalling that the function rowSums gives us the row sums of the adjacency matrix, which is the same as each node’s outdegree.\nWe can check out that the \\(\\mathbf{D}_{out}^{-1}\\) indeed contains the quantities we seek by looking at its first few rows and columns:\n\n   round(D.o[1:10, 1:10], 2)\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n [1,] 0.11  0.0 0.00 0.00  0.0 0.00 0.00  0.0 0.00  0.00\n [2,] 0.00  0.1 0.00 0.00  0.0 0.00 0.00  0.0 0.00  0.00\n [3,] 0.00  0.0 0.17 0.00  0.0 0.00 0.00  0.0 0.00  0.00\n [4,] 0.00  0.0 0.00 0.14  0.0 0.00 0.00  0.0 0.00  0.00\n [5,] 0.00  0.0 0.00 0.00  0.1 0.00 0.00  0.0 0.00  0.00\n [6,] 0.00  0.0 0.00 0.00  0.0 0.14 0.00  0.0 0.00  0.00\n [7,] 0.00  0.0 0.00 0.00  0.0 0.00 0.33  0.0 0.00  0.00\n [8,] 0.00  0.0 0.00 0.00  0.0 0.00 0.00  0.2 0.00  0.00\n [9,] 0.00  0.0 0.00 0.00  0.0 0.00 0.00  0.0 0.17  0.00\n[10,] 0.00  0.0 0.00 0.00  0.0 0.00 0.00  0.0 0.00  0.12\n\n\nWe can then create the \\(\\mathbf{P}\\) matrix corresponding to the undirected version of the Krackhardt friendship network using matrix multiplication like this:\n\n   P <- D.o %*% A\n\nRecalling that %*% is the R matrix multiplication operator.\nSo the resulting \\(\\mathbf{P}\\) is the original adjacency matrix, in which each non-zero entry is equal to one divided by the outdegree of the corresponding node in each row.\nHere are the first 10 rows and columns of the new matrix:\n\n   round(P[1:10, 1:10], 2)\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n [1,] 0.00 0.11 0.00 0.11 0.00 0.00 0.00 0.11 0.00  0.00\n [2,] 0.10 0.00 0.00 0.10 0.10 0.10 0.00 0.00 0.00  0.00\n [3,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.17\n [4,] 0.14 0.14 0.00 0.00 0.00 0.00 0.00 0.14 0.00  0.00\n [5,] 0.00 0.10 0.00 0.00 0.00 0.00 0.00 0.00 0.10  0.10\n [6,] 0.00 0.14 0.00 0.00 0.00 0.00 0.14 0.00 0.14  0.00\n [7,] 0.00 0.00 0.00 0.00 0.00 0.33 0.00 0.00 0.00  0.00\n [8,] 0.20 0.00 0.00 0.20 0.00 0.00 0.00 0.00 0.00  0.20\n [9,] 0.00 0.00 0.00 0.00 0.17 0.17 0.00 0.00 0.00  0.17\n[10,] 0.00 0.00 0.12 0.00 0.12 0.00 0.00 0.12 0.12  0.00\n\n\nNote that the entries are now numbers between zero and one and the matrix is asymmetric; that is, \\(p_{ij}\\) is not necessarily equal to \\(p_{ji}\\). In fact \\(p_{ij}\\) will only be equal to \\(p_{ji}\\) when \\(k_i = k_j\\) (nodes have the same degree). Each cell in the matrix is thus equal to \\(1/k_i\\) where \\(k_i\\) is the degree of the node in row \\(i\\).\nMoreover the rows of \\(\\mathbf{P}\\) sum to one:\n\n   rowSums(P)\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n\nWhich means that the \\(\\mathbf{P}\\) matrix is row stochastic. That is the “outdegree” of each node in the matrix is forced to sum to a fixed number. Substantively this means that we are equalizing the total amount of prestige or status that each node can distribute in the system to a fixed quantity.\nThis means that nodes with a lot of out-neighbors will dissipate this quantity by distributing it across a larger number of recipients (hence their corresponding non-zero entries in the rows of \\(\\mathbf{P}\\)) will be a small number) and nodes with a few out-neighbors will have more to distribute.\nAnother thing to note is that while the sums of the \\(\\mathbf{P}\\) matrix sum to a fixed number (1.0) the sums of the columns of the same matrix do not:\n\n   round(colSums(P), 2)\n\n [1] 1.11 1.34 0.63 0.86 1.56 1.06 0.37 0.51 0.61 1.21 2.33 0.92 0.17 0.87 1.08\n[16] 0.53 2.73 0.54 1.21 0.60 0.77\n\n\nThis means that inequalities in the system will be tied to the indegree of each node in the \\(\\mathbf{P}\\) matrix, which is given by either the column sums of the matrix (as we just saw) or the row sums of the transpose of the same matrix \\(\\mathbf{P}^T\\):\n\n   round(rowSums(t(P)), 2)\n\n [1] 1.11 1.34 0.63 0.86 1.56 1.06 0.37 0.51 0.61 1.21 2.33 0.92 0.17 0.87 1.08\n[16] 0.53 2.73 0.54 1.21 0.60 0.77\n\n\nThis will come in handy in a second.\nThe \\(\\mathbf{P}\\) matrix has many interpretations, but here it just quantifies the idea that the amount of centrality each node can distribute is proportional to their degree, and that the larger the degree, the less there is to distribute (the smaller each cell \\(p_{ij}\\) will be). Meanwhile, it is clear that nodes that are pointed to by many other nodes who themselves don’t point to many others have a larger indegree in \\(\\mathbf{P}\\).\nNow we can just adapt the the model of status distribution we used for eigenvector centrality but this time using the \\(\\mathbf{P}\\) rather than the \\(\\mathbf{A}\\) matrix. Note that because we are interested in the status that comes into each node we use the transpose of \\(\\mathbf{P}\\) rather than \\(\\mathbf{P}\\), just like we did for the Bonacich (1972) status score.\nHere’s our old status game function:\n\n   status1 <- function(w) {\n      x <- rep(1, nrow(w)) #initial status vector set to all ones of length equal to the number of nodes\n      d <- 1 #initial delta\n      k <- 0 #initializing counter\n      while (d > 1e-10) {\n          o.x <- x #old status scores\n          x <- w %*% o.x #new scores a function of old scores and adjacency matrix\n          x <- x/norm(x, type = \"E\") #normalizing new status scoress\n          d <- abs(sum(abs(x) - abs(o.x))) #delta between new and old scores\n          k <- k + 1 #incrementing while counter\n      }\n   return(as.vector(x))\n   }\n\nAt each step, the status of a node is equivalent to the sum of the status scores of their in-neighbors, with more discerning in-neighbors passing along more status than less discerning ones:\n\n   s2 <- status1(t(P))\n   s2 <- s2/max(s2)\n   round(s2, 3)\n\n [1] 0.500 0.556 0.333 0.389 0.556 0.389 0.167 0.278 0.333 0.444 0.778 0.444\n[13] 0.111 0.333 0.500 0.278 1.000 0.222 0.556 0.278 0.333\n\n\nWhat if I told you that these numbers are the same as the leading eigenvector of \\(\\mathbf{P}^T\\)?\n\n   s2 <- abs(eigen(t(P))$vector[, 1])\n   s2 <- s2/max(s2)\n   round(s2, 3) \n\n [1] 0.500 0.556 0.333 0.389 0.556 0.389 0.167 0.278 0.333 0.444 0.778 0.444\n[13] 0.111 0.333 0.500 0.278 1.000 0.222 0.556 0.278 0.333\n\n\nAnd, of course, the (normalized) scores produced by this approach are identical to those computed by the page_rank function in igraph with “damping factor” (to be explained in a second ) set to 1.0:\n\n   pr <- page_rank(g, damping = 1, algo = \"arpack\")$vector\n   round(pr/max(pr), 3)\n\n [1] 0.500 0.556 0.333 0.389 0.556 0.389 0.167 0.278 0.333 0.444 0.778 0.444\n[13] 0.111 0.333 0.500 0.278 1.000 0.222 0.556 0.278 0.333\n\n\nSo the distributional model of status is the same one implemented in the PageRank algorithm!"
  },
  {
    "objectID": "pagerank.html#pagerank-as-a-markov-difussion-model",
    "href": "pagerank.html#pagerank-as-a-markov-difussion-model",
    "title": "PageRank Prestige Scoring",
    "section": "PageRank as a Markov Difussion Model",
    "text": "PageRank as a Markov Difussion Model\nRemember how we just said that there are multiple ways of thinking about \\(\\mathbf{P}\\)? Another way of thinking about the \\(\\mathbf{P}\\) matrix is as characterizing the behavior of a random walker in the graph. At any time point \\(t\\) the walker (a piece of information, a virus, or status itself) sits on node \\(i\\) and the with probability \\(p_{ij}\\) jumps to \\(j\\), who is one of node \\(i\\)’s out-neighbors. The probabilities for each \\(i\\) and \\(j\\) combination are stored in the matrix \\(\\mathbf{P}\\).\nSo our status game can best be understood as a special case of a diffusion game, where what’s being diffused through the network is status itself. Let’s see how this would work.\nImagine we want to spread something through the Krackhardt managers friendship network like a rumor or a piece of information. We start with a seed node \\(i\\) and then track “where” the rumor is at each time step in the network (where the location is a person in the network). The rules of the game are the Markov diffusion model we described above. At each time step the rumor sits on some \\(j\\) and it diffuses to one of \\(j\\)’s neighbors \\(k\\) with probability \\(p_{jk}\\). Where the rumor has been before that time step does not affect where it goes in the present.\nThis sequence of transmission events is called a markov chain, and when it happens in a graph it is called a random walk on the graph. The node at which the rumor sits at a given time \\(t\\) is called the state of the markov chain at \\(t\\). For instance, the following function prints out every state of the markov chain for some series of steps \\(q\\), given a network transition matrix \\(\\mathbf{P}\\) (the w argument in the function):\n\n   markov.chain1 <- function(w, seed = 1, q = 100) {\n      state <- 0 #initializing state vector\n      i <- seed #setting seed node\n      for (t in 1:q) {\n         state[t] <- sample(1:ncol(w), 1, prob = w[i, ]) #new state of the chain\n         i <- state[t] #new source node\n      }\n   return(state)\n   }\n\nThe function above sets the “seed” node to that given in the argument seed (by default, node 1) in line 2. Then, in line 4, it enters the for loop to run q times (in this case 100 times). In line 6 the state vector at t is set to a random node \\(j\\) sampled from the \\(\\mathbf{P}\\) matrix with probability equal to the entry \\(p_{ij}\\) in the \\(i^{th}\\) row of the matrix.\nFor instance when it comes to first node that row looks like:\n\n   round(P[1, ], 2)\n\n [1] 0.00 0.11 0.00 0.11 0.00 0.00 0.00 0.11 0.00 0.00 0.11 0.11 0.00 0.00 0.11\n[16] 0.11 0.11 0.00 0.11 0.00 0.00\n\n\nWhich means that nodes {2, 4, 8, 11, 12, 15, 16, 17, 19} (the neighbors of node 1) have an 11% chance each of being sampled in line 6 and the other ones have no chance. Then in line 7 the new source node is set to whatever neighbor of \\(i\\) was sampled in line 6.\nHere’s a markov chain state sequence of length 100 from the managers friendship network, starting with node 1 as the seed:\n\n   markov.chain1(P)\n\n  [1]  4  8 11 13 11  9 17 20 17 14 17  2 16 10  9 11  5 13  5 15 14 15 17  6 17\n [26] 20 17 21 17  7 14 17  3 10 12 10  3 10 17  5 13  5  2 17 14 17 16 17  1 12\n [51] 17 14  5 13  5  9 17 16  2  1  4 12  4 17 16  1  4 16 10 16 10  9 11 20 19\n [76] 14  7 17 19 11 18 11  5 13  5 19  2  1  4 12  6 12 19 17 14 15 17  8 11  5\n\n\nWe can of course create a longer one by changing the q argument while setting the seed node to 5:\n\n   markov.chain1(P, q = 500, seed = 5)\n\n  [1]  2  4 17 16  1 19  3 17  7 17  1 12  6 21 12  4 17  8 10  5 10  9 17  7 17\n [26] 21  2 19 20 10 17 11  5  2 21 17  8 11  5 13  5 19  1  4  2  5 17  5  9 15\n [51]  3 19  5 11 17  5 21 17 15  3 10  8 10  3 19  3 14 19  2 17  1  8 11 18 21\n [76]  6  2 17  8 11  2 21 17 12  1 17 16 10  9 10 16  4 17  5 19  3 17 19  2  1\n[101] 17 19  5 14 19 15  1  8  1 12 21  5  9 11 19 20 10 17  9 11 12  6 15 14 19\n[126]  3 19 17  3 15 17 11  3 19  3 19 12 17  6 17 14 15  6 12  4  1 17  9 11 13\n[151]  5 21  2  6 12 11 20 11  4 11  5 10 20 19  5 15 14  3 11 20 18 20 19 15  9\n[176] 11 20 18 11 19  5  2 16  2  6  2 11 19  2 21  2 11  1  8 10 12  6  2  5 13\n[201]  5 14  5 15  5 13  5 21 17  8 17 14  7 14  3 10  8 10  8  1 17  2 16  2 19\n[226] 17 21 12 11 19 12 17 20 17  2  6  7 17  4 17  2  1  2 16  4 17 11 20 17  5\n[251] 15  9 10  5 14 17  3 11  1 16 10 17  8  4 12 21  6 17  4 11 17 15 19  3 15\n[276] 17 19 17 15 11  2  4 12  4  2  4  1 17 16  4  8 10 20 17  8  4 17 20 19  2\n[301] 16  2 21  5  9  5 10 12 17  6  2  4  1 12 17 16 17  5 11  3 19  5 11 19 11\n[326]  2 16  4  8  1 12 19  2  5 11  8 11 17  1 19 17 20 18  2 19  1 17 15  5 21\n[351]  5 11 13  5 13 11 20 10 16 17 10  8 10 16 17  5 10 12 17  8  4  2 19 14 15\n[376] 19  3 17 15  9 11 20 17  9  5 15 14 15 14  7 17  1 19  5 11  8 11 20 18 21\n[401] 12 17  3 14 15 19 20 10  8  1 16  2 11  4  2  6 15  5 21  2  1 15  1 16  4\n[426] 11  2 17  3 17  6  2 17 20 19 15  1  8 11  4  2 19 15 11  4 12 10  8 11  1\n[451]  4  8 11  9 10 17  4  8  4 17  2  4  2 11 17 20 19 12 11 15 11  4 11  8 10\n[476]  3 11 20 17 20 11  1 17  7 14 15  3 15 11 17  4 16  4  2 18 11  3 17  7 17\n\n\nNote that one thing that happens here is that the rumor goes through some nodes more often than others. Another thing that you may be thinking is that the odds that a node will keep repeating itself in this chain has to do with how many neighbors they have since that increases the chances that they will be chosen in the sample line of the function. If you think that, you will be right!\nOne thing we can do with the long vector of numbers above is compute the probability that the chain will be in some state or another after a number of steps \\(q\\). To do that, all we have to do is find out the number of times each of the numbers (from one through twenty one) repeats itself, and then divide by the total number of steps.\nIn R we can do that like this, using q = 50000:\n\n   states <- markov.chain1(P, q = 50000, seed = 1)\n   count <- table(states)\n   p <- count/length(states)\n   names(p) <- names(count)\n   round(p, 2)\n\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n0.06 0.07 0.04 0.04 0.06 0.04 0.02 0.03 0.04 0.05 0.09 0.05 0.01 0.04 0.06 0.03 \n  17   18   19   20   21 \n0.11 0.03 0.06 0.03 0.04 \n\n\nLine 1 computes the states of the markov chain after 50000 iterations using our markov.chain1 function. Then line 2 uses the native R function table as a handy trick to compute how many times each node shows up in the chain stored in the count object:\n\n   count\n\nstates\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n2830 3260 1810 2203 3187 2246  952 1582 1884 2463 4459 2510  645 1826 2756 1607 \n  17   18   19   20   21 \n5648 1318 3205 1644 1965 \n\n\nFinally, line 4 divides these numbers by the length of the chain to get the probability.\nNote that the numbers stored in the p vector are readily interpretable. For instance, the 0.06 in the first spot tells us that if we were to run this chain many times and check where the rumor is at step fifty-thousand, there is 6% chance that the rumor will be sitting on node 1, while there is a 11% chance that it would be sitting on node 17, a 3% chance that it will be on node 18, and so forth.\nLike well behaved probabilities, these numbers sum to 1.0:\n\n   sum(p)\n\n[1] 1\n\n\nWe can incorporate these steps in to a new and improved function like thus:\n\n   markov.chain2 <- function(w, seed = 1, q = 100) {\n      state <- 0\n      i <- seed\n      for (t in 1:q) {\n         state[t] <- sample(1:ncol(w), 1, prob = w[i, ])\n         i <- state[t]\n      }\n   count <- table(states)\n   p <- count/length(states)\n   names(p) <- names(count)\n   return(p)\n   }\n\nWhich now does everything in one step:\n\n   markov.chain2(P, q = 500)\n\n      1       2       3       4       5       6       7       8       9      10 \n0.05660 0.06520 0.03620 0.04406 0.06374 0.04492 0.01904 0.03164 0.03768 0.04926 \n     11      12      13      14      15      16      17      18      19      20 \n0.08918 0.05020 0.01290 0.03652 0.05512 0.03214 0.11296 0.02636 0.06410 0.03288 \n     21 \n0.03930 \n\n\nThere is another way to compute these probabilities more directly from the \\(\\mathbf{P}\\) matrix. The basic idea is that at any time \\(t\\), the distribution of probabilities across nodes in the network stored in the vector \\(\\mathbf{x}\\) is given by:\n\\[\n\\mathbf{x}(t) = \\mathbf{P}^T\\mathbf{x}(t-1)\n\\]\nWith the initial probability vector given by:\n\\[\n\\mathbf{x}(0) = \\mathbf{e}^{(i)}\n\\]\nWhere \\(e^{(i)}\\) is a vector containing all zeros except for the \\(i^{th}\\) spot, where it contains a one, indicating the initial seed node.\nHere’s an R function that implements this idea:\n\n   markov.chain3 <- function(w, init = 1, steps = 100) {\n      x <- rep(0, nrow(w))\n      x[init] <- 1\n      P.t <- t(w)\n      for (t in 1:steps) {\n         x <- P.t %*% x\n         }\n   x <- as.vector(x)\n   names(x) <- 1:ncol(w)\n   return(x)\n   }\n\nAnd we can see what it spits out:\n\n   p3 <- markov.chain3(P)\n   round(p3, 2)\n\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n0.06 0.06 0.04 0.04 0.06 0.04 0.02 0.03 0.04 0.05 0.09 0.05 0.01 0.04 0.06 0.03 \n  17   18   19   20   21 \n0.11 0.03 0.06 0.03 0.04 \n\n\nWhich are the same as our more complicated function above.\nNow you may have noticed this already, but these are the same numbers produced by the the PageRank status game!\n\n   names(pr) <- 1:21\n   round(pr, 2)\n\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n0.06 0.06 0.04 0.04 0.06 0.04 0.02 0.03 0.04 0.05 0.09 0.05 0.01 0.04 0.06 0.03 \n  17   18   19   20   21 \n0.11 0.03 0.06 0.03 0.04 \n\n\nThis gives us another (and perhaps) more intuitive interpretation of what the PageRank prestige ranking is all about. Nodes have more prestige if they are more “central” in a network where something is spreading via a random walk process. Higher ranked nodes will be visited more often by the random walker, less-highly-ranked nodes less.\nNote that if a random walker is just a web surfer then it makes sense that a more highly visited page should be more prestigious (and show up toward the top in your Google search) than a less frequently visited page (Brin and Page 1998)."
  },
  {
    "objectID": "pagerank.html#pagerank-with-damping-and-teleportation-in-directed-graphs",
    "href": "pagerank.html#pagerank-with-damping-and-teleportation-in-directed-graphs",
    "title": "PageRank Prestige Scoring",
    "section": "PageRank with Damping and Teleportation in Directed Graphs",
    "text": "PageRank with Damping and Teleportation in Directed Graphs\nPageRank of course was designed to deal with directed graphs (like the World Wide Web). So let’s load up the version of the Krackhardt’s Managers data that contains the advice network which is an unambiguously directed relation.\n\n   g <- ht_advice\n   A <- as.matrix(as_adjacency_matrix(g))\n\nA plot of the advice network is shown in Figure 1.\n\n\n\n\n\nFigure 1: Krackhardt’s managers network\n\n\n\n\nWe then compute the \\(\\mathbf{P}\\) matrix corresponding to this network:\n\n   D.o <- diag(1/rowSums(A))\n   P <- D.o %*% A \n\nOne issue that arises in computing the \\(\\mathbf{P}\\) for directed graphs is that there could be nodes with no out-neighbors (so-called sink nodes) or like node 6 in Figure 1, who has just one out-neighbor (e.g., seeks advice from just one person), in which case the probability is 1.0 that if the random walker is at node 6 it will go to node 21.\nTo avoid this issue the original designers of the PageRank algorithm (Brin and Page 1998) added a “fudge” factor: That is, with probability \\(\\alpha\\) the random walker should hop from node to node following the directed links in the graph. But once in a while with probability \\(1-\\alpha\\) the walker should decide to “teleport” (with uniform probability) to any node in the graph whether it is an out-neighbor of the current node or not.\nHow do we do that? Well we need to “fix” the \\(\\mathbf{P}\\) matrix to allow for such behavior. So instead of \\(\\mathbf{P}\\) we estimate our distributive status model on the matrix \\(\\mathbf{G}\\) (yes, for Google):\n\\[\n   \\mathbf{G} = \\alpha \\mathbf{P} + (1 - \\alpha) \\mathbf{E}\n\\]\nWhere \\(\\mathbf{E}\\) is a matrix of the same dimensions as \\(\\mathbf{P}\\) but containing \\(1/n\\) in every cell indicating that every node has an equal chance of being “teleported” to.\nSo, fixing \\(\\alpha = 0.85\\) (the standard value chosen by Brin and Page (1998) in their original paper) our \\(\\mathbf{G}\\) matrix would be:\n\n   n <- vcount(g)\n   E <- matrix(1/n, n, n)\n   G <- (0.85 * P) + (0.15 * E)\n\nAnd then we just play our status distribution game on the transpose of \\(\\mathbf{G}\\):\n\n   s3 <- round(status1(t(G)), 3)\n   round(s3/max(s3), 3)\n\n [1] 0.302 0.573 0.165 0.282 0.100 0.437 0.635 0.255 0.092 0.180 0.237 0.242\n[13] 0.087 0.268 0.097 0.168 0.258 0.440 0.087 0.200 1.000\n\n\nWhich is the same answer you would get from the igraph function page_rank by setting the “damping” parameter to 0.85:\n\n   pr <- page_rank(g, damping = 0.85, algo = \"arpack\")$vector\n   round(pr/max(pr), 3)\n\n [1] 0.302 0.573 0.165 0.281 0.100 0.437 0.635 0.256 0.091 0.180 0.237 0.242\n[13] 0.086 0.269 0.097 0.169 0.258 0.440 0.086 0.200 1.000\n\n\nWe can see therefore that the damping parameter simply controls the extent to which the PageRank ranking is driven by the directed connectivity of the \\(\\mathbf{P}\\) matrix, versus a stochastic or random component."
  },
  {
    "objectID": "prestige-two-mode-gen.html",
    "href": "prestige-two-mode-gen.html",
    "title": "Generalized Prestige Scoring in Two-Mode Networks",
    "section": "",
    "text": "In the lecture note on Status and Prestige in Two-Mode Netwoks we examine various ways of assigning scores to the row and column objects of the biadjacency matrix to derive status scores for the nodes of a two-mode network.\nThe dual scoring approach due to Bonacich (1991) (the dual “Eigenvector Centrality”) serves as the standard here, with other approaches (e.g., PageRank-style scoring) serving as deviations and tweaks on the general formula. All can be modeled as the “prismatic” (Podolny 2001) distribution of status within a system, where nodes in one mode get status from the status of the nodes in each mode and vice versa.\nIn formulese, the iterative two-mode network status scoring can formalized as follows. At each iteration \\(q\\), the vector of status scores for the row nodes \\(\\mathbf{s}^R\\) and the column nodes \\(\\mathbf{s}^C\\) is given by:\n\\[\ns^R_i(q) = \\sum_j\\mathbf{A}_{ij}s^C_j(q-1)\n\\tag{1}\\]\n\\[\ns^C_j(q) = \\sum_i\\mathbf{A}_{ij}s^R_i(q-1)\n\\tag{2}\\]\nWhere \\(\\mathbf{A}\\) is the two-mode network’s biadjacency matrix, and with the restriction that at the initial step \\(\\mathbf{s}(0)^C = \\mathbf{1}\\) where \\(\\mathbf{1}\\) is the all ones vector of length equals to the number of columns of the biadjacency matrix \\(\\mathbf{A}\\).\nAt each iteration \\(q > 0\\) we normalize both score vectors:\n\\[\n\\mathbf{s}^R(q) = \\frac{\\mathbf{s}^R(q)}{\\left\\langle\\mathbf{s}^R(q)\\right\\rangle}\n\\tag{3}\\]\n\\[\n\\mathbf{s}^C(q) = \\frac{\\mathbf{s}^C(q)}{\\left\\langle\\mathbf{s}^C(q)\\right\\rangle}\n\\tag{4}\\]\nWhere \\(\\left\\langle \\mathbf{s} \\right\\rangle\\) is the mean of the vector scores, indicating that we normalize the status score vector so that its expected value equals one (we could use other normalizations like the maximum, sum, or Euclidean norm).\nAs noted before, one way to generalize the Bonacich scoring is by substituting some other matrix other than \\(\\mathbf{A}\\) like the row-stochastic matrix \\(\\mathbf{P}\\) into the above equations and let the process run until convergence.\nIn a technical paper, Pugliese, Zaccaria, and Pietronero (2016, 1909) hint at an alternative way to generalize the usual status scoring in two-mode networks that involves changing the status scores themselves while keeping the standard biadjacency matrix \\(\\mathbf{A}\\) in place.\nTheir basic idea is that at each time step in the iteration we can substitute the following for the current status scores:\n\\[    \ns^R_i(q) = \\left(\\sum_j\\mathbf{A}_{ij}\\left[s^C_j(q-1)\\right]^{-\\delta}\\right)^{-\\frac{1}{\\delta}}\n\\tag{5}\\]\n\\[\n    s^C_j(q) = \\left(\\sum_i\\mathbf{A}_{ij}\\left[s^R_i(q-1)\\right]^{-\\gamma}\\right)^{-\\frac{1}{\\gamma}}\n\\tag{6}\\]\nThis generalization works like this: When the parameter \\(\\gamma = 1\\) it has the effect of inverting the row object’s status scores (\\(s^R_i(q-1)\\)) that go into the calculation of the column object’s status (inside the parentheses), meaning that column objects that connect to low status row objects receive more status. However, the same parameter outside the parentheses reverses the first inversion, such that column objects receive more status when they connect almost exclusively to high-status row objects and shun the low status ones.\nThe \\(\\delta\\) parameters work similarly. When \\(\\delta = 1\\) the column object’s status scores (\\(s^C_i(q-1)\\)) that go into the calculation of the focal row object’s status are reversed, so that row objects receive more status when they connect to low status row objects (inside the parentheses). The same parameter (outside the parentheses) reverses this inversion, such that the status of a given row object increases when they connect to high status column objects and shun the low status ones.\nNote that when both parameters \\(\\{\\gamma, \\delta\\} = -1\\) the equations above reduce to the usual Bonacich dual scoring. Different parameter value combinations \\(\\{\\gamma, \\delta\\}\\) in the \\(\\{+1, -1\\}\\) range instantiate different ways in which status operates in a given system.\nWhy would we want to generalize status scoring in this way? Recall that the basic principle of Bonacich scoring is based on the equation of status and popularity/activity. In the canonical case of persons and groups (Breiger 1974), an event receives status from being attended by high-status individuals and an individual receives status from being affiliated with a high status event; in each case, status from the point of view of the event means having highly active members, and from the point of view of the individual it means being affiliated with popular events."
  },
  {
    "objectID": "prestige-two-mode-gen.html#column-object-contamination",
    "href": "prestige-two-mode-gen.html#column-object-contamination",
    "title": "Generalized Prestige Scoring in Two-Mode Networks",
    "section": "Column Object Contamination",
    "text": "Column Object Contamination\nBut status may not always work this way. Consider the world-economic network linking countries to the products they have a competitive advantage in producing. Analysts noticed that the most developed countries produce both “complex” (i.e., high status) products that only a select few of other highly developed economies produce (like semiconductors) and also less “complex” (i.e., low status, like extractive natural resources) products that the other less developed economics produce (Tacchella et al. 2012). That means that the “complexity” (i.e., status score) of a product cannot be derived simply taking a summary (e.g., sum or average) of the status score of the countries that produce it, because high status countries engage in both high and low status forms of production.\nIn the more general and sociologically apposite case of persons and groups (Breiger 1974), an equivalent situation would go as follows. Imagine there is a set of elite women and a set of elite events that only the elite women attend. However, elite women are also endowed with a spirit of noblesse oblige, which means that the most elite of them also attend non-elite events. This means that when determining the status of the events it is not very informative to know that elites affiliate with them; rather, we should weigh more heavily whether non-elites affiliate with an event in determining an event’s status, such that as the number of non-elite women who affiliate with an event increases, a given event’s status is downgraded in a non-linear way. In formal terms, the status of column objects is heavily contaminated by having non-elite row-object affiliates, which we may refer to as a column object contamination system.\nNote that the aforementioned dynamics would apply to any status system in which elites are likely to have both elite and non-elite affiliations, and in which elite status is not necessarily threatened by having such non-elite affiliations. However, in this system the status of objects of affiliation (e.g., groups, events, cultural genres) is negatively and steeply affected by having non-elite affiliates. Such a system approximates the description of the “omnivore” regime that, as argued by cultural sociologists, organizes contemporary cultural consumption choices.\nOne approach to modeling this type of elite-omnivorousness status system is to set \\(\\delta = -1\\) and \\(\\gamma = 1\\) in Equation 5 and Equation 6 so that a column object’s status is the sum of the reciprocal of the inverse of the status of the row objects affiliated with it. This means that a column object’s status decreases non-linearly when it is affiliated with low status row objects, and this feeds back into the calculation of each row object’s status."
  },
  {
    "objectID": "prestige-two-mode-gen.html#row-object-contamination",
    "href": "prestige-two-mode-gen.html#row-object-contamination",
    "title": "Generalized Prestige Scoring in Two-Mode Networks",
    "section": "Row Object Contamination",
    "text": "Row Object Contamination\nSetting \\(\\delta = 1\\) and \\(\\gamma = -1\\) in Equation 5 and Equation 6 produce scores consistent with a different status distribution system. This system operates according to the same “status contamination” logic we just described, but it does so asymmetrically in determining the status of the row objects, not the column objects in the biadjacency matrix. In formal terms, the status of row objects is heavily contaminated by having non-elite column-object affiliations; accordingly, we may refer to this status accounting process as a row object contamination system.\nHow would this work? In the case of person and groups, imagine a status system in which the status of persons is fragile, so that any affiliation with low status events will drive a person’s status downward non-linearly. This then feeds back into the determination of each event’s status, such that events likely to be attended by these contaminated individuals themselves rapidly lose status. Note, however, that since we are not transforming the way we calculate status for events, high-status in the case of events reverts to Bonacich-style popularity among high status persons; the highest status events are those who draw a relatively big crowd of high-status people.\nThis status system is therefore in many ways the opposite of the elite omnivorousness one. At the level of individuals, this system rewards being picky about which event to attend, but this pickiness is driven by event popularity. This means that rather than being “omnivores,” the highest status individuals in this accounting system will be popularity seeking univores; that is, people who restrict their attendance only to a select set of highly popular events. For instance, social systems like high-schools or fashion (were status is determined by selecting the most popular objects) may operate in this way."
  },
  {
    "objectID": "prestige-two-mode-gen.html#contaminating-dualities",
    "href": "prestige-two-mode-gen.html#contaminating-dualities",
    "title": "Generalized Prestige Scoring in Two-Mode Networks",
    "section": "Contaminating Dualities",
    "text": "Contaminating Dualities\nOf course, it is likely that the most empirically accurate model of really existing status systems combines the two logics just described; that is following the principle of duality (Breiger 1974), the status of persons is contaminated (and thus non-linearly driven downwards) by affiliation with low status objects, and the status of objects is contaminated by affiliation with low status persons. This is what Wood and Ashby (2025) refer to as the principle of “contaminating dualities,” which mutually feed into one another in determining the status of both persons and groups.\nAccordingly, to model the contaminating dualities status accounting system, we would have to set both \\(\\delta = 1\\) and \\(\\gamma = 1\\) in Equation 5 and Equation 6.\nThe generalized status scoring model above thus allows us to consider analytically distinct status distribution regimes, which we can summarize as in Table 1.\n\n\nTable 1: Parameter settings for different status distribution regimes\n\n\n\\(\\gamma\\)\n\\(\\delta\\)\nStatus Distribution Regime\n\n\n\n\n\n\n-1\n-1\nSymmetric Duality of Status via Activity and Popularity\n\n\n\n\n-1\n1\nAsymmetric Row Low-Status Contamination for Column Objects\n\n\n\n\n-1\n1\nAsymmetric Column Low-Status Contamination for Row Objects\n\n\n\n\n1\n1\nContaminating Dualities for Row and Column Objects\n\n\n\n\n\n\nLet’s see how this would work with real data. Let’s load up the trusty Southern Women (SW) data:\n\n   library(networkdata)\n   library(igraph)\n   g <- southern_women\n   A <- as.matrix(as_biadjacency_matrix(g))\n\nHere’s the SW biadjacency matrix in table form:\n\n\n\n\nTable 2:  Southern Women Data Biadjacency Matrix. \n \n  \n     \n    9/16 \n    4/8 \n    3/15 \n    2/25 \n    5/19 \n    4/12 \n    4/7 \n    6/10 \n    9/26 \n    2/23 \n    6/27 \n    3/2 \n    11/21 \n    8/3 \n  \n \n\n  \n    EVELYN \n    1 \n    1 \n    0 \n    1 \n    1 \n    1 \n    0 \n    0 \n    1 \n    0 \n    1 \n    1 \n    0 \n    0 \n  \n  \n    THERESA \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    0 \n    0 \n    1 \n    0 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    NORA \n    0 \n    1 \n    1 \n    0 \n    1 \n    0 \n    1 \n    1 \n    0 \n    1 \n    0 \n    0 \n    1 \n    1 \n  \n  \n    LAURA \n    1 \n    0 \n    1 \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n  \n  \n    BRENDA \n    1 \n    0 \n    1 \n    1 \n    1 \n    1 \n    0 \n    0 \n    1 \n    0 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    SYLVIA \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n  \n  \n    KATHERINE \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n  \n  \n    HELEN \n    1 \n    0 \n    1 \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    CHARLOTTE \n    0 \n    0 \n    1 \n    1 \n    0 \n    1 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    FRANCES \n    1 \n    0 \n    0 \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    ELEANOR \n    1 \n    0 \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    RUTH \n    1 \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    VERNE \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    MYRNA \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    PEARL \n    1 \n    1 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    DOROTHY \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    OLIVIA \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    FLORA \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n\n\n* Row and Columns Ordered by Degree Centrality.\n\n\n\n\n\nAnd here’s a function that modifies the status distribution game function to incorporate generalized status scores parameters:\n\n   tm.status <- function(w, gamma = -1, delta = -1) {\n      y <- matrix(1, ncol(w), 1) #initial group status column vector set to a constant\n      z <- t(w)\n      epsilon <- 1 \n      k <- 0\n      while (epsilon > 1e-10) {\n         o.y <- y \n         x <- (w %*% (o.y^-delta))^-(1/delta) #generalized status scores for people\n         x <- x/norm(x, type = \"F\") #normalizing new people status scores \n         y <- (z %*% (x^-gamma))^-(1/gamma) #generalized status scores for groups\n         y <- y/norm(y, type = \"F\") #normalizing new group status scores \n         if (k > 1) {\n            epsilon <- abs(sum(abs(y) - abs(o.y))) \n            }\n         k <- k + 1\n         }\n   return(list(p.s = x, g.s = y, k = k))\n   }\n\nWe then run the function for the four status distribution regimes:\n\n   s1 <- tm.status(A, gamma = -1, delta = -1) #Bonacich Scores\n   s2 <- tm.status(A, gamma = 0.5, delta = -1) #column contamination\n   s3 <- tm.status(A, gamma = -0.5, delta = 1) #row contamination\n   s4 <- tm.status(A, gamma = 0.5, delta = 1) #double contamination\n\nNow we check out the different status orderings. For people, this looks like this:\n\n\n\n\nTable 3:  Status Scores for Persons \n \n  \n    Person \n    Bonacich \n    Person \n    Col. Contamination \n    Person \n    Row Contamination \n    Person \n    Double Contamination \n  \n \n\n  \n    THERESA \n    1.00 \n    NORA \n    1.00 \n    DOROTHY \n    1.00 \n    OLIVIA \n    1.00 \n  \n  \n    EVELYN \n    0.90 \n    SYLVIA \n    1.00 \n    PEARL \n    0.53 \n    FLORA \n    1.00 \n  \n  \n    BRENDA \n    0.84 \n    KATHERINE \n    1.00 \n    RUTH \n    0.30 \n    CHARLOTTE \n    0.61 \n  \n  \n    LAURA \n    0.83 \n    HELEN \n    0.02 \n    ELEANOR \n    0.29 \n    DOROTHY \n    0.41 \n  \n  \n    SYLVIA \n    0.75 \n    MYRNA \n    0.02 \n    FRANCES \n    0.20 \n    HELEN \n    0.37 \n  \n  \n    NORA \n    0.71 \n    VERNE \n    0.00 \n    CHARLOTTE \n    0.06 \n    FRANCES \n    0.36 \n  \n  \n    RUTH \n    0.64 \n    OLIVIA \n    0.00 \n    BRENDA \n    0.02 \n    NORA \n    0.36 \n  \n  \n    ELEANOR \n    0.62 \n    FLORA \n    0.00 \n    THERESA \n    0.02 \n    MYRNA \n    0.35 \n  \n  \n    KATHERINE \n    0.59 \n    EVELYN \n    0.00 \n    LAURA \n    0.01 \n    PEARL \n    0.34 \n  \n  \n    VERNE \n    0.59 \n    LAURA \n    0.00 \n    EVELYN \n    0.01 \n    KATHERINE \n    0.34 \n  \n  \n    FRANCES \n    0.56 \n    THERESA \n    0.00 \n    OLIVIA \n    0.00 \n    ELEANOR \n    0.31 \n  \n  \n    HELEN \n    0.54 \n    BRENDA \n    0.00 \n    FLORA \n    0.00 \n    VERNE \n    0.29 \n  \n  \n    MYRNA \n    0.50 \n    CHARLOTTE \n    0.00 \n    VERNE \n    0.00 \n    RUTH \n    0.27 \n  \n  \n    PEARL \n    0.49 \n    FRANCES \n    0.00 \n    MYRNA \n    0.00 \n    LAURA \n    0.27 \n  \n  \n    CHARLOTTE \n    0.45 \n    ELEANOR \n    0.00 \n    HELEN \n    0.00 \n    BRENDA \n    0.27 \n  \n  \n    DOROTHY \n    0.35 \n    RUTH \n    0.00 \n    SYLVIA \n    0.00 \n    SYLVIA \n    0.27 \n  \n  \n    OLIVIA \n    0.19 \n    PEARL \n    0.00 \n    KATHERINE \n    0.00 \n    EVELYN \n    0.25 \n  \n  \n    FLORA \n    0.19 \n    DOROTHY \n    0.00 \n    NORA \n    0.00 \n    THERESA \n    0.22 \n  \n\n\n* Scores normalized by dividing by the maximum.\n\n\n\n\n\nAnd for the groups:\n\n\n\n\nTable 4:  Status Scores for Groups \n \n  \n    Group \n    Bonacich \n    Group \n    Col. Contamination \n    Group \n    Row Contamination \n    Group \n    Double Contamination \n  \n \n\n  \n    9/16 \n    1.00 \n    11/21 \n    1.00 \n    9/16 \n    1.00 \n    2/23 \n    1.00 \n  \n  \n    3/15 \n    0.76 \n    8/3 \n    1.00 \n    4/8 \n    0.45 \n    11/21 \n    1.00 \n  \n  \n    4/8 \n    0.75 \n    6/10 \n    0.03 \n    2/25 \n    0.37 \n    8/3 \n    1.00 \n  \n  \n    5/19 \n    0.65 \n    4/7 \n    0.01 \n    5/19 \n    0.35 \n    6/27 \n    0.83 \n  \n  \n    2/25 \n    0.64 \n    2/23 \n    0.00 \n    3/15 \n    0.21 \n    3/2 \n    0.77 \n  \n  \n    4/12 \n    0.50 \n    3/2 \n    0.00 \n    4/12 \n    0.10 \n    9/26 \n    0.52 \n  \n  \n    4/7 \n    0.40 \n    6/27 \n    0.00 \n    9/26 \n    0.03 \n    6/10 \n    0.38 \n  \n  \n    9/26 \n    0.35 \n    9/26 \n    0.00 \n    6/27 \n    0.01 \n    4/7 \n    0.26 \n  \n  \n    6/10 \n    0.34 \n    4/12 \n    0.00 \n    3/2 \n    0.01 \n    4/12 \n    0.24 \n  \n  \n    3/2 \n    0.30 \n    3/15 \n    0.00 \n    2/23 \n    0.00 \n    2/25 \n    0.13 \n  \n  \n    6/27 \n    0.28 \n    5/19 \n    0.00 \n    4/7 \n    0.00 \n    5/19 \n    0.13 \n  \n  \n    11/21 \n    0.22 \n    2/25 \n    0.00 \n    6/10 \n    0.00 \n    3/15 \n    0.09 \n  \n  \n    8/3 \n    0.22 \n    4/8 \n    0.00 \n    11/21 \n    0.00 \n    4/8 \n    0.07 \n  \n  \n    2/23 \n    0.18 \n    9/16 \n    0.00 \n    8/3 \n    0.00 \n    9/16 \n    0.04 \n  \n\n\n* Scores normalized by dividing by the maximum."
  },
  {
    "objectID": "prestige-two-mode-gen.html#bonacich-rankings",
    "href": "prestige-two-mode-gen.html#bonacich-rankings",
    "title": "Generalized Prestige Scoring in Two-Mode Networks",
    "section": "Bonacich Rankings",
    "text": "Bonacich Rankings\nAs we can see in the first two columns of Table 3 and Table 4, the Bonacich Eigenvector ranking is pretty similar to the ranking we obtain using degree. Theresa, Evelyn, Brenda, and Laura are the top women, and events 9/16, 3/15, 4/8, 5/19, and 2/25 are ranked toward the top."
  },
  {
    "objectID": "prestige-two-mode-gen.html#asymmetric-column-contamination-rankings",
    "href": "prestige-two-mode-gen.html#asymmetric-column-contamination-rankings",
    "title": "Generalized Prestige Scoring in Two-Mode Networks",
    "section": "Asymmetric Column Contamination Rankings",
    "text": "Asymmetric Column Contamination Rankings\nHowever, as the third and fourth columns of Table 4 shows, the event ranking obtained in the asymmetric column contamination model is completely different from the usual Eigenvector ranking and in many ways reversed (\\(r =\\) -0.42). Events 3/2, 6/27, 11/21, and 8/3 come toward the top and most of the events ranked highly by the eigenvector score are toward the bottom. The reason for that, is that while events 3/2, 6/27, 11/21, and 8/3 are relatively small events by attendance, they are attended almost exclusively by the highest status women and are avoided by the lower status women. The column contamination model thus uncovers the most elite set of events.\nBecause women’s status scores are computed in the usual way (\\(\\delta = -1\\)), they end up being similar to those obtained via the standard Bonacich scoring; elite women attend many elite events and non-elite women just a few non-elite ones. This is shown in the first four columns of Table 3. Ranking the women using the column contamination model’s event scores serves to magnify the Bonacich dual scoring status differences between them, while also partially re-ordering the top of the ranks. The top four women receive very high normalized scores (\\(\\mathbf{s}^R > 0.8\\)) and the bottom seven meager scores (\\(\\mathbf{s}^R < 0.2\\)). This ranking reveals Evelyn as the top woman, followed closely by Nora (who was sixth by the Bonacich scoring), Laura, and Sylvia. Note that the reason for Nora’s rise in standing according to the column contamination metric is that she attends almost all of the elite events uncovered by the same metric, as we can see in Table 2."
  },
  {
    "objectID": "prestige-two-mode-gen.html#asymmetric-row-contamination-rankings",
    "href": "prestige-two-mode-gen.html#asymmetric-row-contamination-rankings",
    "title": "Generalized Prestige Scoring in Two-Mode Networks",
    "section": "Asymmetric Row Contamination Rankings",
    "text": "Asymmetric Row Contamination Rankings\nRows five and six of Table 3 show how the person ranking would change if instead we computed it using the asymmetric row contamination model. As we can see, the asymmetric row contamination status order of women is almost a complete reversal from that implied by the asymmetric column contamination model (\\(r =\\) -0.27). Here, Dorothy emerges as to the top person, followed somewhat distantly by Olivia, Flora, and Pearl, all undisputed low status women according to the Bonacich and especially the column contamination model’s scores. Instead, all of the high status women according to these other two status metrics are lumped in the bottom.\nChecking the event ranking in columns five and six of Table 2, we can see that the reason for this inversion of the status order is that only two events, namely, 4/8 and 9/16 emerge as high status events in the asymmetric row contamination metric, and these, as expected given the earlier discussion, are the top-two most popular events. Dorothy attends both and shuns every other event, which puts her at the top. She is a popular event univore, and thus the top dog by this metric. Olivia and Flora have a similar event profile to Dorothy. They attend 4/8 (the second most popular event) and just one other event (2/23). Pearl, the fourth woman ranked by this metric, attends only two events, and two of them are high-status. However, she also attends a relatively low status event (5/19) which degrades her position. Overall, it is easy to see that the asymmetric row contamination status metric rewards high status selectivity toward popular events and heavily punishes the omnivore strategy."
  },
  {
    "objectID": "prestige-two-mode-gen.html#contaminating-dualities-rankings",
    "href": "prestige-two-mode-gen.html#contaminating-dualities-rankings",
    "title": "Generalized Prestige Scoring in Two-Mode Networks",
    "section": "Contaminating Dualities Rankings",
    "text": "Contaminating Dualities Rankings\nAs we can see in the last two columns of Table 4, the contaminating dualities model, combining the elite omnivorousness and popular univorousness status dynamics, provides yet another picture of the event status system, which is, in fact, almost an exact reversal of that implied by the usual Bonacich scores (\\(r =\\) -0.88). Events 4/8, 3/15, and 9/16, the top events in the asymmetric row-contaminaton model, are now towards the bottom of the status order, while events 2/23, 11/21, and 8/3 (the lowest status events according to the Eigenvector centrality scoring) are now the highest status events. As we ca see in Table 2, all of this last set of events are relatively restrictive in terms of attendance (they are on the right side of the table); what they have in common is that they combine memberships of the two types of elite women uncovered by the asymmetric row and column-object contamination models; that is, their membership include both high-status membership omnivores like Nora, and popular event exclusivists like Olivia and Flora, the last two of whom emerge as the top status women in the contaminating dualities dynamic, just like they did in the asymmetric row contamination scoring. Nevertheless, even though Flora and Olivia appear toward the top in the row contamination and contaminating dualities models, the two scorings imply very different rankings on the person side (\\(r =\\) -0.65)."
  },
  {
    "objectID": "prestige.html",
    "href": "prestige.html",
    "title": "Status and Prestige",
    "section": "",
    "text": "In the the centrality lecture notes, we saw how to compute the most popular centrality measures. Freeman’s “big three” have strong graph-theoretic foundation and do a good job of formalizing and quantifying the idea that a node is central if it is “well-placed” in the network, where being well-placed resolves into either being able to reach others (directly as with degree or indirectly as with closeness) or being able to intermediate between others (as with betweenness)."
  },
  {
    "objectID": "prestige.html#networks-as-prisms",
    "href": "prestige.html#networks-as-prisms",
    "title": "Status and Prestige",
    "section": "Networks as Prisms",
    "text": "Networks as Prisms\nThere is, however, another strong and well-motivated intuition as to what it means to be “well-placed” in a network. Here the ties in the network are seen less as “pipes” that transmit stuff and more like “prisms” that reflect on you (Podolny 2001).\nOne way to think about this second version of well-placedness is that what is transmitted through the network is the network itself, or more accurately, the importance, status, and prestige of the people you are connected to, preferably flowing from them (high status people) to you.\nUnder this interpretation, actors get status and prestige in the network from being connected to prestigious and high status others. Those others, in turn, get their status from being connected to high status others, and so on ad infinitum.\nOne way of quantifying this idea goes like this. If \\(\\mathbf{x}\\) is a vector containing the desired status scores, then the status of actor \\(i\\) should be equal to:\n\\[\n   x_i = \\sum_{j} a_{ij}x_j\n\\tag{1}\\]\nWhere \\(a_{ij} = 1\\) if \\(i\\) is adjacent to \\(j\\) in the network. Note that this formula just sums up the status scores of all the others each actor is connected to.\nIn matrix notation, if \\(\\mathbf{x}\\) is a column vector of status scores then:\n\\[\n   \\mathbf{x} = A\\mathbf{x}\n\\]\nBecause \\(\\mathbf{A}\\) is an \\(n \\times n\\) matrix and \\(\\mathbf{x}\\) is \\(n \\times 1\\) column vector, the multiplication \\(A\\mathbf{x}\\) will return another column vector of dimensions \\(n \\times 1\\), in this case \\(\\mathbf{x}\\) itself!\nNote the problem that this formulation poses: \\(\\mathbf{x}\\) appears on both sides of the equation, which means that in order to know the status of any one node we would need to know the status of the others, but calculating the status of the others depends on knowing the status of the focal node, and so on. There’s a chicken and the egg problem here.\nNow, there is an obvious (to the math majors) mathematical solution to this problem, because there’s a class of solvable (under some mild conditions imposed on the matrix \\(\\mathbf{A}\\)) linear algebra problems that take the form:\n\\[\n   \\lambda\\mathbf{x} = A\\mathbf{x}\n\\]\nWhere \\(\\lambda\\) is just a plain old number (a scalar). Once again conditional of the aforementioned mild conditions being met, we can iteratively search for a value \\(\\lambda\\), fix it, then fill up the \\(\\mathbf{x}\\) vector with another set of values, fix those, search for a new \\(\\lambda\\), and continue until we have values of \\(\\lambda\\) and \\(\\mathbf{x}\\) that make the above equality true.\nWhen we do that successfully, we say that the value of \\(\\lambda\\) we hit upon is an eigenvalue of the matrix \\(\\mathbf{A}\\) and the values of the vector \\(\\mathbf{x}\\) we came up with are an eigenvector of the same matrix (technically in the above equation a right eigenvector).\nEigenvalues and eigenvectors, like Don Quixote and Sancho Panza, come in pairs, because you need a unique combination of both to solve the equation. Typically, a given matrix (like an adjacency matrix) will have multiple \\(\\lambda/\\mathbf{x}\\) pairs that will solve the equation. Together the whole set \\(\\lambda/\\mathbf{x}\\) pairs that make the equation true are the eigenvalues and eigenvectors of the matrix."
  },
  {
    "objectID": "prestige.html#eigenvalues-eigenvectors-oh-my",
    "href": "prestige.html#eigenvalues-eigenvectors-oh-my",
    "title": "Status and Prestige",
    "section": "Eigenvalues, Eigenvectors, Oh My!",
    "text": "Eigenvalues, Eigenvectors, Oh My!\nNote that all of this obscure talk about eigenvalues and eigenvectors is just matrix linear algebra stuff. It has nothing to do with networks and social structure.\nIn contrast, because the big three centrality measures have a direct foundation in graph theory, and graph theory is an isomorphic model of social structures (points map to actors/people and lines map to relations) the “math” we do with graph theory is directly meaningful as a model of networks (the counts of the number of edges incident to a node is the count of other actors they someone is directly connected to).\nEigenvalues and eigenvectors are not a model of social structure in the way graph theory is (their first scientific application was in Chemistry and Physics). They are just a mechanical math fix to a circular equation problem.\nThis is why it’s a mistake to introduce network measures of status and prestige by jumping directly to the machinery of linear algebra (or worse talk about the idea of eigenvector centrality which means nothing to most people, and combines two obscure terms into one even more obscure compound term).\nA better approach is to see if we can motivate the use of measures like the ones above using the simple model of the distribution of status and prestige we started with earlier. We will see that we can, and that doing that leads us back to solutions that are the mathematical equivalent of all the eigenvector stuff."
  },
  {
    "objectID": "prestige.html#distributing-status-to-others",
    "href": "prestige.html#distributing-status-to-others",
    "title": "Status and Prestige",
    "section": "Distributing Status to Others",
    "text": "Distributing Status to Others\nLet’s start with the simplest model of how people can get their status from the status of others in a network. It is the simplest because it is based on degree.\nImagine everyone has the same “quantum” of status to begin with (this can be stored in a vector containing the same number of length equals to number of actors in the network). Then, at each step, people “send” the same amount of status to all their alters in the network. Then we repeat, with everyone sending the amount of status they now have after receiving it from others. We repeat this many times. At the end of each step, we compute people’s new status scores using Equation 1. We stop doing this after the status scores of people stop changing across each iteration.\nLet us see a real-life example at work.\nWe will use a data set collected by David Krackhardt on the friendships of 21 managers in a high tech company in the West coast (see the description here). The data are reported as directed ties (\\(i\\) nominates \\(j\\) as a friend) but we will constrain ties to be undirected:\n\n   library(networkdata)\n   library(igraph)\n   g <- as_undirected(ht_friends, mode = \"collapse\")\n\nThe undirected friendship network is shown in Figure 1.\n\n\n\n\n\nFigure 1: Krackhardt’s managers network\n\n\n\n\nWe then extract the adjacency matrix corresponding to this network:\n\n   A <- as.matrix(as_adjacency_matrix(g))\n\nAnd here’s a simple custom function using a while loop that exemplifies the process of status distribution through the network we just talked about:\n\n   status1 <- function(w) {\n      x <- rep(1, nrow(w)) #initial status vector set to all ones of length equal to the number of nodes\n      d <- 1 #initial delta\n      k <- 0 #initializing counter\n      while (d > 1e-10) {\n          o.x <- x #old status scores\n          x <- w %*% o.x #new scores a function of old scores and adjacency matrix\n          x <- x/norm(x, type = \"E\") #normalizing new status scoress\n          d <- abs(sum(abs(x) - abs(o.x))) #delta between new and old scores\n          k <- k + 1 #incrementing while counter\n      }\n   return(as.vector(x))\n   }\n\nLines 2-4 initialize various quantities, most importantly the initial status vector for each node to just a series of ones:\n\n   rep(1, nrow(A))\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n\nThen lines 5-12 implement a little loop of how status is distributed through the network, with the most important piece of code being line 7 where the current status scores for each node are just the sum of the status scores of its neighbors computed one iteration earlier. The program stops when the difference between the old and the new scores is negligible (\\(\\delta < 10^{-10}\\)) as checked in line 9.\nNote the normalization step on line 8, where we divide the each status score by a normalized sum of all of the scores. This is required in order to prevent the sum of status scores from getting bigger and bigger indefinitely (in mathese, this is referred to as the sum “diverging”). In base R, the type = \"E\" normalization implements the Euclidean vector norm (also sometimes confusingly called the Frobenieus norm), by which we divide each value of the status scores by after each update.1\nAnd here’s the resulting (row) vector of status scores for each node:\n\n   s <- status1(A)\n   s <- s/max(s) #normalizing by maximum\n   round(s, 3)\n\n [1] 0.619 0.635 0.446 0.489 0.629 0.430 0.205 0.380 0.444 0.468 0.814 0.549\n[13] 0.162 0.401 0.613 0.360 1.000 0.247 0.680 0.360 0.392\n\n\nWhat if I told you that this vector is the same as that given by the leading (first) eigenvector of the adjacency matrix?\n\n   s.eig <- abs(eigen(A)$vector[, 1]) #computing the first eigenvector\n   s.eig <- s.eig/max(s.eig) #normalizing by maximum\n   round(s.eig, 3)\n\n [1] 0.619 0.635 0.446 0.489 0.629 0.430 0.205 0.380 0.444 0.468 0.814 0.549\n[13] 0.162 0.401 0.613 0.360 1.000 0.247 0.680 0.360 0.392\n\n\nWhich is of course what is computed by the eigen_centrality function in igraph:\n\n   round(eigen_centrality(g)$vector, 3) #igraph automatically normalizes the scores\n\n [1] 0.619 0.635 0.446 0.489 0.629 0.430 0.205 0.380 0.444 0.468 0.814 0.549\n[13] 0.162 0.401 0.613 0.360 1.000 0.247 0.680 0.360 0.392\n\n\nSo, the “eigenvector centralities” are just the limit scores produced by the status distribution process implemented in the status1 function!\nWhen treated as a structural index of connectivity in a graph (i.e., a centrality measure) the eigenvector status scores induce an ordering of the nodes which we may be interested in looking at:\n\n   nodes <- 1:vcount(g)\n   eig.dat <- data.frame(Nodes = nodes, Eigen.Cent = s, Deg.Cent = degree(g))\n   eig.dat <- eig.dat[order(eig.dat$Eigen.Cent, decreasing = TRUE), ]\n   library(kableExtra)\n   kbl(eig.dat[1:10, ], \n       format = \"html\", align = \"c\", row.names = FALSE,\n       caption = \"Top Ten Eigenvector Scores.\",\n       digits = 3) %>%    \n   kable_styling(bootstrap_options = \n                    c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\nTop Ten Eigenvector Scores.\n \n  \n    Nodes \n    Eigen.Cent \n    Deg.Cent \n  \n \n\n  \n    17 \n    1.000 \n    18 \n  \n  \n    11 \n    0.814 \n    14 \n  \n  \n    19 \n    0.680 \n    10 \n  \n  \n    2 \n    0.635 \n    10 \n  \n  \n    5 \n    0.629 \n    10 \n  \n  \n    1 \n    0.619 \n    9 \n  \n  \n    15 \n    0.613 \n    9 \n  \n  \n    12 \n    0.549 \n    8 \n  \n  \n    4 \n    0.489 \n    7 \n  \n  \n    10 \n    0.468 \n    8 \n  \n\n\n\n\n\nAs we can see, for instance, there is a strong correlation between degree and the eigenvector score. But they are not the same. For instance, nodes 19, 2, 5 are tied according to degree, but 19 is highest in the eigenvector scoring, indicating that even though they all have the same number of friends, 19 is connected to better-connected others.\nAs we will see, most other measures of prestige/status rank for nodes in networks are constructed using similar principles as the ones just described. What changes is the model assumptions of how status is distributed in the system. That’s why scary and non-intuitive stuff about eigenvectors or whatever is misleading.\nOther measures of node status and prestige are designed such that they either change the quantum of status that is distributed through the network by making it dependent on some node characteristic (like degree) or differentiate between different routes of distribution in directed graphs, by for instance, differentiating status derived from outgoing links from that derived from incoming links.\nLet’s see some examples of these alternative cases."
  },
  {
    "objectID": "prestige.html#combining-prestige-and-similarity",
    "href": "prestige.html#combining-prestige-and-similarity",
    "title": "Status and Prestige",
    "section": "Combining Prestige and Similarity",
    "text": "Combining Prestige and Similarity\nIn a neat paper, Alvarez-Socorro, Herrera-Almarza, and González-Dı́az (2015) combine ideas of node similarity analysis with prestige ranking to derive an interesting twist on the usual Eigenvector-based prestige score.\nTheir idea is simple and intuitive, linking the “getting status from others” idea we just talked about, but weighting those others by their similarity to ego. Their point is that linking to others who are not similar to you should give you more status than linking to others that are similar, which is the principle behind such fundamental network theories as Strength of Weak Ties (Granovetter 1973) and Structural Holes (Burt 1992).\nHow do we do it? First, we just need to compute one of the many similarity metrics, to come up with a dissimilarity matrix between nodes. We choose the Jaccard similarity, which can be computed using the following function:\n\n   jaccard.sim <- function(x) {\n      A <- as.matrix(as_adjacency_matrix(x))\n      A.p <- A %*% A \n      A.q <- A %*% (1 - A) \n      A.r <- (1 - A) %*% A \n      return(A.p / (A.p + A.q + A.r))\n      }\n\nThe dissimilarity matrix for the Krackhardt managers friendship network is just:\n\n   D <- 1 - jaccard.sim(g)\n\nThen Alvarez-Socorro, Herrera-Almarza, and González-Dı́az (2015) create a new matrix W which equals the element wise product between the adjacency matrix and the distance matrix:\n\n   W <- A * D\n\nAnd the (dis)similarity weighted status scores are obtained by playing our status game on the W matrix:\n\n   s.diss <- status1(W)\n   s.diss <- s.diss/max(s.diss)\n\nAnd here are the top ten nodes by this dissimilarity weighted prestige measure:\n\n\n\n\nTop Ten Dissimilarity-Weighted Eigenvector Scores.\n \n  \n    Nodes \n    Eigen.Cent \n    Deg.Cent \n  \n \n\n  \n    17 \n    1.000 \n    18 \n  \n  \n    11 \n    0.821 \n    14 \n  \n  \n    5 \n    0.667 \n    10 \n  \n  \n    2 \n    0.656 \n    10 \n  \n  \n    19 \n    0.637 \n    10 \n  \n  \n    12 \n    0.619 \n    8 \n  \n  \n    10 \n    0.608 \n    8 \n  \n  \n    15 \n    0.579 \n    9 \n  \n  \n    1 \n    0.563 \n    9 \n  \n  \n    6 \n    0.514 \n    7 \n  \n\n\n\n\n\nNote that while the top two nodes (17 and 11) does not change, taking dissimilarity into account alters the rankings. Node 19 drops from third to fifth place and node 1 from fifth to ninth, and node 5 goes from sixth to third, indicating that node 5 connects to well-connected others who are themselves not connected to their neighbors.\nOf course, it is possible that in some settings, it might make sense to say that people receive more status points by being connected to others who are themselves connected to the people they are connected to (similar others). This situation would apply when status is generated not via brokerage, but by belonging to well-delimited cohesive groups.\nTo accommodate this possibility, we can generalize the above approach as follows. Recall that for Alvarez-Socorro, Herrera-Almarza, and González-Dı́az (2015) the W matrix is given by:\n\\[\n\\mathbf{W} = \\mathbf{A} \\odot (1 - \\mathbf{S})\n\\]\nWhere \\(\\mathbf{A}\\) is the adjacency matrix and \\(\\mathbf{S}\\) is a similarity matrix (making (\\(1-\\mathbf{S}\\) a dissimilarity matrix)) and \\(\\odot\\) is the element-wise matrix multiplication operator.\nA generalization that accommodates a wider range of dependencies between status and similarity goes like this:\n\\[\n\\mathbf{W} = \\mathbf{A} \\odot \\left[\\mathbf{S}^\\delta \\odot (1 - \\mathbf{S})^\\gamma\\right]\n\\]\nWith the restriction that \\(0 \\geq \\delta \\leq 1\\) and \\(0 \\geq \\gamma \\leq 1\\).\nWhen \\(\\delta = \\gamma = 0\\), then \\(\\mathbf{W} = \\mathbf{A}\\) and we recover the standard Eigenvector scores we computed earlier. When \\(\\delta = 0\\) and \\(\\gamma > 0\\), then we recover the dissimilarity-weighted eigenvector scores of Alvarez-Socorro, Herrera-Almarza, and González-Dı́az (2015). When \\(\\delta > 0\\) and \\(\\gamma = 0\\) then we compute scores that weigh the eigenvector scores by similarity rather than dissimilarity.\nHere is a function that packages everything above into the generalized version:\n\n   prestige.sim <- function(x, delta = 0, gamma = 0) {\n      A <- as.matrix(as_adjacency_matrix(x))\n      A.p <- A %*% A \n      A.q <- A %*% (1 - A) \n      A.r <- (1 - A) %*% A \n      S <- A.p / (A.p + A.q + A.r)\n      D < - 1 - S\n      W <- A * S^delta * D^gamma\n      return(abs(eigen(W)$vectors[, 1]))\n   }\n\nAnd now we can run through the various possibilities.\nTop-ten regular eigenvector:\n\n   s <- prestige.sim(g)\n   names(s) <- 1:vcount(g)\n   round(sort(s/max(s), decreasing = TRUE)[1:10], 3)\n\n   17    11    19     2     5     1    15    12     4    10 \n1.000 0.814 0.680 0.635 0.629 0.619 0.613 0.549 0.489 0.468 \n\n\nTop-ten dissimilarity-weighted eigenvector:\n\n   s <- prestige.sim(g, gamma = 1)\n   names(s) <- 1:vcount(g)\n   round(sort(s/max(s), decreasing = TRUE)[1:10], 3)\n\n   17    11     5     2    19    12    10    15     1     6 \n1.000 0.821 0.667 0.656 0.637 0.619 0.608 0.579 0.563 0.514 \n\n\nTop-ten similarity-weighted eigenvector:\n\n   s <- prestige.sim(g, delta = 1)\n   names(s) <- 1:vcount(g)\n   round(sort(s/max(s), decreasing = TRUE)[1:10], 3)\n\n   17    11    19     1    15     2     5     4    12     3 \n1.000 0.830 0.768 0.714 0.660 0.581 0.565 0.526 0.412 0.375 \n\n\nNeat!"
  },
  {
    "objectID": "prestige.html#bonacich-prestige-in-directed-graphs",
    "href": "prestige.html#bonacich-prestige-in-directed-graphs",
    "title": "Status and Prestige",
    "section": "Bonacich Prestige in Directed Graphs",
    "text": "Bonacich Prestige in Directed Graphs\nIn a classic paper, Philip Bonacich (1972) noted the above connection between different ways people conceptualized status and prestige in networks and the leading eigenvector of the adjacency matrix. He then noted that we can extend similar ideas to the directed case.\nHere, people get status from receiving nominations from high status others (i.e., those who receive a lot of nominations), whose partners also get status from receiving a lot of nominations from high status others, and so forth.\nThis means that in a directed system of relations, status distribution operates primarily via the indegree of each node, so that if \\(\\mathbf{A}\\) is the asymmetric adjacency matrix corresponding to the directed graph, then if we play our status game on the transpose of this matrix \\(\\mathbf{A}^T\\) we will get the scores we seek (Fouss, Saerens, and Shimbo 2016, 204).\nRecall that in transposing the matrix of a directed graph, we change it from being a from/to matrix (nodes in the rows send ties to nodes in the columns) to a to/from matrix: Nodes in the rows receive ties from nodes in the columns. So we want to play our status game in this matrix, because we want to rank nodes according to their receipt of ties from high-status others.\nLet’s see a real-life example, this time using the directed version of the Krackhardt friendship nomination network among the high-tech managers:\n\n   g <- ht_friends\n   A <- as.matrix(as_adjacency_matrix(g))\n   s <- status1(t(A))\n   s <- s/max(s)\n   round(s, 3)\n\n [1] 0.922 1.000 0.379 0.639 0.396 0.190 0.240 0.531 0.411 0.117 0.413 0.769\n[13] 0.082 0.428 0.368 0.450 0.592 0.440 0.425 0.225 0.583\n\n\nWhich are the same scores we would have gotten using the eigen_centrality function in igraph with the argument directed set to TRUE:\n\n   round(eigen_centrality(g, directed = TRUE)$vector, 3)\n\n [1] 0.922 1.000 0.379 0.639 0.396 0.190 0.240 0.531 0.411 0.117 0.413 0.769\n[13] 0.082 0.428 0.368 0.450 0.592 0.440 0.425 0.225 0.583\n\n\nAnd, like before, we can treat these scores as centrality measures and rank the nodes in the graph according to them.\nHere are the top ten nodes:\n\n\n\n\nTop Ten Eigenvector Scores for a Directed Graph.\n \n  \n    Nodes \n    Eigen.Cent \n    In.Deg.Cent \n  \n \n\n  \n    2 \n    1.000 \n    10 \n  \n  \n    1 \n    0.922 \n    8 \n  \n  \n    12 \n    0.769 \n    8 \n  \n  \n    4 \n    0.639 \n    5 \n  \n  \n    17 \n    0.592 \n    6 \n  \n  \n    21 \n    0.583 \n    5 \n  \n  \n    8 \n    0.531 \n    5 \n  \n  \n    16 \n    0.450 \n    4 \n  \n  \n    18 \n    0.440 \n    4 \n  \n  \n    14 \n    0.428 \n    5 \n  \n\n\n\n\n\nWhile the top indegree centrality node (2) also gets the top Eigenvector Centrality scores, we see many cases of nodes with equal indegree centrality that get substantively different Eigenvector scores. So who you are connected matters in addition to how many incoming connections you have."
  },
  {
    "objectID": "qap.html",
    "href": "qap.html",
    "title": "Network Regression Using Permutation",
    "section": "",
    "text": "Consider the case where we measure two or more ties on the same set of actors. In that case, we could entertain hypotheses of the form: Is having a tie of type \\(A\\) positively or negatively correlated with a tie of type \\(B\\)?\nWe can use an approach similar to the edge-swapping strategy to answer this question, since a bivariate correlation between ties between two networks is just like the univariate statistics we computed earlier (in that case the modularity, but it could been any network level statistic like the diameter or centralization).\nLet’s see an example. Suppose we have a hypothesis that the relation of “friendship” should be correlated to that of “advice”; that is, people are more likely to seek advice from friends.\nThe Krackhardt High-Tech Managers data contains network information on both the advice and friendship relations between the actors. Let’s load up the advice network:\n\n   library(networkdata)\n   library(igraph)\n   g <- as_undirected(ht_friends, mode = \"collapse\")\n   g2 <- as_undirected(ht_advice, mode = \"collapse\")\n\nAnd extract the corresponding adjacency matrices:\n\n   A.f <- as.matrix(as_adjacency_matrix(g))\n   A.d <- as.matrix(as_adjacency_matrix(g2))\n\nThe correlation between these two ties is then:\n\n   r <- cor(as.vector(A.f), as.vector(A.d))\n   round(r, 2)\n\n[1] 0.26\n\n\nWhich is obtained by “vectorizing” the matrices and computing the Pearson correlation between the dyad vector (of length \\(21 \\times 21 = 441\\)) corresponding to friendship ties and the dyad vector corresponding to advice ties.\nHere, we see that there is a positive correlation between the two (\\(r = 0.26\\)) suggesting people tend to seek advice from friends (or befriend their advisors/advisees, as correlations don’t imply directionality).\nHowever, is this correlation larger than we would expect by chance? For that, we need to know the range of correlations that would be obtained between on of the networks (e.g., advice) and a suitably randomized version of the other (e.g., friendship).\nThe “permutation” approach (Borgatti et al. 2024, chap. 14)—sometimes referred to by the less-than-wieldy name of the Quadratic Assignment Procedure (QAP) (Krackhardt 1988)—allows us to do that.\nRecall from our discussion of blockmodeling that a permutation of an adjacency matrix is just a reshuffling of its rows and columns.\nFor instance the original adjacency matrix of the Krackhardt Managers friendship nomination network is shown in Table 1 (a).\nWe can permute the rows and columns of the matrix by randomly sampling numbers between 1 and 21 (the number of nodes in the network) and re-ordering the rows and columns of the matrix according to this new vector:\n\n   s <- sample(1:nrow(A.f))\n   s\n\n [1] 19  6  4 14  9  8 11 21 17 12  5  1 13 15  2 20 16  3 10  7 18\n\n   A.f.perm <- A.f[s, s]\n\nThe new permuted matrix is shown in Table 1 (b).\n\n\nTable 1: An adjacency matrix and a permuted version\n\n\n\n\n(a) Original. \n\n  \n    0 \n    1 \n    0 \n    1 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    1 \n    1 \n    1 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    1 \n    0 \n    0 \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n    1 \n    0 \n    1 \n  \n  \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    1 \n    1 \n    0 \n    1 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n    0 \n    1 \n    1 \n    1 \n    0 \n    1 \n    0 \n    1 \n    0 \n    1 \n  \n  \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    0 \n    0 \n    1 \n    0 \n    0 \n    1 \n    0 \n    1 \n    0 \n    0 \n    0 \n    1 \n  \n  \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    1 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    0 \n    0 \n    1 \n    0 \n    1 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    1 \n    0 \n  \n  \n    1 \n    1 \n    1 \n    1 \n    1 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    1 \n    1 \n    0 \n    1 \n    0 \n    1 \n    1 \n    1 \n    1 \n    0 \n  \n  \n    1 \n    0 \n    0 \n    1 \n    0 \n    1 \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    0 \n    1 \n  \n  \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    0 \n    0 \n    1 \n    0 \n    1 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    1 \n    0 \n    1 \n    0 \n    1 \n    1 \n    0 \n    0 \n    1 \n    0 \n    1 \n    0 \n    0 \n    1 \n    0 \n    0 \n    1 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    1 \n    1 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    0 \n    1 \n    1 \n    1 \n    0 \n    0 \n    1 \n    1 \n    1 \n  \n  \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n  \n  \n    1 \n    1 \n    1 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    1 \n    1 \n    0 \n    1 \n    0 \n    0 \n    1 \n    0 \n  \n  \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n    0 \n    0 \n  \n  \n    0 \n    1 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n  \n\n\n\n\n\n\n(b) Permuted. \n\n  \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    1 \n    0 \n    1 \n    1 \n    1 \n    1 \n    0 \n    1 \n    1 \n    1 \n    0 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n  \n  \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    1 \n    1 \n    0 \n    1 \n    0 \n    0 \n    1 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    0 \n  \n  \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    0 \n    1 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    1 \n    0 \n    1 \n    0 \n    1 \n    1 \n    0 \n    0 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    0 \n    1 \n    0 \n    0 \n    1 \n  \n  \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n  \n  \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    0 \n    1 \n    1 \n    1 \n    0 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    0 \n  \n  \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    1 \n    0 \n    0 \n    1 \n    1 \n    0 \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    1 \n    0 \n    1 \n    0 \n    0 \n    1 \n    1 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    1 \n    1 \n    0 \n    1 \n    1 \n    0 \n    1 \n    0 \n    1 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    1 \n  \n  \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n  \n  \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    1 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    1 \n    0 \n    0 \n    1 \n    0 \n    0 \n    1 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    0 \n    1 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n\n\n\n\n\n\nNote that when we lose the labels on the nodes the permuted matrix contains that same information as the original, but scrambles the node labels.\nSo, in contrast to the edge swapping approach, which preserves some graph characteristics but loses others, in the permutation approach, every graph-level characteristic stays the same:\n\n   g.perm <- graph_from_adjacency_matrix(A.f.perm, mode = \"undirected\")\n   degree(g.perm)\n\n [1] 10  7  7  6  6  5 14  6 18  8 10  9  2  9 10  5  5  6  8  3  4\n\n   degree(g)\n\n [1]  9 10  6  7 10  7  3  5  6  8 14  8  2  6  9  5 18  4 10  5  6\n\n   #degree sequence\n   sort(degree(g.perm), decreasing = TRUE)\n\n [1] 18 14 10 10 10  9  9  8  8  7  7  6  6  6  6  5  5  5  4  3  2\n\n   sort(degree(g), decreasing = TRUE)\n\n [1] 18 14 10 10 10  9  9  8  8  7  7  6  6  6  6  5  5  5  4  3  2\n\n   #diameter\n   mean(distances(g.perm))\n\n[1] 1.569161\n\n   mean(distances(g))\n\n[1] 1.569161\n\n   #betweenness centralization\n   round(centr_betw(g.perm)$centralization, 2)\n\n[1] 0.21\n\n   round(centr_betw(g)$centralization, 2)\n\n[1] 0.21\n\n\nA function that takes an undirected graph as input, permutes the corresponding adjacency matrix and returns the new graph as output looks like:\n\n   perm <- function(z) {\n      x <- as.matrix(as_adjacency_matrix(z))\n      s <- sample(1:nrow(x))\n      x <- x[s, s] #permuting matrix\n      w <- graph_from_adjacency_matrix(x, mode = \"undirected\")\n      return(w)\n   }\n\nSo to get a p-value for our cross-tie correlation all we need to do is to compute the correlation between the original advice network and a set of permuted graphs of the friendship network.\nFirst, we create a 1000 graph ensemble of permuted friendship networks:\n\n   set.seed(12345)\n   G <- replicate(1000, perm(g), simplify = FALSE)\n\nThen we write a function for computing the correlation between two networks:\n\n   cor.net <- function(x1, x2) {\n   a <- as.matrix(as_adjacency_matrix(x1))\n   b <- as.matrix(as_adjacency_matrix(x2))      \n   r <- cor(as.vector(a), as.vector(b))\n   return(r)\n   }\n\nAnd now we compute our correlation vector showing the first 100 elements:\n\n   corrs <- round(sapply(G, cor.net, x2 = g2), 2)\n   corrs[1:100]\n\n  [1]  0.12 -0.12  0.10 -0.06  0.14  0.02 -0.02 -0.04  0.06  0.12  0.08  0.08\n [13]  0.08 -0.02  0.22  0.04 -0.12  0.06 -0.02  0.02  0.12  0.00  0.16  0.06\n [25]  0.28  0.16  0.10  0.08  0.00  0.04  0.02  0.16  0.16  0.10  0.08  0.10\n [37]  0.02  0.12  0.02  0.16  0.02  0.02 -0.02  0.12  0.10  0.10  0.08  0.08\n [49]  0.14 -0.06  0.04 -0.02 -0.06  0.12 -0.02  0.20 -0.02  0.08 -0.10  0.06\n [61]  0.00  0.04  0.14  0.06 -0.04  0.04 -0.02  0.10 -0.02  0.06  0.12 -0.08\n [73]  0.04  0.06  0.14  0.12  0.08  0.08  0.16 -0.06  0.02  0.06  0.22  0.14\n [85] -0.04  0.06  0.00 -0.10  0.16 -0.08  0.08  0.16  0.04  0.02  0.08 -0.02\n [97]  0.16  0.16 -0.08  0.12\n\n\nAnd here’s a plot of where our estimated value sits on the grand scheme of things:\n\n   library(ggplot2)\n   p <- ggplot(data = data.frame(corrs), aes(x = corrs))\n   p <- p + geom_histogram(binwidth = 0.025, stat = \"bin\", fill = \"darkblue\")\n   p <- p + geom_vline(xintercept = r, \n                       color = \"red\", linetype = 1, linewidth = 1.5)\n   p <- p + geom_vline(xintercept = 0, linetype = 1, \n                       color = \"purple\", linewidth = 1.5)\n   p <- p + theme_minimal() + labs(x = \"Friendship/Advice Correlation\", y = \"Freq.\")\n   p <- p + theme(axis.text = element_text(size = 12))\n   p <- p + annotate(\"text\", x=-0.04, y=150, label= \"Zero Point\", color = \"purple\")\n   p <- p + annotate(\"text\", x=0.22, y=150, label= \"Obs. Value\", color = \"red\")\n   p\n\n\n\n\nThis looks pretty good, with only a tiny proportion of the correlations exceeding the observed value.\nWe can check our p-value like before:\n\n   quantile(corrs, probs = 0.99)\n\n 99% \n0.22 \n\n   r > quantile(corrs, probs = 0.99)\n\n 99% \nTRUE \n\n   1 - ecdf(corrs)(r)\n\n[1] 0.001\n\n\nWhich shows that the correlation between advice and friendship is significant at the \\(p = 0.001\\) level (\\(p<0.01\\)).\nNote that the result is also statistically significant using a two-tailed test:\n\n   1 - ecdf(abs(corrs))(r)\n\n[1] 0.001"
  },
  {
    "objectID": "qap.html#cross-network-regression-using-permutation",
    "href": "qap.html#cross-network-regression-using-permutation",
    "title": "Network Regression Using Permutation",
    "section": "Cross-Network Regression Using Permutation",
    "text": "Cross-Network Regression Using Permutation\nThe permutation (QAP) approach can be extended to considering more than two network variables at the same time, which moves us into multivariable territory. The Krackhardt Managers data also contains information on formal links in the organization in the form of a graph of unidirectional “reports to whom” network, shown below:\n\n   plot(ht_reports)\n\n\n\n\nWho reports to whom in the Krackhardt Manager’s data.\n\n\n\n\nWe can see that there are four middle managers (2, 14, 18, and 21) all of whom report to the top manager (7).\nSo let’s say we wanted to predict advice ties from friendship ties while adjusting for the present of formal reporting ties. We could just run a regression like this:\n\n   A.r <- as.matrix(as_adjacency_matrix(\n      as_undirected(ht_reports, mode = \"collapse\")))\n   ht.dat <- data.frame(frs = as.vector(A.f), adv = as.vector(A.d), rep = as.vector(A.r))\n   reg.res <- lm(adv ~ frs + rep, data = ht.dat)\n   summary(reg.res)\n\n\nCall:\nlm(formula = adv ~ frs + rep, data = ht.dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.7748 -0.5475  0.2252  0.4525  0.4525 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.54750    0.02714  20.172  < 2e-16 ***\nfrs          0.22727    0.04535   5.011 7.88e-07 ***\nrep          0.31614    0.07572   4.175 3.60e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4509 on 438 degrees of freedom\nMultiple R-squared:  0.1033,    Adjusted R-squared:  0.09925 \nF-statistic: 25.24 on 2 and 438 DF,  p-value: 4.22e-11\n\n\nWhich shows that friendship has a statistically significant positive effect on being ties via an advice tie, net of the formal hierarchical link, which also has a positive effect.\nOf course, if you know anything about classical inference, you know the standard errors and associated t-statistics are useless, because the “cases” in the regression above are dyads, and they are not independent from one another because they form…a network!\nSo we can use the permutation approach to get more credible p-values like we did with the correlation, turning the above into a “QAP Regression.”\nThe basic idea is just to run the regression many times, while permuting the outcome variable; this would produce a distribution of coefficient estimates, and we can select the p-value non-parametrically like we did earlier.\nHere’s a function that will run a regression and collect the coefficients:\n\n   run.reg <- function(x, indvars, dat) {\n      w <- as.matrix(as_adjacency_matrix(x))\n      dat <- data.frame(y.perm = as.vector(w), dat)\n      reg.res <- lm(reformulate(indvars, \"y.perm\"), data = dat)\n      return(as.vector(reg.res$coefficients))\n   }\n\nWe now create a graph ensemble of 1000 permuted advice networks:\n\n   set.seed(12345)\n   G2 <- replicate(1000, perm(g2), simplify = FALSE)\n\nAnd now we run the regression on our 1000 strong ensemble of permuted advice network graphs:\n\n   reg.list <- lapply(G2, run.reg, \n                      indvars = c(\"frs\", \"rep\"), dat = ht.dat)\n   betas <- as.matrix(do.call(rbind, reg.list))\n   colnames(betas) <- c(\"Int.Est\", \"Fr.Est\", \"Rep.Est\")\n   head(betas) \n\n       Int.Est     Fr.Est     Rep.Est\n[1,] 0.6346486 0.02616358  0.14965327\n[2,] 0.6446012 0.01130525  0.09861561\n[3,] 0.6641230 0.01258719 -0.12167530\n[4,] 0.6366876 0.12453946 -0.26141124\n[5,] 0.6378383 0.03154367  0.09323552\n[6,] 0.6100193 0.17449454 -0.16471601\n\n\nAnd now we compute the p-values as before and gather them into a table:\n\n   p.vals <- c(1 - ecdf(abs(betas[, 1]))(reg.res$coefficients[1]),\n               1 - ecdf(abs(betas[, 2]))(reg.res$coefficients[2]),\n               1 - ecdf(abs(betas[, 3]))(reg.res$coefficients[3])\n               )\n   z.scores <- round(abs(qnorm(p.vals)), 3)\n   z.scores[1] <- \"--\"\n   reg.tab <- data.frame(reg.res$coefficients, z.scores, p.vals)\n   rownames(reg.tab) <- c(\"Intercept\", \"Friend Tie\", \"Formal Tie\")\n   names(reg.tab) <- c(\"Estimate\", \"z-score\", \"p-value\")\n   kbl(reg.tab[c(2:3), ], rownames = TRUE, format = \"html\", digits = 3,\n       caption = \"Predictors of Advice Ties\") %>% \n             column_spec(1, italic = TRUE) %>% \n      kable_styling(full_width = TRUE,\n                     bootstrap_options = c(\"hover\", \n                                           \"condensed\", \n                                           \"responsive\")) \n\n\n\nPredictors of Advice Ties\n \n  \n      \n    Estimate \n    z-score \n    p-value \n  \n \n\n  \n    Friend Tie \n    0.227 \n    2.257 \n    0.012 \n  \n  \n    Formal Tie \n    0.316 \n    2.226 \n    0.013 \n  \n\n\n\n\n\nWhich shows that indeed friendship and formal reporting ties are both statistically significant predictors of advice ties after accounting for network interdependencies."
  },
  {
    "objectID": "random-walk.html",
    "href": "random-walk.html",
    "title": "Random Walk Concepts in Networks",
    "section": "",
    "text": "Imagine something is diffusing through a network starting with some seed node \\(i\\) following a series of discrete time steps. If the graph is connected, the thing will eventually reach every node in the network. However, depending on the connectivity structure of the graph, it will reach some nodes (e.g., those at a smaller graph theoretic distance from the seed node) sooner than others.\nHere’s a function called first.pass1 that records the minimum number of steps that it takes for something to get to each node in a graph starting from a given seed node:\n\n   first.pass1 <- function(w, seed = 1) {\n      fp <- rep(0, ncol(w))\n      k <- 1\n      i <- seed\n      while(sum(fp[-seed] == 0) != 0) {\n         j <- sample(c(1:ncol(w)), 1, prob = w[i, ]) \n         if (fp[j] == 0 & j != seed) {fp[j] <- k}\n         i <- j\n         k <- k + 1\n         }\n   names(fp) <- 1:ncol(w)\n   return(fp)\n   }\n\nThe function takes a transition matrix \\(\\mathbf{P}\\) as the primary input (of the sort we discussed when talking about status and prestige) and returns a vector containing the time step at which the thing that diffused through the network got to the \\(j^{th}\\) node.\nLet’s test it out using the friendship network from the Krackhardt high-tech managers data:\n\n   library(networkdata)\n   library(igraph)\n   g <- as_undirected(ht_friends, mode = \"collapse\")\n   A <- as.matrix(as_adjacency_matrix(g))\n   P <- diag(1/rowSums(A)) %*% A    \n   set.seed(456)\n   first.pass1(P)\n\n  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20 \n  0   2 137  24  14  28  43  36   8   9  16  29  15  26   7   1  25  33   6  32 \n 21 \n 12 \n\n\nWe can see for instance that the rumor got to node 16 in just one time step, but that it took 24 time steps to get to node 3 and 137 (!) to get to node 3.\nSo it seems like this, the minimum time it takes for something that starts with me to get to you (Fouss, Pirotte, and Saerens 2004), is a good measure of the proximity between me and you in the graph."
  },
  {
    "objectID": "random-walk.html#averaging-across-realizations",
    "href": "random-walk.html#averaging-across-realizations",
    "title": "Random Walk Concepts in Networks",
    "section": "Averaging Across Realizations",
    "text": "Averaging Across Realizations\nHowever, we wouldn’t want to use just one run of the diffusion process to calculate this proximity measure. Instead a better approach is to use the average time it takes for something to get to the other nodes when it starts from a given node.\nTo do that, we can just replicate the first.pass1 function some number of times (e.g., \\(n = 100\\)) and take an average:\n\n   set.seed(456)\n   first.res <- replicate(100, first.pass1(P))\n   round(rowMeans(first.res), 2)\n\n    1     2     3     4     5     6     7     8     9    10    11    12    13 \n 0.00 16.14 32.88 21.24 17.17 24.35 59.98 27.83 25.85 17.75 10.16 15.78 86.78 \n   14    15    16    17    18    19    20    21 \n36.32 15.43 32.94  8.65 44.59 19.17 31.47 33.85 \n\n\nWe can see that according to this measure, called the average first passage time (Fouss, Saerens, and Shimbo 2016, 36), when we start with node 1, things get relatively quickly to node 17 (\\(\\hat{t} = 8.7\\)) but they take forever to get to node 13 (\\(\\hat{t} = 86.8\\)).\nWe can of course, write a wrapper around the first.pass1 function to compute the average first passage time from one node to another using every node in the graph as the initial seed:\n\n   first.pass2 <- function(w, n) {\n      m <- rowMeans(replicate(n, first.pass1(w)))\n      for (i in 2:nrow(w)) {\n         m <- rbind(m, rowMeans(replicate(n, first.pass1(w, seed = i))))\n      }\n   rownames(m) <- 1:nrow(w)\n   return(m)\n   }\n\nAnd the result (showing the first ten rows and columns) is:\n\n   set.seed(456)\n   round(first.pass2(P, 100), 1)[1:10, 1:10]\n\n      1    2    3    4    5    6    7    8    9   10\n1   0.0 16.1 32.9 21.2 17.2 24.4 60.0 27.8 25.9 17.8\n2  18.1  0.0 23.4 24.0 14.6 20.5 60.6 32.6 32.6 16.6\n3  20.6 22.2  0.0 19.9 17.1 29.1 63.1 29.8 26.8 19.0\n4  14.1 14.1 27.4  0.0 18.1 21.7 57.8 28.9 22.7 21.8\n5  16.9 16.7 30.0 25.1  0.0 25.0 52.9 36.5 19.9 24.1\n6  19.5 18.1 24.4 28.2 15.2  0.0 41.5 38.6 19.5 25.2\n7  20.7 15.9 26.8 24.8 16.3 14.2  0.0 33.0 26.1 23.7\n8  14.1 14.9 27.2 16.6 17.2 29.2 73.3  0.0 23.8 19.1\n9  18.1 16.1 27.5 23.9 16.3 20.7 52.1 32.7  0.0 20.1\n10 20.0 18.7 26.2 23.3 16.2 23.8 64.7 27.0 21.2  0.0\n\n\nThe accuracy of the average first passage time estimate we get depends on the number of replications we use to get the average. The bigger, the more accurate. However, it can take a lot of computing time if we increased n to a giant number."
  },
  {
    "objectID": "random-walk.html#iterating-until-convergence",
    "href": "random-walk.html#iterating-until-convergence",
    "title": "Random Walk Concepts in Networks",
    "section": "Iterating Until Convergence",
    "text": "Iterating Until Convergence\nThere’s another approach to computing the average first passage time that involves iterating through a matrix using the information in the transition matrix. The basic idea is that the average first passage time for a random walker that starts at node \\(i\\) and ends at node \\(j\\) is given by:\n\\[\nm_{ij} = 1 + \\sum^N_{j \\neq k}p_{ik}m_{k,j}\n\\]\nWith the proviso that \\(m_{ii}\\) is always equal to zero. What this tells us is that the average first passage time between any two nodes in the graph \\(i\\) and \\(j\\), is given by one plus the product probability that something can transition from a sender node \\(i\\) to an intermediary node \\(k\\)—given by \\(p_{ik}\\)—and \\(m_{kj}\\) which is the average first passage time from that intermediary node \\(k\\) to the destination node \\(j\\).\nNote that since we need to know \\(m_{kj}\\) to get \\(m_{ij}\\), then this opens up a chicken or the egg problem that we can solve through iteration like we did for the status scores in the prestige lesson. That is, we first start with a null value for all the entries of the \\(\\mathbf{M}\\) matrix (e.g., \\(m_{ij} = 0\\) for all \\(i\\) and \\(j\\)), compute an initial round of \\(m_{ij}\\) estimates for all pairs of nodes \\(i\\) and \\(j\\) using the equation above, recompute \\(m_{ij}\\) using those new values, rinse, repeat, and stop after we don’t get any changes between successive versions of the \\(\\mathbf{M}\\) matrix.\nA function that implement this idea goes like this:\n\n   first.pass3 <- function(w) {\n      n <- nrow(w)\n      nodes <- 1:n\n      m <- matrix(0, n, n) #initializing M matrix values\n      colnames(m) <- 1:n\n      rownames(m) <- 1:n\n      d <- 1\n      z <- 1\n      while(d > 1e-05) {\n      old.m <- m\n      for (i in nodes) { #loop through every node\n         for (j in nodes[-i]) { #loop through every node except i\n            m[i,j] <- 1\n            for (k in nodes[-j]) { #loop through every node except i and j\n               m[i,j] <- m[i,j] + (w[i,k]*old.m[k,j]) #update M matrix values\n               }\n            }\n         }\n      z <- z + 1\n      d <- abs(sum(abs(m) - abs(old.m))) #difference between current and previous M matrix\n      } #end while loop\n      return(m)\n   }\n\nThis function takes the probability transition matrix \\(\\mathbf{P}\\) as input and returns \\(\\mathbf{M}\\) a matrix of average first passage times between every pair of nodes in the network.\nAnd the result is:\n\n   round(first.pass3(P), 1)[1:10, 1:10]\n\n      1    2    3    4    5    6    7    8    9   10\n1   0.0 15.0 28.5 21.1 17.7 25.4 59.7 29.9 28.5 21.2\n2  17.0  0.0 29.0 22.3 16.1 23.2 59.0 33.8 28.2 21.8\n3  19.1 17.7  0.0 25.7 16.6 25.4 57.6 34.1 27.4 18.6\n4  15.1 14.4 29.1  0.0 18.0 25.6 59.9 28.6 28.7 20.9\n5  19.4 15.8 27.6 25.7  0.0 24.8 58.4 34.5 25.1 19.9\n6  19.3 15.0 28.6 25.3 16.9  0.0 50.5 35.0 24.2 21.8\n7  19.9 17.3 27.1 26.1 17.0 16.9  0.0 35.2 27.1 22.2\n8  14.9 16.8 28.3 19.5 17.8 26.1 59.9  0.0 28.0 17.7\n9  19.4 17.2 27.7 25.6 14.4 21.3 57.8 34.1  0.0 18.4\n10 18.7 17.3 25.4 24.3 15.7 25.4 59.4 30.2 24.9  0.0\n\n\nWhich are values pretty close to the ones we obtained by averaging earlier."
  },
  {
    "objectID": "random-walk.html#using-the-laplacian-matrix",
    "href": "random-walk.html#using-the-laplacian-matrix",
    "title": "Random Walk Concepts in Networks",
    "section": "Using the Laplacian Matrix",
    "text": "Using the Laplacian Matrix\nFinally, there is a way to use matrix algebra magic to compute the average first pass at the limit (\\(n \\approx \\infty\\)) in closed form without averaging or iterations.\nTo do that, first we need to compute a matrix called the graph Laplacian, which is defined as:\n\\[\n\\mathbf{L} = \\mathbf{D} - \\mathbf{A}\n\\]\nWhere \\(\\mathbf{D}\\) is a diagonal matrix containing the degrees of each node in the graph along the diagonals and zeroes everywhere else.\nIn R we can compute \\(\\mathbf{L}\\) like this:\n\n   D <- diag(rowSums(A))\n   L <- D - A\n\nOnce we have \\(\\mathbf{L}\\), we need to compute a variation known by the less than memorable name of the Moore-Penrose Pseudo-Inverse of the Laplacian, written as \\(\\mathbf{L}^+\\).\nDespite the terrible name, \\(\\mathbf{L}^+\\) is easy to compute:\n\n   E <- matrix(1/nrow(A), nrow(A), nrow(A))\n   L.plus <- solve(L - E) + E\n\nAs we can see, E is just a matrix containing the inverse of the number of nodes in the network in every cell.\nNow, the average first passage time between every pair of nodes in the network, contained in the matrix \\(\\mathbf{M}\\) is given by:\n\\[\n\\mathbf{M} = vol(A)(\\mathbf{e}\\mathbf{L}^+_{diag})^T - vol(A)\\mathbf{L}^+ + (\\mathbf{L}^+\\mathbf{d})\\mathbf{e}^T -\\mathbf{e}(\\mathbf{d}^T\\mathbf{L}^+)\n\\]\nNow, this formula looks long and monstrous but it is composed of simple quantities we know and love. We have already been introduced to \\(\\mathbf{L}^+\\), while \\(vol(A) = \\sum_{ij} a_{ij}\\) is just the sum of the non-zero entries in the adjacency matrix, \\(\\mathbf{e}\\) is a column vector of ones with as many rows as the number of nodes in the graph, \\(\\mathbf{d}\\) is a vector of the same length as the number of nodes in the graph containing the degrees of each node at each position, and \\(\\mathbf{L}^+_{diag}\\) is a vector containing the diagonal entries of \\(\\mathbf{L}^+\\) at each position.\nThe following R code constructs \\(\\mathbf{M}\\) step by step:\n\n   e <- matrix(1, nrow(A), 1)\n   d <- rowSums(A)\n   M <- sum(A) * t(e %*% diag(L.plus))\n   M <- M - (sum(A) * L.plus)\n   M <- M + (L.plus %*% d) %*% t(e)\n   M <- M - (e %*% (t(d) %*% L.plus))\n   rownames(M) <- 1:nrow(A)\n   colnames(M) <- 1:nrow(A)\n   M <- t(M)\n\nAnd now for the big reveal:\n\n   round(M, 1)[1:10, 1:10]\n\n      1    2    3    4    5    6    7    8    9   10\n1   0.0 14.9 27.7 20.6 17.3 24.5 56.1 28.8 27.6 20.6\n2  17.1  0.0 28.4 22.0 15.8 22.3 55.5 32.8 27.6 21.3\n3  19.9 18.3  0.0 26.0 16.9 25.3 54.7 33.7 27.3 18.8\n4  15.6 14.8 28.8  0.0 18.1 25.1 56.8 27.9 28.4 20.8\n5  19.9 16.1 27.3 25.6  0.0 24.3 55.3 33.8 24.7 19.8\n6  20.2 15.8 28.8 25.8 17.5  0.0 47.8 34.8 24.3 22.1\n7  23.6 20.8 30.0 29.2 20.2 19.6  0.0 37.7 29.9 25.2\n8  16.0 17.8 28.7 20.1 18.5 26.3 57.4  0.0 28.4 18.2\n9  20.2 17.9 27.7 26.0 14.7 21.2 55.0 33.7  0.0 18.6\n10 19.3 17.8 25.3 24.4 15.9 25.1 56.4 29.7 24.7  0.0\n\n\nWhich shows entries pretty close in value to the ones we obtained by iteration and averaging."
  },
  {
    "objectID": "random-walk.html#the-average-commute-distance",
    "href": "random-walk.html#the-average-commute-distance",
    "title": "Random Walk Concepts in Networks",
    "section": "The Average Commute Distance",
    "text": "The Average Commute Distance\nOnce we have the average first passage time, we can compute another important quantity called the average commute distance between two nodes \\(i\\) and \\(j\\) (\\(n_{ij}\\)). This is the number of steps it takes for a random walker to start at node \\(i\\), reach another specific node \\(j\\) and then get back to the original node \\(i\\) (hence commuting, like going fromm to work and back home again).\nIt turns out that \\(n_{ij}\\) is pretty simple to compute, once we know the average first passage time between every pair of nodes \\(m_{ij}\\), since it is given by:\n\\[\n   n_{ij} = m_{ij} + m_{ji}\n\\]\nSo the Average Commute Distance is just the entries of \\(\\mathbf{M}\\) on the upper triangle added to the corresponding entries in the lower triangle:\n\n   N <- M + t(M)\n   round(N[1:10, 1:10], 1)\n\n      1    2    3    4    5    6    7    8    9   10\n1   0.0 32.0 47.6 36.2 37.1 44.7 79.6 44.8 47.9 39.9\n2  32.0  0.0 46.7 36.7 31.9 38.2 76.3 50.6 45.5 39.0\n3  47.6 46.7  0.0 54.8 44.2 54.0 84.7 62.4 55.0 44.0\n4  36.2 36.7 54.8  0.0 43.7 50.9 86.0 48.1 54.4 45.2\n5  37.1 31.9 44.2 43.7  0.0 41.8 75.4 52.3 39.5 35.6\n6  44.7 38.2 54.0 50.9 41.8  0.0 67.4 61.1 45.5 47.2\n7  79.6 76.3 84.7 86.0 75.4 67.4  0.0 95.1 84.9 81.6\n8  44.8 50.6 62.4 48.1 52.3 61.1 95.1  0.0 62.1 47.9\n9  47.9 45.5 55.0 54.4 39.5 45.5 84.9 62.1  0.0 43.3\n10 39.9 39.0 44.0 45.2 35.6 47.2 81.6 47.9 43.3  0.0\n\n\nNote that the entries in this matrix are symmetric (it takes as long for something to go from to you to me and back to you as from me to you and back to me). They thus function as a similarity metric between nodes; the lower the average commute distance, the closer or more similar the two nodes are.\nOf course, there’s also math to compute \\(\\mathbf{N}\\) directly from \\(\\mathbf{L}^+\\) using the same ingredients we used before. It goes like this:\n\\[\n\\mathbf{N} = (\\mathbf{L}^+_{diag}\\mathbf{e}^T + \\mathbf{e}\\mathbf{L}^+_{diag} - 2\\mathbf{L}^+)vol(A)\n\\]\nWhich is actually a much less monstrous and simpler expression than before.\nThe following R code constructs \\(\\mathbf{N}\\) step by step:\n\n   N <- diag(L.plus) %*% t(e)\n   N <- N + (e %*% diag(L.plus))\n   N <- N - (2 * L.plus)\n   N <- N * sum(A)\n   round(N, 1)[1:10, 1:10]\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n [1,]  0.0 32.0 47.6 36.2 37.1 44.7 79.6 44.8 47.9  39.9\n [2,] 32.0  0.0 46.7 36.7 31.9 38.2 76.3 50.6 45.5  39.0\n [3,] 47.6 46.7  0.0 54.8 44.2 54.0 84.7 62.4 55.0  44.0\n [4,] 36.2 36.7 54.8  0.0 43.7 50.9 86.0 48.1 54.4  45.2\n [5,] 37.1 31.9 44.2 43.7  0.0 41.8 75.4 52.3 39.5  35.6\n [6,] 44.7 38.2 54.0 50.9 41.8  0.0 67.4 61.1 45.5  47.2\n [7,] 79.6 76.3 84.7 86.0 75.4 67.4  0.0 95.1 84.9  81.6\n [8,] 44.8 50.6 62.4 48.1 52.3 61.1 95.1  0.0 62.1  47.9\n [9,] 47.9 45.5 55.0 54.4 39.5 45.5 84.9 62.1  0.0  43.3\n[10,] 39.9 39.0 44.0 45.2 35.6 47.2 81.6 47.9 43.3   0.0\n\n\nWhich as you can see, gives us the results we seek.\nInterestingly, we can obtain the average commute time distance between any pair nodes yet another way. For instance from the above matrix, we know the average commute time distances between nodes 3 and 8 is 62.38.\nLet’s construct two vectors full of zeros of the same length as the number of nodes in the graph, except they have a one in the third and eighth spot respectively:\n\n   n <- ncol(A)\n   i <- rep(0, n)\n   j <- rep(0, n)\n   i[3] <- 1\n   j[8] <- 1\n\nFouss, Pirotte, and Saerens (2004) show that the average commute time distance between nodes 3 and 8 is also given by:\n\n   round(sum(A) * (t((i - j)) %*% L.plus %*% (i - j)), 2)\n\n      [,1]\n[1,] 62.38\n\n\nNeat!"
  },
  {
    "objectID": "schedule-208A-F24.html",
    "href": "schedule-208A-F24.html",
    "title": "SOCIOL 208A Reading Schedule (Fall 2024)",
    "section": "",
    "text": "Prell, C. & Schaefer, D. R. (2023). Introducing Social Network Analysis. In J. McLevey, J. Scott, P. J. Carrington (Eds.) The SAGE Handbook of Social Network Analysis. Sage Publications. link\nLight, R. & Moody, J. (2021). Network Basics: Points, Lines, and Positions. In R. Light, and J. Moody (Eds.) The Oxford Handbook of Social Networks Oxford University Press. link\nHarary, F. & Norman., R. Z. (1953). Graph Theory as a Mathematical Model in Social Science. Research Center for Group Dynamics, University of Michigan. link\n\n\n\n\n\nBasic Network Concepts and Definitions Cheat Sheet.\nBasic Introduction to R\nThe Basics of the R Programming Language Short Intergraph Tutorial\n\nPackage networkdata"
  },
  {
    "objectID": "schedule-208A-F24.html#week-2-october-9-centrality",
    "href": "schedule-208A-F24.html#week-2-october-9-centrality",
    "title": "SOCIOL 208A Reading Schedule (Fall 2024)",
    "section": "Week 2, October 9: Centrality",
    "text": "Week 2, October 9: Centrality\n\nReadings\n\nMartin G. Everett & Steve P. Borgatti (2023). “Centrality.” In J. McLevey, J. Scott, P. J. Carrington (Eds.) The SAGE Handbook of Social Network Analysis. Sage Publications. link\nFreeman, L. C. (1978). Centrality in Social Networks Conceptual Clarification. Social Networks, 1(3), 215-239. pdf\nKoschützki, D., Lehmann, K.A., Peeters, L., Richter, S., Tenfelde-Podehl, D., Zlotowski, O. (2005). Centrality Indices. In: Brandes, U., Erlebach, T. (eds) Network Analysis. Lecture Notes in Computer Science, vol 3418. Springer, Berlin, Heidelberg (secs. 3.2, 3.3, and 3.4). link\nNeal, Z. P. (2014). A network perspective on the processes of empowered organizations. American Journal of Community Psychology, 53, 407-418. https://doi.org/10.1007/s10464-013-9623-1\nAgneessens, F., Borgatti, S. P., & Everett, M. G. (2017). Geodesic based centrality: Unifying the local and the global. Social Networks, 49, 12-26. link\n\n\n\nExplainers\n\nLizardo, O. (n.d). Centralities based on Degree. link\nLizardo, O. (n.d). Centralities based on the Geodesic Distance. link\nLizardo, O. (n.d). Centralities based on Shortest Paths. link\n\n\n\nOther Material & Further Reading\n\nBorgatti, S. P., & Everett, M. G. (2006). A graph-theoretic perspective on centrality. Social Networks, 28(4), 466-484. link\nBrandes, U., Borgatti, S. P., & Freeman, L. C. (2016). Maintaining the duality of closeness and betweenness centrality. Social networks, 44, 153-159. link\nKoschützki, D., Lehmann, K.A., Tenfelde-Podehl, D., Zlotowski, O. (2005). Advanced Centrality Concepts. In: Brandes, U., Erlebach, T. (eds) Network Analysis. Lecture Notes in Computer Science, vol 3418. Springer, Berlin, Heidelberg. link\nComprehensive list of centrality measures with formulas and software"
  },
  {
    "objectID": "schedule-208A-F24.html#week-3-october-16-status-and-prestige",
    "href": "schedule-208A-F24.html#week-3-october-16-status-and-prestige",
    "title": "SOCIOL 208A Reading Schedule (Fall 2024)",
    "section": "Week 3, October 16: Status and Prestige",
    "text": "Week 3, October 16: Status and Prestige\n\nFranceschet, M. (2011). PageRank: standing on the shoulders of giants. Communications of the ACM, 54(6), 92-101. link\nMartin, J. L. & Murphy, J. P. (2021). Networks, Status, and Inequality. In R. Light, and J. Moody (Eds.) The Oxford Handbook of Social Networks Oxford University Press. link\nKoschützki, D., Lehmann, K.A., Peeters, L., Richter, S., Tenfelde-Podehl, D., Zlotowski, O. (2005). Centrality Indices. In: Brandes, U., Erlebach, T. (eds) Network Analysis. Lecture Notes in Computer Science, vol 3418. Springer, Berlin, Heidelberg (sec. 3.9). link"
  },
  {
    "objectID": "schedule-208A-F24.html#further-substantive-reading",
    "href": "schedule-208A-F24.html#further-substantive-reading",
    "title": "SOCIOL 208A Reading Schedule (Fall 2024)",
    "section": "Further (Substantive) Reading",
    "text": "Further (Substantive) Reading\n\nRossman, G., Esparza, N., & Bonacich, P. (2010). I’d Like To Thank The Academy, Team Spillovers, and Network Centrality. American Sociological Review, 75(1), 31-51. link\n\n\nFurther (Mathy) Reading\n\nVigna, S. (2016). Spectral ranking. Network Science, 4(4), 433-445. pdf\nBaltz, A., Kliemann, L. (2005). Spectral Analysis. In: Brandes, U., Erlebach, T. (eds) Network Analysis. Lecture Notes in Computer Science, vol 3418. Springer, Berlin, Heidelberg. link\nBonacich, P. (1972). Factoring and Weighting Approaches to Status Scores and Clique Identification. Journal of Mathematical Sociology, 2(1), 113-120. pdf\nKatz, L. (1953). A New Status Index Derived from Sociometric Analysis. Psychometrika, 18(1), 39-43. pdf"
  },
  {
    "objectID": "schedule-208A-F24.html#week-4-october-23",
    "href": "schedule-208A-F24.html#week-4-october-23",
    "title": "SOCIOL 208A Reading Schedule (Fall 2024)",
    "section": "Week 4, October 23:",
    "text": "Week 4, October 23:\n\nNo Class (Traveling)"
  },
  {
    "objectID": "schedule-208A-F24.html#week-5-october-30-similarity-roles-and-positions",
    "href": "schedule-208A-F24.html#week-5-october-30-similarity-roles-and-positions",
    "title": "SOCIOL 208A Reading Schedule (Fall 2024)",
    "section": "Week 5, October 30: Similarity, Roles, and Positions",
    "text": "Week 5, October 30: Similarity, Roles, and Positions\n\nReadings\n\nBurt, R. S. (1976). Positions in networks. Social Forces, 55(1), 93-122. link\nBreiger, R. L., Boorman, S. A., & Arabie, P. (1975). An algorithm for clustering relational data with applications to social network analysis and comparison with multidimensional scaling. Journal of Mathematical Psychology, 12(3), 328-383. link\nLü, L., Jin, C. H., & Zhou, T. (2009). Similarity index based on local paths for link prediction of complex networks. Physical Review E—Statistical, Nonlinear, and Soft Matter Physics, 80(4), 046122. link\nJeh, G., & Widom, J. (2002). Simrank: a measure of structural-context similarity. In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 538-543). link\nLeicht, E. A., Holme, P., & Newman, M. E. (2006). Vertex similarity in networks. Physical Review E—Statistical, Nonlinear, and Soft Matter Physics, 73(2), 026120. link\n\n\n\nFurther Reading\n\nFouss, F., Pirotte, A., Renders, J. M., & Saerens, M. (2007). Random-walk computation of similarities between nodes of a graph with application to collaborative recommendation. IEEE Transactions on knowledge and data engineering, 19(3), 355-369. link\nKovács, B. (2010). A generalized model of relational similarity. Social Networks, 32(3), 197-211. link\nLiben-Nowell, D., & Kleinberg, J. (2003). The link prediction problem for social networks. In Proceedings of the Twelfth Annual ACM International Conference on Information and Knowledge Management (CIKM’03) (pp. 556-559). link to longer paper\nLü, L., & Zhou, T. (2011). Link prediction in complex networks: A survey. Physica A: statistical mechanics and its applications, 390(6), 1150-1170. link\n\n\n\nCheat Sheets:\n\nCentrality, Status, and Node Similarity Cheat Sheet.\nChroł, B & Bojanowski, M. (2018). Proximity-based Methods for Link Prediction. https://cran.r-project.org/web/packages/linkprediction/vignettes/proxfun.html"
  },
  {
    "objectID": "schedule-208A-F24.html#week-6-november-6-cohesive-subgroups-and-communities",
    "href": "schedule-208A-F24.html#week-6-november-6-cohesive-subgroups-and-communities",
    "title": "SOCIOL 208A Reading Schedule (Fall 2024)",
    "section": "Week 6, November 6: Cohesive Subgroups and Communities",
    "text": "Week 6, November 6: Cohesive Subgroups and Communities\n\nReadings\n\nMoody, J., & Mucha, P. J. (2023). Structural Cohesion and Cohesive Groups. In J. McLevey, J. Scott, P. J. Carrington (Eds.) The SAGE Handbook of Social Network Analysis. Sage Publications. link\nShai, S., Stanley, N., Granell, C., Taylor, D. & Mucha, P. J. (2021). Case Studies in Network Community Detection. In R. Light, and J. Moody (Eds.) The Oxford Handbook of Social Networks Oxford University Press. link\nNewman, M. E. (2018). Community Structure. In Networks, 2nd Edition. Oxford, Online Edition, Oxford Academic. link\n\n\n\nFurther (Substantive) Readings\n\nMelamed, D. (2015). Communities of classes: A network approach to social mobility. Research in Social Stratification and Mobility, 41, 56-65. link\nShwed, U., & Bearman, P. S. (2010). The temporal structure of scientific consensus formation. American Sociological Review, 75(6), 817-840. link\n\n\n\nFurther (Mathy) Reading\n\nClauset, A., Newman, M. E., & Moore, C. (2004). Finding community structure in very large networks. Physical Review E—Statistical, Nonlinear, and Soft Matter Physics, 70(6), 066111. link\nFortunato, S. (2010). Community Detection in Graphs. Physics Reports, 486(3-5), 75-174. link\nGirvan, M., & Newman, M. E. (2002). Community Structure in Social and Biological Networks. Proceedings of the National academy of Sciences, 99(12), 7821-7826. link\nLeicht, E. A., and Newman, M. E. (2008). Community Structure in Directed Networks. Physical Review Letters 100, 118703. link\nNewman, M. E. (2006). Modularity and Community Structure in Networks. Proceedings of the National Academy of Sciences, 103(23), 8577-8582. link\nNewman, M. E., & Girvan, M. (2003). Mixing patterns and community structure in networks. In Statistical mechanics of complex networks (pp. 66-87). Berlin, Heidelberg: Springer Berlin Heidelberg.\nNewman, M. E. (2003). Mixing Patterns in Networks. Physical review E 67(2), 026126. link\nNewman, M. E., & Girvan, M. (2004). Finding and evaluating community structure in networks. Physical review E, 69(2), 026113. link"
  },
  {
    "objectID": "schedule-208A-F24.html#week-7-november-13-analyzing-two-mode-networks",
    "href": "schedule-208A-F24.html#week-7-november-13-analyzing-two-mode-networks",
    "title": "SOCIOL 208A Reading Schedule (Fall 2024)",
    "section": "Week 7, November 13: Analyzing Two-Mode Networks",
    "text": "Week 7, November 13: Analyzing Two-Mode Networks\n\nReadings\n\nBreiger, R. L. (1974). The Duality of Persons and Groups. Social Forces, 53(2), 181–190. link\nBorgatti, S. P., & Everett, M. G. (1997). Network Analysis of 2-Mode Data. Social Networks, 19(3), 243-269. pdf\nEverett, M. G., & Borgatti, S. P. (2013). The Dual-Projection Approach for Two-Mode Networks. Social Networks, 35(2), 204-210. link\nNeal, Z. (2014). The backbone of bipartite projections: Inferring relationships from co-authorship, co-sponsorship, co-attendance and other co-behaviors. Social Networks, 39, 84-97. link"
  },
  {
    "objectID": "schedule-208A-F24.html#further-reading-1",
    "href": "schedule-208A-F24.html#further-reading-1",
    "title": "SOCIOL 208A Reading Schedule (Fall 2024)",
    "section": "Further Reading",
    "text": "Further Reading\n\nBorgatti, S., & Halgin, D. (2014). Analyzing affiliation networks. In The SAGE Handbook of Social Network Analysis, First Edition (pp. 417-433), SAGE Publications Ltd. link\nFaust, K. (1997). Centrality in affiliation networks. Social Networks, 19(2), 157-191. link\n\n\nOther Material\n\nMurphy, Phil, and Brendan Knapp. (2018). Bipartite/two-mode networks in igraph. RPubs https://rpubs.com/pjmurphy/317838\nDomagalski, R., Neal, Z. P., & Sagan, B. (2021). Backbone: An R package for extracting the backbone of bipartite projections. Plos one, 16(1), e0244363. link\nNeal, Z. P. (2022). backbone: An R package to extract network backbones. PloS one, 17(5), e0269137. link"
  },
  {
    "objectID": "schedule-208A-F24.html#week-8-november-20-ego-networks",
    "href": "schedule-208A-F24.html#week-8-november-20-ego-networks",
    "title": "SOCIOL 208A Reading Schedule (Fall 2024)",
    "section": "Week 8, November 20: Ego Networks",
    "text": "Week 8, November 20: Ego Networks\n\nReadings\n\nSmith, J. A. (2021). The Continued Relevance of Ego Network Data. In R. Light, and J. Moody (Eds.) The Oxford Handbook of Social Networks Oxford University Press. link"
  },
  {
    "objectID": "schedule-208A-F24.html#week-9-november-27-statistical-models-of-network-structure-i",
    "href": "schedule-208A-F24.html#week-9-november-27-statistical-models-of-network-structure-i",
    "title": "SOCIOL 208A Reading Schedule (Fall 2024)",
    "section": "Week 9, November 27: Statistical Models of Network Structure I",
    "text": "Week 9, November 27: Statistical Models of Network Structure I\n\nReadings\n\nRobins, G., Pattison, P., Kalish, Y., & Lusher, D. (2007). An introduction to exponential random graph (p*) models for social networks. Social Networks, 29(2), 173-191. link\nMorris, M., Handcock, M. S., & Hunter, D. R. (2008). Specification of exponential-family random graph models: terms and computational aspects. Journal of Statistical Software, 24(4), 1548. link\nPattison, P., & Robins, G. (2002). Neighborhood–based models for social networks. Sociological Methodology, 32(1), 301-337. link"
  },
  {
    "objectID": "schedule-208A-F24.html#further-reading-2",
    "href": "schedule-208A-F24.html#further-reading-2",
    "title": "SOCIOL 208A Reading Schedule (Fall 2024)",
    "section": "Further Reading",
    "text": "Further Reading\n\nLusher D., Wang, P., Brennecke, J., Brailly J., Faye, M., Gallagher, C. (2021). Advances in Exponential Random Graph Models. In R. Light, and J. Moody (Eds.) The Oxford Handbook of Social Networks, Oxford University Press. link\nOrsini, C., Dankulov, M. M., Colomer-de-Simón, P., Jamakovic, A., Mahadevan, P., Vahdat, A., … & Krioukov, D. (2015). Quantifying randomness in real networks. Nature communications, 6(1), 8627. link"
  },
  {
    "objectID": "schedule-208A-F24.html#week-10-december-4-statistical-models-of-network-structure-ii",
    "href": "schedule-208A-F24.html#week-10-december-4-statistical-models-of-network-structure-ii",
    "title": "SOCIOL 208A Reading Schedule (Fall 2024)",
    "section": "Week 10, December 4: Statistical Models of Network Structure II",
    "text": "Week 10, December 4: Statistical Models of Network Structure II"
  },
  {
    "objectID": "schedule-208B-S22.html",
    "href": "schedule-208B-S22.html",
    "title": "SOCIOL 208B Reading Schedule (Spring 2022)",
    "section": "",
    "text": "Marsden, P. V., & Laumann, E. O. (1984). Mathematical Ideas In Social Structural Analysis. Journal of Mathematical Sociology, 10(3-4), 271-294. [pdf]\nWellman, B. (1988). Structural Analysis: From Method and Metaphor To Theory and Substance. In B. Wellman & S. D. Berkowitz (Eds.), Social Structures: A Network Approach. (Pp. 19–61). Cambridge University Press. [pdf]\nEmirbayer, M. (1997). Manifesto For A Relational Sociology. American Journal of Sociology, 103(2), 281–317. [pdf]\n\n\n\n\n\nErikson, E. (2013). Formalist and Relationalist Theory In Social Network Analysis. Sociological Theory, 31(3), 219–242. [pdf]\nEmirbayer, M., & Goodwin, J. (1994). Network Analysis, Culture, and The Problem of Agency. American Journal of Sociology, 99(6), 1411–1454. [pdf]\nMische, A. (2011). Relational Sociology, Culture, and Agency. In J. Scott & P. J. Carrington (Eds.), The Sage Handbook of Social Network Analysis (Pp. 80–97). Sage. [pdf]"
  },
  {
    "objectID": "schedule-208B-S22.html#week-2-the-greatest-hits",
    "href": "schedule-208B-S22.html#week-2-the-greatest-hits",
    "title": "SOCIOL 208B Reading Schedule (Spring 2022)",
    "section": "Week 2: The Greatest Hits",
    "text": "Week 2: The Greatest Hits\n\nTue., Apr. 5\n\nWhite, H. C., Boorman, S. A., & Breiger, R. L. (1976). Social Structure From Multiple Networks. I. Blockmodels of Roles and Positions. American Journal of Sociology, 81(4), 730–780. [pdf]\nBreiger, R. L. (1974). The Duality of Persons and Groups. Social Forces, 53(2), 181–190. [pdf]\nFreeman, L. C. (1978). Centrality in social networks conceptual clarification. Social networks, 1(3), 215-239. [pdf]\n\n\n\nThu., Apr. 7\n\nGranovetter, M. S. (1973). The Strength of Weak Ties. American Journal of Sociology, 78(3), 1360–1380. [pdf]\n\nRejection letter from American Sociological Review of the first (1969) version of the paper [pdf]\nGranovetter, M. S. (1969) ``Alienation Reconsidered: The Strength of Weak Ties.’’ Reprinted in Connections 5(2): 4-16. [pdf]\n\nCentola, D., & Macy, M. (2007). Complex Contagions and the Weakness of Long Ties. American Journal of Sociology, 113(3), 702-734. [pdf]\nMcPherson, M., Smith-Lovin, L., & Cook, J. M. (2001). Birds of a feather: Homophily in social networks. Annual Review of Sociology, 27(1), 415-444. [pdf]"
  },
  {
    "objectID": "schedule-208B-S22.html#week-3-brokerage-and-intermediation",
    "href": "schedule-208B-S22.html#week-3-brokerage-and-intermediation",
    "title": "SOCIOL 208B Reading Schedule (Spring 2022)",
    "section": "Week 3: Brokerage and Intermediation",
    "text": "Week 3: Brokerage and Intermediation\n\nTue., Apr. 12\n\nGould, R. V., & Fernandez, R. M. (1989). Structures of Mediation: A Formal Approach To Brokerage In Transaction Networks. Sociological Methodology, 89-126. [pdf]\nObstfeld, D., Borgatti, S.P. and Davis, J. (2014). Brokerage As a Process: Decoupling Third Party Action From Social Network Structure. Research In The Sociology of Organizations, 40), 135-159. [pdf]\nStovel, K., & Shaw, L. (2012). Brokerage. Annual Review of Sociology, 38, 139-158. [pdf]\n\n\n\nThu., Apr. 14\n\nBurt, R. S. (2004). Structural Holes and Good Ideas. American Journal of Sociology, 110(2), 349-399.[pdf]\nGoldberg, A., Srivastava, S. B., Manian, V. G., Monroe, W., & Potts, C. (2016). Fitting In Or Standing Out? The Tradeoffs of Structural and Cultural Embeddedness. American Sociological Review, 81(6), 1190-1222. [pdf]\nAral, S., & Van Alstyne, M. (2011). The diversity-bandwidth trade-off. American Journal of Sociology, 117(1), 90-171. [pdf]"
  },
  {
    "objectID": "schedule-208B-S22.html#week-4-networks-in-science",
    "href": "schedule-208B-S22.html#week-4-networks-in-science",
    "title": "SOCIOL 208B Reading Schedule (Spring 2022)",
    "section": "Week 4: Networks In Science",
    "text": "Week 4: Networks In Science\n\nTue., Apr. 19\n\nMoody, J. (2004). The Structure of A Social Scientific Collaboration Network. American Sociological Review, 69, 213-238. [pdf]\nShwed, U., & Bearman, P. S. (2010). The Temporal Structure of Scientific Consensus Formation. American Sociological Review, 75(6), 817-840. [pdf]\nMoody, J., & Light, R. (2006). A view from above: The evolving sociological landscape. The American Sociologist, 37(2), 67-86. [pdf]\n\n\n\nThu., Apr. 21\n\nFoster, J. G., Rzhetsky, A., & Evans, J. A. (2015). Tradition and Innovation In Scientists’ Research Strategies. American Sociological Review, 80(5), 875-908. [pdf]\nMcmahan, P., & Mcfarland, D. A. (2021). Creative Destruction: The Structural Consequences of Scientific Curation. American Sociological Review, 86(2), 341-376. [pdf]"
  },
  {
    "objectID": "schedule-208B-S22.html#week-5-collaboration-creativity-and-field-dynamics",
    "href": "schedule-208B-S22.html#week-5-collaboration-creativity-and-field-dynamics",
    "title": "SOCIOL 208B Reading Schedule (Spring 2022)",
    "section": "Week 5: Collaboration, Creativity, and Field Dynamics",
    "text": "Week 5: Collaboration, Creativity, and Field Dynamics\n\nTue., Apr. 26\n\nVedres, B., & Stark, D. (2010). Structural Folds: Generative Disruption In Overlapping Groups. American Journal of Sociology, 115(4), 1150-1190. [pdf]\nUzzi, B, & Spiro, J. (2005). Collaboration and Creativity: The Small World Problem. American Journal of Sociology, 111, 447-504. [pdf]\n\n\n\nThu., Apr. 28\n\nPowell, W. W., White, D. R., Koput, K. W., & Owen-Smith, J. (2005). Network Dynamics and Field Evolution: The Growth of Interorganizational Collaboration In The Life Sciences. American Journal of Sociology, 110(4), 1132-1205. [pdf]\nRossman, G., Esparza, N., & Bonacich, P. (2010). I’d Like To Thank The Academy, Team Spillovers, and Network Centrality. American Sociological Review, 75(1), 31-51. [pdf]"
  },
  {
    "objectID": "schedule-208B-S22.html#week-6-networks-and-culture-and-culture-in-networks",
    "href": "schedule-208B-S22.html#week-6-networks-and-culture-and-culture-in-networks",
    "title": "SOCIOL 208B Reading Schedule (Spring 2022)",
    "section": "Week 6: Networks and Culture and Culture in Networks",
    "text": "Week 6: Networks and Culture and Culture in Networks\n\nTue., May. 3\n\nBreiger, R. L. (2010). Dualities of culture and structure: Seeing through cultural holes. Pp. 37-47 in Relationale soziologie. VS Verlag für Sozialwissenschaften. [pdf]\nLizardo, 0. (2023). Culture and Networks. Pp. 188-201 in The SAGE Handbook of Social Network Analysis. SAGE Publications Ltd. [pdf]\nLewis, K., & Kaufman, J. (2018). The Conversion of Cultural Tastes Into Social Network Ties. American Journal of Sociology, 123(6), 1684-1742. [pdf]\n\n\n\nThu., May. 5\n\nFuhse, J. A. (2009). The Meaning Structure of Social Networks. Sociological Theory, 27(1), 51-73. [pdf]\nIkegami, E. (2000). A Sociological Theory of Publics: Identity and Culture As Emergent Properties In Networks. Social Research, 989-1029. [pdf]\nMützel, S., & Breiger, R. (2021). Duality Beyond Persons and Groups. In The Oxford Handbook of Social Networks. Oxford University Press. [pdf]"
  },
  {
    "objectID": "schedule-208B-S22.html#week-7-difussion-in-networks",
    "href": "schedule-208B-S22.html#week-7-difussion-in-networks",
    "title": "SOCIOL 208B Reading Schedule (Spring 2022)",
    "section": "Week 7: Difussion In Networks",
    "text": "Week 7: Difussion In Networks\n\nTue., May. 10\n\nDellaposta, D., Shi, Y., & Macy, M. (2015) Why Do Liberals Drink Lattes?. American Journal of Sociology, 120(5), 1473-1511. [pdf]\nCentola, D. (2015). The social origins of networks and diffusion. American Journal of Sociology, 120(5), 1295-1338. [pdf]\nBecker, S. O., Hsiao, Y., Pfaff, S., & Rubin, J. (2020). Multiplex Network Ties and the Spatial Diffusion of Radical Innovations: Martin Luther’s Leadership In The Early Reformation. American Sociological Review, 85(5), 857-894. [pdf]\n\n\n\nThu., May. 12\n\nGoldberg, A., & Stein, S. K. (2018). Beyond Social Contagion: Associative Diffusion and The Emergence of Cultural Variation. American Sociological Review, 83(5), 897-932. [pdf]\nBail, C. A., Brown, T. W., & Mann, M. (2017). Channeling hearts and minds: Advocacy organizations, cognitive-emotional currents, and public conversation. American Sociological Review, 82(6), 1188-1213. [pdf]"
  },
  {
    "objectID": "schedule-208B-S22.html#week-8-networks-in-history",
    "href": "schedule-208B-S22.html#week-8-networks-in-history",
    "title": "SOCIOL 208B Reading Schedule (Spring 2022)",
    "section": "Week 8: Networks In History",
    "text": "Week 8: Networks In History\n\nTue., May. 17\n\nPadgett, J. F., & Ansell, C. K. (1993). Robust Action and the Rise of the Medici, 1400-1434. American Journal of Sociology, 98(6), 1259–1319. [pdf]\nGould, R. V. (1991). Multiple Networks and Mobilization In The Paris Commune, 1871. American Sociological Review, 56(6), 716-729. [pdf]\nErikson, E., & Feltham, E. (2021). Historical Network Research. In The Oxford Handbook of Social Networks. Oxford University Press. [pdf]\n\n\n\nThu., May. 19\n\nErikson, E., & Bearman, P. (2006). Malfeasance and the Foundations For Global Trade: The Structure of English Trade In The East Indies, 1601–1833. American Journal of Sociology, 112(1), 195-230. [pdf]\nBearman, P., Faris, R., & Moody, J. (1999). Blocking The Future: New Solutions For Old Problems In Historical Social Science. Social Science History, 23(4), 501-533. [pdf]"
  },
  {
    "objectID": "schedule-208B-S22.html#week-9-networks-and-inequality",
    "href": "schedule-208B-S22.html#week-9-networks-and-inequality",
    "title": "SOCIOL 208B Reading Schedule (Spring 2022)",
    "section": "Week 9: Networks and Inequality",
    "text": "Week 9: Networks and Inequality\n\nTue., May. 24\n\nGould, R. V. (2002). The Origins of Status Hierarchies: A Formal Theory and Empirical Test. American Journal of Sociology, 107(5), 1143-1178. [pdf]\nGondal, N. (2015). Inequality preservation through uneven diffusion of Cultural materials across stratified groups. Social Forces, 93(3), 1109-1137. [pdf]\nThomas, R. J., & Mark, N. P. (2013). Population size, network density, and the emergence of inherited inequality. Social Forces, 92(2), 521-544. [pdf]\n\n\n\nThu., May. 26\n\nBurris, V. (2004). The Academic Caste System: Prestige Hierarchies In PhD Exchange Networks. American Sociological Review, 69, 239-264. [pdf]\nFowler, J. H., Grofman, B., & Masuoka, N. (2007). Social networks in political science: Hiring and placement of Ph.Ds, 1960–2002. PS: Political Science & Politics, 40(4), 729-739. [pdf]\nClauset, A., Arbesman, S., & Larremore, D. B. (2015). Systematic inequality and hierarchy in faculty hiring networks. Science Advances, 1(1), e1400005. [pdf]\nGondal, N. (2018). Duality of departmental specializations and PhD exchange: A Weberian analysis of status in interaction using multilevel exponential random graph models (mERGM). Social Networks, 55, 202-212. [pdf]"
  },
  {
    "objectID": "schedule-208B-S22.html#week-10-networks-and-the-micromacro-link",
    "href": "schedule-208B-S22.html#week-10-networks-and-the-micromacro-link",
    "title": "SOCIOL 208B Reading Schedule (Spring 2022)",
    "section": "Week 10: Networks and The Micro/Macro Link",
    "text": "Week 10: Networks and The Micro/Macro Link\n\nTue., May. 31\n\nMcfarland, D. A., Moody, J., Diehl, D., Smith, J. A., & Thomas, R. J. (2014). Network Ecology and Adolescent Social Structure. American Sociological Review, 79(6), 1088-1121. [pdf]\nBearman, P. S., Moody, J., & Stovel, K. (2004). Chains of Affection: The Structure of Adolescent Romantic and Sexual Networks. American Journal of Sociology, 110(1), 44-91. [pdf]\n\n\n\nThu., Jun. 2\n\nLevy, B. L., Phillips, N. E., & Sampson, R. J. (2020). Triple Disadvantage: Neighborhood Networks of Everyday Urban Mobility and Violence In US Cities. American Sociological Review, 85(6), 925-956. [pdf]\nPapachristos, A. V. (2009). Murder By Structure: Dominance Relations and the Social Structure of Gang Homicide. American Journal of Sociology, 115(1), 74-128. [pdf]"
  },
  {
    "objectID": "schedule-208B-S24.html",
    "href": "schedule-208B-S24.html",
    "title": "SOCIOL 208B Reading Schedule (Spring 2024)",
    "section": "",
    "text": "Wellman, B. (1988). Structural Analysis: From Method and Metaphor To Theory and Substance. In B. Wellman & S. D. Berkowitz (Eds.), Social Structures: A Network Approach. (Pp. 19–61). Cambridge University Press.\nFreeman, L. (2004). The Development of Social Network Analysis. Vancouver, BC: Empirical Press (Introduction, and Chap. 9)\n\n\n\n\n\nEmirbayer, M. (1997). Manifesto For A Relational Sociology. American Journal of Sociology, 103(2), 281–317.\nErikson, E. (2013). Formalist and Relationalist Theory In Social Network Analysis. Sociological Theory, 31(3), 219–242.\nMische, A. (2011). Relational Sociology, Culture, and Agency. In J. Scott & P. J. Carrington (Eds.), The Sage Handbook of Social Network Analysis (Pp. 80–97). Sage."
  },
  {
    "objectID": "schedule-208B-S24.html#week-2-the-greatest-hits",
    "href": "schedule-208B-S24.html#week-2-the-greatest-hits",
    "title": "SOCIOL 208B Reading Schedule (Spring 2024)",
    "section": "Week 2: The Greatest Hits",
    "text": "Week 2: The Greatest Hits\n\nTuesday, April 9\n\nWhite, H. C., Boorman, S. A., & Breiger, R. L. (1976). Social Structure From Multiple Networks. I. Blockmodels of Roles and Positions. American Journal of Sociology, 81(4), 730–780.\nBreiger, R. L. (1974). The Duality of Persons and Groups. Social Forces, 53(2), 181–190.\nFreeman, L. C. (1978). Centrality in social networks conceptual clarification. Social networks, 1(3), 215-239.\n\n\n\nThursday, April 11\n\nGranovetter, M. S. (1973). The Strength of Weak Ties. American Journal of Sociology, 78(3), 1360–1380.\n\nRejection letter from American Sociological Review of the first (1969) version of the paper\nGranovetter, M. S. (1969) “Alienation Reconsidered: The Strength of Weak Ties.” Reprinted in Connections 5(2): 4-16.\n\nWang, D., & Uzzi, B. (2022). Weak ties, failed tries, and success. Science, 377(6612), 1256-1258.\nMcPherson, M., Smith-Lovin, L., & Cook, J. M. (2001). Birds of a feather: Homophily in social networks. Annual Review of Sociology, 27(1), 415-444."
  },
  {
    "objectID": "schedule-208B-S24.html#week-3-brokerage",
    "href": "schedule-208B-S24.html#week-3-brokerage",
    "title": "SOCIOL 208B Reading Schedule (Spring 2024)",
    "section": "Week 3: Brokerage",
    "text": "Week 3: Brokerage\n\nTuesday, April 16 (Virtual Meeting)\n\nGould, R. V., & Fernandez, R. M. (1989). Structures of Mediation: A Formal Approach To Brokerage In Transaction Networks. Sociological Methodology, 89-126.\nObstfeld, D., Borgatti, S.P. and Davis, J. (2014). Brokerage As a Process: Decoupling Third Party Action From Social Network Structure. Research In The Sociology of Organizations, 40), 135-159.\nStovel, K., & Shaw, L. (2012). Brokerage. Annual Review of Sociology, 38, 139-158.\n\n\n\nThursday, April 18 (Virtual Meeting)\n\nBurt, R. S. (2004). Structural Holes and Good Ideas. American Journal of Sociology, 110(2), 349-399.\nGoldberg, A., Srivastava, S. B., Manian, V. G., Monroe, W., & Potts, C. (2016). Fitting In Or Standing Out? The Tradeoffs of Structural and Cultural Embeddedness. American Sociological Review, 81(6), 1190-1222.\nAral, S. (2016). The future of weak ties. American Journal of Sociology, 121(6), 1931-1939."
  },
  {
    "objectID": "schedule-208B-S24.html#week-4-science",
    "href": "schedule-208B-S24.html#week-4-science",
    "title": "SOCIOL 208B Reading Schedule (Spring 2024)",
    "section": "Week 4: Science",
    "text": "Week 4: Science\n\nTuesday, April 23\n\nMoody, J. (2004). The Structure of A Social Scientific Collaboration Network. American Sociological Review, 69, 213-238.\nClauset, A., Arbesman, S., & Larremore, D. B. (2015). Systematic inequality and hierarchy in faculty hiring networks. Science Advances, 1(1), e1400005.\n\n\n\nThursday, April 25\n\nGondal, N. (2018). Duality of departmental specializations and PhD exchange: A Weberian analysis of status in interaction using multilevel exponential random graph models (mERGM). Social Networks, 55, 202-212.\nMcmahan, P., & Mcfarland, D. A. (2021). Creative Destruction: The Structural Consequences of Scientific Curation. American Sociological Review, 86(2), 341-376.\nShwed, U., & Bearman, P. S. (2010). The Temporal Structure of Scientific Consensus Formation. American Sociological Review, 75(6), 817-840."
  },
  {
    "objectID": "schedule-208B-S24.html#week-5-collaboration-creativity",
    "href": "schedule-208B-S24.html#week-5-collaboration-creativity",
    "title": "SOCIOL 208B Reading Schedule (Spring 2024)",
    "section": "Week 5: Collaboration & Creativity",
    "text": "Week 5: Collaboration & Creativity\n\nTuesday, April 30\n\nUzzi, B, & Spiro, J. (2005). Collaboration and Creativity: The Small World Problem. American Journal of Sociology, 111, 447-504.\nDe Vaan, M., Stark, D., & Vedres, B. (2015). Game changer: The topology of creativity. American Journal of Sociology, 120(4), 1144-1194.\n\n\n\nThursday, May 2\n\nRossman, G., Esparza, N., & Bonacich, P. (2010). I’d Like To Thank The Academy, Team Spillovers, and Network Centrality. American Sociological Review, 75(1), 31-51.\nChoi, Y., Ingram, P., & Han, S. W. (2023). Cultural breadth and embeddedness: The individual adoption of organizational culture as a determinant of creativity. Administrative Science Quarterly, 68(2), 429-464.\nSilver, D., Childress, C., Lee, M., Slez, A., & Dias, F. (2022). Balancing categorical conventionality in music. American Journal of Sociology, 128(1), 224-286."
  },
  {
    "objectID": "schedule-208B-S24.html#week-6-difussion",
    "href": "schedule-208B-S24.html#week-6-difussion",
    "title": "SOCIOL 208B Reading Schedule (Spring 2024)",
    "section": "Week 6: Difussion",
    "text": "Week 6: Difussion\n\nTuesday, May 7\n\nDellaPosta, D., Shi, Y., & Macy, M. (2015). Why do liberals drink lattes?. American Journal of Sociology, 120(5), 1473-1511.\nDellaPosta, D. (2020). Pluralistic collapse: The “oil spill” model of mass opinion polarization. American Sociological Review, 85(3), 507-536.\nGoldberg, A., & Stein, S. K. (2018). Beyond Social Contagion: Associative Diffusion and The Emergence of Cultural Variation. American Sociological Review, 83(5), 897-932.\n\n\n\nThursday, May 9\n\nCentola, D., & Macy, M. (2007). Complex Contagions and the Weakness of Long Ties. American Journal of Sociology, 113(3), 702-734.\nGondal, N. (2023). Diffusion of innovations through social networks: Determinants and implications. Sociology Compass, 17(5), e13084.\nBail, C. A., Brown, T. W., & Wimmer, A. (2019). Prestige, proximity, and prejudice: how Google search terms diffuse across the world. American Journal of Sociology, 124(5), 1496-1548."
  },
  {
    "objectID": "schedule-208B-S24.html#week-7-culture",
    "href": "schedule-208B-S24.html#week-7-culture",
    "title": "SOCIOL 208B Reading Schedule (Spring 2024)",
    "section": "Week 7: Culture",
    "text": "Week 7: Culture\n\nTuesday, May 14\n\nLizardo, O. (2023). Culture and Networks. Pp. 188-201 in The SAGE Handbook of Social Network Analysis. SAGE Publications Ltd.\nLewis, K., & Kaufman, J. (2018). The Conversion of Cultural Tastes Into Social Network Ties. American Journal of Sociology, 123(6), 1684-1742.\nRawlings, C. M., & Childress, C. (2023). The Polarization of Popular Culture: Tracing the Size, Shape, and Depth of the “Oil Spill”. Social Forces, soad150.\n\n\n\nThursday, May 16 (No Meeting)"
  },
  {
    "objectID": "schedule-208B-S24.html#week-8-history",
    "href": "schedule-208B-S24.html#week-8-history",
    "title": "SOCIOL 208B Reading Schedule (Spring 2024)",
    "section": "Week 8: History",
    "text": "Week 8: History\n\nTuesday, May 21 (Virtual Meeting)\n\nPadgett, J. F., & Ansell, C. K. (1993). Robust Action and the Rise of the Medici, 1400-1434. American Journal of Sociology, 98(6), 1259–1319.\nGould, R. V. (1991). Multiple Networks and Mobilization In The Paris Commune, 1871. American Sociological Review, 56(6), 716-729.\nBearman, P., Moody, J., & Faris, R. (2002). Networks and History. Complexity, 8(1), 61-71.\n\n\n\nThursday, May 23\n\nErikson, E., & Feltham E., (2021) Historical Network Research, Pp. 432–442 in R Light, and J Moody (eds), The Oxford Handbook of Social Networks. Oxford University Press.\nErikson, E., & Bearman, P. (2006). Malfeasance and the Foundations For Global Trade: The Structure of English Trade In The East Indies, 1601–1833. American Journal of Sociology, 112(1), 195-230.\nBecker, S. O., Hsiao, Y., Pfaff, S., & Rubin, J. (2020). Multiplex Network Ties and the Spatial Diffusion of Radical Innovations: Martin Luther’s Leadership In The Early Reformation. American Sociological Review, 85(5), 857-894."
  },
  {
    "objectID": "schedule-208B-S24.html#week-9-inequality",
    "href": "schedule-208B-S24.html#week-9-inequality",
    "title": "SOCIOL 208B Reading Schedule (Spring 2024)",
    "section": "Week 9: Inequality",
    "text": "Week 9: Inequality\n\nTuesday, May 28\n\nDiMaggio, P., & Garip, F. (2012). Network effects and social inequality. Annual Review of Sociology, 38, 93-118.\nPedulla, D. S., & Pager, D. (2019). Race and networks in the job search process. American Sociological Review, 84(6), 983-1012.\n\n\n\nThursday, May 30\n\nErikson, E., & Occhiuto, N. (2017). Social networks and macrosocial change. Annual Review of Sociology, 43, 229-248.\nZhang, J., & Centola, D. (2019). Social networks and health: New developments in diffusion, online and offline. Annual Review of Sociology, 45, 91-109.\nHofstra, B., Corten, R., Van Tubergen, F., & Ellison, N. B. (2017). Sources of segregation in social networks: A novel approach using Facebook. American Sociological Review, 82(3), 625-656."
  },
  {
    "objectID": "schedule-208B-S24.html#week-10-linking-micro-and-macro",
    "href": "schedule-208B-S24.html#week-10-linking-micro-and-macro",
    "title": "SOCIOL 208B Reading Schedule (Spring 2024)",
    "section": "Week 10: Linking Micro and Macro",
    "text": "Week 10: Linking Micro and Macro\n\nTuesday, June 4\n\nMcfarland, D. A., Moody, J., Diehl, D., Smith, J. A., & Thomas, R. J. (2014). Network Ecology and Adolescent Social Structure. American Sociological Review, 79(6), 1088-1121.\nBearman, P. S., Moody, J., & Stovel, K. (2004). Chains of Affection: The Structure of Adolescent Romantic and Sexual Networks. American Journal of Sociology, 110(1), 44-91.\n\n\n\nThursday, June 6\n\nLevy, B. L., Phillips, N. E., & Sampson, R. J. (2020). Triple Disadvantage: Neighborhood Networks of Everyday Urban Mobility and Violence In US Cities. American Sociological Review, 85(6), 925-956.\nPapachristos, A. V., & Bastomski, S. (2018). Connected in crime: the enduring effect of neighborhood networks on the spatial patterning of violence. American Journal of Sociology, 124(2), 517-568."
  },
  {
    "objectID": "similarity.html",
    "href": "similarity.html",
    "title": "Vertex Similarity",
    "section": "",
    "text": "As we noted in the role equivalence handout, for odd reasons, classical approaches to structural similarity in networks used distance metrics but did not measure similarity directly. More recent approaches from network and information science prefer to define vertex similarity using direct similarity measures based on local structural characteristics, like node neighborhoods.\nMathematically, similarity is a less stringent (but also less well-defined compared to distances) relation between pairs of nodes in a graph than distance, so it can be easier to work with in most applications.\nFor instance, similarity is required to be symmetric (\\(s_{ij} = s_{ji}\\) for all \\(i\\) and \\(j\\)) and most metrics have reasonable bounds (e.g., 0 for least similar and 1.0 for maximally similar). Given such a bounded similarity we can get to dissimilarity by subtracting one: \\(d_{ij} = 1 - s_{ij}\\)"
  },
  {
    "objectID": "similarity.html#basic-ingredients-of-vertex-similarity-metrics",
    "href": "similarity.html#basic-ingredients-of-vertex-similarity-metrics",
    "title": "Vertex Similarity",
    "section": "Basic Ingredients of Vertex Similarity Metrics",
    "text": "Basic Ingredients of Vertex Similarity Metrics\nConsider two nodes and the set of nodes that are the immediate neighbors to each. In this case, most vertex similarity measures will make use of three pieces of information:\n\nThe number of common neighbors \\(p\\).\nThe number of actors \\(q\\) who are connected to node \\(i\\) but not to node \\(j\\).\nThe number of actors \\(r\\) who are connected to node \\(j\\) but not to node \\(i\\).\n\nIn the simplest case of the binary undirected graph then these are given by:\n\\[\n   p = \\sum_{k = 1}^{n} a_{ik} a_{jk}\n\\]\n\\[\n   q = \\sum_{k = 1}^{n} a_{ik} (1 - a_{jk})\n\\]\n\\[\n   r = \\sum_{k = 1}^{n} (1- a_{ik}) a_{jk}\n\\]\nIn matrix form:\n\\[\n   \\mathbf{A}(p) = \\mathbf{A} \\mathbf{A} = \\mathbf{A}^2\n\\]\n\\[\n   \\mathbf{A}(q) = \\mathbf{A} (1 - \\mathbf{A})\n\\]\n\\[\n   \\mathbf{A}(r) = (1 - \\mathbf{A}) \\mathbf{A}\n\\]\nLet’s look at an example:\n\n   library(networkdata)\n   library(igraph)\n   g.flint <- movie_267\n   g.flint <- delete_vertices(g.flint, degree(g.flint) <= 3)\n   A <- as.matrix(as_adjacency_matrix(g.flint))\n   A.p <- A %*% A #common neighbors matrix\n   A.q <- A %*% (1 - A) #neighbors of i not connected to j\n   A.r <- (1 - A) %*% A #neighbors of j not connected to i\n   A.p[1:10, 1:10]\n\n             BAM-BAM BARNEY BETTY FELDSPAR FRED HEADMISTRESS HERDMASTER LAVA\nBAM-BAM            6      4     5        2    5            2          2    5\nBARNEY             4     13     9        1   11            5          3    7\nBETTY              5      9    13        2    9            3          4    8\nFELDSPAR           2      1     2        2    1            0          2    2\nFRED               5     11     9        1   14            4          3    9\nHEADMISTRESS       2      5     3        0    4            5          0    3\nHERDMASTER         2      3     4        2    3            0          4    4\nLAVA               5      7     8        2    9            3          4   11\nLEACH              2      3     3        1    2            2          1    2\nMORRIS             2      3     2        0    2            3          0    2\n             LEACH MORRIS\nBAM-BAM          2      2\nBARNEY           3      3\nBETTY            3      2\nFELDSPAR         1      0\nFRED             2      2\nHEADMISTRESS     2      3\nHERDMASTER       1      0\nLAVA             2      2\nLEACH            3      1\nMORRIS           1      3\n\n   A.q[1:10, 1:10]\n\n             BAM-BAM BARNEY BETTY FELDSPAR FRED HEADMISTRESS HERDMASTER LAVA\nBAM-BAM            0      2     1        4    1            4          4    1\nBARNEY             9      0     4       12    2            8         10    6\nBETTY              8      4     0       11    4           10          9    5\nFELDSPAR           0      1     0        0    1            2          0    0\nFRED               9      3     5       13    0           10         11    5\nHEADMISTRESS       3      0     2        5    1            0          5    2\nHERDMASTER         2      1     0        2    1            4          0    0\nLAVA               6      4     3        9    2            8          7    0\nLEACH              1      0     0        2    1            1          2    1\nMORRIS             1      0     1        3    1            0          3    1\n             LEACH MORRIS\nBAM-BAM          4      4\nBARNEY          10     10\nBETTY           10     11\nFELDSPAR         1      2\nFRED            12     12\nHEADMISTRESS     3      2\nHERDMASTER       3      4\nLAVA             9      9\nLEACH            0      2\nMORRIS           2      0\n\n   A.r[1:10, 1:10]\n\n             BAM-BAM BARNEY BETTY FELDSPAR FRED HEADMISTRESS HERDMASTER LAVA\nBAM-BAM            0      9     8        0    9            3          2    6\nBARNEY             2      0     4        1    3            0          1    4\nBETTY              1      4     0        0    5            2          0    3\nFELDSPAR           4     12    11        0   13            5          2    9\nFRED               1      2     4        1    0            1          1    2\nHEADMISTRESS       4      8    10        2   10            0          4    8\nHERDMASTER         4     10     9        0   11            5          0    7\nLAVA               1      6     5        0    5            2          0    0\nLEACH              4     10    10        1   12            3          3    9\nMORRIS             4     10    11        2   12            2          4    9\n             LEACH MORRIS\nBAM-BAM          1      1\nBARNEY           0      0\nBETTY            0      1\nFELDSPAR         2      3\nFRED             1      1\nHEADMISTRESS     1      0\nHERDMASTER       2      3\nLAVA             1      1\nLEACH            0      2\nMORRIS           2      0\n\n\nNote that while \\(\\mathbf{A}(p)\\) is necessarily symmetric, neither \\(q\\) nor \\(r\\) have to be. Barney has many more neighbors that Bam-Bam is not connected to than vice versa. Also note that the \\(\\mathbf{A}(r)\\) matrix is just the transpose of the \\(\\mathbf{A}(q)\\) matrix in the undirected case.\nSo the most obvious measure of similarity between two nodes is simply the number of common neighbors (Leicht, Holme, and Newman 2006):\n\\[\n   s_{ij} = p_{ij}\n\\]\nWe have already seen a version of this in the directed case when talking about the HITS algorithm (Kleinberg 1999), which computes a spectral (eigenvector-based) ranking based on the matrices of common in and out-neighbors in a directed graph.\n\\[\n   p^{in}_{ij} = \\sum_{k = 1}^{n} a_{ki} a_{kj}\n\\]\n\\[\n   p^{out}_{ij} = \\sum_{k = 1}^{n} a_{ik} a_{jk}\n\\]\nWhich in matrix form is:\n\\[\n   \\mathbf{A}(p^{out}) = \\mathbf{A}^T \\mathbf{A}\n\\]\n\\[\n   \\mathbf{A}(p^{in}) = \\mathbf{A} \\mathbf{A}^T\n\\]\nIn this case, similarity can be measured either by the number of common in-neighbors or the number of common out-neighbors.\nIf the network under consideration is a (directed) citation network with nodes being papers and links between papers defined as a citation from paper \\(i\\) to paper \\(j\\), then the number of common in-neighbors between two papers is their co-citation similarity (the number of other papers that cite both papers), and the number of common out-neighbors is their bibliographic coupling similarity (the overlap in their list of references).\nOne problem with using unbounded quantities like the sheer number of common (in or out) neighbors to define node similarity is that they are only limited by the number of nodes in the network (Leicht, Holme, and Newman 2006). Thus, an actor with many neighbors will end up having lots of other neighbors in common with lots of other nodes, which will mean we would count them as “similar” to almost everyone."
  },
  {
    "objectID": "similarity.html#normalized-similarity-metrics",
    "href": "similarity.html#normalized-similarity-metrics",
    "title": "Vertex Similarity",
    "section": "Normalized Similarity Metrics",
    "text": "Normalized Similarity Metrics\nNormalized similarity metrics deal with this issue by adjusting the raw similarity based on \\(p\\) using the number of non-shared neighbors \\(q\\) and \\(r\\).\nThe two most popular versions of normalized vertex similarity scores are the Jaccard index and the cosine similarity (sometimes also referred to as the Salton Index).\nThe Jacccard index is given by:\n\\[\n   s_{ij} = \\frac{p}{p + q + r}\n\\]\nWhich is the ratio of the size of the intersection of the neighborhoods of the two nodes (number of common neighbors) divided by the size of the union of the two neighborhoods.\nIn our example, this would be:\n\n   J <- A.p / (A.p + A.q + A.r)\n   round(J[1:10, 1:10], 2)\n\n             BAM-BAM BARNEY BETTY FELDSPAR FRED HEADMISTRESS HERDMASTER LAVA\nBAM-BAM         1.00   0.27  0.36     0.33 0.33         0.22       0.25 0.42\nBARNEY          0.27   1.00  0.53     0.07 0.69         0.38       0.21 0.41\nBETTY           0.36   0.53  1.00     0.15 0.50         0.20       0.31 0.50\nFELDSPAR        0.33   0.07  0.15     1.00 0.07         0.00       0.50 0.18\nFRED            0.33   0.69  0.50     0.07 1.00         0.27       0.20 0.56\nHEADMISTRESS    0.22   0.38  0.20     0.00 0.27         1.00       0.00 0.23\nHERDMASTER      0.25   0.21  0.31     0.50 0.20         0.00       1.00 0.36\nLAVA            0.42   0.41  0.50     0.18 0.56         0.23       0.36 1.00\nLEACH           0.29   0.23  0.23     0.25 0.13         0.33       0.17 0.17\nMORRIS          0.29   0.23  0.14     0.00 0.13         0.60       0.00 0.17\n             LEACH MORRIS\nBAM-BAM       0.29   0.29\nBARNEY        0.23   0.23\nBETTY         0.23   0.14\nFELDSPAR      0.25   0.00\nFRED          0.13   0.13\nHEADMISTRESS  0.33   0.60\nHERDMASTER    0.17   0.00\nLAVA          0.17   0.17\nLEACH         1.00   0.20\nMORRIS        0.20   1.00\n\n\nHere showing that Barney is more similar to Fred and Betty than he is to Bam-Bam.\nThe cosine similarity is given by:\n\\[\n   s_{ij} = \\frac{p}{\\sqrt{p + q} \\sqrt{p + r}}\n\\]\nWhich is the ratio of the number of common neighbors divided by the product of the square root of the degrees of each node (or the square root of the product which is the same thing), since \\(p\\) + \\(q\\) is the degree of node \\(i\\) and \\(p\\) + \\(r\\) is the degree of node \\(j\\).\nIn our example, this would be:\n\n   C <- A.p / (sqrt(A.p + A.q) * sqrt(A.p + A.r))\n   round(C[1:10, 1:10], 2)\n\n             BAM-BAM BARNEY BETTY FELDSPAR FRED HEADMISTRESS HERDMASTER LAVA\nBAM-BAM         1.00   0.45  0.57     0.58 0.55         0.37       0.41 0.62\nBARNEY          0.45   1.00  0.69     0.20 0.82         0.62       0.42 0.59\nBETTY           0.57   0.69  1.00     0.39 0.67         0.37       0.55 0.67\nFELDSPAR        0.58   0.20  0.39     1.00 0.19         0.00       0.71 0.43\nFRED            0.55   0.82  0.67     0.19 1.00         0.48       0.40 0.73\nHEADMISTRESS    0.37   0.62  0.37     0.00 0.48         1.00       0.00 0.40\nHERDMASTER      0.41   0.42  0.55     0.71 0.40         0.00       1.00 0.60\nLAVA            0.62   0.59  0.67     0.43 0.73         0.40       0.60 1.00\nLEACH           0.47   0.48  0.48     0.41 0.31         0.52       0.29 0.35\nMORRIS          0.47   0.48  0.32     0.00 0.31         0.77       0.00 0.35\n             LEACH MORRIS\nBAM-BAM       0.47   0.47\nBARNEY        0.48   0.48\nBETTY         0.48   0.32\nFELDSPAR      0.41   0.00\nFRED          0.31   0.31\nHEADMISTRESS  0.52   0.77\nHERDMASTER    0.29   0.00\nLAVA          0.35   0.35\nLEACH         1.00   0.33\nMORRIS        0.33   1.00\n\n\nShowing results similar (pun intended) to those obtained using the Jaccard index.\nA less commonly used option is the Dice Coefficient (sometimes called the Sorensen Index) given by:\n\\[\n   s_{ij} = \\frac{2p}{2p + q + r}\n\\]\nWhich is given by the ratio of twice the number of common neighbors divided by twice the same quantity plus the sum of the non-shared neighbors (and thus a variation of the Jaccard measure).\nIn our example, this would be:\n\n   D <- (2 * A.p) / ((2 * A.p) + A.q + A.r)\n   round(D[1:10, 1:10], 2)\n\n             BAM-BAM BARNEY BETTY FELDSPAR FRED HEADMISTRESS HERDMASTER LAVA\nBAM-BAM         1.00   0.42  0.53     0.50 0.50         0.36       0.40 0.59\nBARNEY          0.42   1.00  0.69     0.13 0.81         0.56       0.35 0.58\nBETTY           0.53   0.69  1.00     0.27 0.67         0.33       0.47 0.67\nFELDSPAR        0.50   0.13  0.27     1.00 0.12         0.00       0.67 0.31\nFRED            0.50   0.81  0.67     0.12 1.00         0.42       0.33 0.72\nHEADMISTRESS    0.36   0.56  0.33     0.00 0.42         1.00       0.00 0.38\nHERDMASTER      0.40   0.35  0.47     0.67 0.33         0.00       1.00 0.53\nLAVA            0.59   0.58  0.67     0.31 0.72         0.38       0.53 1.00\nLEACH           0.44   0.38  0.38     0.40 0.24         0.50       0.29 0.29\nMORRIS          0.44   0.38  0.25     0.00 0.24         0.75       0.00 0.29\n             LEACH MORRIS\nBAM-BAM       0.44   0.44\nBARNEY        0.38   0.38\nBETTY         0.38   0.25\nFELDSPAR      0.40   0.00\nFRED          0.24   0.24\nHEADMISTRESS  0.50   0.75\nHERDMASTER    0.29   0.00\nLAVA          0.29   0.29\nLEACH         1.00   0.33\nMORRIS        0.33   1.00\n\n\nOnce again, showing results comparable to the previous.\nNote, that all three of these pairwise measures of similarity are bounded between zero and one, with nodes being maximally similar to themselves and with pairs of distinct nodes being maximally similar when they have the same set of neighbors (e.g., they are structurally equivalent).\nLeicht, Holme, and Newman (2006) introduce a variation on the theme of normalized structural similarity scores. Their point is that maybe we should care about nodes that are surprisingly similar given some suitable null model. They propose the configuration model as such a null model. This model takes a graph with the same degree distribution as the original but with connections formed at random as reference.\nThe LHN similarity index (for Leicht, Holme, and Newman) is then given by:\n\\[\ns_{ij} = \\frac{p}{(p + q)(p + r)}\n\\]\nWhich can be seen as a variation of the cosine similarity defined earlier.\nIn our example, this would be:\n\n   D <- A.p / ((A.p + A.q) * (A.p + A.r))\n   round(D[1:10, 1:10], 2)\n\n             BAM-BAM BARNEY BETTY FELDSPAR FRED HEADMISTRESS HERDMASTER LAVA\nBAM-BAM         0.17   0.05  0.06     0.17 0.06         0.07       0.08 0.08\nBARNEY          0.05   0.08  0.05     0.04 0.06         0.08       0.06 0.05\nBETTY           0.06   0.05  0.08     0.08 0.05         0.05       0.08 0.06\nFELDSPAR        0.17   0.04  0.08     0.50 0.04         0.00       0.25 0.09\nFRED            0.06   0.06  0.05     0.04 0.07         0.06       0.05 0.06\nHEADMISTRESS    0.07   0.08  0.05     0.00 0.06         0.20       0.00 0.05\nHERDMASTER      0.08   0.06  0.08     0.25 0.05         0.00       0.25 0.09\nLAVA            0.08   0.05  0.06     0.09 0.06         0.05       0.09 0.09\nLEACH           0.11   0.08  0.08     0.17 0.05         0.13       0.08 0.06\nMORRIS          0.11   0.08  0.05     0.00 0.05         0.20       0.00 0.06\n             LEACH MORRIS\nBAM-BAM       0.11   0.11\nBARNEY        0.08   0.08\nBETTY         0.08   0.05\nFELDSPAR      0.17   0.00\nFRED          0.05   0.05\nHEADMISTRESS  0.13   0.20\nHERDMASTER    0.08   0.00\nLAVA          0.06   0.06\nLEACH         0.33   0.11\nMORRIS        0.11   0.33\n\n\nWhich, once again, produces similar results to what we found before. Note, however, that the LHN is not naturally maximal for self-similar nodes."
  },
  {
    "objectID": "similarity.html#similarity-and-structural-equivalence",
    "href": "similarity.html#similarity-and-structural-equivalence",
    "title": "Vertex Similarity",
    "section": "Similarity and Structural Equivalence",
    "text": "Similarity and Structural Equivalence\nAll normalized similarity measures bounded between zero and one (like Jaccard, Cosine, and Dice) also define a distance on each pair of nodes which is equal to one minus the similarity. So the cosine distance between two nodes is one minus the cosine similarity, and so on for the Jaccard and Dice indexes.\nBecause they define distances, this also means that these approaches can be used to find approximately structurally equivalent classes of nodes in a graph just like we did with the Euclidean and correlation distances.\nFor instance, consider our toy graph from before with four structurally equivalent sets of nodes:\n\n\n\n\n\nA toy graph demonstrating structural equivalence.\n\n\n\n\nThe cosine similarity matrix for this graph is:\n\n   A <- as.matrix(as_adjacency_matrix(g))\n   A.p <- A %*% A \n   A.q <- A %*% (1 - A) \n   A.r <- (1 - A) %*% A \n   C <- A.p / (sqrt(A.p + A.q) * sqrt(A.p + A.r))\n   round(C, 2)\n\n     A    B    C    D    E    F    G    H    I\nA 1.00 0.26 0.26 0.58 0.58 0.58 0.67 0.00 0.00\nB 0.26 1.00 1.00 0.00 0.00 0.00 0.26 0.67 0.67\nC 0.26 1.00 1.00 0.00 0.00 0.00 0.26 0.67 0.67\nD 0.58 0.00 0.00 1.00 1.00 1.00 0.58 0.25 0.25\nE 0.58 0.00 0.00 1.00 1.00 1.00 0.58 0.25 0.25\nF 0.58 0.00 0.00 1.00 1.00 1.00 0.58 0.25 0.25\nG 0.67 0.26 0.26 0.58 0.58 0.58 1.00 0.00 0.00\nH 0.00 0.67 0.67 0.25 0.25 0.25 0.00 1.00 0.75\nI 0.00 0.67 0.67 0.25 0.25 0.25 0.00 0.75 1.00\n\n\nNote that structurally equivalent nodes have similarity scores equal to 1.0. In this case, the distance matrix is given by:\n\n   D <- 1 - C\n\nAnd a hierarchical clustering on this matrix reveals the structurally equivalent classes:\n\n   D <- dist(D) \n   h.res <- hclust(D, method = \"ward.D2\")\n   plot(h.res)\n\n\n\n\nWe can package all that we said before into a handy function that takes a graph as input and returns all four normalized similarity metrics as output:\n\n   vertex.sim <- function(x) {\n      A <- as.matrix(as_adjacency_matrix(x))\n      A.p <- A %*% A \n      A.q <- A %*% (1 - A) \n      A.r <- (1 - A) %*% A \n      J <- A.p / (A.p + A.q + A.r)\n      C <- A.p / (sqrt(A.p + A.q) * sqrt(A.p + A.r))\n      D <- (2 * A.p) / ((2 * A.p) + A.q + A.r)\n      L <- A.p / ((A.p + A.q) * (A.p + A.r))\n      return(list(J = J, C = C, D = D, L = L))\n      }\n\nIn the Flintstones network, we could then find structurally equivalent blocks from the similarity analysis as follows (using Cosine):\n\n   g.flint <- movie_267\n   g.flint <- delete_vertices(g.flint, degree(g.flint) <= 3)\n   D <- dist(1- vertex.sim(g.flint)$C)\n   h.res <- hclust(D, method = \"ward.D2\") #hierarchical clustering\n   plot(h.res)\n\n\n\n\nWe can then extract our blocks just like we did with the Euclidean distance hierarchical clustering results in the previous handout:\n\n   blocks  <- cutree(h.res, k = 6)\n   blocks\n\n        BAM-BAM          BARNEY           BETTY        FELDSPAR            FRED \n              1               2               2               3               2 \n   HEADMISTRESS      HERDMASTER            LAVA           LEACH          MORRIS \n              4               3               2               5               4 \n      MRS SLATE         PEBBLES PEBBLES BAM-BAM        PILTDOWN      POINDEXTER \n              2               1               5               6               5 \n         PYRITE           SLATE           WILMA \n              6               2               2 \n\n\nThis analysis now puts \\(\\{Barney, Betty, Fred, Lava, Mrs. Slate, Slate, Wilma\\}\\) in the second block of equivalent actors (shown as the right-most cluster of actors in the dendrogram)."
  },
  {
    "objectID": "similarity.html#generalized-similarities",
    "href": "similarity.html#generalized-similarities",
    "title": "Vertex Similarity",
    "section": "Generalized Similarities",
    "text": "Generalized Similarities\nSo far, we have defined distances and similarities mainly as a function of the number of shared neighbors between two nodes. Structural equivalence is an idealized version of this, obtaining whenever people share exactly the same set of neighbors.\nYet, similarities based on local neighborhood information only have been criticized (e.g., by Borgatti and Everett (1992)) for not quite capturing the sociological intuition behind the idea of a role which is usually what they are deployed for. That is, two doctors don’t occupy the same role because they treat the same set of patients (in fact, this would be weird); instead, the set of doctors occupy the same role to the extent they treat a set of actors in the same (or similar) role: Namely, patients, regardless of whether those actors are literally the same or not.\nThis worry led social network analysts to try to generalize the concept of structural equivalence using less stringent algebraic definition, resulting in something of a proliferation of different equivalences such as automorphic or regular equivalence (Borgatti and Everett 1992).\nUnfortunately, none of this work led (unlike the work on structural equivalence) to useful or actionable tools for data analysis, with some even concluding that some definitions of equivalence (like regular equivalence) are unlikely to be found in any interesting substantive setting.\nA better approach here is to use the more relaxed notion of similarity like we did above to come up with a more general definition that can capture our role-based intuitions. Note that all the similarity notions studied earlier have structural equivalence as the limiting case of maximum similarity. So they are still based on the idea that nodes are similar to the extent they connect to the same others.\nWe can generalize this idea (to deal with the doctor/patient role problem) in the following way: nodes are similar to the extent they connect to similar others, with the restriction that we can only use endogenous (structural connectivity) information—like with structural equivalence or common-neighbor approaches—to define everyone’s similarity (no exogenous attribute stuff).\nAs Jeh and Widom (2002) note, this approach does lead to a coherent generalization of the idea of similarity for nodes in graphs because we can just define similarity recursively and iterate through the graph until the similarity scores stop changing.1 More specifically, they propose to measure the similarity between two nodes \\(i\\) and \\(j\\) at each time-step in the iteration using the formula:\n\\[\\begin{equation}\n   s_{ij}(t) = \\frac{\\alpha}{d_i d_j} \\sum_{k \\in N(i)} \\sum_{l \\in N(j)} s_{kl}(t-1)\n\\end{equation}\\]\nSo the similarity between two nodes at step \\(t\\) is just the sum of the pairwise similarities between each of their neighbors (computed in the previous step, \\(t-1\\)), weighted by the ratio of a free parameter \\(\\alpha\\) (a number between zero and one) to the product of their degrees (to take a weighted average).\nThis measure nicely captures the idea that nodes are similar to the extent they both connect to people who are themselves similar. Note that it doesn’t matter whether these neighbors are shared between the two nodes (the summation occurs over each pair of nodes in \\(p\\) + \\(q\\) versus \\(p\\) + \\(r\\) as defined earlier) or whether the set of neighbors are themselves neighbors, which deals with the doctor/patient problem.\nA function that implements this idea looks like:\n\n   SimRank.in <- function(A, C = 0.8, iter = 10) {\n      d <- colSums(A)\n      n <- nrow(A)\n      S <- diag(1, n, n)\n      rownames(S) <- rownames(A)\n      colnames(S) <- colnames(A)\n      m <- 1\n      while(m < iter) {\n          for(i in 1:n) {\n               for(j in 1:n) {\n                    if (i < j) {\n                        a <- names(which(A[, i] == 1)) \n                        b <- names(which(A[, j] == 1)) \n                        Sij <- 0\n                        for (k in a) {\n                           for (l in b) {\n                              Sij <- Sij + S[k, l] #i's similarity to j\n                           }\n                        }\n                        S[i, j] <- C/(d[i] * d[j]) * Sij\n                        S[j, i] <- C/(d[i] * d[j]) * Sij\n                    }\n               }\n          }\n         m <- m + 1\n      }\n   return(S)\n}\n\nNote that this function calculates SimRank using each node’s in-neighbors (this doesn’t matter if the graph is undirected).\nLet’s try it out in the Flintstones graph using \\(\\alpha = 0.95\\):\n\n   A <- as.matrix(as_adjacency_matrix(g.flint))\n   S <- SimRank.in(A, 0.95)\n   round(S[, 1:5], 2)\n\n                BAM-BAM BARNEY BETTY FELDSPAR FRED\nBAM-BAM            1.00   0.46  0.47     0.52 0.47\nBARNEY             0.46   1.00  0.47     0.45 0.48\nBETTY              0.47   0.47  1.00     0.48 0.47\nFELDSPAR           0.52   0.45  0.48     1.00 0.46\nFRED               0.47   0.48  0.47     0.46 1.00\nHEADMISTRESS       0.47   0.48  0.46     0.44 0.48\nHERDMASTER         0.48   0.47  0.48     0.57 0.48\nLAVA               0.48   0.47  0.47     0.49 0.48\nLEACH              0.49   0.48  0.48     0.53 0.47\nMORRIS             0.49   0.48  0.47     0.44 0.47\nMRS SLATE          0.47   0.46  0.47     0.48 0.47\nPEBBLES            0.51   0.47  0.48     0.53 0.48\nPEBBLES BAM-BAM    0.49   0.48  0.48     0.48 0.48\nPILTDOWN           0.47   0.48  0.47     0.52 0.48\nPOINDEXTER         0.47   0.47  0.48     0.51 0.47\nPYRITE             0.47   0.48  0.47     0.52 0.48\nSLATE              0.47   0.47  0.48     0.50 0.48\nWILMA              0.47   0.47  0.48     0.48 0.47\n\n\nWe can transform the generalized similarities to distances and plot:\n\n   D <- dist(1- S)\n   h.res <- hclust(D, method = \"ward.D2\") #hierarchical clustering\n   plot(h.res)\n\n\n\n   blocks  <- cutree(h.res, k = 6)\n   blocks\n\n        BAM-BAM          BARNEY           BETTY        FELDSPAR            FRED \n              1               2               3               4               2 \n   HEADMISTRESS      HERDMASTER            LAVA           LEACH          MORRIS \n              5               4               3               6               5 \n      MRS SLATE         PEBBLES PEBBLES BAM-BAM        PILTDOWN      POINDEXTER \n              3               1               6               4               6 \n         PYRITE           SLATE           WILMA \n              4               3               3 \n\n\nWhich returns a somewhat different role partition than the metrics relying on structural equivalence. In the SimRank equivalence partition, \\(\\{Fred, Barney\\}\\) are in their own standalone class, while \\(\\{Betty, Wilma, Slate, Mrs. Slate, Lava\\}\\) appear as a separate cluster."
  },
  {
    "objectID": "similarity.html#global-similarity-indices",
    "href": "similarity.html#global-similarity-indices",
    "title": "Vertex Similarity",
    "section": "Global Similarity Indices",
    "text": "Global Similarity Indices\nAs we saw earlier, the most important ingredient of structural similarity measures between pairs of nodes is the number of common of neighbors (followed by the degrees of each node), and this quantity is given by the square of the adjacency matrix \\(\\mathbf{A}^2\\). So we can say that this matrix gives us a basic similarity measure between nodes, namely, the common neighbors similarity:\n\\[\n\\mathbf{S} = \\mathbf{A}^2\n\\]\nAnother way of seeing this is that a common neighbor defines a path of length two between a pair of nodes. So the number of common neighbors between two nodes is equivalent to the number of paths of length two between them. We are thus saying that the similarity between two nodes increases as the number of paths of length two between them increases, and that info is also recorded in the \\(\\mathbf{A}^2\\) matrix.\n\nThe Local Path Similarity Index\nBut if the similarity between node pairs increases in the number of paths of length two between them, wouldn’t nodes be even more similar if they also have a bunch of paths of length three between them?\nLü, Jin, and Zhou (2009) asked themselves the same question and proposed the following as a similarity metric based on paths:\n\\[\\begin{equation}\n\\mathbf{S} = \\mathbf{A}^2 + \\alpha \\mathbf{A}^3\n\\end{equation}\\]\nThis is the so-called local path similarity index (Lü, Jin, and Zhou 2009). Obviously, structurally equivalent nodes will be counted as similar by this metric (lots of paths of length two between them) but also nodes indirectly connected by many paths of length three, but to a lesser extent given the discount parameter \\(\\alpha\\) (a number between zero and one).\nA function that does this is:\n\n   local.path <- function(A, alpha = 0.5) {\n      A2 <- A %*% A\n      S <- A2 + alpha*(A2 %*% A)\n   return(S)\n   }\n\nHere’s how the local path similarity looks in the Flintstones network:\n\n   A <- as.matrix(as_adjacency_matrix(g.flint))\n   S <- local.path(A)\n   S[1:10, 1:10]\n\n             BAM-BAM BARNEY BETTY FELDSPAR FRED HEADMISTRESS HERDMASTER LAVA\nBAM-BAM         20.0   29.5  30.5      6.5 31.5         12.5        9.5 25.0\nBARNEY          29.5   50.0  51.0     13.0 52.0         21.0       21.0 46.5\nBETTY           30.5   51.0  52.0     11.0 53.0         24.0       18.0 46.0\nFELDSPAR         6.5   13.0  11.0      3.0 13.5          4.5        5.0 10.0\nFRED            31.5   52.0  53.0     13.5 55.0         21.5       21.5 47.5\nHEADMISTRESS    12.5   21.0  24.0      4.5 21.5         13.0        6.5 21.5\nHERDMASTER       9.5   21.0  18.0      5.0 21.5          6.5       10.0 17.0\nLAVA            25.0   46.5  46.0     10.0 47.5         21.5       17.0 42.0\nLEACH            9.5   15.5  16.0      3.5 17.5          7.5        5.5 15.5\nMORRIS           8.5   13.0  15.5      2.5 13.0          8.0        3.5 12.5\n             LEACH MORRIS\nBAM-BAM        9.5    8.5\nBARNEY        15.5   13.0\nBETTY         16.0   15.5\nFELDSPAR       3.5    2.5\nFRED          17.5   13.0\nHEADMISTRESS   7.5    8.0\nHERDMASTER     5.5    3.5\nLAVA          15.5   12.5\nLEACH          6.0    4.0\nMORRIS         4.0    6.0\n\n\nOf course as \\(\\alpha\\) approaches zero, then the local path measure reduces to the number of common neighbors, while numbers closer to one count paths of length three more.\nAnother thing people may wonder if why not keep going and add paths of length four:\n\\[\\begin{equation}\n\\mathbf{S} = \\mathbf{A}^2 + \\alpha \\mathbf{A}^3 + \\alpha^2 \\mathbf{A}^4\n\\end{equation}\\]\nOr paths of length whatever:\n\\[\n\\mathbf{S} = \\mathbf{A}^2 + \\alpha \\mathbf{A}^3 + \\alpha^2 \\mathbf{A}^4 ... + \\alpha^{k-2} \\mathbf{A}^k\n\\]\nWhere \\(k\\) is the length of the maximum path considered. Lü, Jin, and Zhou (2009) argue that these higher order paths don’t matter, so maybe we don’t have to worry about them.\n\n\nThe Katz Similarity\nAnother issue is that there was already an all-paths similarity measure in existence, one developed by the mathematical social scientist Leo Katz (1953) in the 1950s (!).\nThe basic idea was to use linear algebra tricks to solve:\n\\[\n\\mathbf{S} = \\sum_{k=1}^{\\infty} \\alpha^k A^k\n\\]\nWhich would theoretically count the paths of all possible lengths between two nodes while discounting the contribution of the longer paths in proportion to their length (as \\(k\\) gets bigger, with \\(\\alpha\\) a number between zero and one, \\(\\alpha^k\\) gets smaller and smaller).\nLinear algebra hocus-pocus (non-technical explainer here) turns the above infinite sum into the more tractable:\n\\[\n\\mathbf{S} = (\\mathbf{I} - \\alpha A)^{-1}\n\\]\nWhere \\(\\mathbf{I}\\) is the identity matrix (a matrix of all zeros except that it has the number one in each diagonal cell) of the same dimensions as the original. Raising the result of the subtraction in parentheses to minus one indicates the matrix inverse operation (most matrices are invertible, unless they are weird).\nA function to compute the Katz similarity between all node pairs is:\n\n   katz.sim <- function(A, min.alpha = 0.05) {\n      I <- diag(nrow(A)) #creating identity matrix\n      alpha <- runif(1, min = min.alpha, max = 1/eigen(A)$values[1])\n      S <- solve(I - alpha * A) \n      return(S)\n   }\n\nFor technical reasons (e.g., guarantee that the infinite sum converges) we need to choose \\(\\alpha\\) to be a number larger than zero but smaller than the reciprocal of the first eigenvalue of the matrix. Hence we just pick a random value in that interval in line 3 (set the seed to get reproducible answers). Line 4 computes the actual Katz similarity using the native R function solve to find the relevant matrix inverse.2\nIn the Flintstones network the Katz similarity looks like:\n\n   set.seed(456)\n   S <- katz.sim(A)\n   round(S[1:10, 1:10], 2)\n\n             BAM-BAM BARNEY BETTY FELDSPAR FRED HEADMISTRESS HERDMASTER LAVA\nBAM-BAM         1.03   0.08  0.09     0.01 0.09         0.01       0.01 0.03\nBARNEY          0.08   1.07  0.11     0.07 0.12         0.03       0.08 0.10\nBETTY           0.09   0.11  1.07     0.01 0.11         0.08       0.02 0.11\nFELDSPAR        0.01   0.07  0.01     1.01 0.07         0.00       0.01 0.01\nFRED            0.09   0.12  0.11     0.07 1.07         0.02       0.08 0.11\nHEADMISTRESS    0.01   0.03  0.08     0.00 0.02         1.02       0.00 0.08\nHERDMASTER      0.01   0.08  0.02     0.01 0.08         0.00       1.02 0.02\nLAVA            0.03   0.10  0.11     0.01 0.11         0.08       0.02 1.06\nLEACH           0.01   0.02  0.02     0.00 0.07         0.01       0.01 0.07\nMORRIS          0.01   0.02  0.07     0.00 0.01         0.01       0.00 0.01\n             LEACH MORRIS\nBAM-BAM       0.01   0.01\nBARNEY        0.02   0.02\nBETTY         0.02   0.07\nFELDSPAR      0.00   0.00\nFRED          0.07   0.01\nHEADMISTRESS  0.01   0.01\nHERDMASTER    0.01   0.00\nLAVA          0.07   0.01\nLEACH         1.01   0.01\nMORRIS        0.01   1.01\n\n\n\n\nDegree-Weighted Katz Similarity (AKA Leich-Holme-Newman)\nLeicht, Holme, and Newman (2006) argue that the Katz approach is fine and dandy as a similarity measure, but note that is an unweighted index (like the raw number of common neighbors). This means that nodes with large degree will end up being “similar” to a buch of other nodes in the graph, just because they have lots of paths of length whatever between them and those nodes.\nLeicht, Holme, and Newman (2006) propose a “fix” for this weakness in the Katz similarity, resulting in the matrix linear algebra equivalent of a degree-normalized similarity measure like the Jaccard or Cosine.\nSo instead of Katz they suggest we compute:\n\\[\n\\mathbf{S} = \\mathbf{D}^{-1} \\left( \\frac{\\alpha A}{\\lambda_1} \\right)^{-1} \\mathbf{D}^{-1}\n\\]\nHere \\(\\mathbf{D}\\) is a matrix containing the degrees of each node along the diagonal. The inverse of this matrix \\(\\mathbf{D}^{-1}\\) will contain the reciprocal of each degree \\(1/k_i\\) along the diagonals. \\(\\lambda_1\\), on the other hand, is just the first eigenvalue of the adjacency matrix.\nSo, the LHN Similarity is just the Katz similarity weighted by the degree of the sender and receiver node along each path, further discounting paths featuring high-degree nodes at either or both ends.\nWhich leads to the function:\n\n   LHN.sim <- function(A, alpha = 0.9) {\n      D <- solve(diag(rowSums(A))) #inverse of degree matrix\n      lambda <- eigen(A)$values[1] #first eigenvalue of adjacency matrix\n      S <- D %*% solve((alpha * A)/lambda) %*% D #LHN index\n      rownames(S) <- rownames(A)\n      colnames(S) <- colnames(A)\n      return(S)\n   }\n\nAnd the Flintstones result:\n\n   S <- LHN.sim(A)\n   round(S[, 1:5], 2)\n\n                BAM-BAM BARNEY BETTY FELDSPAR  FRED\nBAM-BAM           -0.09   0.01  0.01    -0.16 -0.01\nBARNEY             0.01  -0.02  0.00     0.11  0.01\nBETTY              0.01   0.00 -0.04    -0.48  0.00\nFELDSPAR          -0.16   0.11 -0.48    -2.55  0.25\nFRED              -0.01   0.01  0.00     0.25 -0.01\nHEADMISTRESS       0.20   0.05 -0.13    -2.96 -0.05\nHERDMASTER         0.16   0.08  0.10     0.07 -0.07\nLAVA               0.00  -0.02 -0.02    -0.42  0.02\nLEACH             -0.27  -0.10  0.04     2.01  0.09\nMORRIS            -0.25  -0.03  0.12     1.90  0.03\nMRS SLATE         -0.05   0.01  0.04     0.74 -0.01\nPEBBLES            0.19   0.01  0.01    -0.16 -0.01\nPEBBLES BAM-BAM    0.03   0.00  0.03     0.16  0.00\nPILTDOWN           0.00   0.00  0.00    -0.35  0.00\nPOINDEXTER         0.01   0.03  0.04     0.66 -0.02\nPYRITE             0.00   0.00  0.00    -0.35  0.00\nSLATE             -0.08  -0.01  0.04     0.91  0.01\nWILMA              0.01   0.00  0.02     0.09  0.00\n\n\nNote that as Leicht, Holme, and Newman (2006) discuss, the entries in the LHN version of the similarity \\(\\mathbf{S}\\) can be either positive or negative. Negative entries are nodes that are surprisingly dissimilar given their degrees, and positive numbers indicating node pairs that are surprisingly similar. Numbers closer to zero are nodes that are neither similar nor dissimilar given their degrees.\nHere we can see that Barney and Fred are actually not that similar to one another (after we take into account their very high degree) and that Barney is actually most similar to Feldspar (and Fred even more so).\nBecause the LHN similarities have negative and positive values, they already define a distance between nodes in the graph, like the correlation distance. So if we wanted to find blocks of actors using this similarity criterion, all we need to do is:\n\n   D <- dist(S)\n   h.res <- hclust(D, method = \"ward.D2\")\n   plot(h.res)\n\n\n\n   blocks <- cutree(h.res, k = 8)\n   blocks\n\n        BAM-BAM          BARNEY           BETTY        FELDSPAR            FRED \n              1               2               1               3               2 \n   HEADMISTRESS      HERDMASTER            LAVA           LEACH          MORRIS \n              4               5               1               6               7 \n      MRS SLATE         PEBBLES PEBBLES BAM-BAM        PILTDOWN      POINDEXTER \n              8               1               2               1               8 \n         PYRITE           SLATE           WILMA \n              1               8               2 \n\n\nIt seems like this approach is geared towards finding smaller, more fine-grained clusters of similar actors in the data."
  },
  {
    "objectID": "similarity.html#appendix",
    "href": "similarity.html#appendix",
    "title": "Vertex Similarity",
    "section": "Appendix",
    "text": "Appendix\nA more general version of the function to compute SimRank looks like this (partly based on Fouss, Saerens, and Shimbo 2016, 84, algorithm 2.4):\n\n   SimRank <- function(A, sigma = 0.0001, alpha = 0.8) {\n      n <- nrow(A)\n      d <- as.numeric(colSums(A) > 0)\n      e <- matrix(1, n, 1)\n      Q <- diag(as.vector(t(e) %*% A), n, n)\n      diag(Q) <- 1/diag(Q)\n      Q <- A %*% Q\n      Q[is.nan(Q)] <- 0\n      diff <- 1\n      k <- 1\n      K <- diag(1, n, n)\n      while(diff > sigma) {\n         K.old <- K\n         K.p <- alpha * t(Q) %*% K %*% Q\n         K <- K.p - diag(as.vector(diag(K.p)), n, n) + diag(d, n, n)\n         diff <- abs(sum(abs(K.old)) - sum(abs(K)))\n         k <- k + 1\n      }\n      rownames(K) <- rownames(A)\n      colnames(K) <- colnames(A)\n      return(K)\n   }"
  },
  {
    "objectID": "spectral.html",
    "href": "spectral.html",
    "title": "Community Detection Using Spectral Clustering",
    "section": "",
    "text": "In the lecture on communities, we saw how to use the leading (first) eigenvector of the modularity matrix to split a network in two (and then split those partitions in two and so on).\nElsewhere, we saw that the eigendecomposition of a square matrix actually results in \\(p\\) eigenvectors and eigenvalues (not just the leading or first).\nWe also learned from the eigendecomposition lesson that selecting some number \\(k < p\\) of eigenvectors and eigenvalues helps us reconstruct the best approximation of the original matrix given that number of dimensions.\nPutting these three lessons together suggests a simple way to detect multiple communities in a network using the eigenvalues of a suitable matrix, along with standard clustering algorithms such as k-means clustering. This approach does not have to iterate between binary divisions based on a single (leading) eigenvector, but can instead use multiple eigenvectors at once to partition the data into any desired number of clusters.\nThis general approach to community detection is sometimes referred to as spectral clustering.1"
  },
  {
    "objectID": "spectral.html#spectral-clustering-using-the-eigenvectors-of-the-laplacian",
    "href": "spectral.html#spectral-clustering-using-the-eigenvectors-of-the-laplacian",
    "title": "Community Detection Using Spectral Clustering",
    "section": "Spectral Clustering Using the Eigenvectors of the Laplacian",
    "text": "Spectral Clustering Using the Eigenvectors of the Laplacian\nThe issue then becomes, which matrix should we use the eigenvalues and eigenvectors of? As suggested by Von Luxburg (2007), an obvious candidate is what is called the graph Laplacian (\\(\\mathbf{L}\\)), which is given by:\n\\[\n\\mathbf{L} = \\mathbf{D} - \\mathbf{A}\n\\]\nWhere \\(\\mathbf{D}\\) is the graph’s degree matrix a matrix with the degree vector of the graph along the diagonals and zeros everywhere else.\nLike the modularity matrix, the \\(\\mathbf{L}\\) matrix is doubly-centered (rows and columns sum to zero), which means, via some math other smarter people have worked out, that, if \\(\\mathbf{A}\\) is connected, one of the eigenvalues of \\(\\mathbf{L}\\) is guaranteed to be zero.2\nThe other interesting thing is that the second smallest (non-zero) eigenvalue of \\(\\mathbf{L}\\) (sometimes called the Fiedler vector) provides an optimal (e.g., minimizing the number of cross-community edges) two-community partition of the graph (just like we saw the leading eigenvector of the modularity matrix does).\nIf we want to check for the existence of multiple groups, therefore, what we need to do is to create a new \\(n \\times k\\) matrix \\(\\mathbf{U}\\) composed of the \\(k\\) eigenvectors of \\(\\mathbf{L}\\) associated with the \\(k\\) smallest eigenvalues, arranged from smallest to biggest.\nWe then normalize the node-specific row-vector of values of the\\(\\mathbf{U}\\) matrix (e.g., using the Euclidean norm), and the use normalized \\(\\mathbf{U}\\) matrix, which is an embedding of the node set of the graph in an k-dimensional Euclidean space, as input to a k-means algorithm with a known number of clusters (Fouss, Saerens, and Shimbo 2016, 320).\nLet’s see how that would work. Let’s load the law_friends data from the networkdata package containing information on the friendship nominations of 68 lawyers in a firm. We have already analyzed these data using other community detection algorithms in a previous handout. The original data are directed, so we constrain them to be undirected and remove nodes with degree less than two:\n\n   library(networkdata)\n   library(igraph)\n   g <- law_friends\n   g <- as_undirected(g, mode = \"collapse\")\n   g <- subgraph(g, degree(g)>=2) #removing low degree nodes\n\nWe can then write a function called ratio.cut that accomplishes the eigendecomposition of the graph Laplacian matrix described earlier. It looks like this:\n\n   ratio.cut <- function(x, k = 6) {\n         A <- as.matrix(as_adjacency_matrix(x))\n         D <- diag(degree(x))\n         n <- vcount(x)\n         L <- D - A\n         eig.L <- eigen(L)\n         b <- n - k\n         e <- n - 1\n         U <- matrix(eig.L$vectors[, b:e], nrow = n, ncol = k)\n         if (k > 1) {\n            U <- U[, k:1] #re-ordering columns from smallest to biggest\n            }\n         for (i in 1:nrow(U)) {\n            U[i, ] <- U[i, ]/norm(as.matrix(U[i, ]), type = \"E\")\n            }\n      return(U)\n   }\n\nThis function takes a graph as input and produces the \\(\\mathbf{U}\\) matrix with \\(n\\) rows and \\(k\\) columns, with \\(k=6\\) by default. It first computes the adjacency matrix (line 2), then the degree matrix (line 3), then the Laplacian matrix (line 5). It then computes the eigendecomposition of the Laplacian (line 6), row normalizes the values taken by the eigenvectors corresponding to the \\(k\\) smallest eigenvalues in lines 10-13 (reverse-ordered from smallest to biggest in line 9), and returns the resulting matrix \\(\\mathbf{U}\\) in line 14.\nLet’s see the function at work:\n\n   U <- ratio.cut(g)\n   round(U, 2)[1:10, ]\n\n       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]\n [1,]  0.08  0.34  0.66  0.37 -0.54  0.13\n [2,]  0.07  0.29  0.60  0.37 -0.53  0.37\n [3,]  0.47  0.13  0.08 -0.87  0.03  0.04\n [4,]  0.34  0.38  0.71  0.16 -0.45  0.10\n [5,]  0.57  0.26  0.06 -0.78  0.00  0.01\n [6,]  0.58 -0.76 -0.21  0.20 -0.05  0.03\n [7,]  0.37  0.20 -0.01 -0.90  0.08  0.03\n [8,] -0.02  0.33  0.65  0.41 -0.55 -0.01\n [9,]  0.14  0.37  0.67  0.36 -0.52  0.04\n[10,]  0.05  0.36  0.66  0.42 -0.50  0.06\n\n\nWhich shows the corresponding values of \\(\\mathbf{U}\\) for each node across the six eigenvectors of the Laplacian we selected.\nNow, the basic idea is to treat each of the normalized scores along the six eigenvectors as if they were “variables” or “features” in a standard k-means clustering problem. The algorithm will then group nodes based on how similar their scores are on each six-dimensional vector. Similar nodes will correspond to communities (k-means clusters) in our data.\nK-means clustering requires knowing how many groups we want in advance (differently from hierarchical clustering). Since we don’t know which is the best community partition beforehand, we instead compute a bunch of partitions and check the modularity of each.\nThe following function computes cluster assignments of the nodes in the graph up to ten partitions (starting from the minimum two):\n\n   k.cuts <- function(x, max = 9) {\n      clus <- list()\n      for (i in 1:max) {\n         set.seed(456) #setting seed because kmeans uses random starting nodes for cluster centroids\n         k <- i + 1\n         clus[[i]] <- kmeans(x, k)$cluster\n         }\n      return(clus)\n   }\n\nThis function takes the \\(\\mathbf{U}\\) matrix as input and returns a nine-element list (with vectors of length \\(n\\) as its elements) of cluster assignments for each node in the graph, corresponding to partitions \\(k = \\{2, 3, \\ldots 10\\}\\) respectively.\nLet’s see the clustering function at work:\n\n   clus <- k.cuts(U)\n   clus\n\n[[1]]\n [1] 2 2 1 2 1 1 1 2 2 2 2 2 2 1 2 2 2 1 1 2 2 2 2 2 2 2 2 1 2 2 1 1 1 2 1 2 2 2\n[39] 2 1 1 2 1 1 2 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n[[2]]\n [1] 3 3 1 3 1 1 1 3 3 3 3 3 3 1 3 3 3 1 1 3 3 3 3 3 3 3 3 1 3 3 1 1 1 3 1 3 3 3\n[39] 3 2 2 3 2 1 3 2 1 1 2 3 2 2 2 2 1 1 1 1 2 1 2 2 2 2 2 2 2 2\n\n[[3]]\n [1] 3 3 4 3 4 4 4 3 3 3 3 3 3 4 3 3 3 4 1 3 3 3 3 3 3 3 3 4 3 3 4 4 4 3 1 3 3 3\n[39] 3 2 2 3 2 1 3 2 1 1 2 3 2 2 2 1 1 1 1 1 2 4 2 2 2 2 2 2 2 2\n\n[[4]]\n [1] 3 3 5 3 5 5 5 3 3 3 3 3 3 5 3 3 3 5 1 3 3 3 3 3 3 3 3 5 3 4 4 4 4 3 1 3 3 3\n[39] 3 2 2 3 2 1 3 2 1 1 2 3 2 2 2 1 1 1 1 1 2 4 2 2 2 2 2 2 2 2\n\n[[5]]\n [1] 3 3 5 3 5 5 5 3 3 3 3 3 3 5 3 3 3 5 1 3 3 3 3 3 4 3 3 5 3 4 6 6 6 3 1 4 3 3\n[39] 3 2 2 3 2 1 3 2 1 1 2 3 2 2 2 1 1 1 1 1 2 6 2 2 2 2 2 2 2 2\n\n[[6]]\n [1] 3 3 5 3 5 5 5 3 3 3 3 3 3 5 3 3 3 5 1 3 3 3 3 3 4 3 3 5 3 4 6 6 6 3 1 4 3 3\n[39] 3 2 2 3 2 1 3 2 1 1 2 3 2 2 2 1 1 1 1 1 2 6 7 2 7 7 7 7 7 7\n\n[[7]]\n [1] 8 8 5 8 5 5 5 8 8 8 8 8 8 5 8 8 8 5 1 8 8 8 8 8 4 8 8 5 8 4 6 6 6 3 1 3 3 3\n[39] 3 2 2 3 2 1 3 2 1 1 2 8 2 2 2 1 1 1 1 1 2 6 7 2 7 7 7 7 7 7\n\n[[8]]\n [1] 8 8 5 8 5 5 5 8 8 8 8 8 8 5 8 8 8 5 1 8 8 8 8 8 4 8 8 5 8 4 6 6 6 3 1 3 3 3\n[39] 3 2 2 3 7 1 3 2 1 1 2 8 2 2 2 7 1 1 1 1 7 6 7 2 9 9 9 9 9 9\n\n[[9]]\n [1]  3  3  5  3  5  5  5  3  3  3  3  3  3  5  8  3  3  5  1  3  3  3  3  3  4\n[26]  3  3  5  3  4  6  6  6 10  1 10 10 10 10  2  2 10  7  1 10  2  1  1  2  8\n[51]  2  2  2  7  1  1  1  1  7  6  7  2  9  9  9  9  9  9\n\n\nGreat! Now that we have our partitions, we need to check the modularity corresponding to each one of them.\nWe can do this with the following quick function:\n\n   mod.check <- function(x, c) {\n      k <- length(c)\n      m <- rep(0, k)\n      for (i in 1:k) {\n         m[i] <- modularity(x, c[[i]])\n         }\n      names(m) <- 2:(k+1)\n      return(m)\n      }\n\nWhich takes an igraph graph object and the list of cluster assignments as input and produces a vector of the same length as the list of cluster assignments with the modularity corresponding to that assignment.\nLet’s see this function at work in our running example:\n\n   mod.res <- mod.check(g, clus)\n   round(mod.res, 3)\n\n    2     3     4     5     6     7     8     9    10 \n0.305 0.356 0.328 0.319 0.316 0.300 0.302 0.293 0.286 \n\n\nWhich suggests that the three-cluster partition of the network (shown in Figure 1 (b)) does pretty well in separating densely connected subgraphs (\\(Q = 0.36\\)).\nNote that this partition looks a lot like the one we settled on using Newman’s divisive leading eigenvector approach based on the modularity matrix: One core dense group of lawyers surrounded by a looser couple of communities.\n\n\n\n\n\n\n\n(a) Original Network\n\n\n\n\n\n\n\n(b) Maximum Modularity Solution)\n\n\n\n\n\n\n\n\n\n(c) Four Community Solution\n\n\n\n\n\n\n\n(d) Five Community Solution\n\n\n\n\n\n\n\n\n\n(e) Six Community Solution\n\n\n\n\n\n\n\n(f) Seven Community Solution\n\n\n\n\nFigure 1: Clustering of Nodes in the Law Firm Friendship Network Using the Ratio Cut\n\n\nNote also that the four, five, six and even seven-community partitions are not too shabby either. We can see those in Figure 1 (c)-Figure 1 (f) as they reveal further insights into the group structure of the network beyond the main three-community division.\nNote that further subdivisions of the network split the more loosely structured community in the upper-right, while the more densely linked community in the lower-left remains largely undisturbed."
  },
  {
    "objectID": "spectral.html#spectral-clustering-using-the-eigenvectors-of-the-normalized-laplacian",
    "href": "spectral.html#spectral-clustering-using-the-eigenvectors-of-the-normalized-laplacian",
    "title": "Community Detection Using Spectral Clustering",
    "section": "Spectral Clustering Using the Eigenvectors of the Normalized Laplacian",
    "text": "Spectral Clustering Using the Eigenvectors of the Normalized Laplacian\nWe saw how to cluster a network in a way that results in a good community partition using the Laplacian of the adjacency matrix \\(\\mathbf{L}\\). Another approach is to use a (degree) normalized version of the same matrix \\(\\mathbf{\\hat{L}}\\), defined as follows:\n\\[\n\\mathbf{\\hat{L}} = \\mathbf{I} - \\mathbf{D}^{-\\frac{1}{2}}\\mathbf{A}\\mathbf{D}^{-\\frac{1}{2}}\n\\]\nWhere everything else is as before and \\(\\mathbf{I}\\) is the identity matrix (an \\(n \\times n\\) matrix with ones along the diagonals and zero everywhere else), and \\(\\mathbf{D}^{-\\frac{1}{2}}\\) is a matrix containing the inverse of the square root of degrees of each node (\\(1/\\sqrt{k_i}\\)) in the diagonals and zeros everywhere else.\nWe can just adapt our previous ratio.cut function code to perform this new job:\n\n   norm.ratio.cut <- function(x, k = 6) {\n         A <- as.matrix(as_adjacency_matrix(x))\n         n <- vcount(x)\n         I <- diag(1, n, n)\n         D <- diag(1/sqrt(degree(x)))\n         L <- I - (D %*% A %*% D)\n         eig.L <- eigen(L)\n         b <- n - k\n         e <- n - 1\n         U <- matrix(eig.L$vectors[, b:e], nrow = n, ncol = k)\n         if (k > 1) {\n            U <- U[, k:1] #re-ordering columns from smallest to biggest\n            }         \n         for (i in 1:nrow(U)) {\n            U[i, ] <- U[i, ]/norm(as.matrix(U[i, ]), type = \"E\")\n            }\n      return(U)\n   }\n\nWhere we just have to modify the way we define the \\(\\mathbf{D}\\) and \\(\\mathbf{L}\\) matrices in lines 5 and 6 respectively, after creating the \\(\\mathbf{I}\\) matrix in line 4.\nNow let’s see if the normalized cut can help us finds some communities:\n\n   U <- norm.ratio.cut(g)\n   clus <- k.cuts(U)\n   mod.res <- mod.check(g, clus)\n   round(mod.res, 3)\n\n    2     3     4     5     6     7     8     9    10 \n0.250 0.316 0.320 0.352 0.350 0.331 0.293 0.284 0.246 \n\n\n\n\n\n\n\n\n\n(a) Five Community Solution\n\n\n\n\n\n\n\n(b) Six Community Solution\n\n\n\n\nFigure 2: Clustering of Nodes in the Law Firm Friendship Network Using the Normalized Ratio Cut\n\n\nAs we can see, the normalized ratio cut approach performs almost as well as the ratio cut approach in terms of the maximum modularity it finds (\\(Q = 0.35\\)), but suggests a finer grained partition, with the maximum at either five or six communities.\nThe resulting node clusters are shown in Figure 2 (a) and Figure 2 (b)."
  },
  {
    "objectID": "spectral.html#spectral-clustering-using-the-eigenvectors-of-the-degree-normalized-laplacian",
    "href": "spectral.html#spectral-clustering-using-the-eigenvectors-of-the-degree-normalized-laplacian",
    "title": "Community Detection Using Spectral Clustering",
    "section": "Spectral Clustering Using the Eigenvectors of the Degree-Normalized Laplacian",
    "text": "Spectral Clustering Using the Eigenvectors of the Degree-Normalized Laplacian\nAn alternative version of the normalized Laplacian is given by:\n\\[\n\\mathbf{\\bar{L}} = \\mathbf{D}^{-1}\\mathbf{L}\n\\]\nWhich is just the original Laplacian as defined earlier with each entry divided by the degree of the corresponding node in that row.\nA function that extracts the relevant eigenvectors of this version of the normalized Laplacian goes as follows:\n\n   d.norm.ratio.cut <- function(x, k = 6) {\n         A <- as.matrix(as_adjacency_matrix(x))\n         n <- vcount(x)\n         I <- diag(1, n, n)\n         D <- diag(degree(x))\n         L <- solve(D) %*% (D - A)\n         eig.L <- eigen(L)\n         b <- n - k\n         e <- n - 1\n         U <- matrix(eig.L$vectors[, b:e], nrow = n, ncol = k)\n         if (k > 1) {\n            U <- U[, k:1] #re-ordering columns from smallest to biggest\n            }         \n         for (i in 1:nrow(U)) {\n            U[i, ] <- U[i, ]/norm(as.matrix(U[i, ]), type = \"E\")\n            }\n      return(U)\n   }\n\nWhere, once again, we only need to modify a couple of lines (5 and 6) from before to compute the new version of \\(\\mathbf{L}\\).\nTo see the quality of the partitions obtained via this method, we just type:\n\n   clus <- k.cuts(d.norm.ratio.cut(g))\n   mod.res <- mod.check(g, clus)\n   round(mod.res, 3)\n\n    2     3     4     5     6     7     8     9    10 \n0.250 0.355 0.339 0.327 0.340 0.331 0.294 0.287 0.252 \n\n\nThe degree-normalized Laplacian once again prefers the three-community partition, but also shows that the six community partition produces a high-quality clustering. Here’s how those look like:\n\n\n\n\n\n\n\n(a) Three Community Solution\n\n\n\n\n\n\n\n(b) Six Community Solution\n\n\n\n\nFigure 3: Clustering of Nodes in the Law Firm Friendship Network Using the Degree-Normalized Ratio Cut"
  },
  {
    "objectID": "spectral.html#clustering-using-the-eigenvectors-of-the-modularity-matrix",
    "href": "spectral.html#clustering-using-the-eigenvectors-of-the-modularity-matrix",
    "title": "Community Detection Using Spectral Clustering",
    "section": "Clustering Using the Eigenvectors of the Modularity Matrix",
    "text": "Clustering Using the Eigenvectors of the Modularity Matrix\nAs noted by Fender et al. (2017, 1796), we can extend the spectral clustering approach based on the Laplacian and the normalized Laplacian to the modularity matrix \\(\\mathbf{B}\\). That is, we cluster the graph by embedding the nodes in a set of dimensions defined by the eigendecomposition of \\(\\mathbf{B}\\).\nThe main difference is that rather than using the eigenvectors corresponding to the smallest eigenvalues (as we did with \\(\\mathbf{L}\\)) we proceed in more typical fashion (as done with PCA and CA) and choose the eigenvectors corresponding to the largest ones.\nThis approach, once again, only requires small modifications to the one we used for the Laplacian:\n\n   mod.mat.cut <- function(x, k = 2) {\n         A <- as.matrix(as_adjacency_matrix(x))\n         n <- vcount(x)\n         d <- as.matrix(degree(x))\n         B <- A - (d %*% t(d))/sum(A)\n         eig.B <- eigen(B)\n         U <- eig.B$vectors[, 1:k]\n         for (i in 1:nrow(U)) {\n            U[i, ] <- U[i, ]/norm(as.matrix(U[i, ]), type = \"E\")\n            }\n      return(U)\n   }\n\nThe key difference here is that we compute the modularity matrix \\(\\mathbf{B}\\) rather than \\(\\mathbf{L}\\) from the adjacency matrix \\(\\mathbf{A}\\) in line 5; we then plug \\(\\mathbf{B}\\) into the eigen function and proceed with normalizing in the same way as before.\nAnother difference is that rather than using a large number of eigenvalues (e.g., \\(k = 6\\)), as we did when we were picking from the smallest ones, we now go for parsimony and pick a small rank (two-dimensional) representation of the original modularity matrix (\\(k = 2\\)).\nLet’s see how this works:\n\n   U <- mod.mat.cut(g)\n   clus <- k.cuts(U)\n   mod.res <- mod.check(g, clus)\n   round(mod.res, 3)\n\n    2     3     4     5     6     7     8     9    10 \n0.264 0.366 0.331 0.317 0.306 0.304 0.224 0.187 0.166 \n\n\nWe can see that the three-cluster solution does really well modularity-wise (\\(Q = 0.37\\)), however, the four cluster solution also seems promising. The resulting communities are shown in Figure 3 (a) and Figure 3 (b).\n\n\n\n\n\n\n\n(a) Three Community Solution\n\n\n\n\n\n\n\n(b) Four Community Solution\n\n\n\n\nFigure 4: Clustering of Nodes in the Law Firm Friendship Network Using the Spectral Decomposition of the Modularity Matrix."
  },
  {
    "objectID": "structequiv.html",
    "href": "structequiv.html",
    "title": "Role Equivalence and Structural Similarity",
    "section": "",
    "text": "One of the earlier “proofs of concept” of the power of social network analysis came from demonstrating that you could formalize the fuzzy idea of “role” central to functionalist sociology and British social anthropology using the combined tools of graph theoretical and matrix representations of networks (White, Boorman, and Breiger 1976).\nThis and other contemporaneous work (Breiger, Boorman, and Arabie 1975) set off an entire sub-tradition of data analysis of networks focused on the idea that one could partition the set of vertices in a graph into meaningful classes based on some mathematical (e.g., graph theoretic) criterion.\nThese classes would in turn would be isomorphic with the concept of role as social position and the classes thereby derived as indicating the number of such positions in the social structure under investigation as well as which actors belonged to which positions."
  },
  {
    "objectID": "structequiv.html#structural-equivalence",
    "href": "structequiv.html#structural-equivalence",
    "title": "Role Equivalence and Structural Similarity",
    "section": "Structural Equivalence",
    "text": "Structural Equivalence\nThe earliest work pursued simultaneously by analysts at Harvard (White, Boorman, and Breiger 1976) and Chicago (Burt 1976) relied on the idea of structural equivalence.\nIn a graph \\(G = \\{E, V\\}\\) two nodes \\(v_i, v_j\\) are structurally equivalent if they are connected to the same others in the network; that is, if \\(N(v_i)\\) is the set of nodes adjacent to node \\(v_i\\) and \\(N(v_j)\\) is the set of nodes adjacent to node \\(v_j\\), then:\n\\[\n   v_i \\equiv v_j \\iff N(v_i) = N(v_j)\n\\]\nIn a graph, an equivalence class \\(C\\) is just a set of nodes that are structurally equivalent, such that if \\(v_i \\in C_i\\) and \\(v_j \\in C_i\\) then \\(v_i \\equiv v_j\\) for all pairs \\((v_i, v_j) \\in C_i\\).\nThe partitioning of the vertex set into a set of equivalence classes \\(\\{C_1, C_2 \\ldots C_k\\}\\) as well as the adjacency relations between nodes in the same class and nodes in different classes defines the role structure of the network."
  },
  {
    "objectID": "structequiv.html#structural-equivalence-in-an-ideal-world",
    "href": "structequiv.html#structural-equivalence-in-an-ideal-world",
    "title": "Role Equivalence and Structural Similarity",
    "section": "Structural Equivalence in an Ideal World",
    "text": "Structural Equivalence in an Ideal World\nLet us illustrate these concepts. Consider the following toy graph:\n\n\n\n\n\nFigure 1: A toy graph demonstrating structural equivalence.\n\n\n\n\nWith associated adjacency matrix:\n\n\n\n\n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n  \n \n\n  \n    A \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    B \n    1 \n    0 \n    0 \n    1 \n    1 \n    1 \n    1 \n    0 \n    0 \n  \n  \n    C \n    1 \n    0 \n    0 \n    1 \n    1 \n    1 \n    1 \n    0 \n    0 \n  \n  \n    D \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n  \n  \n    E \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n  \n  \n    F \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n  \n  \n    G \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    H \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n    0 \n    0 \n    1 \n  \n  \n    I \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n    0 \n    1 \n    0 \n  \n\n\n\n\n\nA simple function to check for structural equivalence in the graph, relying on the native R function setequal would be:\n\n   check.equiv <- function(x) {\n      n <- vcount(x)\n      v <- V(x)$name\n      E <- matrix(0, n, n)\n      for (i in v) {\n         for (j in v) {\n            if (i != j & E[which(v == j), which(v == i)] != 1) {\n               N.i <- neighbors(x, i)\n               N.j <- neighbors(x, j)\n               if (are_adjacent(x, i, j) == TRUE) {\n                  N.i <- c(names(N.i), i)\n                  N.j <- c(names(N.j), j)\n                  } #end sub-if\n               if (setequal(N.i, N.j) == TRUE) {\n                  E[which(v == i), which(v == j)] <- 1\n                  E[which(v == j), which(v == i)] <- 1\n                  } #end sub-if\n               } #end main if\n            } #end j loop\n         } #end i loop\n      rownames(E) <- v\n      colnames(E) <- v\n   return(E)\n   }\n\nThis function creates an empty “equivalence” matrix \\(\\mathbf{E}\\) in line 4, loops through each pair of nodes in the graph in lines 5-20. The main condition restricts the checking to nodes that are not the same or have not yet to be found to be equivalent (line 7). Lines 8-9 extract the node neighborhoods using the igraph function neighbors.\nLines 10-13 check to see if the pair of nodes that are being checked for equivalence are themselves adjacent. If they are indeed adjacent (the conditional in line 10 is TRUE) then we need to use the so-called closed neighborhood of \\(v_i\\) and \\(v_j\\), written \\(N[v_i], N[v_j]\\), to do the equivalence check, or otherwise we get the wrong answer.1\nThe equivalence check is done in line 14 using the native R function setequal. This function takes two inputs (e.g., two vectors) and will return a value of TRUE if the elements in the first vector are the same as the elements in the second vector. In that case we update the matrix \\(\\mathbf{E}\\) accordingly.\nAfter writing our function, we can then type:\n\n   Equiv <- check.equiv(g)\n\nAnd the resulting equivalence matrix \\(\\mathbf{E}\\) corresponding to the graph in Figure 1 is:\n\n\n\n\n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n  \n \n\n  \n    A \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    B \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    C \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    D \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    E \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    F \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    G \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    H \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n  \n  \n    I \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n  \n\n\n\n\n\nIn this matrix, there is a 1 in the corresponding cell if the row node is structurally equivalent to the column node.\nOne thing we can do with this matrix is re-order the rows and columns, so that rows(columns) corresponding to nodes that are “adjacent” in the equivalence relation appear next to one another in the matrix.\nTo do that we can use the corrMatOrder function from the corrplot package, designed to work with correlation matrices, but works with any matrix of values:\n\n   #install.packages(\"corrplot\")\n   library(corrplot)\n   SE.ord <- corrMatOrder(Equiv, order = \"hclust\", hclust.method = \"ward.D2\")\n   SE.ord\n\n[1] 6 4 5 8 9 1 7 2 3\n\n\nThe corrplot function corrMatorder takes a matrix as input and returns a vector of reordered values of the rows(columns) as output. We use a hierarchical clustering algorithm using Ward’s method to do the job.\nWe can see that the new re-ordered vector has the previous row(column) 6 in fist position, 4 at second, five at third, 8 at fourth, and so forth.;\nWe can then re-order rows and columns of the old equivalence matrix using this new ordering by typing:\n\n   Equiv <- Equiv[SE.ord, SE.ord]\n\nThe resulting re-ordered matrix looks like:\n\n\n\n\n \n  \n      \n    F \n    D \n    E \n    H \n    I \n    A \n    G \n    B \n    C \n  \n \n\n  \n    F \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    D \n    1 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    E \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    H \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    I \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    A \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    G \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    B \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n  \n  \n    C \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n  \n\n\n\n\n\nOnce the equivalence matrix is re-ordered we can see that sets of structurally equivalent nodes in Figure 1, appear clustered along the diagonals. This type of re-arranged matrix is said to be in block-diagonal form (e.g., non-zero entries clustered along the diagonals).\nEven more interestingly, we can do the same re-arranging on the original adjacency matrix, to reveal:\n\n\n\n\n \n  \n      \n    F \n    D \n    E \n    H \n    I \n    A \n    G \n    B \n    C \n  \n \n\n  \n    F \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    1 \n    1 \n  \n  \n    D \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    1 \n    1 \n  \n  \n    E \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    1 \n    1 \n  \n  \n    H \n    1 \n    1 \n    1 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    I \n    1 \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    A \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n  \n  \n    G \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    1 \n  \n  \n    B \n    1 \n    1 \n    1 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n  \n  \n    C \n    1 \n    1 \n    1 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n  \n\n\n\n\n\nThis is called a blocked adjacency matrix. As you can see, once the structural equivalence relations in the network are revealed by permuting the rows and columns, the adjacency matrix shows an orderly pattern.\nThe way to interpret the blocked adjacency matrix is as follows:\n\nThe block diagonals of the matrix reveal the intra-block relations between sets of structurally equivalent nodes. If the block diagonal is empty–called a zero block–it means that set of structurally equivalent nodes does not connect with one another directly. If it has ones–called a one block–it it means that members of that set of structurally equivalent nodes are also neighbors.\nThe off diagonal blocks reveals the inter-block adjacency relations between different clusters of structurally equivalent nodes. If an off-diagonal block is a one-block, it means that members of block \\(C_i\\) send ties to members of block \\(C_j\\). If and off diagonal block is a zero-block, it means that members of block \\(C_i\\) avoid associating with members of block \\(C_j\\).\n\nSo if:\n\\[\nC_1 = \\{D, E, F\\}\n\\]\n\\[\nC_2 = \\{H, I\\}\n\\]\n\\[\nC_3 = \\{A, G\\}\n\\]\n\\[\nC_4 = \\{B, C\\}\n\\]\nThen we can see that:\n\nMembers of \\(C_1\\) connect with members of \\(C_2\\) and \\(C_4\\) but not among themselves.\nMembers of \\(C_2\\) connect among themselves and with \\(C_1\\).\nMembers of \\(C_3\\) connect among themselves and with \\(C_4\\).\nMembers of \\(C_4\\) connect with \\(C_1\\) and \\(C_3\\) but avoid associating with their own block.\n\nThese intra and inter-block relations can then be represented in the reduced image matrix:\n\n\n\n\n \n  \n      \n    C_1 \n    C_2 \n    C_3 \n    C_4 \n  \n \n\n  \n    C_1 \n    0 \n    1 \n    0 \n    1 \n  \n  \n    C_2 \n    1 \n    1 \n    0 \n    0 \n  \n  \n    C_3 \n    0 \n    0 \n    1 \n    1 \n  \n  \n    C_4 \n    1 \n    0 \n    1 \n    0 \n  \n\n\n\n\n\nWhich reveals a more economical representation of the system based on structural equivalence."
  },
  {
    "objectID": "structequiv.html#structural-equivalence-in-the-real-world",
    "href": "structequiv.html#structural-equivalence-in-the-real-world",
    "title": "Role Equivalence and Structural Similarity",
    "section": "Structural Equivalence in the Real World",
    "text": "Structural Equivalence in the Real World\nOf course, in real data, it is very unlikely that two nodes will meet the exact mathematical criterion of structural equivalence. They might have three out of five, or eight out of nine common neighbors but would still get a big zero in the \\(\\mathbf{E}\\) defined using our strict function.\nSo, a lot of role analysis in real networks follows instead searches for a near cousin to structural equivalence. This leads us to the large class of distance and similarity metrics, and the task is to pick one such that structural equivalence falls off as a special case of the given metric.\nFor random reasons, early work in social network analysis focused on distance metrics, while more recent work inspired by network science focuses on similarity metrics. The end goal is the same though; to cluster nodes in a graph such that those in the same class are the most structurally similar to one another.\nLet us, therefore, begin with the distance approach. Here the goal is simply to pick a distance metric \\(d\\) with a well defined minimum \\(d_{min}\\) or maximum value \\(d_{max}\\), such that:\n\\[\n   v_i \\equiv v_j \\iff d(v_i, v_j) = d_{min} \\lor d(v_i, v_j) = d_{max}\n\\]\nWhere whether we pick the maximum or minimum value depends on the particularities of the measure \\(d\\).\nWe then populate the \\(\\mathbf{E}\\) matrix with the values of \\(d\\) for each pair of nodes \\((v_i, v_j)\\), do some kind of clustering on the matrix, and use our clusters assignments to re-arrange the original adjacency matrix to find our blocks, and so forth.\nA very obvious candidate for \\(d\\) is the Euclidean Distance (Burt 1976):\n\\[\n   d_{i,j} = \\sqrt{\\sum_{k \\neq i,j} (a_{ik} - a_{jk})^2}\n\\]\nWhere \\(a_{ik}\\) and \\(a_{jk}\\) are the corresponding entries in the graph’s adjacency matrix \\(\\mathbf{A}\\). The minimum for this measure is \\(d_{min} = 0\\), so this is the value we should find for structurally equivalent nodes.\nA function that does this for any graph is:\n\n   d.euclid <- function(x) {\n      A <- as.matrix(as_adjacency_matrix(x))\n      n <- nrow(A)\n      E <- matrix(0, n, n)\n      for (i in 1:n) {\n         for (j in 1:n) {\n            if (i < j & i != j) {\n               d.ij <- 0\n               for (k in 1:n) {\n                  if (k != i & k != j) {\n                     d.ij <- d.ij + (A[i,k] - A[j,k])^2\n                     }\n                  }\n               E[i,j] <- sqrt(d.ij)\n               E[j,i] <- sqrt(d.ij)\n            }\n         }\n      }\n   rownames(E) <- rownames(A)\n   colnames(E) <- colnames(A)\n   return(E)\n   }\n\nAnd we can try it out with our toy graph:\n\n   E <- d.euclid(g)\n   round(E, 1)\n\n    A   B   C   D   E   F   G   H   I\nA 0.0 2.0 2.0 1.7 1.7 1.7 0.0 2.6 2.6\nB 2.0 0.0 0.0 2.6 2.6 2.6 2.0 1.7 1.7\nC 2.0 0.0 0.0 2.6 2.6 2.6 2.0 1.7 1.7\nD 1.7 2.6 2.6 0.0 0.0 0.0 1.7 2.0 2.0\nE 1.7 2.6 2.6 0.0 0.0 0.0 1.7 2.0 2.0\nF 1.7 2.6 2.6 0.0 0.0 0.0 1.7 2.0 2.0\nG 0.0 2.0 2.0 1.7 1.7 1.7 0.0 2.6 2.6\nH 2.6 1.7 1.7 2.0 2.0 2.0 2.6 0.0 0.0\nI 2.6 1.7 1.7 2.0 2.0 2.0 2.6 0.0 0.0\n\n\nAnd it looks like indeed it detected the structurally equivalent nodes in the graph. We can see it clearly by re-ordering the rows and columns according to our known ordering:\n\n   E <- E[SE.ord, SE.ord]\n   round(E, 1)\n\n    F   D   E   H   I   A   G   B   C\nF 0.0 0.0 0.0 2.0 2.0 1.7 1.7 2.6 2.6\nD 0.0 0.0 0.0 2.0 2.0 1.7 1.7 2.6 2.6\nE 0.0 0.0 0.0 2.0 2.0 1.7 1.7 2.6 2.6\nH 2.0 2.0 2.0 0.0 0.0 2.6 2.6 1.7 1.7\nI 2.0 2.0 2.0 0.0 0.0 2.6 2.6 1.7 1.7\nA 1.7 1.7 1.7 2.6 2.6 0.0 0.0 2.0 2.0\nG 1.7 1.7 1.7 2.6 2.6 0.0 0.0 2.0 2.0\nB 2.6 2.6 2.6 1.7 1.7 2.0 2.0 0.0 0.0\nC 2.6 2.6 2.6 1.7 1.7 2.0 2.0 0.0 0.0\n\n\nHere the block-diagonals of the matrix contain zeroes because the \\(d\\) is a distance function with a minimum of zero. If we wanted it to contain ones instead we would normalize:\n\\[\n   d^* = 1-\\left(\\frac{d}{max(d)}\\right)\n\\]\nWhich would give us:\n\n   E.norm <- 1 - E/max(E)\n   round(E.norm, 1)\n\n    F   D   E   H   I   A   G   B   C\nF 1.0 1.0 1.0 0.2 0.2 0.3 0.3 0.0 0.0\nD 1.0 1.0 1.0 0.2 0.2 0.3 0.3 0.0 0.0\nE 1.0 1.0 1.0 0.2 0.2 0.3 0.3 0.0 0.0\nH 0.2 0.2 0.2 1.0 1.0 0.0 0.0 0.3 0.3\nI 0.2 0.2 0.2 1.0 1.0 0.0 0.0 0.3 0.3\nA 0.3 0.3 0.3 0.0 0.0 1.0 1.0 0.2 0.2\nG 0.3 0.3 0.3 0.0 0.0 1.0 1.0 0.2 0.2\nB 0.0 0.0 0.0 0.3 0.3 0.2 0.2 1.0 1.0\nC 0.0 0.0 0.0 0.3 0.3 0.2 0.2 1.0 1.0\n\n\nAs noted, a distance function defines a clustering on the nodes, and the results of the clustering generate our blocks. In R we can use the functions dist and hclust to do the job:\n\n   E <- dist(E) #transforming E to a dist object\n   h.res <- hclust(E, method = \"ward.D2\")\n   plot(h.res)\n\n\n\n\nWhich are our original clusters of structurally equivalent nodes!\nLet’s see how this would work in real data. Let’s take the Flintstones (film) co-appearance network as an example:\n\n   library(networkdata)\n   g.flint <- movie_267\n   g.flint <- delete_vertices(g.flint, degree(g.flint) <= 3)\n   E <- d.euclid(g.flint)\n   E <- dist(E) #transforming E to a dist object\n   h.res <- hclust(E, method = \"ward.D2\") #hierarchical clustering\n   #install.packages(\"dendextend\")\n   library(dendextend) #nice dendrogram plotting\n   dend <- as.dendrogram(h.res) %>% \n      color_branches(k = 8) %>% \n      color_labels(k = 8) %>% \n      set(\"labels_cex\", 0.65) %>% \n      set(\"branches_lwd\", 2) %>% \n      plot\n\n\n\n\nThe Flintstones is a film based on a family, and families are the prototypes of social roles in networks, so if structural equivalence gets at roles, then it should recover known kin roles here, and indeed it does to some extent. One cluster is the focal parents separated into “moms” and “dads” roles, and another has the kids.\nAfter we have our clustering, we may wish to extract the structurally equivalent blocks from the hierarchical clustering results. To do that, we need to cut the dendrogram at a height that will produce a given number of clusters. Of course because hierarchical clustering is agglomerative, it begins with all nodes in the same cluster and ends with all nodes in a single cluster. So a reasonable solution is some (relatively) small number of clusters \\(k\\) such that \\(1 > k < n\\) that is, some number larger than one but smaller than the number of nodes in the graph.\nChoosing the number of clusters after a hierarchical clustering is not a well-defined problem, so you have to use a combination of pragmatic and domain-specific knowledge criteria to decide. Here, it looks like four blocks provides enough resolution and substantive interpretability so let’s do that. To do the job we use a function from the dendextend package we used above to draw our pretty colored dendrogram data viz, called cutree which as its name implies cuts the dendrogram at a height that produces the required number of classes:\n\n   blocks  <- cutree(h.res, k = 4)\n   blocks\n\n        BAM-BAM          BARNEY           BETTY        FELDSPAR            FRED \n              1               2               2               3               2 \n   HEADMISTRESS      HERDMASTER            LAVA           LEACH          MORRIS \n              4               3               2               1               4 \n      MRS SLATE         PEBBLES PEBBLES BAM-BAM        PILTDOWN      POINDEXTER \n              4               1               1               3               4 \n         PYRITE           SLATE           WILMA \n              3               2               2 \n\n\nNote that the result is a named vector with the node labels as the names and a value of \\(k\\) for each node, where \\(k\\) indicates the class of that node. For instance, \\(\\{Barney, Betty, Fred, Lava, Slate, Wilma\\}\\) all belong to \\(k = 2\\). Remember this is a purely nominal classification so the order of the numbers doesn’t matter."
  },
  {
    "objectID": "structequiv.html#concor",
    "href": "structequiv.html#concor",
    "title": "Role Equivalence and Structural Similarity",
    "section": "CONCOR",
    "text": "CONCOR\nThe other (perhaps less obvious) way of defining a distance between nodes in the network based on their connectivity patterns to other nodes is the correlation distance:\n\\[\n    d_{i,j} =\n    \\frac{\n    \\sum_{i \\neq k}\n    (a_{ki} - \\overline{a}_{i}) \\times\n    \\sum_{j \\neq k}\n    (a_{kj} - \\overline{a}_{j})\n    }\n    {\n    \\sqrt{\n    \\sum_{i \\neq k}\n    (a_{ki} - \\overline{a}_{i})^2 \\times\n    \\sum_{j \\neq k}\n    (a_{kj} - \\overline{a}_{j})^2\n        }\n    }\n\\]\nA more involved but still meaningful formula compared of the Euclidean distance. Here \\(\\overline{a}_{i}\\) is the column mean of the entries of node \\(i\\) in the affiliation matrix.\nSo the correlation distance is the ratio of the covariance of the column vectors corresponding to each node in the adjacency matrix and the product of their standard deviations.\nThe correlation distance between nodes in our toy network is given by simply typing:\n\n   m <- as.matrix(as_adjacency_matrix(g))\n   C <- cor(m)\n   round(C, 2)\n\n      A     B     C     D     E     F     G     H     I\nA  1.00 -0.32 -0.32  0.32  0.32  0.32  0.50 -0.63 -0.63\nB -0.32  1.00  1.00 -1.00 -1.00 -1.00 -0.32  0.35  0.35\nC -0.32  1.00  1.00 -1.00 -1.00 -1.00 -0.32  0.35  0.35\nD  0.32 -1.00 -1.00  1.00  1.00  1.00  0.32 -0.35 -0.35\nE  0.32 -1.00 -1.00  1.00  1.00  1.00  0.32 -0.35 -0.35\nF  0.32 -1.00 -1.00  1.00  1.00  1.00  0.32 -0.35 -0.35\nG  0.50 -0.32 -0.32  0.32  0.32  0.32  1.00 -0.63 -0.63\nH -0.63  0.35  0.35 -0.35 -0.35 -0.35 -0.63  1.00  0.55\nI -0.63  0.35  0.35 -0.35 -0.35 -0.35 -0.63  0.55  1.00\n\n\nWhich gives the Pearson product moment correlation of each pair of columns in the adjacency matrix.\nThe key thing that was noticed by Breiger, Boorman, and Arabie (1975) is that we can iterate this process, and compute correlation distances of correlation distances between nodes in the graph. If we do this for our toy network a few (e.g., three) times we get:\n\n   C1 <- cor(C)\n   C2 <- cor(C1)\n   C3 <- cor(C2)\n   round(C3, 2)\n\n   A  B  C  D  E  F  G  H  I\nA  1 -1 -1  1  1  1  1 -1 -1\nB -1  1  1 -1 -1 -1 -1  1  1\nC -1  1  1 -1 -1 -1 -1  1  1\nD  1 -1 -1  1  1  1  1 -1 -1\nE  1 -1 -1  1  1  1  1 -1 -1\nF  1 -1 -1  1  1  1  1 -1 -1\nG  1 -1 -1  1  1  1  1 -1 -1\nH -1  1  1 -1 -1 -1 -1  1  1\nI -1  1  1 -1 -1 -1 -1  1  1\n\n\nInterestingly the positive correlations converge to 1.0 and the negative correlations converge to -1.0!\nIf we sort the rows and columns of the new matrix according to these values, we get:\n\n   C.ord <- corrMatOrder(C3, order = \"hclust\", hclust.method = \"ward.D2\")\n   C3 <- C3[C.ord, C.ord]\n   round(C3, 2)\n\n   B  C  H  I  F  D  E  A  G\nB  1  1  1  1 -1 -1 -1 -1 -1\nC  1  1  1  1 -1 -1 -1 -1 -1\nH  1  1  1  1 -1 -1 -1 -1 -1\nI  1  1  1  1 -1 -1 -1 -1 -1\nF -1 -1 -1 -1  1  1  1  1  1\nD -1 -1 -1 -1  1  1  1  1  1\nE -1 -1 -1 -1  1  1  1  1  1\nA -1 -1 -1 -1  1  1  1  1  1\nG -1 -1 -1 -1  1  1  1  1  1\n\n\nAha! The iterated correlations seems to have split the matrix into two blocks \\(C_1 = \\{G, F, D, A, G\\}\\) and \\(C_2 = \\{I, H, B, C\\}\\). Each of the blocks is composed of two sub-blocks that we know are structurally equivalent from our previous analysis.\nA function implementing this method of iterated correlation distances until convergence looks like:\n\n   con.cor <- function(x) {\n      C <- x\n      while (mean(abs(C)) != 1) {\n         C <- cor(C)\n         }\n      b1 <- C[, 1] > 0\n      b2 <- !b1\n      return(list(x[, b1, drop = FALSE], x[, b2, drop = FALSE]))\n      }\n\nThis function takes a graph’s adjacency matrix as input, creates a copy of the adjacency matrix in line 2 (to be put through the iterated correlations meat grinder). The three-line (3-5) while loop goes through the iterated correlations (stopping when the matrix is full of ones). Then the resulting two blocks are returned as the columns of a couple of matrices stored in a list in line 8.\nFor instance, to use our example above:\n\n   con.cor(m)\n\n[[1]]\n  A D E F G\nA 0 0 0 0 1\nB 1 1 1 1 1\nC 1 1 1 1 1\nD 0 0 0 0 0\nE 0 0 0 0 0\nF 0 0 0 0 0\nG 1 0 0 0 0\nH 0 1 1 1 0\nI 0 1 1 1 0\n\n[[2]]\n  B C H I\nA 1 1 0 0\nB 0 0 0 0\nC 0 0 0 0\nD 1 1 1 1\nE 1 1 1 1\nF 1 1 1 1\nG 1 1 0 0\nH 0 0 0 1\nI 0 0 1 0\n\n\nThe columns of these two matrices are the two blocks we found before. Of course, to implement this method as a divisive clustering algorithm, what we want is to split these two blocks into two finer grained blocks, by iterative correlations of the columns of these two sub-matrices (to reveal two further sub-matrices each) and thus find our original four structurally equivalent groups.\nThe following function–simplified and adapted from Adam Slez’s work—which includes the con.cor function shown earlier inside of it, will do it:\n\n  blocks <- function(g, s = 2) {\n     A <- as.matrix(as_adjacency_matrix(g))\n     B <- list(A)\n     for(i in 1:s) {\n       B <- unlist(lapply(B, con.cor), recursive = FALSE)\n       }\n     return(lapply(B, colnames))\n   }\n\nThis function takes the graph as input and returns a list of column names containing the structurally equivalent blocks as output:\n\n   blocks(g)\n\n[[1]]\n[1] \"A\" \"G\"\n\n[[2]]\n[1] \"D\" \"E\" \"F\"\n\n[[3]]\n[1] \"B\" \"C\"\n\n[[4]]\n[1] \"H\" \"I\"\n\n\nWhich are the original structurally equivalent classes. The argument s controls the number of splits. When it is equal to one, the function produces two blocks, and when it is equal to two it produces four blocks, when it is equal to three, six blocks, and so on.\nThis is the algorithm called CONCOR (Breiger, Boorman, and Arabie 1975), short for convergence of iterate correlations, and can be used to cluster the rows(columns) of any valued square data matrix.\nFor instance, if we wanted to split the Flintstones network into four blocks we would proceed as follows:\n\n   g.flint <- movie_267\n   g.flint <- delete_vertices(g.flint, degree(g.flint) <= 3)\n   blocks(g.flint)\n\n[[1]]\n[1] \"BAM-BAM\"    \"BETTY\"      \"PEBBLES\"    \"POINDEXTER\" \"WILMA\"     \n\n[[2]]\n[1] \"FELDSPAR\"   \"HERDMASTER\" \"LAVA\"       \"PILTDOWN\"   \"PYRITE\"    \n[6] \"SLATE\"     \n\n[[3]]\n[1] \"BARNEY\"          \"FRED\"            \"LEACH\"           \"PEBBLES BAM-BAM\"\n\n[[4]]\n[1] \"HEADMISTRESS\" \"MORRIS\"       \"MRS SLATE\"   \n\n\nWhich is similar to the results we got from the Euclidean distance method, except that now the children are put in the same blocks as the moms.\nWe could then visualize the results as follows:\n\n   #install.packages(\"ggcorrplot\")\n   library(ggcorrplot)\n   A <- as.matrix(as_adjacency_matrix(g.flint))\n   ord <- unlist(blocks(g.flint, 2))\n   A <- A[ord, ord]\n   p <- ggcorrplot(A, colors = c(\"white\", \"white\", \"black\")) \n   p <- p + theme(legend.position = \"none\", \n                  axis.text.y = element_text(size = 8),\n                  axis.text.x = element_text(size = 8),\n                  )\n   p <- p + geom_hline(yintercept = 5.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_vline(xintercept = 5.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_hline(yintercept = 11.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_vline(xintercept = 11.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_hline(yintercept = 15.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_vline(xintercept = 15.5, linewidth = 2, color = \"blue\")\n   p\n\n\n\n\nWhich is just a tile plot of the original adjacency matrix (adjacent cells in black) with rows and columns re-ordered according to the four-block solution and ggplot vertical and horizontal lines highlighting the boundaries of the blocked matrix.\nAs you can see, characters with similar patterns of scene co-appearances (like Barney and Fred) are drawn next to one another, revealing the larger “roles” in the network."
  },
  {
    "objectID": "structequiv.html#structural-equivalence-in-directed-graphs",
    "href": "structequiv.html#structural-equivalence-in-directed-graphs",
    "title": "Role Equivalence and Structural Similarity",
    "section": "Structural Equivalence in Directed Graphs",
    "text": "Structural Equivalence in Directed Graphs\nLike before, the main complication introduced by the directed case is the “doubling” of the relations considered.\nIn the Euclidean distance case, we have to decide whether we want to compute two set of distances between nodes, one based on the in-distance vectors and the other on the out-distance vectors, and the two sets of hierarchical clustering partitions.\nAnother approach is simply to combine both according to the formula:\n\\[\n   d_{i,j} = \\sqrt{\n                  \\sum_{k \\neq i,j} (a_{ik} - a_{jk})^2 +\n                  \\sum_{k \\neq i,j} (a_{ki} - a_{kj})^2\n                  }\n\\]\nWhich just computes the Euclidean distances between nodes using both in and out neighbors. Here nodes would be structurally equivalent only if they have the same set of in and out-neighbors. This would mean changing line 11 in the function d.euclid above with the following:\n\n   d.ij <- d.ij + ((A[i,k] - A[j,k])^2 + (A[k,i] - A[k,j])^2)\n\nThis way, distances are computed on both the row and columns of the directed graph’s adjacency matrix.\nIf we are using the correlation distance approach in a directed graph, then the main trick is to stack the original adjacency matrix against its transpose, and then compute the correlation distance on the columns of the stacked matrices, which by definition combines information in incoming and outgoing ties.\nLet’s see a brief example. Let’s load up the Krackhardt’s high-tech managers data on advice relations and look at the adjacency matrix:\n\n   g <- ht_advice\n   A <- as.matrix(as_adjacency_matrix(g))\n   A\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]\n [1,]    0    1    0    1    0    0    0    1    0     0     0     0     0\n [2,]    0    0    0    0    0    1    1    0    0     0     0     0     0\n [3,]    1    1    0    1    0    1    1    1    1     1     1     1     0\n [4,]    1    1    0    0    0    1    0    1    0     1     1     1     0\n [5,]    1    1    0    0    0    1    1    1    0     1     1     0     1\n [6,]    0    0    0    0    0    0    0    0    0     0     0     0     0\n [7,]    0    1    0    0    0    1    0    0    0     0     1     1     0\n [8,]    0    1    0    1    0    1    1    0    0     1     1     0     0\n [9,]    1    1    0    0    0    1    1    1    0     1     1     1     0\n[10,]    1    1    1    1    1    0    0    1    0     0     1     0     1\n[11,]    1    1    0    0    0    0    1    0    0     0     0     0     0\n[12,]    0    0    0    0    0    0    1    0    0     0     0     0     0\n[13,]    1    1    0    0    1    0    0    0    1     0     0     0     0\n[14,]    0    1    0    0    0    0    1    0    0     0     0     0     0\n[15,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[16,]    1    1    0    0    0    0    0    0    0     1     0     0     0\n[17,]    1    1    0    1    0    0    1    0    0     0     0     0     0\n[18,]    1    1    1    1    1    0    1    1    1     1     1     0     1\n[19,]    1    1    1    0    1    0    1    0    0     1     1     0     0\n[20,]    1    1    0    0    0    1    0    1    0     0     1     1     0\n[21,]    0    1    1    1    0    1    1    1    0     0     0     1     0\n      [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21]\n [1,]     0     0     1     0     1     0     0     1\n [2,]     0     0     0     0     0     0     0     1\n [3,]     1     0     0     1     1     0     1     1\n [4,]     0     0     1     1     1     0     1     1\n [5,]     1     0     1     1     1     1     1     1\n [6,]     0     0     0     0     0     0     0     1\n [7,]     1     0     0     1     1     0     0     1\n [8,]     0     0     0     0     1     0     0     1\n [9,]     1     0     1     1     1     0     0     1\n[10,]     0     1     1     1     1     1     1     0\n[11,]     0     0     0     0     0     0     0     0\n[12,]     0     0     0     0     0     0     0     1\n[13,]     1     0     0     0     1     0     0     0\n[14,]     0     0     0     0     1     0     0     1\n[15,]     1     0     1     1     1     1     1     1\n[16,]     0     0     0     0     1     0     0     0\n[17,]     0     0     0     0     0     0     0     1\n[18,]     1     1     1     0     0     1     1     1\n[19,]     1     1     0     0     1     0     1     0\n[20,]     1     1     1     1     1     0     0     1\n[21,]     1     0     0     1     1     0     1     0\n\n\nRecall that in these data a tie goes from an advice seeker to an advisor. So the standard correlation distance on the columns computes the in-correlation, or structural equivalence based on incoming ties (two managers are equivalent if they are nominated as advisors by the same others).\nWe may also be interested in the out-correlation, that is structural equivalence based on out-going ties. Here two managers are structurally equivalent is they seek advice from the same others. This information is contained in the transpose of the original adjacency matrix:\n\n   A.t <- t(A)\n   A.t\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]\n [1,]    0    0    1    1    1    0    0    0    1     1     1     0     1\n [2,]    1    0    1    1    1    0    1    1    1     1     1     0     1\n [3,]    0    0    0    0    0    0    0    0    0     1     0     0     0\n [4,]    1    0    1    0    0    0    0    1    0     1     0     0     0\n [5,]    0    0    0    0    0    0    0    0    0     1     0     0     1\n [6,]    0    1    1    1    1    0    1    1    1     0     0     0     0\n [7,]    0    1    1    0    1    0    0    1    1     0     1     1     0\n [8,]    1    0    1    1    1    0    0    0    1     1     0     0     0\n [9,]    0    0    1    0    0    0    0    0    0     0     0     0     1\n[10,]    0    0    1    1    1    0    0    1    1     0     0     0     0\n[11,]    0    0    1    1    1    0    1    1    1     1     0     0     0\n[12,]    0    0    1    1    0    0    1    0    1     0     0     0     0\n[13,]    0    0    0    0    1    0    0    0    0     1     0     0     0\n[14,]    0    0    1    0    1    0    1    0    1     0     0     0     1\n[15,]    0    0    0    0    0    0    0    0    0     1     0     0     0\n[16,]    1    0    0    1    1    0    0    0    1     1     0     0     0\n[17,]    0    0    1    1    1    0    1    0    1     1     0     0     0\n[18,]    1    0    1    1    1    0    1    1    1     1     0     0     1\n[19,]    0    0    0    0    1    0    0    0    0     1     0     0     0\n[20,]    0    0    1    1    1    0    0    0    0     1     0     0     0\n[21,]    1    1    1    1    1    1    1    1    1     0     0     1     0\n      [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21]\n [1,]     0     1     1     1     1     1     1     0\n [2,]     1     1     1     1     1     1     1     1\n [3,]     0     1     0     0     1     1     0     1\n [4,]     0     1     0     1     1     0     0     1\n [5,]     0     1     0     0     1     1     0     0\n [6,]     0     1     0     0     0     0     1     1\n [7,]     1     1     0     1     1     1     0     1\n [8,]     0     1     0     0     1     0     1     1\n [9,]     0     1     0     0     1     0     0     0\n[10,]     0     1     1     0     1     1     0     0\n[11,]     0     1     0     0     1     1     1     0\n[12,]     0     1     0     0     0     0     1     1\n[13,]     0     1     0     0     1     0     0     0\n[14,]     0     1     0     0     1     1     1     1\n[15,]     0     0     0     0     1     1     1     0\n[16,]     0     1     0     0     1     0     1     0\n[17,]     0     1     0     0     0     0     1     1\n[18,]     1     1     1     0     0     1     1     1\n[19,]     0     1     0     0     1     0     0     0\n[20,]     0     1     0     0     1     1     0     1\n[21,]     1     1     0     1     1     0     1     0\n\n\nCorrelating the columns of this matrix would thus give us the out-correlation distance based on advice seeking relations.\n“Stacking” is a way to combine both in and out-going ties and compute a single distance based on both. It just means that we literally bind the rows of the firs matrix and its transpose:\n\n   A.stack <- rbind(A, A.t)\n   A.stack\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]\n [1,]    0    1    0    1    0    0    0    1    0     0     0     0     0\n [2,]    0    0    0    0    0    1    1    0    0     0     0     0     0\n [3,]    1    1    0    1    0    1    1    1    1     1     1     1     0\n [4,]    1    1    0    0    0    1    0    1    0     1     1     1     0\n [5,]    1    1    0    0    0    1    1    1    0     1     1     0     1\n [6,]    0    0    0    0    0    0    0    0    0     0     0     0     0\n [7,]    0    1    0    0    0    1    0    0    0     0     1     1     0\n [8,]    0    1    0    1    0    1    1    0    0     1     1     0     0\n [9,]    1    1    0    0    0    1    1    1    0     1     1     1     0\n[10,]    1    1    1    1    1    0    0    1    0     0     1     0     1\n[11,]    1    1    0    0    0    0    1    0    0     0     0     0     0\n[12,]    0    0    0    0    0    0    1    0    0     0     0     0     0\n[13,]    1    1    0    0    1    0    0    0    1     0     0     0     0\n[14,]    0    1    0    0    0    0    1    0    0     0     0     0     0\n[15,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[16,]    1    1    0    0    0    0    0    0    0     1     0     0     0\n[17,]    1    1    0    1    0    0    1    0    0     0     0     0     0\n[18,]    1    1    1    1    1    0    1    1    1     1     1     0     1\n[19,]    1    1    1    0    1    0    1    0    0     1     1     0     0\n[20,]    1    1    0    0    0    1    0    1    0     0     1     1     0\n[21,]    0    1    1    1    0    1    1    1    0     0     0     1     0\n[22,]    0    0    1    1    1    0    0    0    1     1     1     0     1\n[23,]    1    0    1    1    1    0    1    1    1     1     1     0     1\n[24,]    0    0    0    0    0    0    0    0    0     1     0     0     0\n[25,]    1    0    1    0    0    0    0    1    0     1     0     0     0\n[26,]    0    0    0    0    0    0    0    0    0     1     0     0     1\n[27,]    0    1    1    1    1    0    1    1    1     0     0     0     0\n[28,]    0    1    1    0    1    0    0    1    1     0     1     1     0\n[29,]    1    0    1    1    1    0    0    0    1     1     0     0     0\n[30,]    0    0    1    0    0    0    0    0    0     0     0     0     1\n[31,]    0    0    1    1    1    0    0    1    1     0     0     0     0\n[32,]    0    0    1    1    1    0    1    1    1     1     0     0     0\n[33,]    0    0    1    1    0    0    1    0    1     0     0     0     0\n[34,]    0    0    0    0    1    0    0    0    0     1     0     0     0\n[35,]    0    0    1    0    1    0    1    0    1     0     0     0     1\n[36,]    0    0    0    0    0    0    0    0    0     1     0     0     0\n[37,]    1    0    0    1    1    0    0    0    1     1     0     0     0\n[38,]    0    0    1    1    1    0    1    0    1     1     0     0     0\n[39,]    1    0    1    1    1    0    1    1    1     1     0     0     1\n[40,]    0    0    0    0    1    0    0    0    0     1     0     0     0\n[41,]    0    0    1    1    1    0    0    0    0     1     0     0     0\n[42,]    1    1    1    1    1    1    1    1    1     0     0     1     0\n      [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21]\n [1,]     0     0     1     0     1     0     0     1\n [2,]     0     0     0     0     0     0     0     1\n [3,]     1     0     0     1     1     0     1     1\n [4,]     0     0     1     1     1     0     1     1\n [5,]     1     0     1     1     1     1     1     1\n [6,]     0     0     0     0     0     0     0     1\n [7,]     1     0     0     1     1     0     0     1\n [8,]     0     0     0     0     1     0     0     1\n [9,]     1     0     1     1     1     0     0     1\n[10,]     0     1     1     1     1     1     1     0\n[11,]     0     0     0     0     0     0     0     0\n[12,]     0     0     0     0     0     0     0     1\n[13,]     1     0     0     0     1     0     0     0\n[14,]     0     0     0     0     1     0     0     1\n[15,]     1     0     1     1     1     1     1     1\n[16,]     0     0     0     0     1     0     0     0\n[17,]     0     0     0     0     0     0     0     1\n[18,]     1     1     1     0     0     1     1     1\n[19,]     1     1     0     0     1     0     1     0\n[20,]     1     1     1     1     1     0     0     1\n[21,]     1     0     0     1     1     0     1     0\n[22,]     0     1     1     1     1     1     1     0\n[23,]     1     1     1     1     1     1     1     1\n[24,]     0     1     0     0     1     1     0     1\n[25,]     0     1     0     1     1     0     0     1\n[26,]     0     1     0     0     1     1     0     0\n[27,]     0     1     0     0     0     0     1     1\n[28,]     1     1     0     1     1     1     0     1\n[29,]     0     1     0     0     1     0     1     1\n[30,]     0     1     0     0     1     0     0     0\n[31,]     0     1     1     0     1     1     0     0\n[32,]     0     1     0     0     1     1     1     0\n[33,]     0     1     0     0     0     0     1     1\n[34,]     0     1     0     0     1     0     0     0\n[35,]     0     1     0     0     1     1     1     1\n[36,]     0     0     0     0     1     1     1     0\n[37,]     0     1     0     0     1     0     1     0\n[38,]     0     1     0     0     0     0     1     1\n[39,]     1     1     1     0     0     1     1     1\n[40,]     0     1     0     0     1     0     0     0\n[41,]     0     1     0     0     1     1     0     1\n[42,]     1     1     0     1     1     0     1     0\n\n\nNote that this matrix has the same number of columns as the original adjacency matrix and double the number of rows. This doesn’t matter since the correlation distance works on the columns, meaning that it will return a matrix of the same dimensions as the original:\n\n   round(cor(A.stack), 2)\n\n       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7] [,8]  [,9] [,10] [,11] [,12]\n [1,]  1.00  0.43  0.00  0.09  0.09  0.22  0.14 0.37  0.13  0.25  0.37  0.22\n [2,]  0.43  1.00 -0.19  0.00 -0.19  0.49  0.24 0.38 -0.15 -0.24  0.51  0.52\n [3,]  0.00 -0.19  1.00  0.52  0.62 -0.24  0.19 0.33  0.57  0.00  0.03 -0.03\n [4,]  0.09  0.00  0.52  1.00  0.43 -0.03  0.29 0.33  0.57  0.10  0.03 -0.03\n [5,]  0.09 -0.19  0.62  0.43  1.00 -0.35  0.00 0.14  0.67  0.20  0.03 -0.15\n [6,]  0.22  0.49 -0.24 -0.03 -0.35  1.00  0.27 0.36 -0.16  0.00  0.50  0.74\n [7,]  0.14  0.24  0.19  0.29  0.00  0.27  1.00 0.19  0.24 -0.05  0.10  0.06\n [8,]  0.37  0.38  0.33  0.33  0.14  0.36  0.19 1.00  0.27  0.01  0.41  0.49\n [9,]  0.13 -0.15  0.57  0.57  0.67 -0.16  0.24 0.27  1.00  0.07  0.03  0.04\n[10,]  0.25 -0.24  0.00  0.10  0.20  0.00 -0.05 0.01  0.07  1.00  0.24 -0.11\n[11,]  0.37  0.51  0.03  0.03  0.03  0.50  0.10 0.41  0.03  0.24  1.00  0.49\n[12,]  0.22  0.52 -0.03 -0.03 -0.15  0.74  0.06 0.49  0.04 -0.11  0.49  1.00\n[13,]  0.17 -0.11  0.36  0.14  0.25 -0.08  0.11 0.19  0.22  0.17  0.32 -0.16\n[14,]  0.47  0.51  0.13  0.03  0.13  0.50  0.30 0.51  0.24  0.03  0.57  0.62\n[15,] -0.08 -0.48  0.63  0.25  0.63 -0.47 -0.19 0.07  0.42  0.18 -0.10 -0.25\n[16,]  0.38  0.21  0.14  0.24  0.14  0.22  0.00 0.62  0.12  0.15  0.56  0.18\n[17,]  0.37  0.40  0.13  0.03 -0.07  0.61  0.00 0.61  0.03  0.03  0.68  0.74\n[18,]  0.06  0.11 -0.03 -0.14  0.09  0.21 -0.45 0.15 -0.11  0.28  0.28  0.29\n[19,] -0.08 -0.25  0.38  0.18  0.38 -0.22 -0.05 0.26  0.30  0.28  0.21 -0.15\n[20,]  0.28  0.00  0.52  0.52  0.43  0.08  0.38 0.33  0.57  0.29  0.24  0.08\n[21,]  0.02  0.10 -0.04  0.06 -0.23  0.24  0.29 0.18  0.05 -0.02  0.24  0.17\n      [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21]\n [1,]  0.17  0.47 -0.08  0.38  0.37  0.06 -0.08  0.28  0.02\n [2,] -0.11  0.51 -0.48  0.21  0.40  0.11 -0.25  0.00  0.10\n [3,]  0.36  0.13  0.63  0.14  0.13 -0.03  0.38  0.52 -0.04\n [4,]  0.14  0.03  0.25  0.24  0.03 -0.14  0.18  0.52  0.06\n [5,]  0.25  0.13  0.63  0.14 -0.07  0.09  0.38  0.43 -0.23\n [6,] -0.08  0.50 -0.47  0.22  0.61  0.21 -0.22  0.08  0.24\n [7,]  0.11  0.30 -0.19  0.00  0.00 -0.45 -0.05  0.38  0.29\n [8,]  0.19  0.51  0.07  0.62  0.61  0.15  0.26  0.33  0.18\n [9,]  0.22  0.24  0.42  0.12  0.03 -0.11  0.30  0.57  0.05\n[10,]  0.17  0.03  0.18  0.15  0.03  0.28  0.28  0.29 -0.02\n[11,]  0.32  0.57 -0.10  0.56  0.68  0.28  0.21  0.24  0.24\n[12,] -0.16  0.62 -0.25  0.18  0.74  0.29 -0.15  0.08  0.17\n[13,]  1.00  0.20  0.26  0.51  0.20  0.05  0.63  0.36 -0.02\n[14,]  0.20  1.00 -0.10  0.34  0.57  0.16  0.11  0.24  0.14\n[15,]  0.26 -0.10  1.00  0.02 -0.10  0.08  0.34  0.25 -0.18\n[16,]  0.51  0.34  0.02  1.00  0.45  0.11  0.41  0.24  0.17\n[17,]  0.20  0.57 -0.10  0.45  1.00  0.40  0.11  0.24  0.14\n[18,]  0.05  0.16  0.08  0.11  0.40  1.00  0.18 -0.03 -0.32\n[19,]  0.63  0.11  0.34  0.41  0.11  0.18  1.00  0.28 -0.03\n[20,]  0.36  0.24  0.25  0.24  0.24 -0.03  0.28  1.00 -0.04\n[21,] -0.02  0.14 -0.18  0.17  0.14 -0.32 -0.03 -0.04  1.00\n\n\nAnd we would then do a blockmodel based on these distances:\n\n   blocks2 <- function(A, s = 2) {\n     colnames(A) <- 1:ncol(A) #use only if the original matrix has no names\n     B <- list(A)\n     for(i in 1:s) {\n       B <- unlist(lapply(B, con.cor), recursive = FALSE)\n       }\n     return(lapply(B, colnames))\n   }\n   blocks2(A.stack)\n\n[[1]]\n [1] \"1\"  \"2\"  \"6\"  \"8\"  \"11\" \"12\" \"14\" \"16\" \"17\" \"18\"\n\n[[2]]\n[1] \"7\"  \"21\"\n\n[[3]]\n[1] \"3\"  \"4\"  \"5\"  \"9\"  \"15\" \"20\"\n\n[[4]]\n[1] \"10\" \"13\" \"19\""
  },
  {
    "objectID": "structequiv.html#structural-equivalence-in-multigraphs",
    "href": "structequiv.html#structural-equivalence-in-multigraphs",
    "title": "Role Equivalence and Structural Similarity",
    "section": "Structural Equivalence in Multigraphs",
    "text": "Structural Equivalence in Multigraphs\nNote that we would apply the same trick if we wanted to do a blockmodel based on multiple relations like friendship and advice. Here’s a blockmodel based on the stacked matrices of incoming ties of both types:\n\n   A.f <- as.matrix(as_adjacency_matrix(ht_friends))\n   A.stack <- rbind(A, A.f)\n   blocks2(A.stack)\n\n[[1]]\n[1] \"1\"  \"3\"  \"5\"  \"14\" \"15\" \"20\"\n\n[[2]]\n[1] \"4\"  \"9\"  \"13\" \"19\"\n\n[[3]]\n[1] \"2\"  \"8\"  \"12\" \"16\" \"17\" \"18\"\n\n[[4]]\n[1] \"6\"  \"7\"  \"10\" \"11\" \"21\"\n\n\nAnd one combining incoming and outgoing friendship and advice ties:\n\n   A.stack <- rbind(A, A.t, A.f, t(A.f))\n   blocks2(A.stack)\n\n[[1]]\n[1] \"1\"  \"2\"  \"8\"  \"12\" \"16\" \"21\"\n\n[[2]]\n[1] \"6\"  \"11\" \"14\" \"17\"\n\n[[3]]\n[1] \"3\"  \"4\"  \"5\"  \"7\"  \"9\"  \"20\"\n\n[[4]]\n[1] \"10\" \"13\" \"15\" \"18\" \"19\"\n\n\nNote that here the stacked matrix has four sub-matrices: (1) Incoming advice, (2) outgoing advice, (3) incoming friendship, and (4) Outgoing friendship."
  },
  {
    "objectID": "subgraph-select.html",
    "href": "subgraph-select.html",
    "title": "Selecting a Subgraph Based on Edge Conditions",
    "section": "",
    "text": "A great question asked in class goes as follows: What if I want to create a subgraph based on selecting a subset of a nodes, and then the other nodes in the graph that are that set of node’s in-neighbors?\nLet’s see how that would work.\nFirst, we create a vector with the node ids of our focal nodes, which will be women under 40 in the law_advice network from the package networkdata.\n\n   library(networkdata)\n   library(igraph)\n   library(ggraph)\n   yw <- which(V(law_advice)$gender == 2 & V(law_advice)$age < 40)\n   yw\n\n [1] 29 34 39 48 51 57 59 60 61 67 69 71\n\n\nSecond, we need to collect the node ids of the people who point to these set of nodes; that is, each of these node’s in-neighbors. For that, we use the igraph function neighbors:\n\n   yw_in <- list() #creating empty list\n   k <- 1 #counter for list position\n   for (i in yw) {\n      nei <- neighbors(law_advice, i, mode = \"in\") \n      nei <- as.vector(nei) #vector of in-neighbors ids\n      yw_in[[k]] <- nei #adding to list\n      k <- k + 1 #incrementing counter\n   }\n\nLine one creates an (empty) list object in R. The beauty of a list object is that it is an object that can hold other objects (vectors, matrices, igraph graph objects, etc.) as members (it can also have other lists as members, with lists all the way down). For a primer on how to work with R lists see here\nThe idea is to populate this initially empty list with the vectors of the in-neighbor ids of each node listed in the vector yw. Lines 2-8 do that using a simple for loop starring the igraph command neighbors, a function which takes two inputs: an igraph graph object, and a node id. The argument “mode” (“in” or “out” for directed graphs), tells it which kid of neighbors you want (not necessary for undirected graphs). Here we want the in-neighbors, so mode = “in”.\nNow we have a list object in R of length equal to the number of younger women (12 in this case) with each entry equal to the ids of those women’s in-neighbors.\n\n   length(yw_in)\n\n[1] 12\n\n   head(yw_in)\n\n[[1]]\n [1]  4 10 12 15 16 17 19 26 27 28 34 36 41 42 45 48 70\n\n[[2]]\n [1]  7 10 12 13 14 15 16 17 19 22 26 27 28 29 33 36 42 44 45 46 48 56 60 61 64\n\n[[3]]\n [1] 13 17 30 40 41 42 48 51 52 55 56 57 65 66 67 69 71\n\n[[4]]\n[1] 16 17 39 42\n\n[[5]]\n[1] 58 59\n\n[[6]]\n [1] 27 39 41 51 55 56 62 65 66 67 71\n\n\nNow we need to create a vector of the unique ids of these nodes. To do this, we just to “unlist” all of the node ids to create a simple vector from the list object.\nThe unlist native R function does that for us, taking a list as input and returning all of the elements inside each of the separate objects stored in the list as output. Here we wrap that call in the unique native R function to eliminate repeats (common in-neighbors across women):\n\n   yw_in <- unique(unlist(yw_in))\n   yw_in\n\n [1]  4 10 12 15 16 17 19 26 27 28 34 36 41 42 45 48 70  7 13 14 22 29 33 44 46\n[26] 56 60 61 64 30 40 51 52 55 57 65 66 67 69 71 39 58 59 62 35\n\n\nOf course, because the younger women are their own in-neighbors, they are included in this vector, so we need to get rid of them:\n\n   yw_in <- setdiff(yw_in, yw)\n   yw_in\n\n [1]  4 10 12 15 16 17 19 26 27 28 36 41 42 45 70  7 13 14 22 33 44 46 56 64 30\n[26] 40 52 55 65 66 58 62 35\n\n\nWe use the native command setdiff to find the elements in vector yw_in that are not contained in the vector of young women ids yw or the difference between the set of nodes ids stored in yw_in and the set of node ids stored in yw.\nNow that we have the vector of ids of the focal nodes and the vector of ids of their in-neighbors, we are ready to create our subgraph! All we need to do is specify we want both the younger law firm women and their in-neighbors in our node-induced subgraph:\n\n   g <- subgraph(law_advice, c(yw, yw_in))\n\nWe can even specify a new vertex attribute, differentiating the focal network from the in-neighbor network.\n\n   V(g)$net_status <- c(rep(1, length(yw)), rep(2, length(c(yw, yw_in)) - length(yw)))\n   vertex_attr(g)\n\n$status\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[39] 2 2 2 2 2 2 2\n\n$gender\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 1 1 2 1 1 2 1 1 1 2 1 2 2 2 1 1 1 2 1 2 2 2 1\n[39] 2 1 1 2 2 1 2\n\n$office\n [1] 1 2 1 1 1 2 3 1 1 1 1 1 1 2 1 2 2 1 2 1 1 1 1 1 3 1 2 1 2 1 1 1 1 2 2 1 1 1\n[39] 1 1 1 1 1 1 1\n\n$seniority\n [1] 31 29 25 24 22  1 21 20 23 19  9 15 13 11 10  7  8  8  8  5  6  6  5  4  5\n[26]  3  3  1  4  4  3  3  3  3  2  2  2  2  2  2  1  1  1  1  1\n\n$age\n [1] 59 63 53 52 57 56 48 46 50 46 49 41 47 38 38 39 37 36 33 43 37 34 31 31 53\n[26] 38 42 35 29 29 34 38 33 33 30 31 34 32 45 28 43 35 38 31 26\n\n$practice\n [1] 2 2 2 2 1 2 2 2 2 2 1 1 1 2 2 1 1 2 2 1 1 1 1 2 2 2 2 2 1 1 1 1 1 1 1 2 2 2\n[39] 2 1 1 1 1 2 1\n\n$law_school\n [1] 3 3 3 2 2 1 3 1 1 1 3 3 1 2 3 3 3 2 3 3 1 1 2 2 1 3 2 3 3 3 3 2 2 3 2 2 3 2\n[39] 2 3 3 2 3 2 2\n\n$net_status\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[39] 2 2 2 2 2 2 2\n\n\nFinally, we create an edge deleted subgraph including only the incoming advice edges from nodes who are not younger women in the firm to younger women and deleting everything other link:\n\n   e.rem <- E(g)[V(g)[net_status==1] %->% V(g)[net_status==1]] \n   #selecting edges from younger women to younger women\n   g.r <- delete_edges(g, e.rem) #removing edges\n   e.rem <- E(g.r)[V(g.r)[net_status==1] %->% V(g.r)[net_status==2]] \n   #selecting edges from younger women to non-younger women\n   g.r <- delete_edges(g.r, e.rem) #removing edges\n   e.rem <- E(g.r)[V(g.r)[net_status==2] %->% V(g.r)[net_status==2]] \n   #selecting edges from non-younger women to non-younger women\n   g.r <- delete_edges(g.r, e.rem) #removing edges\n   Iso = which(degree(g.r)==0) #selecting isolates\n   g.r <- delete_vertices(g.r, Iso) #removing isolates\n\nHere we can see both the delete_edges and delete_vertices functions from igraph in action. Both take some graph object as input followed by either an edge sequence (in this case produced by E(g))or a vector of node ids respectively. In both cases those particular edges or nodes are removed from the graph.\nThe other neat functionality we see on display here is the igraph %->% operator for directed graph edges (the equivalent for undirected graphs is the double dash %–%). This allows us to select a set of edges according to a vertex condition (e.g., homophilous (same group) edges or edges that link a member from group a to a member from group b).\nSo the code chunk:\n\n   E(g)[V(g)[net_status==1] %->% V(g)[net_status==1]] \n\n+ 69/436 edges from ce22436:\n [1]  1-> 4  1-> 5  1-> 6  1-> 9  1->10  1->11  1->12  2-> 3  3-> 8  3-> 9\n[11]  3->12  4-> 1  4-> 3  4-> 5  4-> 6  4-> 7  4-> 8  4-> 9  4->10  4->12\n[21]  5-> 4  5-> 7  5-> 8  5-> 9  5->12  6-> 1  6-> 4  6-> 7  6-> 8  6-> 9\n[31]  7-> 5  7-> 6  7-> 8  7->11  7->12  8-> 1  8-> 2  8-> 3  8-> 4  8-> 6\n[41]  8-> 7  8-> 9  8->10  8->12  9-> 1  9-> 4  9-> 6  9-> 8  9->11  9->12\n[51] 10-> 1 10-> 4 10-> 5 10-> 7 10-> 9 10->11 10->12 11-> 1 11-> 7 11-> 8\n[61] 11-> 9 11->10 12-> 1 12-> 4 12-> 5 12-> 7 12-> 8 12-> 9 12->11\n\n\nTakes the edge set of the graph g (E(g)) and gives us the subset of edges that go from a vertex with net_status equal to one to another vertex that also has net_status equal to one (in this case edges directed from one of our focal nodes to another one of our focal nodes). This, of course, happens to be all the directed edges linking nodes one through twelve in the network. The same go for the other calls to the same function using different logical combinations of values of net_status between nodes.\nFINALLY, can now plot the incoming advice network to younger women (in red):\n\n\n\n\n\nWomen lawyers advice network showing incoming links from outside the group (blue nodes) to younger women (red nodes)"
  },
  {
    "objectID": "swap.html",
    "href": "swap.html",
    "title": "Graph Ensembles in Networks",
    "section": "",
    "text": "So far we have treated the observed ties in a given network as given, along with any metrics computed from those ties (e.g., attribute correlations, centralities, etc.). The idea of uncertainty around an estimate, foundational to traditional statistics, has so far not been applicable.\nAnother approach, and one that brings in concerns about inference, statistical significance, and so forth, is to think of the observed network data as a realization of some underlying random or probabilistic process."
  },
  {
    "objectID": "swap.html#random-networks",
    "href": "swap.html#random-networks",
    "title": "Graph Ensembles in Networks",
    "section": "Random Networks",
    "text": "Random Networks\nThere are many ways to go about this, but in the following handouts we will cover two important ones. First we can treat the whole network as a realization of some random process. That leads to models of random networks and graph ensembles (Orsini et al. 2015). This approach is useful for testing single hypotheses against a plausible null (e.g., are the levels of homophily that I am observing in this network more or less than we would expect by chance?). A variation of this strategy uses permutations to do a version of multiple regression in network data (Krackhardt 1988).\nSecond, there are models that treat each link in the network as a realization of some random process (Pattison and Robins 2002). Because a network is just a set of links, these models are also treating the network as random, but allow us to test more fine-grained multivariate hypotheses by conditioning on multiple characteristics of the ties and the nodes that are involved in each link at the same time. A popular, and extremely flexible approach to this type of multivariate statistical analysis of networks is the family of exponential random graph models usually abbreviated as ERGMs and pronounced “ergums.”\nIn this handout we will discuss the group of strategies focused on randomizing networks via edge swapping. This other handout covers network regression methods via permutation."
  },
  {
    "objectID": "swap.html#graph-ensembles",
    "href": "swap.html#graph-ensembles",
    "title": "Graph Ensembles in Networks",
    "section": "Graph Ensembles",
    "text": "Graph Ensembles\nA graph ensemble is just a set of graphs that share some graph level property, like the ones we studied in the very first handout. For instance, they might have the same number of nodes, or the same number of edges, or both, but are different in terms of which particular nodes connect to which.\nNote that if an ensemble of graphs has the same number of nodes and edges, they will also be identical with regard to any property that is a function of these two things, like the density.\nUsually, what we would want is a graph ensemble that matches the properties that we observe in a graph that corresponds to the network of interest. The idea is to create an ensemble of graphs that are similar to our original data. We can then use the ensemble to test whether some quantity we observe in our data is larger or smaller than we would expect by chance. Here “chance” is simply the probability of observing that value in the ensemble of graphs we created, “net” of the property we are holding constant in the ensemble.\nAs Orsini et al. (2015) note, a simple way to create an ensemble is just by using an edge swapping algorithm on the original graph. Each edge swapping algorithm works by preserving some property of the original graph across each realization, so that all the graphs in the ensemble share that property, while everything else is scrambled."
  },
  {
    "objectID": "swap.html#the-erdos-renyi-model",
    "href": "swap.html#the-erdos-renyi-model",
    "title": "Graph Ensembles in Networks",
    "section": "The Erdos-Renyi Model",
    "text": "The Erdos-Renyi Model\nIn the simplest case, we create an ensemble that preserves the number of nodes and edges of the original graph (and everything that is a function of these, like density and average degree) and randomizes everything else in the ensemble relative to the observed graph.\nLet’s see how this would work. First we load up a network, in this case Krackardt’s high tech managers data:\n\n   library(networkdata)\n   library(igraph)\n   g <- as_undirected(ht_friends, mode = \"collapse\")\n\nThis is the undirected version of the friendship nominations network. Let’s review some graph properties:\n\n   vcount(g)\n\n[1] 21\n\n   ecount(g)\n\n[1] 79\n\n   edge_density(g)\n\n[1] 0.3761905\n\n   mean(degree(g))\n\n[1] 7.52381\n\n\nWe can see that we have \\(|V| = 21\\), \\(|E|=79\\), \\(d=0.38\\), and \\(\\bar{k}=7.5\\).\nA rewiring of this graph, which preserves these statistics, goes like this: Pick a random edge, detach it from the vertices that are incident to it, and attach it to a new pair of vertices that are currently disconnected (to avoid multiples), making sure they are distinct vertices (to avoid loops). Do that for some proportion of the edges in the graph.\nThe result is a new “rewired” graph, that still has the same number of nodes and edges as the original (hence same density and average degree), but with whatever original tie-formation tendencies in the original graph scrambled up.\nIn igraph we can use the rewire function, along with the each_edge rewiring algorithm to accomplish this.\nLet’s say we wanted to rewire up to 50% of the edges of the original network. We would then type:\n\n   set.seed (123)\n   g.swap1 <- rewire(g, each_edge(prob = .5, loops = FALSE, multiple = FALSE))\n\nWhich creates a new graph called g.swap1 with 50% of the edges scrambled (note that we set the seed if we want to get the same graph every time we run it).\nAs we can see, the new graph preserves the density and average degree of the original:\n\n   vcount(g.swap1)\n\n[1] 21\n\n   ecount(g.swap1)\n\n[1] 79\n\n   edge_density(g.swap1)\n\n[1] 0.3761905\n\n   mean(degree(g.swap1))\n\n[1] 7.52381\n\n\nAnd here’s a side-by-side comparison:\n\n\n\n\n\n\nOriginal Friendship Network.\n\n\n\n\n\n\n\nOne Density Preserving Swap.\n\n\n\n\n\nNote that while we preferred some structural features of the original network, the swapped graph has a very different structure!\nHere are two other graphs that also have the same average degree and density as the original but with 100% of the edges rewired:\n\n   set.seed (456)\n   g.swap2 <- rewire(g, each_edge(prob = 1, loops = FALSE, multiple = FALSE))\n   set.seed (789)\n   g.swap3 <- rewire(g, each_edge(prob = 1, loops = FALSE, multiple = FALSE))\n\n\n\n\n\n\n\nAnother Density Preserving Swap.\n\n\n\n\n\n\n\nYet Another Density Preserving Swap.\n\n\n\n\n\nOne of the reasons why these graphs looks so different from the original is that while these graphs all have the same average degree, the specific degrees of each node are not preserved. This is clear if you compare node 13 in the original graph (which is poorly-connected, with only two friends) to node 13 in the graph to the right of the original, where they now have many (random) friends.\nNote that other non-local graph properties (which don’t depend directly on the number of nodes and edges) are also not preserved. For instance, the graph diameter is different across swaps:\n\n   mean(distances(g))\n\n[1] 1.569161\n\n   mean(distances(g.swap1))\n\n[1] 1.560091\n\n   mean(distances(g.swap2))\n\n[1] 1.609977\n\n   mean(distances(g.swap3))\n\n[1] 1.573696\n\n\nWe will learn how to create graph ensembles that preserve specific node degrees in a bit. But first, let’s see what we can do with graph ensembles."
  },
  {
    "objectID": "swap.html#null-hypothesis-testing-in-networks",
    "href": "swap.html#null-hypothesis-testing-in-networks",
    "title": "Graph Ensembles in Networks",
    "section": "Null Hypothesis Testing in Networks",
    "text": "Null Hypothesis Testing in Networks\nRemember that one thing we wanted to do is null hypothesis testing; that is, we want to see if something computed in the original graph is more or less than we would expect by chance, given a suitable “null model”, which in our case is a network where the density—and thus expected degree—is preserved but people connect at random (sometimes called an Erdos-Renyi model).\nSo let’s compute something:\n\n   assortativity(g, V(g)$level)\n\n[1] 0.1116466\n\n\nThis is Newman’s assortativity coefficient (a.k.a., the modularity), which we covered here, for the nominal category of “Level” telling us that ties are more likely to form between managers of the same level in the company. Is this a “statistically significant” level of assortativity?\nWell compared to what? Compared to what we observe in an ensemble of graphs with the same number of nodes, edges, and density!\nHow do we do that comparison?\nFirst, let’s create an ensemble of 1000 edge-swapped graphs, putting them all into a list object called G. To do that, we use the R function replicate which simulates a loop applying a function a specified number of times, in this case it will be our trusty rewire function with the graph g as input.\n\n   set.seed(12345)\n   G <- replicate(500, \n                  rewire(g, each_edge(prob = 1, loops = FALSE, multiple = FALSE)),\n                  simplify = FALSE\n                  )\n\nWe can now we compute our assortativity coefficient in each one of them using sapply:\n\n   assort <- sapply(G, assortativity, values = V(g)$level)\n   round(assort, 2)[1:100] #first 100 values\n\n  [1] -0.08 -0.02 -0.07 -0.08 -0.14 -0.11  0.01  0.04 -0.05  0.33 -0.09  0.00\n [13] -0.07  0.11 -0.04  0.16 -0.14  0.04 -0.10 -0.04  0.03 -0.13 -0.01 -0.04\n [25] -0.20  0.00 -0.10  0.00  0.03 -0.03 -0.05 -0.01 -0.11 -0.02  0.02 -0.06\n [37] -0.15 -0.07 -0.21 -0.15 -0.08 -0.21 -0.18 -0.04 -0.02 -0.17 -0.19 -0.06\n [49]  0.04 -0.12 -0.11 -0.02 -0.05 -0.15 -0.01 -0.04 -0.11  0.03  0.04  0.09\n [61] -0.10 -0.10 -0.10  0.09 -0.09  0.01 -0.12  0.04 -0.13 -0.05 -0.04 -0.08\n [73] -0.07 -0.21 -0.09 -0.13  0.08 -0.10 -0.07 -0.02 -0.22 -0.08 -0.06 -0.13\n [85] -0.12 -0.10 -0.12  0.03 -0.12 -0.13 -0.10 -0.11 -0.05 -0.07 -0.04 -0.16\n [97] -0.07 -0.04  0.04 -0.04\n\n\nNote that the modularity is actually negative in most of these graphs, suggesting that when people form ties at random they are unlikely to magically end up being assortative by level.\nSo let’s see how our observed value stacks up in the grand scheme:\n\n   library(ggplot2)\n   p <- ggplot(data = data.frame(round(assort, 2)), aes(x = assort))\n   p <- p + geom_histogram(binwidth = 0.015, stat = \"bin\", fill = \"darkblue\")\n   p <- p + geom_vline(xintercept = assortativity(g, V(g)$level), \n                       color = \"red\", linetype = 1, linewidth = 1.5)\n   p <- p + geom_vline(xintercept = 0, linetype = 1, \n                       color = \"purple\", linewidth = 1.5)\n   p <- p + theme_minimal() + labs(x = \"Q by Level\", y = \"Freq.\")\n   p <- p + theme(axis.text = element_text(size = 12))\n   p <- p + annotate(\"text\", x=-0.05, y=47, label= \"Zero Point\", color = \"purple\")\n   p <- p + annotate(\"text\", x=0.06, y=47, label= \"Obs. Value\", color = \"red\")\n   p\n\n\n\n\nSo we can see that only a few of the networks in the ensemble have \\(Q\\) values higher than the observed one, indicating that what we observed is unlikely to have occurred by chance.\nHow unlikely? We can just compute the value that corresponds to the 99th percentile of the assortativity distribution from the ensemble and then see if what observe is above that value (corresponding to \\(p < 0.01\\)).\n\n   quantile(assort, probs = 0.99)\n\n      99% \n0.1533994 \n\n   assortativity(g, V(g)$level) > quantile(assort, probs = 0.99)\n\n  99% \nFALSE \n\n\nWe find that our observed value of assortativity by dept is not significant at this level, as the corresponding value in the ensemble distribution is higher than what we observe.\nOf course we can also try a less stringent standard for statistical significance, like \\(p < 0.05\\):\n\n   quantile(assort, probs = 0.95)\n\n      95% \n0.0665133 \n\n   assortativity(g, V(g)$level) > quantile(assort, probs = 0.95)\n\n 95% \nTRUE \n\n\nWhich, in this case, is “statistically significant”! This is because the observed value is larger than the corresponding value at the 95th percentile slot of the graph ensemble distribution.\nIf we wanted to find the actual “p-value” corresponding to our observed value, we would just type the following, which uses the native R function ecdf:\n\n   1 - ecdf(assort)(assortativity(g, V(g)$level))\n\n[1] 0.014\n\n\nWhich yields \\(p = 0.014\\) for our observed value of assortativity by department, which is good enough to get published.\nNote, however, that this is a one-tailed test of significance (Borgatti et al. 2024, 285), this is fine since we usually expect the modularity to be positive which implies a directional hypothesis.\nIf we wanted a more stringent two-tailed we would need to create a vector with the absolute value of the modularity and use that to construct our test:\n\n   1 - ecdf(abs(assort))(assortativity(g, V(g)$level))\n\n[1] 0.27\n\n\nWhich would fail the statistical significance test by the usual standards (\\(p = 0.27\\))."
  },
  {
    "objectID": "swap.html#preserving-degrees",
    "href": "swap.html#preserving-degrees",
    "title": "Graph Ensembles in Networks",
    "section": "Preserving Degrees",
    "text": "Preserving Degrees\nRecall that our swapping function above preserves the expected (average) degree but not the degree of any particular node. This means that the neither the degree sequence nor the degree distribution of the original network is preserved.\nThis is clear in the following plots:\n\n   library(dplyr)\n   library(tidyr)\n   deg.dat <- data.frame(Original = degree(g), Swap1 = degree(g.swap1), Swap2 = degree(g.swap2), Swap3 = degree(g.swap3)) %>% \n   pivot_longer(1:4, names_to = \"graph\") %>% \n      mutate(v = sort(rep(1:vcount(g), 4)))\n   p <- ggplot(data = deg.dat, aes(x = value, group = graph, fill = graph))\n   p <- p + geom_histogram(binwidth = 0.35)\n   p <- p + facet_wrap(~ graph, nrow = 4)\n   p <- p + theme_minimal()\n   p <- p + theme(legend.position = \"none\",\n                  axis.text = element_text(size = 12),\n                  strip.text = element_text(size = 14),\n                  axis.title = element_text(size = 14)) \n   p <- p + scale_x_continuous(breaks = c(1:18))\n   p <- p + labs(x = \"\", y =\"\")\n   p\n\n\n\n\nThe top shows the original degree distribution, which is different from the ones in the swapped graphs. In fact, it is clear that the swapped degree distribution get rid of extreme high degree nodes and pull everyone to the middle (Erdos-Renyi graphs degrees tend toward a Poisson distribution).\nSo it could be that the plain average degree (Erdos-Renyi) preserving model is too simplistic to use as a plausible null model. A better model is one that would preserve the degrees of each node in the ensemble but make every connection otherwise random.\nIn igraph there is a trusty function called keeping_degseq—which is used alongside the more general function rewire we used earlier—that allows us to scramble the edges in a graph while keeping the degrees of each node the same as the original.\n\n\n\n\n\n\n\n(a) Before swap.\n\n\n\n\n\n\n\n(b) After swap.\n\n\n\n\nFigure 1: One degree preserving swap.\n\n\nThe basic idea is to sample a pair of edges connecting vertices \\(a\\) and \\(b\\) and \\(c\\) and \\(d\\) respectively, delete them, and create two new edges (if they don’t currently exist) between vertices \\(a\\) and \\(c\\) and between \\(b\\) and \\(d\\), repeating this process niter number of times. A toy example of the degree-preserving swap is shown in Figure 1. This approach randomizes connections but keeps the number of edges incident to each node the same (Maslov and Sneppen 2002).\nLet’s see how it works:\n\n   g.swap <- rewire(g, keeping_degseq(niter = 50))\n   degree(g)\n\n [1]  9 10  6  7 10  7  3  5  6  8 14  8  2  6  9  5 18  4 10  5  6\n\n   degree(g.swap)\n\n [1]  9 10  6  7 10  7  3  5  6  8 14  8  2  6  9  5 18  4 10  5  6\n\n\nWe can see that the edge-swapping algorithm creates a new graph with exactly the same degree sequence as the original.\nHere they are side-by-side:\n\n\n\n\n\n\nOriginal Friendship Network.\n\n\n\n\n\n\n\nOne Degree Preserving Swap.\n\n\n\n\n\nAs we can see, the network structure is definitely different between these two graphs, but each node has the same levels of connectivity in each.\nSo now we can use our more stringent null model to test our previous hypothesis: Is the observed level of assortativity by level more than we would observe in the same network where everyone keeps their degree centrality but everything is random?\nLet’s find out.\nFirst, let’s wrap the degree-preserving swap into a function:\n\n   swap2 <- function(x, e = 10^-7) {\n      m <- round((ecount(x)/2) * log(1/e)) #recommended number \n      #of swaps according to \n      #https://en.wikipedia.org/wiki/Degree-preserving_randomization\n      x <- rewire(x, keeping_degseq(niter = m))\n      return(x)\n   }\n\nThen let’s create our graph ensemble using 500 graphs:\n\n   set.seed(12345)\n   G2 <- replicate(500, swap2(g), simplify = FALSE)\n\nAnd see where our observed value falls in the distribution:\n\n   assort <- sapply(G2, assortativity, values = V(g)$level)\n   library(ggplot2)\n   p <- ggplot(data = data.frame(round(assort, 2)), aes(x = assort))\n   p <- p + geom_histogram(binwidth = 0.08, stat = \"bin\", fill = \"darkblue\")\n   p <- p + geom_vline(xintercept = assortativity(g, V(g)$level), \n                       color = \"red\", linetype = 1, linewidth = 1.5)\n   p <- p + geom_vline(xintercept = 0, linetype = 1, \n                       color = \"purple\", linewidth = 1.5)\n   p <- p + theme_minimal() + labs(x = \"Q by Level\", y = \"Freq.\")\n   p <- p + theme(axis.text = element_text(size = 12))\n   p <- p + annotate(\"text\", x=-0.04, y=170, label= \"Zero Point\", color = \"purple\")\n   p <- p + annotate(\"text\", x=0.07, y=170, label= \"Obs. Value\", color = \"red\")\n   p\n\n\n\n\nNote that the values are much more constrained this time around, falling within specific ranges. We can test our hypothesis the same way as before:\n\n   quantile(assort, probs = 0.95)\n\n       95% \n0.05136546 \n\n   assortativity(g, V(g)$level) > quantile(assort, probs = 0.95)\n\n 95% \nTRUE \n\n\nWhich tells us that after account for node degree differences, the observed value we observed is still significantly larger than we would have observed by chance using conventional cutoffs.\nIndeed, the p-value of the estimate is:\n\n   1 - ecdf(assort)(assortativity(g, V(g)$level))\n\n[1] 0.01\n\n\nWhich is pretty good. Maybe our paper will be published!\nBut oh no, some reviewer who doesn’t know what they are talking about asked for a two-tailed test:\n\n   1 - ecdf(abs(assort))(assortativity(g, V(g)$level))\n\n[1] 0.294\n\n\nIt’s a rejection after all :("
  },
  {
    "objectID": "syllabus-208A.html",
    "href": "syllabus-208A.html",
    "title": "208A Syllabus",
    "section": "",
    "text": "THIS IS A LIVE DOCUMENT CHECK REGULARLY FOR CHANGES"
  },
  {
    "objectID": "syllabus-208A.html#class-description",
    "href": "syllabus-208A.html#class-description",
    "title": "208A Syllabus",
    "section": "Class Description",
    "text": "Class Description\nThis class is an introductory graduate-level seminar focused on techniques in Social Network Analysis (SNA). The seminar covers the most common data analytic tasks that people engage in when analyzing “network data.” What is network data? What counts as network data is itself a point of contention—as we will see, for some people all data is network data—but let us say for the sake of this class that network data is data in which the unit of analysis is the relation or the interaction between at least two actors or objects, and the data come typically arranged in this “dyadic” form. At the end of the course, you will be familiar with (and will have acquired some practice) the basic techniques used to analyze social network data."
  },
  {
    "objectID": "syllabus-208A.html#course-content",
    "href": "syllabus-208A.html#course-content",
    "title": "208A Syllabus",
    "section": "Course Content",
    "text": "Course Content\n\nBasic SNA\nSo, what are the things that people usually do when they have network data? Well, they typically want to figure out basic statistics about the interaction system formed by the set of dyads in the data, where a dyad is any two pairs of actors (whether they are connected or not). This task requires computing basic network quantities like the number of nodes and the number of links between entities as well as more advanced statistics based on representing the network as a graph (like the average path length, number of components, etc., all notions we will cover in the first week of class).\n\n\nCentrality and Prestige\nThen come the various things that almost everyone is interested in computing when using network data to answer social science questions. Primarily, this includes measures and indices of a node’s position in the network (e.g., differentiating between more or less central or more or less prestigious nodes), which we will cover in weeks 2 and 3.\n\n\nClasses and Communities\nAfter taking a break in Week 4, we move to the common case of people wanting to see if the nodes in the network fall into definable clusters or classes, where the criterion for being in the same cluster is based on how they connect to other nodes. Here, we want to find clusters of nodes that are similar to one another by some graph theoretic criterion and partition the graph into clusters based on that criterion.\nWeek 6 is dedicated to the next thing we may want to do, and that is to see if we can uncover clumps of densely connected nodes in the network indicating some natural partition into subgroups or communities, defined as nodes that interact more among themselves than they do among those outside the group, leading to the myriad of group and community detection techniques designed to partition a graph into clusters based on the underlying connectivity structure.\n\n\nTwo-Mode and Ego Networks\nThe next two weeks are dedicated to the analysis of some pretty common “non-standard” types of network data (e.g., data that doesn’t use the dyadic relation between objects of the same type as the analytic unit). The first is ego networks, where we first sample a set of units (egos), and then within each ego, we sample a subset of their contacts (e.g., by asking the people who are their most important friends or figuring out the most frequent interaction partners). These data come closest to the traditional data in social science (a rectangular matrix of cases by variables), so various standard techniques—like regression—apply (with some twists).\nThe second type of non-standard network data comes in a two-mode network form, in which some sets of objects are linked to objects of a different set, but there is no data on the links between objects of the same set. Standard cases by variables data in surveys are two-mode data (people connect to variables), as is any web or archival data collecting memberships or interactions between persons and objects (like attendance at events or people buying books on Amazon). We will see that due to a neat mathematical trick, we can transform two-mode into standard dyadic network data and thus deploy the whole panoply of techniques we learned in weeks 1-6 (which means that we can do SNA on all types of data, not just network data, and therefore all data is network data).\n\n\nProbabilistic Models of Networks\nThe bulk of SNA assumes that the ties exist as recorded in the data. Recently (e.g., over the last two decades or so) developed approaches to social network analysis make the ties the dependent variable and thus see the observed network data as a realization of some stochastic process governing the probability that two objects will be linked and thus one that can be modeled statistically. We analyze the theory and methods behind this approach to thinking of network structure from the bottom up and also cover some models designed to treat networks as composed of “relational events” and thus model how events that link entities in networks evolve."
  },
  {
    "objectID": "syllabus-208A.html#requirements",
    "href": "syllabus-208A.html#requirements",
    "title": "208A Syllabus",
    "section": "Requirements",
    "text": "Requirements\nThere are three main requirements in the class. Participation (mainly attendance and contributions made during our seminar meetings), a short weekly data exercise, and a longer data analysis paper due at the end of the quarter.\n\nClass Attendance and Class Discussion (25% of grade)\nAttendance is required not optional. If you need to miss a class meeting please let me know beforehand. It is part of your professional socialization to commit to attending class meetings and to inform me when that’s not possible (if only as a point of courtesy). The informal part of participation will be gauged by your contribution to our class discussion in the form of questions, comments, suggestions, wonderings, problems.\n\n\nWeekly Data Analysis Exercises (25% of grade)\nThese will be short weekly assignments in which I will ask you to take a (small) social network data set of your choice and compute some of the basic statistics or implement some of the techniques that we covered the week before. They will be due on Sunday at the end of each week. What you submit will take the form of a file containing the code and results from your analysis (typically an R Markdown file). These will not be graded, but will just be counted as submitted or not submitted.\n\n\nFinal Data Analysis Paper (50% of grade)\nThe basic goal here is for you to end up with something that could be useful to you at the end of the day. Hopefully a basic data exercise that can be the basis of a longer substantive paper or as a standalone research note.\nThis will be a 2500 to 5000 word (single-spaced, Times New Roman Font, 12pt, 1in margins) write-up of a data analysis, including some type of network data and/or some kind of network analytic technique (to be discussed and cleared by me). The data source can be obtained from a public repository of network data, or it could be network data that you already have access to or collected yourself.\nIn the paper, you will describe the data, provide key network metrics, describe the data-analytic approach that you will use, and provide a summary of the key empirical patterns that you found, along with a brief conclusion. The paper should be written in the style of a “research note” focused on key empirical findings (not long theory windup or literature review).\nYou will submit an extended abstract of your final project, outlining your main research idea (e.g., data source and type of analysis) due on Sunday of week 6. This will be a one-page, single-spaced document with 12pt Times New Roman Font and 1in margins."
  },
  {
    "objectID": "syllabus-208B.html",
    "href": "syllabus-208B.html",
    "title": "208B Syllabus",
    "section": "",
    "text": "THIS IS A LIVE DOCUMENT CHECK REGULARLY FOR CHANGES"
  },
  {
    "objectID": "syllabus-208B.html#participation-50-of-grade",
    "href": "syllabus-208B.html#participation-50-of-grade",
    "title": "208B Syllabus",
    "section": "Participation (50% of grade)",
    "text": "Participation (50% of grade)\n\nWeekly Analytic Memo\nThe formal side of participation will come in the form of you submitting a short memo (500 to 1000 words) where you try to put two or more of the readings for that week in conversation with one another. The first memo will be due starting on week 2.\nMemo specifications:\n\nYou should pick at least one reading from the Tuesday meeting and at least one reading from the Thursday meeting as the focus of your memo.\nIn your memo you should feel free to raise questions or issues the readings brought up for you as well as any questions, problems, or weaknesses you identify in the argument or the analyses.\nYou may also feel free to connect the readings to other work you are familiar with, pointing to key points of commonality and difference. The main point of the memo is for me to see evidence of you thinking thought the material, as well as providing fodder for discussion during our meeting.\nThe memo will be due by 5p the day before our first class meeting of the week (that’s Monday) so that I have a chance to read it and comment on it. You will submit it via the assignments tool on Canvas.\n\n\n\nClass Attendance\nAttendance is required not optional. If you need to miss a class meeting please let me know before hand. It is part of your professional socialization to commit to attending class meetings and to inform me when that’s not possible (if only as a point of courtesy).\n\n\nClass Discussion\nThe informal part of participation will be gauged by your contribution to our class discussion. You can use the thoughts developed in your analytic memo as a take-off point for framing your contribution."
  },
  {
    "objectID": "syllabus-208B.html#paper-50-of-grade",
    "href": "syllabus-208B.html#paper-50-of-grade",
    "title": "208B Syllabus",
    "section": "Paper (50% of grade)",
    "text": "Paper (50% of grade)\nThe basic goal here is for you to end up with something that could be useful to you at the end of the day. As such, I’ll give you a set of options here, but if you none of these work, we can talk about something that can be customized for your needs and goals. So by the end of the course you will submit one of the following:\n\nDraft of a research paper.- This will be a 3500 to 9000 word (double-spaced, Times New Roman Font, 12pt, 1in margins) draft of a research paper. This paper will contain some kind of data analysis, involving networks broadly defined. It will include an introduction reviewing literature and setting up a research problem or question. It will then move on to a methods section describing your data and analytic approach, and will close with a discussion section summarizing key findings, outlining implications for substantive research and theory, and describing potential future work and extensions. The goal here is to come up with a draft of a paper that could one day be submitted to the various social science and sociology-oriented journal focused on substantive research, whether “generalist” (e.g., American Sociological Review) or “specialist” (e.g., Social Networks).\nDraft of a Conceptual Paper.- This will be a 5000 to 10000 word page (double-spaced, Times New Roman Font, 12pt, 1in margins) draft of a research paper. The paper will focus on a set of concepts, theoretical ideas, or overall perspectives for approaching the study of social life that are based, inspired, extend, or incorporate network ideas, network thinking, or network concepts and techniques (broadly defined). The goal here is to come up with a draft of a paper that could one day be submitted to the various social science and sociology-oriented journal focused on “theory.”\nExtended Literature Review Draft.- This will be a 10-15 page (double-spaced, Times New Roman Font, 12pt, 1in Margins) draft of a literature review of work done from a social network perspective on a topic of your interest. The paper will cover what has been done in the field so far, what the strengths and limitations of previous is, and will note gaps or opportunities for future work addressing those limitations or extending the literature to new substantive domains, perhaps linking previous work to the some of the stuff we will be reading in class.\nDraft of a Research Proposal.- This will be a 10 page (single-spaced, Times New Roman Font, 12pt, 1in margins, including references) draft of a research proposal for a project incorporating either network thinking, theories, or techniques that you plan to start in the near future. The proposal will include a background section reviewing previous work, noting their strengths and limitations, and pointing to gaps in the literature. It will also include an “approach” section describing your research project, your main research questions, the data gathering procedures you will use, and the data-analytic techniques you plan to implement once your data is collected. It will close with an implication sections describing what the contributions of your project will be and why it is relevant and important.\nData Analysis Exercise.- This will be a 2500 to 5000 word (single-spaced, Times New Roman Font, 12pt, 1in margins) write-up of a data analysis, including some type of network data and/or some kind of network analytic technique (to be discussed and cleared by me). The data source can be obtained from a public repository of network data, or it could be network data that you already have access to or collected yourself. In the paper, you will describe the data, provide key network metrics, describe the data-analytic approach that you will use, and provide a summary of the key empirical patterns that you found, along with a brief conclusion. The paper should be written in the style of a “research note” focused on key empirical findings (not long theory windup or literature review).\n\nWhatever you decide, you will submit an extended abstract of your final project, due on Friday of week 6. This will be a one-page, single-spaced document with 12pt Times New Roman Font and 1in margins."
  },
  {
    "objectID": "tm-basic.html",
    "href": "tm-basic.html",
    "title": "Analyzing Two-Mode Networks",
    "section": "",
    "text": "This lecture deals with the network analysis of two-mode networks. Note that in the literature there is some terminological slippage. Two-mode networks are a type of social network. By definition two-mode networks can be represented using rectangular adjacency matrices (sometimes called affiliation matrices in sociology).\nIn this case, two-mode networks fall under the general category of “two-mode data.” Any data set that has information on two types of objects (e.g., people and variables) is two-mode data so two-mode networks are just a special case of two-mode data.\nIn this sense, there is a useful a distinction, due to Borgatti and Everett (1997). This is that between the “modes” and the “ways” of a data matrix. So most data matrices are two-ways, in that they have at least two dimensions (e.g., the row and column dimensions).\nBut some data matrices (like the usual adjacency matrix in regular network data) only collect information on a single type of entity, so they are “one mode, two ways.” But sometimes we have network data on two sets of objects, in which case, we use a data matrix that has “two-modes” (sets of nodes) and “two ways” (rows and columns).\nSo what makes a network a “two-mode network”? Well, a two-mode network is different from a regular network, because it has two sets of nodes not just one. So instead of \\(V\\) now we have \\(V_1\\) and \\(V_2\\). Moreover, the edges in a two-mode network only go from nodes in one set to nodes in the other set; there are no within-node-set edges."
  },
  {
    "objectID": "tm-basic.html#bipartite-graphs",
    "href": "tm-basic.html#bipartite-graphs",
    "title": "Analyzing Two-Mode Networks",
    "section": "Bipartite Graphs",
    "text": "Bipartite Graphs\nThis restriction makes the graph that represents a two-mode network a special kind of graph called a bipartite graph. A graph is bipartite if the set of nodes in the graph can be divided into two groups, such that relations go from nodes in one set to nodes in the other set.\nNote that bipartite graphs can be be used to represent both two-mode and regular one mode networks, as long as the above condition holds. For instance, a dating network with 100% heterosexual people will yield a bipartite graph based on the dating relation, with men in one set and women on the other node set, even though it’s a one-mode network.\nSo whether or not a graph is bipartite is something you can check for.\nLet’s see how that works. Let us load the most famous two-mode network data set (kind of the Drosophila of two-mode network analysis; one of the most repeatedly analyzed social structures in history: For a classic sampling of such analyses see here) a network composed of eighteen women from the social elite of a tiny town in the south in the 1930s who attended fourteen social events (Breiger 1974), otherwise know as the Southern Women (SW) data:\n\n   library(igraph)\n   library(networkdata)\n   g <- southern_women\n\nNow we already know this is a bipartite graph. However, let’s say you are new and you’ve never heard of these data. You can check whether the graph you loaded up is bipartite or not by using the igraph function is_bipartite:\n\n   is_bipartite(g)\n\n[1] TRUE\n\n\nWhich returns TRUE as an answer. Had we loaded up any old non-bipartite graph, the answer would have been:\n\n   g.whatever <- movie_45\n   is_bipartite(g.whatever)\n\n[1] FALSE\n\n\nWhich makes sense because that’s just a regular old graph.\nNote that if we check the bipartite graph object, it looks like any other igraph object:\n\n   g\n\nIGRAPH 1074643 UN-B 32 89 -- \n+ attr: type (v/l), name (v/c)\n+ edges from 1074643 (vertex names):\n [1] EVELYN   --6/27 EVELYN   --3/2  EVELYN   --4/12 EVELYN   --9/26\n [5] EVELYN   --2/25 EVELYN   --5/19 EVELYN   --9/16 EVELYN   --4/8 \n [9] LAURA    --6/27 LAURA    --3/2  LAURA    --4/12 LAURA    --2/25\n[13] LAURA    --5/19 LAURA    --3/15 LAURA    --9/16 THERESA  --3/2 \n[17] THERESA  --4/12 THERESA  --9/26 THERESA  --2/25 THERESA  --5/19\n[21] THERESA  --3/15 THERESA  --9/16 THERESA  --4/8  BRENDA   --6/27\n[25] BRENDA   --4/12 BRENDA   --9/26 BRENDA   --2/25 BRENDA   --5/19\n[29] BRENDA   --3/15 BRENDA   --9/16 CHARLOTTE--4/12 CHARLOTTE--9/26\n+ ... omitted several edges\n\n\nBut we can tell that the graph is a two-mode network because we have links starting with people with old southern lady names from the 1930s (which are also the names of a bunch of young girls in middle school today) and ending with events that have dates in them. So the (undirected) edge is \\(person-event\\).\nThe graph is undirected because the “membership” or “attendance” relation between a person and an organization/event doesn’t have a natural directionality:\n\n   is_directed(g)\n\n[1] FALSE\n\n\nAnother way of checking the “bipartiteness” of a graph in igraph is by using the bipartite_mapping function.\nLet’s see what it does:\n\n   bipartite_mapping(g)\n\n$res\n[1] TRUE\n\n$type\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n    FALSE     FALSE     FALSE     FALSE     FALSE     FALSE     FALSE     FALSE \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n    FALSE     FALSE     FALSE     FALSE     FALSE     FALSE     FALSE     FALSE \n   OLIVIA     FLORA      6/27       3/2      4/12      9/26      2/25      5/19 \n    FALSE     FALSE      TRUE      TRUE      TRUE      TRUE      TRUE      TRUE \n     3/15      9/16       4/8      6/10      2/23       4/7     11/21       8/3 \n     TRUE      TRUE      TRUE      TRUE      TRUE      TRUE      TRUE      TRUE \n\n\nThis function takes the candidate bipartite graph as input and returns two objects: res is just a check to see if the graph is actually bipartite (TRUE in this case), type is a logical vector of dimensions \\(M + N\\) (where \\(M\\) is the number of nodes in the person set and \\(N\\) is the number of nodes in the event set) dividing the nodes into two groups. Here people get FALSE and events get TRUE, but this designations are arbitrary (a kind of dummy coding; FALSE = 0 and TRUE = 1).\nWe can add this as a node attribute to our graph so that way we know which node is in which set:\n\n   V(g)$type <- bipartite_mapping(g)$type"
  },
  {
    "objectID": "tm-basic.html#the-biadjacency-affiliation-matrix",
    "href": "tm-basic.html#the-biadjacency-affiliation-matrix",
    "title": "Analyzing Two-Mode Networks",
    "section": "The Biadjacency (Affiliation) Matrix",
    "text": "The Biadjacency (Affiliation) Matrix\nOnce you have your bipartite graph loaded up, you may want (if the graph is small enough) to check out the graph’s affiliation matrix \\(A\\).\nThis works just like before, except that now we use the as_biadjacency_matrix function:\n\n   A <- as.matrix(as_biadjacency_matrix(g))\n   A\n\n          6/27 3/2 4/12 9/26 2/25 5/19 3/15 9/16 4/8 6/10 2/23 4/7 11/21 8/3\nEVELYN       1   1    1    1    1    1    0    1   1    0    0   0     0   0\nLAURA        1   1    1    0    1    1    1    1   0    0    0   0     0   0\nTHERESA      0   1    1    1    1    1    1    1   1    0    0   0     0   0\nBRENDA       1   0    1    1    1    1    1    1   0    0    0   0     0   0\nCHARLOTTE    0   0    1    1    1    0    1    0   0    0    0   0     0   0\nFRANCES      0   0    1    0    1    1    0    1   0    0    0   0     0   0\nELEANOR      0   0    0    0    1    1    1    1   0    0    0   0     0   0\nPEARL        0   0    0    0    0    1    0    1   1    0    0   0     0   0\nRUTH         0   0    0    0    1    0    1    1   1    0    0   0     0   0\nVERNE        0   0    0    0    0    0    1    1   1    0    0   1     0   0\nMYRNA        0   0    0    0    0    0    0    1   1    1    0   1     0   0\nKATHERINE    0   0    0    0    0    0    0    1   1    1    0   1     1   1\nSYLVIA       0   0    0    0    0    0    1    1   1    1    0   1     1   1\nNORA         0   0    0    0    0    1    1    0   1    1    1   1     1   1\nHELEN        0   0    0    0    0    0    1    1   0    1    1   1     0   0\nDOROTHY      0   0    0    0    0    0    0    1   1    0    0   0     0   0\nOLIVIA       0   0    0    0    0    0    0    0   1    0    1   0     0   0\nFLORA        0   0    0    0    0    0    0    0   1    0    1   0     0   0\n\n\nIn this matrix we list one set of nodes in the rows and the other set is in the columns. Each cell \\(a_{ij} = 1\\) if row node \\(i\\) is affiliated with column node \\(j\\), otherwise \\(a_{ij} = 0\\)."
  },
  {
    "objectID": "tm-basic.html#the-bipartite-adjacency-matrix",
    "href": "tm-basic.html#the-bipartite-adjacency-matrix",
    "title": "Analyzing Two-Mode Networks",
    "section": "The Bipartite Adjacency Matrix",
    "text": "The Bipartite Adjacency Matrix\nNote that if we were to use the regular as_adjacency_matrix function on a bipartite graph, we get a curious version of the adjacency matrix:\n\n   B <- as.matrix(as_adjacency_matrix(g))\n   B\n\n          EVELYN LAURA THERESA BRENDA CHARLOTTE FRANCES ELEANOR PEARL RUTH\nEVELYN         0     0       0      0         0       0       0     0    0\nLAURA          0     0       0      0         0       0       0     0    0\nTHERESA        0     0       0      0         0       0       0     0    0\nBRENDA         0     0       0      0         0       0       0     0    0\nCHARLOTTE      0     0       0      0         0       0       0     0    0\nFRANCES        0     0       0      0         0       0       0     0    0\nELEANOR        0     0       0      0         0       0       0     0    0\nPEARL          0     0       0      0         0       0       0     0    0\nRUTH           0     0       0      0         0       0       0     0    0\nVERNE          0     0       0      0         0       0       0     0    0\nMYRNA          0     0       0      0         0       0       0     0    0\nKATHERINE      0     0       0      0         0       0       0     0    0\nSYLVIA         0     0       0      0         0       0       0     0    0\nNORA           0     0       0      0         0       0       0     0    0\nHELEN          0     0       0      0         0       0       0     0    0\nDOROTHY        0     0       0      0         0       0       0     0    0\nOLIVIA         0     0       0      0         0       0       0     0    0\nFLORA          0     0       0      0         0       0       0     0    0\n6/27           1     1       0      1         0       0       0     0    0\n3/2            1     1       1      0         0       0       0     0    0\n4/12           1     1       1      1         1       1       0     0    0\n9/26           1     0       1      1         1       0       0     0    0\n2/25           1     1       1      1         1       1       1     0    1\n5/19           1     1       1      1         0       1       1     1    0\n3/15           0     1       1      1         1       0       1     0    1\n9/16           1     1       1      1         0       1       1     1    1\n4/8            1     0       1      0         0       0       0     1    1\n6/10           0     0       0      0         0       0       0     0    0\n2/23           0     0       0      0         0       0       0     0    0\n4/7            0     0       0      0         0       0       0     0    0\n11/21          0     0       0      0         0       0       0     0    0\n8/3            0     0       0      0         0       0       0     0    0\n          VERNE MYRNA KATHERINE SYLVIA NORA HELEN DOROTHY OLIVIA FLORA 6/27 3/2\nEVELYN        0     0         0      0    0     0       0      0     0    1   1\nLAURA         0     0         0      0    0     0       0      0     0    1   1\nTHERESA       0     0         0      0    0     0       0      0     0    0   1\nBRENDA        0     0         0      0    0     0       0      0     0    1   0\nCHARLOTTE     0     0         0      0    0     0       0      0     0    0   0\nFRANCES       0     0         0      0    0     0       0      0     0    0   0\nELEANOR       0     0         0      0    0     0       0      0     0    0   0\nPEARL         0     0         0      0    0     0       0      0     0    0   0\nRUTH          0     0         0      0    0     0       0      0     0    0   0\nVERNE         0     0         0      0    0     0       0      0     0    0   0\nMYRNA         0     0         0      0    0     0       0      0     0    0   0\nKATHERINE     0     0         0      0    0     0       0      0     0    0   0\nSYLVIA        0     0         0      0    0     0       0      0     0    0   0\nNORA          0     0         0      0    0     0       0      0     0    0   0\nHELEN         0     0         0      0    0     0       0      0     0    0   0\nDOROTHY       0     0         0      0    0     0       0      0     0    0   0\nOLIVIA        0     0         0      0    0     0       0      0     0    0   0\nFLORA         0     0         0      0    0     0       0      0     0    0   0\n6/27          0     0         0      0    0     0       0      0     0    0   0\n3/2           0     0         0      0    0     0       0      0     0    0   0\n4/12          0     0         0      0    0     0       0      0     0    0   0\n9/26          0     0         0      0    0     0       0      0     0    0   0\n2/25          0     0         0      0    0     0       0      0     0    0   0\n5/19          0     0         0      0    1     0       0      0     0    0   0\n3/15          1     0         0      1    1     1       0      0     0    0   0\n9/16          1     1         1      1    0     1       1      0     0    0   0\n4/8           1     1         1      1    1     0       1      1     1    0   0\n6/10          0     1         1      1    1     1       0      0     0    0   0\n2/23          0     0         0      0    1     1       0      1     1    0   0\n4/7           1     1         1      1    1     1       0      0     0    0   0\n11/21         0     0         1      1    1     0       0      0     0    0   0\n8/3           0     0         1      1    1     0       0      0     0    0   0\n          4/12 9/26 2/25 5/19 3/15 9/16 4/8 6/10 2/23 4/7 11/21 8/3\nEVELYN       1    1    1    1    0    1   1    0    0   0     0   0\nLAURA        1    0    1    1    1    1   0    0    0   0     0   0\nTHERESA      1    1    1    1    1    1   1    0    0   0     0   0\nBRENDA       1    1    1    1    1    1   0    0    0   0     0   0\nCHARLOTTE    1    1    1    0    1    0   0    0    0   0     0   0\nFRANCES      1    0    1    1    0    1   0    0    0   0     0   0\nELEANOR      0    0    1    1    1    1   0    0    0   0     0   0\nPEARL        0    0    0    1    0    1   1    0    0   0     0   0\nRUTH         0    0    1    0    1    1   1    0    0   0     0   0\nVERNE        0    0    0    0    1    1   1    0    0   1     0   0\nMYRNA        0    0    0    0    0    1   1    1    0   1     0   0\nKATHERINE    0    0    0    0    0    1   1    1    0   1     1   1\nSYLVIA       0    0    0    0    1    1   1    1    0   1     1   1\nNORA         0    0    0    1    1    0   1    1    1   1     1   1\nHELEN        0    0    0    0    1    1   0    1    1   1     0   0\nDOROTHY      0    0    0    0    0    1   1    0    0   0     0   0\nOLIVIA       0    0    0    0    0    0   1    0    1   0     0   0\nFLORA        0    0    0    0    0    0   1    0    1   0     0   0\n6/27         0    0    0    0    0    0   0    0    0   0     0   0\n3/2          0    0    0    0    0    0   0    0    0   0     0   0\n4/12         0    0    0    0    0    0   0    0    0   0     0   0\n9/26         0    0    0    0    0    0   0    0    0   0     0   0\n2/25         0    0    0    0    0    0   0    0    0   0     0   0\n5/19         0    0    0    0    0    0   0    0    0   0     0   0\n3/15         0    0    0    0    0    0   0    0    0   0     0   0\n9/16         0    0    0    0    0    0   0    0    0   0     0   0\n4/8          0    0    0    0    0    0   0    0    0   0     0   0\n6/10         0    0    0    0    0    0   0    0    0   0     0   0\n2/23         0    0    0    0    0    0   0    0    0   0     0   0\n4/7          0    0    0    0    0    0   0    0    0   0     0   0\n11/21        0    0    0    0    0    0   0    0    0   0     0   0\n8/3          0    0    0    0    0    0   0    0    0   0     0   0\n\n\nThis bipartite adjacency matrix \\(\\mathbf{B}\\) is of dimensions \\((M + N) \\times (M + N)\\), which is \\((18 + 14) \\times (18 + 14) = 32 \\times 32\\) in the SW data; it has the following block structure (Fouss, Saerens, and Shimbo 2016, 12):\n\\[\n\\mathbf{B} = \\left[\n\\begin{matrix}\n\\mathbf{O}_{M \\times M} & \\mathbf{A}_{M \\times N} \\\\\n\\mathbf{A}^T_{N \\times M} & \\mathbf{O}_{N \\times N}\n\\end{matrix}\n\\right]\n\\]\nWhere \\(\\mathbf{O}\\) is just the all zeros matrix of the relevant dimensions, and \\(\\mathbf{A}\\) is the biadjacency (affiliation) matrix as defined earlier. Thus, the bipartite adjacency matrix necessarily has two big diagonal “zero blocks” in it (upper-left and lower-right) corresponding to where the links between nodes in the same set would be (but necessarily aren’t because this is a two-mode network). The non-zero blocks are just the affiliation matrix (upper-right) and its transpose(lower-left)."
  },
  {
    "objectID": "tm-basic.html#bipartiteness-as-anti-community",
    "href": "tm-basic.html#bipartiteness-as-anti-community",
    "title": "Analyzing Two-Mode Networks",
    "section": "Bipartiteness as “Anti-Community”",
    "text": "Bipartiteness as “Anti-Community”\nRecall from the community structure lecture notes, that community structure is defined by clusters of nodes that have more connections among themselves than they do with outsiders. If you think about it, a bipartite graph has the opposite of this going on. Nodes of the same type have zero connections among themselves, and they have all their connections with nodes of the other group!\nSo that means that bipartite structure is the mirror image of community structure (in the two group case). This also means that if we were to compute the modularity of a bipartite graph, using the node type as the grouping variable we should get the theoretical minimum of this measure (which you may recall is \\(Q = -\\frac{1}{2}\\)).\nLet’s try it out, by computing the modularity from the bipartite adjacency matrix of the SW data, using node type as the grouping variable:\n\n   V(g)$comm <- as.numeric(bipartite_mapping(g)$type) + 1\n   modularity(g, V(g)$comm)\n\n[1] -0.5\n\n\nAnd indeed, we recover the theoretical minimum value of the modularity (Brandes et al. 2007, 173)! This also means that this method can be used to test whether a graph is bipartite, or whether any network approximates bipartiteness (Newman 2006, 13). Values that are close to \\(-0.5\\) would indicate that the network in question has bipartite structure."
  },
  {
    "objectID": "tm-basic.html#basic-two-mode-network-statistics",
    "href": "tm-basic.html#basic-two-mode-network-statistics",
    "title": "Analyzing Two-Mode Networks",
    "section": "Basic Two-Mode Network Statistics",
    "text": "Basic Two-Mode Network Statistics\nWe can calculate some basic network statistics from the affiliation (biadjacency) matrix. We have two number of nodes to calculate, but only one quantity for the number of edges.\n\nNumber of Nodes and Edges\nThe number of nodes on the people side \\(N\\) is just the number of rows of \\(A\\):\n\n   nrow(A)\n\n[1] 18\n\n\nAnd the number of events/groups \\(M\\) is just the number of columns:\n\n   ncol(A)\n\n[1] 14\n\n\nFinally, the number of edges \\(E\\) is just the sum of all the entries of \\(A\\):\n\n   sum(A)\n\n[1] 89\n\n\nNote that if you were to use the igraph function vcount on the original graph object, you get the wrong answer:\n\n   vcount(g)\n\n[1] 32\n\n\nThat’s because vcount is working with the \\(32 \\times 32\\) regular adjacency matrix, not the biadjacency matrix. Here, vcount is returning the total number of nodes in the graph summing across the two sets, which is \\(M + N\\).\nIf you wanted to get the right answer for each set of edges from the regular igraph graph object, you could use the type node attribute we defined earlier along with the subgraph function:\n\n   vcount(subgraph(g, V(g)$type == FALSE))\n\n[1] 18\n\n\nWhich gives us the number of women. For the events we do the same thing:\n\n   vcount(subgraph(g, V(g)$type == TRUE))\n\n[1] 14\n\n\nHowever, because there’s only one set of edges, ecount still gives us the right answer:\n\n   ecount(g)\n\n[1] 89\n\n\nWhich is the same as:\n\n   sum(A)\n\n[1] 89\n\n\n\n\nDensity\nAs we saw in the case of one-mode networks, one of the most basic network statistics that can be derived from the above quantities is the density (observed number of edges divided by maximum possible number of edges in the graph).\nIn a two-mode network, density is given by:\n\\[\nd = \\frac{|E|}{N \\times M}\n\\]\nWhere \\(|E|\\) is the number of edges in the network. In our case we can compute the density as follows:\n\n   d <- sum(A)/(nrow(A) * ncol(A))\n   d\n\n[1] 0.3531746"
  },
  {
    "objectID": "tm-basic.html#degree-based-statistics",
    "href": "tm-basic.html#degree-based-statistics",
    "title": "Analyzing Two-Mode Networks",
    "section": "Degree-Based Statistics",
    "text": "Degree-Based Statistics\nBecause we have two sets of degrees, all the basic degree statistics in the network double up. So we have two mean degrees, two maximum degrees, and two minimum degree to take care of:\n\n   mean.d.p <- mean(rowSums(A))\n   mean.d.g <- mean(colSums(A))\n   max.d.p <- max(rowSums(A))\n   max.d.g <- max(colSums(A))\n   min.d.p <- min(rowSums(A))\n   min.d.g <- min(colSums(A))\n\nSo we have:\n\n   round(mean.d.p, 1)\n\n[1] 4.9\n\n   round(mean.d.g, 1)\n\n[1] 6.4\n\n   max.d.p\n\n[1] 8\n\n   max.d.g\n\n[1] 14\n\n   min.d.p\n\n[1] 2\n\n   min.d.g\n\n[1] 3\n\n\nHowever, note that because there’s only one set of undirected edges, the total number of edges incident to each node in each of the two sets is always going to be the same.\nThat means that there’s only one sum of degrees. So the sum of degrees for people:\n\n   sum(rowSums(A))\n\n[1] 89\n\n\nIs the same as the sum of degrees of events:\n\n   sum(colSums(A))\n\n[1] 89\n\n\nNote that in a bipartite graph, therefore, the sum of degrees of nodes in each node set is equal to the \\(|E|\\), the number of edges in the graph!\n\nDegree Centrality\nIn a two-mode network, there are two degree sets, each corresponding to one set of nodes. For the people, in this case, their degree (centrality) is just the number of events they attend, and for the groups, it’s just the number of people that attend each event.\nAs we have already seen, we can get each from the affiliation matrix. The degree of the people are just the row sums:\n\n   rowSums(A)\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n        8         7         8         7         4         4         4         3 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n        4         4         4         6         7         8         5         2 \n   OLIVIA     FLORA \n        2         2 \n\n\nAnd the degree of the events are just the column sums:\n\n   colSums(A)\n\n 6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10  2/23   4/7 11/21 \n    3     3     6     4     8     8    10    14    12     5     4     6     3 \n  8/3 \n    3 \n\n\nThe igraph function degree will also give us the right answer, but in the form of a single vector including both people and events:\n\n   degree(g)\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n        8         7         8         7         4         4         4         3 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n        4         4         4         6         7         8         5         2 \n   OLIVIA     FLORA      6/27       3/2      4/12      9/26      2/25      5/19 \n        2         2         3         3         6         4         8         8 \n     3/15      9/16       4/8      6/10      2/23       4/7     11/21       8/3 \n       10        14        12         5         4         6         3         3 \n\n\nAs Borgatti and Everett (1997) note, if we want normalized degree centrality measures, we need to divide by either \\(M\\) (for people) or \\(N\\) (for events). That is, for people we use the number of events as the norm (as this is the theoretical maximum) and for events the number of people.\nSo for people, normalized degree is:\n\n   round(rowSums(A)/ncol(A), 3)\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n    0.571     0.500     0.571     0.500     0.286     0.286     0.286     0.214 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n    0.286     0.286     0.286     0.429     0.500     0.571     0.357     0.143 \n   OLIVIA     FLORA \n    0.143     0.143 \n\n\nAnd for events:\n\n   round(colSums(A)/nrow(A), 3)\n\n 6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10  2/23   4/7 11/21 \n0.167 0.167 0.333 0.222 0.444 0.444 0.556 0.778 0.667 0.278 0.222 0.333 0.167 \n  8/3 \n0.167 \n\n\nOr with igraph:\n\n   round(degree(g)/c(rep(14, 18), rep(18, 14)), 3)\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n    0.571     0.500     0.571     0.500     0.286     0.286     0.286     0.214 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n    0.286     0.286     0.286     0.429     0.500     0.571     0.357     0.143 \n   OLIVIA     FLORA      6/27       3/2      4/12      9/26      2/25      5/19 \n    0.143     0.143     0.167     0.167     0.333     0.222     0.444     0.444 \n     3/15      9/16       4/8      6/10      2/23       4/7     11/21       8/3 \n    0.556     0.778     0.667     0.278     0.222     0.333     0.167     0.167 \n\n\n\n\nAverage Nearest Neighbor Degree\nFor each person (group) we may also be interested in whether they connect to more or less central groups (persons). As such, we can compute the average nearest neighbor degree for persons and groups.\nFor people this is equivalent to multiplying the vector of group degrees by the entries of the affiliation matrix, and then dividing by the degrees of each person:\n\n   knn.p <- A * colSums(A)\n   knn.p\n\n          6/27 3/2 4/12 9/26 2/25 5/19 3/15 9/16 4/8 6/10 2/23 4/7 11/21 8/3\nEVELYN       3   8   12    3    6   10    0    3   8    0    0   0     0   0\nLAURA        3   8    5    0    4   14    6    3   0    0    0   0     0   0\nTHERESA      0  10    4    3    8   12    3    6  10    0    0   0     0   0\nBRENDA       4   0    6    3    8    5    3    4   0    0    0   0     0   0\nCHARLOTTE    0   0    3    6   10    0    3    0   0    0    0   0     0   0\nFRANCES      0   0    3    0   14    6    0    8   0    0    0   0     0   0\nELEANOR      0   0    0    0   12    3    6   10   0    0    0   0     0   0\nPEARL        0   0    0    0    0    3    0   14   6    0    0   0     0   0\nRUTH         0   0    0    0    4    0    8   12   3    0    0   0     0   0\nVERNE        0   0    0    0    0    0    8    5   3    0    0   6     0   0\nMYRNA        0   0    0    0    0    0    0    4   3    8    0   3     0   0\nKATHERINE    0   0    0    0    0    0    0    6   3    8    0   3     4  14\nSYLVIA       0   0    0    0    0    0   12    3   6   10    0   3     8  12\nNORA         0   0    0    0    0    8    5    0   4   14    6   3     8   5\nHELEN        0   0    0    0    0    0    4    3   0   12    3   6     0   0\nDOROTHY      0   0    0    0    0    0    0    3   8    0    0   0     0   0\nOLIVIA       0   0    0    0    0    0    0    0  10    0    3   0     0   0\nFLORA        0   0    0    0    0    0    0    0  14    0    3   0     0   0\n\n   knn.p <- rowSums(knn.p)/rowSums(A)\n   round(knn.p, 2)\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n     6.62      6.14      7.00      4.71      5.50      7.75      7.75      7.67 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n     6.75      5.50      4.50      6.33      7.71      6.62      5.60      5.50 \n   OLIVIA     FLORA \n     6.50      8.50 \n\n\nWe can see that for Flora, the average number of members of the groups she connects to is very high, while the opposite is the case for Myrna.\nWe can do the same for groups:\n\n   knn.g <- t(A) * rowSums(A)\n   knn.g\n\n      EVELYN LAURA THERESA BRENDA CHARLOTTE FRANCES ELEANOR PEARL RUTH VERNE\n6/27       8     5       0      4         0       0       0     0    0     0\n3/2        7     2       6      0         0       0       0     0    0     0\n4/12       8     2       7      4         4       8       0     0    0     0\n9/26       7     0       8      4         4       0       0     0    0     0\n2/25       4     8       5      4         4       8       2     0    4     0\n5/19       4     7       2      6         0       7       2     8    0     0\n3/15       0     8       2      7         4       0       8     0    4     4\n9/16       3     7       2      8         0       4       7     2    6     3\n4/8        4     0       8      0         0       0       0     2    7     4\n6/10       0     0       0      0         0       0       0     0    0     0\n2/23       0     0       0      0         0       0       0     0    0     0\n4/7        0     0       0      0         0       0       0     0    0     6\n11/21      0     0       0      0         0       0       0     0    0     0\n8/3        0     0       0      0         0       0       0     0    0     0\n      MYRNA KATHERINE SYLVIA NORA HELEN DOROTHY OLIVIA FLORA\n6/27      0         0      0    0     0       0      0     0\n3/2       0         0      0    0     0       0      0     0\n4/12      0         0      0    0     0       0      0     0\n9/26      0         0      0    0     0       0      0     0\n2/25      0         0      0    0     0       0      0     0\n5/19      0         0      0    3     0       0      0     0\n3/15      0         0      7    4     4       0      0     0\n9/16      7         2      8    0     4       7      0     0\n4/8       4         8      5    4     0       8      2     7\n6/10      4         7      2    6     3       0      0     0\n2/23      0         0      0    7     4       0      8     5\n4/7       3         7      2    8     4       0      0     0\n11/21     0         4      8    5     0       0      0     0\n8/3       0         4      7    2     0       0      0     0\n\n   knn.g <- rowSums(knn.g)/colSums(A)\n   round(knn.g, 2)\n\n 6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10  2/23   4/7 11/21 \n 5.67  5.00  5.50  5.75  4.88  4.88  5.20  5.00  5.25  4.40  6.00  5.00  5.67 \n  8/3 \n 4.33 \n\n\n\n\nDegree Correlation\nFinally, we can also compute the degree correlation between the nodes in each mode. This tell us whether people with more memberships connect to larger (positive correlation) or smaller (negative correlation) groups.\nHere’s a function to compute the degree correlation in a two mode network from the bipartite matrix:\n\n   tm.deg.corr <- function(x) {\n      d <- data.frame(e = as.vector(x), \n                      rd = rep(rowSums(x), ncol(x)), \n                      cd = rep(colSums(x), each = nrow(x)),\n                      rn = rep(rownames(x), ncol(x)),\n                      cn = rep(colnames(x), each = nrow(x))\n                      )\n      r <- cor(d[d$e == 1, ]$rd, d[d$e == 1, ]$cd)\n      return(list(r = r, d = d))\n   }\n\nThe tm.deg.corr function creates a data frame set with as many rows as there are entries in the bipartite matrix, and three columns: e recording whether there is a one or a zero for that particular combination of nodes (line 2), rd recording the degree of that node (line 3), and cd recording other node (line 4); lines 5 and 6 record the node labels for that dyad. Then in line 8 the function computes the Pearson correlations between the degrees of persons and groups that are connected in the data frame (e.g., e = 1).\nWe can now apply our function to the SW data:\n\n   tm.deg.corr(B)$r\n\n[1] -0.3369979\n\n\nWhich tells us that there is degree anti-correlation in this network: People with more memberships tend to belong to smaller groups, and people with less memberships connect to bigger groups."
  },
  {
    "objectID": "tm-ca.html",
    "href": "tm-ca.html",
    "title": "Correspondence Analysis (CA) of Two-Mode Networks",
    "section": "",
    "text": "Correspondence Analysis (CA) a relatively simple way to analyze and visualize two-mode data. As you might have already guessed, two-mode CA boils down to the eigendecomposition of a suitable matrix, derived from the original affiliation (bi-adjacency) matrix of a two-mode network.\nThe goal is to come up with a low-rank (usually two-dimensional) approximation of the original affiliation network using the eigenvectors and eigenvalues obtained from the decomposition, as we did above with our toy example.\nSo which matrix should be use for CA?\nLet’s find out:\nFirst we need to create row stochastic versions of the affiliation matrix and its transpose \\(\\mathbf{A}\\) and \\(\\mathbf{A}^T\\). Recall that a matrix is row stochastic if their rows sum to one.\nFor the people, we can do this by taking the original affiliation matrix, and pre-multiplying it by a diagonal square matrix \\(\\mathbf{D}_P^{-1}\\) of dimensions \\(M \\times M\\) containing the inverse of the degrees of each person in the affiliation network along the diagonals and zeros everywhere else, yielding the row-stochastic matrix \\(\\mathbf{P}_{PG}\\) of dimensions \\(M \\times N\\):\n\\[\n\\mathbf{P}_{PG} = \\mathbf{D}_P^{-1}\\mathbf{A}\n\\]\nAnd we can do the same with the groups, except that we pre-multiply the transpose of the original affiliation matrix by \\(\\mathbf{D}_G^{-1}\\) which is an \\(N \\times N\\) matrix containing the inverse of the size of each group along the diagonals and zero everywhere else, this yields the matrix \\(\\mathbf{P}_{GP}\\) of dimensions \\(N \\times M\\):\n\\[\n\\mathbf{P}_{GP} = \\mathbf{D}_G^{-1}\\mathbf{A}^T\n\\]\nIn R can compute \\(\\mathbf{P}_{PG}\\) and \\(\\mathbf{P}_{GP}\\), using the classic Southern Women two-mode data, as follows:\nAnd we can check that both \\(\\mathbf{P}_{PG}\\) and \\(\\mathbf{P}_{GP}\\) are indeed row stochastic:\nAnd that they are of the predicted dimensions:\nGreat! Now, we can obtain the degree-normalized projections for people by multiplying \\(\\mathbf{P}_{PG}\\) times \\(\\mathbf{P}_{GP}\\):\n\\[\n\\mathbf{P}_{PP} = \\mathbf{P}_{PG}\\mathbf{P}_{GP}\n\\]\nWhich produces the matrix \\(\\mathbf{P}_{PP}\\) a square \\(M \\times M\\) matrix containing the degree-normalized similarities between each pair of people.\nWe then do the same for groups:\n\\[\n\\mathbf{P}_{GG} = \\mathbf{P}_{GP}\\mathbf{P}_{PG}\n\\]\nWhich produces the matrix \\(\\mathbf{P}_{GG}\\) a square \\(N \\times N\\) matrix containing the degree-normalized similarities between each pair of groups.\nIn R we obtain these matrices as follows:\nWhich are still row stochastic–but now square–matrices:\nLet’s peek inside one of these matrices:\nWhat are these numbers? Well, they can be interpreted as probabilities that a random walker starting at the row node and, following any sequence of \\(person-group-person'-group'\\) hops, will reach the column person. Thus, higher values indicate an affinity or proximity between the people (and the groups in the corresponding matrix)."
  },
  {
    "objectID": "tm-ca.html#performing-ca",
    "href": "tm-ca.html#performing-ca",
    "title": "Correspondence Analysis (CA) of Two-Mode Networks",
    "section": "Performing CA",
    "text": "Performing CA\nWe went through all these steps because CA is equivalent to the eigendecomposition of the last two square matrices we obtained, namely, \\(\\mathbf{P_{PP}}\\) and \\(\\mathbf{P_{GG}}\\):\n\n   CA.p <- lapply(eigen(P.pp), Re)\n   CA.g <- lapply(eigen(P.gg), Re)\n\nLet’s see what we have here:\n\n   round(CA.p$values, 2)\n\n [1] 1.00 0.63 0.32 0.18 0.14 0.11 0.10 0.06 0.04 0.04 0.02 0.01 0.01 0.00 0.00\n[16] 0.00 0.00 0.00\n\n   round(CA.g$values, 2)\n\n [1] 1.00 0.63 0.32 0.18 0.14 0.11 0.10 0.06 0.04 0.04 0.02 0.01 0.01 0.00\n\n\nSo the two matrices have identical eigenvalues, and the first one is 1.0.\nLet’s check out the first three eigenvectors:\n\n   rownames(CA.p$vectors) <- rownames(A)\n   rownames(CA.g$vectors) <- colnames(A)\n   round(CA.p$vectors[, 1:3], 2)\n\n           [,1]  [,2]  [,3]\nEVELYN    -0.24 -0.24  0.03\nLAURA     -0.24 -0.25 -0.01\nTHERESA   -0.24 -0.20  0.02\nBRENDA    -0.24 -0.26 -0.01\nCHARLOTTE -0.24 -0.29 -0.01\nFRANCES   -0.24 -0.24 -0.02\nELEANOR   -0.24 -0.15 -0.03\nPEARL     -0.24 -0.01  0.06\nRUTH      -0.24 -0.05  0.03\nVERNE     -0.24  0.13 -0.04\nMYRNA     -0.24  0.25 -0.10\nKATHERINE -0.24  0.31 -0.22\nSYLVIA    -0.24  0.26 -0.20\nNORA      -0.24  0.26 -0.03\nHELEN     -0.24  0.24  0.07\nDOROTHY   -0.24  0.09  0.09\nOLIVIA    -0.24  0.33  0.66\nFLORA     -0.24  0.33  0.66\n\n   round(CA.g$vectors[, 1:3], 2)\n\n      [,1]  [,2]  [,3]\n6/27  0.27 -0.30 -0.01\n3/2   0.27 -0.28 -0.04\n4/12  0.27 -0.30  0.00\n9/26  0.27 -0.30 -0.02\n2/25  0.27 -0.25  0.00\n5/19  0.27 -0.16  0.00\n3/15  0.27 -0.04  0.05\n9/16  0.27 -0.01  0.05\n4/8   0.27  0.15 -0.19\n6/10  0.27  0.32  0.22\n2/23  0.27  0.35 -0.79\n4/7   0.27  0.29  0.20\n11/21 0.27  0.34  0.35\n8/3   0.27  0.34  0.35\n\n\nSo this is interesting. The first eigenvector of the decomposition of both \\(\\mathbf{P_{PP}}\\) and \\(\\mathbf{P_{GG}}\\) is just the same number for each person and group. Note that this is the eigenvector that is associated with the first eigenvalue which happens to be \\(\\lambda_1 = 1.0\\).\nSo it looks like the first eigenvector is a pretty useless quantity (a constant) so we can discard it, keeping all the other ones. Now the old second eigenvector is the first, the old third is the second, and so on:\n\n   eig.vec.p <- CA.p$vectors[, 2:ncol(CA.p$vectors)]\n   eig.vec.g <- CA.g$vectors[, 2:ncol(CA.g$vectors)]\n\nNote that the rest of the eigenvalues (discarding the 1.0 one) are arranged in descending order:\n\n   eig.vals <- CA.p$values[2:length(CA.p$values)]\n   round(eig.vals, 3)\n\n [1] 0.627 0.319 0.179 0.138 0.107 0.099 0.064 0.044 0.036 0.021 0.012 0.005\n[13] 0.000 0.000 0.000 0.000 0.000\n\n\nThe magnitude of the eigenvalue tells us how important is the related eigenvector in containing information about the original matrix. It looks like here, the first two eigenvectors contain a good chunk of the info.\nWe can check how much exactly by computing the ratio between the sum of the first two eigenvalues over the sum of all the eigenvalues:\n\n   round(sum(eig.vals[1:2])/sum(eig.vals), 2)\n\n[1] 0.57\n\n\nWhich tells us that the first two eigenvectors account for about 57% of the action (or more precisely we could reconstruct the original matrix with 57% accuracy using just these two eigenvectors and associated eigenvalues).\nBecause the magnitude of the CA eigenvectors don’t have a natural scale, it is common to normalize them to have a variance of 1.0 (Fouss, Saerens, and Shimbo 2016, 399), and the multiplying them by the square root of the eigenvalue corresponding to that dimension, so that the new variance is scaled to the importance of that dimension.\nWe can perform the normalization of the raw CA scores using the following function, which performs CA on the affiliation matrix:\n\n   norm.CA.vec <- function(x) {\n      D.p <- diag(rowSums(x)) #degree matrix for persons\n      D.g <- diag(colSums(x)) #degree matrix for groups\n      i.Dp <- solve(D.p) #inverse of degree matrix for persons\n      i.Dg <- solve(D.g) #inverse of degree matrix for groups\n      CA.p <- lapply(eigen(i.Dp %*% x %*% i.Dg %*% t(x)), Re) #person CA\n      CA.g <- lapply(eigen(i.Dg %*% t(x) %*% i.Dp %*% x), Re) #group CA\n      ev <- CA.p$values[2:length(CA.g$values)]\n      m <- length(ev)\n      CA.p <- CA.p$vectors[, 2:ncol(CA.p$vectors)]\n      CA.g <- CA.g$vectors[, 2:ncol(CA.g$vectors)]\n      rownames(CA.p) <- rownames(A)\n      rownames(CA.g) <- colnames(A)\n      Z.u.p <- matrix(0, nrow(x), m)\n      Z.u.g <- matrix(0, ncol(x), m)\n      Z.v.p <- matrix(0, nrow(x), m)\n      Z.v.g <- matrix(0, ncol(x), m)\n      rownames(Z.u.p) <- rownames(x)\n      rownames(Z.u.g) <- colnames(x)\n      rownames(Z.v.p) <- rownames(x)\n      rownames(Z.v.g) <- colnames(x)\n      for (i in 1:m) {\n         ev.p <- as.matrix(CA.p[, i])\n         ev.g <- as.matrix(CA.g[, i])\n         norm.p <- as.numeric(t(ev.p) %*% D.p %*% ev.p) #person norm\n         Z.u.p[, i] <- ev.p * sqrt(sum(A)/norm.p) #normalizing to unit variance\n         Z.v.p[, i] <- Z.u.p[, i] * sqrt(ev[i]) #normalizing to square root of eigenvalue\n         norm.g <- as.numeric(t(ev.g) %*% D.g %*% ev.g) #group norm\n         Z.u.g[, i] <- ev.g * sqrt(sum(A)/norm.g) #normalizing to unit variance\n         Z.v.g[, i] <- Z.u.g[, i] * sqrt(ev[i]) #normalizing to square root of eigenvalue\n      }\n   return(list(Z.u.p = Z.u.p, Z.u.g = Z.u.g,\n               Z.v.p = Z.v.p, Z.v.g = Z.v.g))\n   }\n\nThis function takes the bi-adjacency matrix as input and returns two new set of normalized CA scores for both persons and groups as output. The normalized CA scores are stored in four separate matrices: \\(\\mathbf{Z_P^U}, \\mathbf{Z_G^U}, \\mathbf{Z_P^V}, \\mathbf{Z_G^V}\\).\nOne person-group set of scores is normalized to unit variance (Z.u.p and Z.u.g) and the other person-group set of scores is normalized to the scale of the eigenvalue corresponding to each CA dimension for both persons and groups (Z.v.p and Z.v.g).\nLet’s see the normalization function at work, extracting the first two dimensions for persons and groups (the first two columns of each \\(\\mathbf{Z}\\) matrix):\n\n   CA.res <- norm.CA.vec(A)\n   uni.p <- CA.res$Z.u.p[, 1:2]\n   uni.g <- CA.res$Z.u.g[, 1:2]\n   val.p <- CA.res$Z.v.p[, 1:2]\n   val.g <- CA.res$Z.v.g[, 1:2]\n   round(uni.p, 2)\n\n           [,1]  [,2]\nEVELYN    -1.01  0.20\nLAURA     -1.06 -0.07\nTHERESA   -0.83  0.14\nBRENDA    -1.08 -0.09\nCHARLOTTE -1.23 -0.07\nFRANCES   -1.01 -0.10\nELEANOR   -0.65 -0.21\nPEARL     -0.05  0.37\nRUTH      -0.21  0.17\nVERNE      0.55 -0.23\nMYRNA      1.04 -0.58\nKATHERINE  1.32 -1.33\nSYLVIA     1.10 -1.20\nNORA       1.10 -0.19\nHELEN      1.02  0.44\nDOROTHY    0.38  0.55\nOLIVIA     1.38  3.98\nFLORA      1.38  3.98\n\n   round(uni.g, 2)\n\n       [,1]  [,2]\n6/27  -1.33 -0.02\n3/2   -1.22 -0.16\n4/12  -1.31  0.00\n9/26  -1.31 -0.08\n2/25  -1.12  0.00\n5/19  -0.72 -0.01\n3/15  -0.16  0.23\n9/16  -0.04  0.24\n4/8    0.65 -0.87\n6/10   1.41  1.01\n2/23   1.54 -3.64\n4/7    1.29  0.91\n11/21  1.48  1.61\n8/3    1.48  1.61\n\n   round(val.p, 2)\n\n           [,1]  [,2]\nEVELYN    -0.80  0.11\nLAURA     -0.84 -0.04\nTHERESA   -0.65  0.08\nBRENDA    -0.86 -0.05\nCHARLOTTE -0.97 -0.04\nFRANCES   -0.80 -0.06\nELEANOR   -0.51 -0.12\nPEARL     -0.04  0.21\nRUTH      -0.17  0.10\nVERNE      0.43 -0.13\nMYRNA      0.83 -0.33\nKATHERINE  1.05 -0.75\nSYLVIA     0.87 -0.68\nNORA       0.87 -0.11\nHELEN      0.81  0.25\nDOROTHY    0.30  0.31\nOLIVIA     1.10  2.25\nFLORA      1.10  2.25\n\n   round(val.g, 2)\n\n       [,1]  [,2]\n6/27  -1.05 -0.01\n3/2   -0.97 -0.09\n4/12  -1.04  0.00\n9/26  -1.04 -0.05\n2/25  -0.88  0.00\n5/19  -0.57 -0.01\n3/15  -0.13  0.13\n9/16  -0.03  0.14\n4/8    0.51 -0.49\n6/10   1.12  0.57\n2/23   1.22 -2.05\n4/7    1.02  0.52\n11/21  1.17  0.91\n8/3    1.17  0.91\n\n\nGreat! Now we have two sets (unit variance versus eigenvalue variance) of normalized CA scores for persons and groups on the first two dimensions."
  },
  {
    "objectID": "tm-ca.html#the-duality-of-persons-and-groups-ca-scores",
    "href": "tm-ca.html#the-duality-of-persons-and-groups-ca-scores",
    "title": "Correspondence Analysis (CA) of Two-Mode Networks",
    "section": "The Duality of Persons and Groups CA Scores",
    "text": "The Duality of Persons and Groups CA Scores\nJust like as we saw when discussing the Bonacich Eigenvector centrality in the two-mode case, there is a duality between the CA scores assigned to the person and the groups on each dimension, such that the scores for each person are a weighted sum of the scores assigned to each group on that dimension and vice versa (Faust 1997, 171).\nThe main difference is that this time we sum scores across the \\(\\mathbf{P_P}\\) and \\(\\mathbf{P_G}\\) matrices rather than the original affiliation matrix and its transpose, resulting in degree-weighted sums of scores for both persons and groups.\nSo for any given person, on any given dimension, let’s say \\(EVELYN\\), her CA score is given by the sum of the (unit variance normalized) CA scores of the groups she belongs to weighted by her degree (done by multiplying each CA score by the relevant cell in Evelyn’s row of the \\(\\mathbf{P}_{PG}\\) matrix):\n\n   sum(P.pg[\"EVELYN\", ] * uni.g[, 1]) \n\n[1] -0.7994396\n\n\nWhich is the same as the (eigenvalue variance) normalized score we obtained via CA for \\(EVELYN\\):\n\n   val.p[\"EVELYN\", 1]\n\n    EVELYN \n-0.7994396 \n\n\nA similar story applies to groups. Each group score is the group-size-weighted sum of the (unit variance normalized) CA scores of the people who join it:\n\n   sum(P.gp[\"6/27\", ] * uni.p[, 1])\n\n[1] -1.051052\n\n\nWhich is the same as the (eigenvalue variance normalized) score we obtained via CA:\n\n    val.g[\"6/27\", 1]  \n\n     6/27 \n-1.051052 \n\n\nNeat! Duality at work."
  },
  {
    "objectID": "tm-ca.html#visualizing-two-mode-networks-using-ca",
    "href": "tm-ca.html#visualizing-two-mode-networks-using-ca",
    "title": "Correspondence Analysis (CA) of Two-Mode Networks",
    "section": "Visualizing Two-Mode Networks Using CA",
    "text": "Visualizing Two-Mode Networks Using CA\nAnd, finally, we can use the first two (eigenvalue variance) normalized CA scores to plot the persons and groups in a common two-dimensional space:\n\n   val.g[, 2] <- val.g[, 2]*-1 #flippling sign of group scores on second dimension for plotting purposes\n   plot.dat <- data.frame(rbind(uni.p, val.g)) %>% \n      cbind(type = as.factor(c(rep(1, 18), rep(2, 14))))\n   library(ggplot2)\n   # install.packages(\"ggrepel\")\n   library(ggrepel)\n   p <- ggplot(data = plot.dat, aes(x = X1, y = X2, color = type))\n   p <- p + geom_hline(aes(yintercept = 0), color = \"gray\")\n   p <- p + geom_vline(aes(xintercept = 0), color = \"gray\")\n   p <- p + geom_text_repel(aes(label = rownames(plot.dat)), \n                            max.overlaps = 20, size = 2.75)\n   p <- p + theme_minimal()\n   p <- p + theme(legend.position = \"none\",\n                  axis.title = element_text(size = 14),\n                  axis.text = element_text(size = 12))\n   p <- p + scale_color_manual(values = c(\"red\", \"blue\"))\n   p <- p + labs(x = \"First Dimension\", y = \"Second Dimension\") \n   p <- p + xlim(-2, 2) + ylim(-2, 4)\n   p\n\n\n\n\nIn this space, people with the most similar patterns of memberships to the most similar groups are placed close to one another. In the same way, groups with the most similar members are placed closed to one another.\nAlso like before, we can use the scores obtained from the CA analysis to re-arrange the rows and columns of the original matrix to reveal blocks of maximally similar persons and events:\n\n   library(ggcorrplot)\n   p <- ggcorrplot(t(A[order(val.p[,1]), order(val.g[,1])]), \n                   colors = c(\"white\", \"white\", \"red\")) \n   p <- p + theme(legend.position = \"none\", \n                  axis.text.y = element_text(size = 8),\n                  axis.text.x = element_text(size = 8, angle = 0),\n                  )\n   p <- p + scale_x_discrete(position = \"top\") \n   p <- p + geom_hline(yintercept = 6.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_hline(yintercept = 11.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_hline(yintercept = 16.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_vline(xintercept = 9.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_vline(xintercept = 6.5, linewidth = 2, color = \"blue\")\n   p\n\n\n\n\nHere CA seems to have detected two separate clusters of actors who preferentially attend two distinct clusters of events!\nThe three events in the middle {3/15, 9/16, 4/8} don’t seem to differentiate between participants in each cluster (everyone attends)–they thus appear near the origin in the CA diagram, indicating a weak association with either dimension.\nHowever, the events to the left (with clusters of participants in the lower-left) and to the right of the x-axis (with clusters of participants in the upper-right) are attended preferentially by distinct groups of participants; they thus appear at the extreme left and right positions of the first dimension of the CA diagram.\nIn the same way, the four people in the middle {Ruth, Dorothy, Pearl, Verne} only attend the undifferentiated, popular events, so that means that they are not strongly associated with either cluster of actors (and thus appear near the origin in the CA diagram). The top and bottom participants, by contrast, appear to the extreme right and left in the CA diagram, indicating a strong association with the underlying dimensions.\nNote the similarity between this blocking and that obtained from the generalized vertex similarity analysis of the same network."
  },
  {
    "objectID": "tm-ca.html#another-way-of-performing-ca-in-a-two-mode-network",
    "href": "tm-ca.html#another-way-of-performing-ca-in-a-two-mode-network",
    "title": "Correspondence Analysis (CA) of Two-Mode Networks",
    "section": "Another way of Performing CA in a Two-Mode Network",
    "text": "Another way of Performing CA in a Two-Mode Network\nSciarra et al. (2020) suggest an alternative way of computing CA for two-mode networks that avoids the issue of having to do the eigendecomposition of an asymmetric matrix (like the \\(\\mathbf{P}_{PP}\\) and \\(\\mathbf{P}_{GG}\\)) above. Their suggestion is to normalize the biadjacency matrix \\(\\mathbf{A}\\), before computing the degree-weighted projections, according to the formula:\n\\[\n\\tilde{\\mathbf{A}} = \\mathbf{D^{-2}_P}\\mathbf{A}\\mathbf{D^{-2}_G}\n\\]\nWhere \\(\\mathbf{D^{-2}}\\) is a matrix containing the inverse of the square root of the degrees in diagonals and zero everywhere else.\nWe can then obtain symmetric versions of the projection matrices, by applying the usual Breiger (1974) projection formula:\n\\[\n\\mathbf{P} = \\tilde{\\mathbf{A}}\\tilde{\\mathbf{A}}^T\n\\]\n\\[\n\\mathbf{G} = \\tilde{\\mathbf{A}}^T\\tilde{\\mathbf{A}}\n\\]\nThe matrices \\(\\mathbf{P}\\) and \\(\\mathbf{G}\\) no longer have the nice interpretation of being Markov transition matrices that the \\(\\mathbf{P}_{pp}\\) \\(\\mathbf{P}_{gg}\\) matrices have, but the fact that they are symmetric can be an advantage in facilitating the eigendecomposition computation, as they are guaranteed to have real-valued solutions.\nLet’s see how this would work:\n\n   D2.p <- diag(1/sqrt(rowSums(A)))\n   D2.g <- diag(1/sqrt(colSums(A)))\n   A2 <- D2.p %*% A %*% D2.g\n   P <- A2 %*% t(A2)\n   G <- t(A2) %*% A2\n\nWe can check that indeed \\(\\mathbf{P}\\) and \\(\\mathbf{G}\\) are symmetric:\n\n   isSymmetric(P)\n\n[1] TRUE\n\n   isSymmetric(G)\n\n[1] TRUE\n\n\nAnd now we can obtain the row and column CA scores via eigendecomposition of these matrices:\n\n   eig.vec.p2 <- eigen(P)$vectors\n   eig.vec.g2 <- eigen(G)$vectors\n\nAs Sciarra et al. (2020, 3) also note, to have the eigenvectors obtained via this procedure match the CA scores, we need to rescale the standard CA scores by multiplying them by the square root of the degrees of the row and column nodes:\n\n   cor(CA.res$Z.u.p[, 1] * sqrt(rowSums(A)), eig.vec.p2[, 2])\n\n[1] 1\n\n   cor(CA.res$Z.u.g[, 1] * sqrt(colSums(A)), eig.vec.g2[, 2])\n\n[1] 1\n\n\nWhich as we can see, results in perfectly correlated scores across the two methods."
  },
  {
    "objectID": "tm-centrality.html",
    "href": "tm-centrality.html",
    "title": "Centrality in Two-Mode Networks",
    "section": "",
    "text": "We have already seen how to calculate the simplest measure of centrality in two-mode networks (degree) in the basic two-mode statistics lecture. Here we extend the discussion to the other two of the big three, closeness and betwennness. As always, we load up the usual Southern Women (SW) data:"
  },
  {
    "objectID": "tm-centrality.html#geodesic-distances",
    "href": "tm-centrality.html#geodesic-distances",
    "title": "Centrality in Two-Mode Networks",
    "section": "Geodesic Distances",
    "text": "Geodesic Distances\nGeodesic distances work a bit different in two-mode networks because of the only between-node-sets edges restriction.\nFor instance, the minimum geodesic distance \\(g_{ii'}\\) between two people is two (a person cannot be adjacent to another person), but it is one between a person and a group (if the person is a member of the group).\nIn the same way, a group \\(g\\) cannot be at geodesic distance less than three from a person \\(p*\\) who is not a member, because the shortest path is \\(g-p-g^*-p^*\\).\nThat is, there has to be some other group \\(g^*\\) shared between a member \\(p\\) of the focal group \\(g\\) and another person \\(p^*\\) for the shortest path between \\(g\\) and the non-member \\(p^*\\) to exist, and that involves three links at minimum: \\(g-p\\), \\(p-g^*\\), and \\(g^*-p^*\\). This means that the links in paths in two-mode networks always alternate between persons and group nodes.\nBeyond that geodesic distances work the same way. In igraph when we use the distances function on a bipartite graph, we get:\n\n   D.pg <- distances(g)\n   head(D.pg)\n\n          EVELYN LAURA THERESA BRENDA CHARLOTTE FRANCES ELEANOR PEARL RUTH\nEVELYN         0     2       2      2         2       2       2     2    2\nLAURA          2     0       2      2         2       2       2     2    2\nTHERESA        2     2       0      2         2       2       2     2    2\nBRENDA         2     2       2      0         2       2       2     2    2\nCHARLOTTE      2     2       2      2         0       2       2     4    2\nFRANCES        2     2       2      2         2       0       2     2    2\n          VERNE MYRNA KATHERINE SYLVIA NORA HELEN DOROTHY OLIVIA FLORA 6/27 3/2\nEVELYN        2     2         2      2    2     2       2      2     2    1   1\nLAURA         2     2         2      2    2     2       2      4     4    1   1\nTHERESA       2     2         2      2    2     2       2      2     2    3   1\nBRENDA        2     2         2      2    2     2       2      4     4    1   3\nCHARLOTTE     2     4         4      2    2     2       4      4     4    3   3\nFRANCES       2     2         2      2    2     2       2      4     4    3   3\n          4/12 9/26 2/25 5/19 3/15 9/16 4/8 6/10 2/23 4/7 11/21 8/3\nEVELYN       1    1    1    1    3    1   1    3    3   3     3   3\nLAURA        1    3    1    1    1    1   3    3    3   3     3   3\nTHERESA      1    1    1    1    1    1   1    3    3   3     3   3\nBRENDA       1    1    1    1    1    1   3    3    3   3     3   3\nCHARLOTTE    1    1    1    3    1    3   3    3    3   3     3   3\nFRANCES      1    3    1    1    3    1   3    3    3   3     3   3\n\n   tail(D.pg)\n\n      EVELYN LAURA THERESA BRENDA CHARLOTTE FRANCES ELEANOR PEARL RUTH VERNE\n4/8        1     3       1      3         3       3       3     1    1     1\n6/10       3     3       3      3         3       3       3     3    3     3\n2/23       3     3       3      3         3       3       3     3    3     3\n4/7        3     3       3      3         3       3       3     3    3     1\n11/21      3     3       3      3         3       3       3     3    3     3\n8/3        3     3       3      3         3       3       3     3    3     3\n      MYRNA KATHERINE SYLVIA NORA HELEN DOROTHY OLIVIA FLORA 6/27 3/2 4/12 9/26\n4/8       1         1      1    1     3       1      1     1    2   2    2    2\n6/10      1         1      1    1     1       3      3     3    4   4    4    4\n2/23      3         3      3    1     1       3      1     1    4   4    4    4\n4/7       1         1      1    1     1       3      3     3    4   4    4    4\n11/21     3         1      1    1     3       3      3     3    4   4    4    4\n8/3       3         1      1    1     3       3      3     3    4   4    4    4\n      2/25 5/19 3/15 9/16 4/8 6/10 2/23 4/7 11/21 8/3\n4/8      2    2    2    2   0    2    2   2     2   2\n6/10     4    2    2    2   2    0    2   2     2   2\n2/23     4    2    2    2   2    2    0   2     2   2\n4/7      4    2    2    2   2    2    2   0     2   2\n11/21    4    2    2    2   2    2    2   2     0   2\n8/3      4    2    2    2   2    2    2   2     2   0\n\n\nWhich is a square matrix of dimensions \\((M + N) \\times (M + N)\\); that’s \\((18 + 14) \\times (18 + 14) = 32 \\times 32\\) in our case.\nWe can check in R:\n\n   dim(D.pg)\n\n[1] 32 32\n\n\nAs we can see in the distance matrix, distances between nodes in the same set are even \\(g_{ii'|jj'} = \\{2, 4, \\ldots\\}\\) but distances in nodes in different sets are odd \\(g_{ij|ji} = \\{1, 3, \\ldots\\}\\). Beyond this hiccup, distances can be interpreted in the same way as one-mode networks."
  },
  {
    "objectID": "tm-centrality.html#closeness-centrality",
    "href": "tm-centrality.html#closeness-centrality",
    "title": "Centrality in Two-Mode Networks",
    "section": "Closeness Centrality",
    "text": "Closeness Centrality\nThis means that (unnormalized) closeness centrality works the same way as it does in regular networks:\n\n   round(closeness(g), 3)\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n    0.017     0.015     0.017     0.015     0.013     0.014     0.014     0.014 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n    0.015     0.015     0.014     0.015     0.016     0.017     0.015     0.014 \n   OLIVIA     FLORA      6/27       3/2      4/12      9/26      2/25      5/19 \n    0.012     0.012     0.012     0.012     0.013     0.012     0.014     0.016 \n     3/15      9/16       4/8      6/10      2/23       4/7     11/21       8/3 \n    0.017     0.019     0.018     0.013     0.012     0.013     0.012     0.012 \n\n\nWhich is just the inverse of the sums of the distances matrix for people and groups counting their geodesic distances to nodes of both sets.\nHowever, as Borgatti and Everett (1997) note, if we want normalized closeness centralities, we can’t use the off-the-shelf normalization for one-mode networks in igraph (\\(n-1\\)) as it will give us non-sense results because now we have two sets of nodes.\nInstead, we need to normalize the closeness score for each node set by its theoretical maximum for each node set.\nFor people, this is:\n\\[\nN + 2(M - 1)\n\\]\nAnd for groups/events this same quantity is:\n\\[\nM + 2(N - 1)\n\\]\nThe basic idea is that nodes can be at minimum geodesic distance \\(g = 1\\) from nodes of the other set (for people, groups; for groups, people) and at minimum distance \\(g = 2\\) from nodes of their own set, with their own presence eliminated by subtraction (Borgatti and Everett 1997).\nIn our case, we create a normalization vector with these quantities of length \\(M + N\\):\n\n   M <- nrow(A)\n   N <- ncol(A)\n   n.p <- N + 2 * (M - 1)\n   n.e <- M + 2 * (N - 1)\n   norm.vec <- c(rep(n.p, M), rep(n.e, N))\n\nAnd normalized closeness is:\n\n   round(norm.vec/rowSums(D.pg), 3)\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n    0.800     0.727     0.800     0.727     0.600     0.667     0.667     0.667 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n    0.706     0.706     0.686     0.727     0.774     0.800     0.727     0.649 \n   OLIVIA     FLORA      6/27       3/2      4/12      9/26      2/25      5/19 \n    0.585     0.585     0.524     0.524     0.564     0.537     0.595     0.688 \n     3/15      9/16       4/8      6/10      2/23       4/7     11/21       8/3 \n    0.733     0.846     0.786     0.550     0.537     0.564     0.524     0.524 \n\n\nWhich are the same numbers in Borgatti and Everett (1997, table 1, column 6)."
  },
  {
    "objectID": "tm-centrality.html#betweenness-centrality",
    "href": "tm-centrality.html#betweenness-centrality",
    "title": "Centrality in Two-Mode Networks",
    "section": "Betweenness Centrality",
    "text": "Betweenness Centrality\nAs Borgatti and Everett (1997) also note, the normalizations for betweenness centrality in the two-mode case are a bit more involved. This is because they depend on which node set is larger than the other.\nFor the larger node set, which in our case is the people, the normalization is:\n\\[\n2(M-1)(N-1)\n\\]\nFor the smaller node set, which in our case is the groups/events, the normalization is:\n\\[\n\\frac{1}{2}(N)(N-1)+\\frac{1}{2}(M-1)(M-2)+(M-1)(N-1)\n\\]\nRemember that you have to switch this around if you are analyzing a network with more groups than people.\nCreating the relevant vectors:\n\n   n.p <- 2*(M-1)*(N-1)\n   n.e <- (1/2)*(N*(N-1))+(1/2)*(M-1)*(M-2)+(M-1)*(N-1)\n   norm.vec <- c(rep(n.p, M), rep(n.e, N))\n\nAnd normalized betweenness is:\n\n   round(betweenness(g)/norm.vec, 4)*100\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n     9.72      5.17      8.82      4.98      1.07      1.08      0.95      0.68 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n     1.69      1.58      1.65      4.77      7.22     11.42      4.27      0.20 \n   OLIVIA     FLORA      6/27       3/2      4/12      9/26      2/25      5/19 \n     0.51      0.51      0.22      0.21      1.84      0.78      3.80      6.56 \n     3/15      9/16       4/8      6/10      2/23       4/7     11/21       8/3 \n    13.07     24.60     22.75      1.15      1.98      1.83      0.23      0.23 \n\n\nWhich are (with some slight differences and rounding errors) the same numbers in Borgatti and Everett (1997, table 2, column 3)."
  },
  {
    "objectID": "tm-community-ca.html",
    "href": "tm-community-ca.html",
    "title": "Community Detection in Two-Mode Networks Using CA",
    "section": "",
    "text": "You may have noticed that the CA analysis of two-mode networks looks a lot like the identification of communities in one-mode networks. The main difference is that in a two-mode network, good communities are composed of clusters of persons and groups well-separated from other clusters of persons and groups.\nAs Barber (2007) noted, we can extend Newman’s modularity approach to ascertain whether a given partition identifies a good “community” in the bipartite case. For that, we need a bi-adjacency analog of the modularity matrix \\(\\mathbf{B}\\). This is given by:\n\\[\n\\mathbf{B}_{(ij)} = \\mathbf{A}_{(ij)} - \\frac{k^p_ik^g_j}{|E|}\n\\]\nWhere \\(k^p_i\\) is the number of memberships of the \\(i^{th}\\) person, \\(k^g_i\\) is the number of members of the \\(j^{th}\\) group, and \\(|E|\\) is the number of edges in the bipartite graph.\nSo in the Southern Women data case, this would be:\n\n    library(networkdata)\n    library(igraph)\n    g <- southern_women\n    A <- as_biadjacency_matrix(g)\n    dp <- as.matrix(rowSums(A))\n    dg <- as.matrix(colSums(A))\n    dpdg <- dp %*% t(dg) #person x group degree product matrix\n    B <- A - dpdg/sum(A)\n    round(B, 2)\n\n           6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10  2/23\nEVELYN     0.73  0.73  0.46  0.64  0.28  0.28 -0.90 -0.26 -0.08 -0.45 -0.36\nLAURA      0.76  0.76  0.53 -0.31  0.37  0.37  0.21 -0.10 -0.94 -0.39 -0.31\nTHERESA   -0.27  0.73  0.46  0.64  0.28  0.28  0.10 -0.26 -0.08 -0.45 -0.36\nBRENDA     0.76 -0.24  0.53  0.69  0.37  0.37  0.21 -0.10 -0.94 -0.39 -0.31\nCHARLOTTE -0.13 -0.13  0.73  0.82  0.64 -0.36  0.55 -0.63 -0.54 -0.22 -0.18\nFRANCES   -0.13 -0.13  0.73 -0.18  0.64  0.64 -0.45  0.37 -0.54 -0.22 -0.18\nELEANOR   -0.13 -0.13 -0.27 -0.18  0.64  0.64  0.55  0.37 -0.54 -0.22 -0.18\nPEARL     -0.10 -0.10 -0.20 -0.13 -0.27  0.73 -0.34  0.53  0.60 -0.17 -0.13\nRUTH      -0.13 -0.13 -0.27 -0.18  0.64 -0.36  0.55  0.37  0.46 -0.22 -0.18\nVERNE     -0.13 -0.13 -0.27 -0.18 -0.36 -0.36  0.55  0.37  0.46 -0.22 -0.18\nMYRNA     -0.13 -0.13 -0.27 -0.18 -0.36 -0.36 -0.45  0.37  0.46  0.78 -0.18\nKATHERINE -0.20 -0.20 -0.40 -0.27 -0.54 -0.54 -0.67  0.06  0.19  0.66 -0.27\nSYLVIA    -0.24 -0.24 -0.47 -0.31 -0.63 -0.63  0.21 -0.10  0.06  0.61 -0.31\nNORA      -0.27 -0.27 -0.54 -0.36 -0.72  0.28  0.10 -1.26 -0.08  0.55  0.64\nHELEN     -0.17 -0.17 -0.34 -0.22 -0.45 -0.45  0.44  0.21 -0.67  0.72  0.78\nDOROTHY   -0.07 -0.07 -0.13 -0.09 -0.18 -0.18 -0.22  0.69  0.73 -0.11 -0.09\nOLIVIA    -0.07 -0.07 -0.13 -0.09 -0.18 -0.18 -0.22 -0.31  0.73 -0.11  0.91\nFLORA     -0.07 -0.07 -0.13 -0.09 -0.18 -0.18 -0.22 -0.31  0.73 -0.11  0.91\n            4/7 11/21   8/3\nEVELYN    -0.54 -0.27 -0.27\nLAURA     -0.47 -0.24 -0.24\nTHERESA   -0.54 -0.27 -0.27\nBRENDA    -0.47 -0.24 -0.24\nCHARLOTTE -0.27 -0.13 -0.13\nFRANCES   -0.27 -0.13 -0.13\nELEANOR   -0.27 -0.13 -0.13\nPEARL     -0.20 -0.10 -0.10\nRUTH      -0.27 -0.13 -0.13\nVERNE      0.73 -0.13 -0.13\nMYRNA      0.73 -0.13 -0.13\nKATHERINE  0.60  0.80  0.80\nSYLVIA     0.53  0.76  0.76\nNORA       0.46  0.73  0.73\nHELEN      0.66 -0.17 -0.17\nDOROTHY   -0.13 -0.07 -0.07\nOLIVIA    -0.13 -0.07 -0.07\nFLORA     -0.13 -0.07 -0.07\n\n\nNeat! Like before the numbers in this matrix represent the expected probability of observing a tie in a world in which people keep their number of memberships and groups keep their observed sizes, but otherwise, people and groups connect at random.\nWe can also create a bipartite matrix version of the bi-adjacency modularity, as follows:\n\n   n <- vcount(g)\n   Np <- nrow(A)\n   names <- c(rownames(A), colnames(A))\n   B2 <- matrix(0, n, n) #all zeros matrix of dimensions (p + g) X (p + g)\n   B2[1:Np, (Np + 1):n] <- B #putting B in the top right block\n   B2[(Np + 1):n, 1:Np] <- t(B) #putting B transpose in the lower-left block\n   rownames(B2) <- names\n   colnames(B2) <- names\n   round(B2, 2)\n\n          EVELYN LAURA THERESA BRENDA CHARLOTTE FRANCES ELEANOR PEARL  RUTH\nEVELYN      0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\nLAURA       0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\nTHERESA     0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\nBRENDA      0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\nCHARLOTTE   0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\nFRANCES     0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\nELEANOR     0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\nPEARL       0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\nRUTH        0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\nVERNE       0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\nMYRNA       0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\nKATHERINE   0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\nSYLVIA      0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\nNORA        0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\nHELEN       0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\nDOROTHY     0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\nOLIVIA      0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\nFLORA       0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00  0.00\n6/27        0.73  0.76   -0.27   0.76     -0.13   -0.13   -0.13 -0.10 -0.13\n3/2         0.73  0.76    0.73  -0.24     -0.13   -0.13   -0.13 -0.10 -0.13\n4/12        0.46  0.53    0.46   0.53      0.73    0.73   -0.27 -0.20 -0.27\n9/26        0.64 -0.31    0.64   0.69      0.82   -0.18   -0.18 -0.13 -0.18\n2/25        0.28  0.37    0.28   0.37      0.64    0.64    0.64 -0.27  0.64\n5/19        0.28  0.37    0.28   0.37     -0.36    0.64    0.64  0.73 -0.36\n3/15       -0.90  0.21    0.10   0.21      0.55   -0.45    0.55 -0.34  0.55\n9/16       -0.26 -0.10   -0.26  -0.10     -0.63    0.37    0.37  0.53  0.37\n4/8        -0.08 -0.94   -0.08  -0.94     -0.54   -0.54   -0.54  0.60  0.46\n6/10       -0.45 -0.39   -0.45  -0.39     -0.22   -0.22   -0.22 -0.17 -0.22\n2/23       -0.36 -0.31   -0.36  -0.31     -0.18   -0.18   -0.18 -0.13 -0.18\n4/7        -0.54 -0.47   -0.54  -0.47     -0.27   -0.27   -0.27 -0.20 -0.27\n11/21      -0.27 -0.24   -0.27  -0.24     -0.13   -0.13   -0.13 -0.10 -0.13\n8/3        -0.27 -0.24   -0.27  -0.24     -0.13   -0.13   -0.13 -0.10 -0.13\n          VERNE MYRNA KATHERINE SYLVIA  NORA HELEN DOROTHY OLIVIA FLORA  6/27\nEVELYN     0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00  0.73\nLAURA      0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00  0.76\nTHERESA    0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00 -0.27\nBRENDA     0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00  0.76\nCHARLOTTE  0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00 -0.13\nFRANCES    0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00 -0.13\nELEANOR    0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00 -0.13\nPEARL      0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00 -0.10\nRUTH       0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00 -0.13\nVERNE      0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00 -0.13\nMYRNA      0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00 -0.13\nKATHERINE  0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00 -0.20\nSYLVIA     0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00 -0.24\nNORA       0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00 -0.27\nHELEN      0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00 -0.17\nDOROTHY    0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00 -0.07\nOLIVIA     0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00 -0.07\nFLORA      0.00  0.00      0.00   0.00  0.00  0.00    0.00   0.00  0.00 -0.07\n6/27      -0.13 -0.13     -0.20  -0.24 -0.27 -0.17   -0.07  -0.07 -0.07  0.00\n3/2       -0.13 -0.13     -0.20  -0.24 -0.27 -0.17   -0.07  -0.07 -0.07  0.00\n4/12      -0.27 -0.27     -0.40  -0.47 -0.54 -0.34   -0.13  -0.13 -0.13  0.00\n9/26      -0.18 -0.18     -0.27  -0.31 -0.36 -0.22   -0.09  -0.09 -0.09  0.00\n2/25      -0.36 -0.36     -0.54  -0.63 -0.72 -0.45   -0.18  -0.18 -0.18  0.00\n5/19      -0.36 -0.36     -0.54  -0.63  0.28 -0.45   -0.18  -0.18 -0.18  0.00\n3/15       0.55 -0.45     -0.67   0.21  0.10  0.44   -0.22  -0.22 -0.22  0.00\n9/16       0.37  0.37      0.06  -0.10 -1.26  0.21    0.69  -0.31 -0.31  0.00\n4/8        0.46  0.46      0.19   0.06 -0.08 -0.67    0.73   0.73  0.73  0.00\n6/10      -0.22  0.78      0.66   0.61  0.55  0.72   -0.11  -0.11 -0.11  0.00\n2/23      -0.18 -0.18     -0.27  -0.31  0.64  0.78   -0.09   0.91  0.91  0.00\n4/7        0.73  0.73      0.60   0.53  0.46  0.66   -0.13  -0.13 -0.13  0.00\n11/21     -0.13 -0.13      0.80   0.76  0.73 -0.17   -0.07  -0.07 -0.07  0.00\n8/3       -0.13 -0.13      0.80   0.76  0.73 -0.17   -0.07  -0.07 -0.07  0.00\n            3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10  2/23   4/7\nEVELYN     0.73  0.46  0.64  0.28  0.28 -0.90 -0.26 -0.08 -0.45 -0.36 -0.54\nLAURA      0.76  0.53 -0.31  0.37  0.37  0.21 -0.10 -0.94 -0.39 -0.31 -0.47\nTHERESA    0.73  0.46  0.64  0.28  0.28  0.10 -0.26 -0.08 -0.45 -0.36 -0.54\nBRENDA    -0.24  0.53  0.69  0.37  0.37  0.21 -0.10 -0.94 -0.39 -0.31 -0.47\nCHARLOTTE -0.13  0.73  0.82  0.64 -0.36  0.55 -0.63 -0.54 -0.22 -0.18 -0.27\nFRANCES   -0.13  0.73 -0.18  0.64  0.64 -0.45  0.37 -0.54 -0.22 -0.18 -0.27\nELEANOR   -0.13 -0.27 -0.18  0.64  0.64  0.55  0.37 -0.54 -0.22 -0.18 -0.27\nPEARL     -0.10 -0.20 -0.13 -0.27  0.73 -0.34  0.53  0.60 -0.17 -0.13 -0.20\nRUTH      -0.13 -0.27 -0.18  0.64 -0.36  0.55  0.37  0.46 -0.22 -0.18 -0.27\nVERNE     -0.13 -0.27 -0.18 -0.36 -0.36  0.55  0.37  0.46 -0.22 -0.18  0.73\nMYRNA     -0.13 -0.27 -0.18 -0.36 -0.36 -0.45  0.37  0.46  0.78 -0.18  0.73\nKATHERINE -0.20 -0.40 -0.27 -0.54 -0.54 -0.67  0.06  0.19  0.66 -0.27  0.60\nSYLVIA    -0.24 -0.47 -0.31 -0.63 -0.63  0.21 -0.10  0.06  0.61 -0.31  0.53\nNORA      -0.27 -0.54 -0.36 -0.72  0.28  0.10 -1.26 -0.08  0.55  0.64  0.46\nHELEN     -0.17 -0.34 -0.22 -0.45 -0.45  0.44  0.21 -0.67  0.72  0.78  0.66\nDOROTHY   -0.07 -0.13 -0.09 -0.18 -0.18 -0.22  0.69  0.73 -0.11 -0.09 -0.13\nOLIVIA    -0.07 -0.13 -0.09 -0.18 -0.18 -0.22 -0.31  0.73 -0.11  0.91 -0.13\nFLORA     -0.07 -0.13 -0.09 -0.18 -0.18 -0.22 -0.31  0.73 -0.11  0.91 -0.13\n6/27       0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n3/2        0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n4/12       0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n9/26       0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n2/25       0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n5/19       0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n3/15       0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n9/16       0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n4/8        0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n6/10       0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n2/23       0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n4/7        0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n11/21      0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n8/3        0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n          11/21   8/3\nEVELYN    -0.27 -0.27\nLAURA     -0.24 -0.24\nTHERESA   -0.27 -0.27\nBRENDA    -0.24 -0.24\nCHARLOTTE -0.13 -0.13\nFRANCES   -0.13 -0.13\nELEANOR   -0.13 -0.13\nPEARL     -0.10 -0.10\nRUTH      -0.13 -0.13\nVERNE     -0.13 -0.13\nMYRNA     -0.13 -0.13\nKATHERINE  0.80  0.80\nSYLVIA     0.76  0.76\nNORA       0.73  0.73\nHELEN     -0.17 -0.17\nDOROTHY   -0.07 -0.07\nOLIVIA    -0.07 -0.07\nFLORA     -0.07 -0.07\n6/27       0.00  0.00\n3/2        0.00  0.00\n4/12       0.00  0.00\n9/26       0.00  0.00\n2/25       0.00  0.00\n5/19       0.00  0.00\n3/15       0.00  0.00\n9/16       0.00  0.00\n4/8        0.00  0.00\n6/10       0.00  0.00\n2/23       0.00  0.00\n4/7        0.00  0.00\n11/21      0.00  0.00\n8/3        0.00  0.00\n\n\nWhich is a bipartite version of modularity matrix (\\(\\mathbf{\\hat{B}}\\)) with the same block structure as the bipartite adjacency matrix:\n\\[\n\\mathbf{\\hat{B}} = \\left[\n\\begin{matrix}\n\\mathbf{O}_{M \\times M} & \\mathbf{B}_{M \\times N} \\\\\n\\mathbf{B}^T_{N \\times M} & \\mathbf{O}_{N \\times N}\n\\end{matrix}\n\\right]\n\\]\nNote that in \\(\\mathbf{\\hat{B}}\\) the modularity (expected number of edges) is set to zero for nodes of the same set (people and people; groups and groups), and to non-zero values for nodes of different sets (persons and groups).\nNow, we can use the same approach we used in the unipartite case to check the modularity of some hypothetical partition of the nodes in the graph.\nTake for instance, the CA scores in the first dimension that we saw in the lecture on CA. They do seem to divide the persons and groups into distinct communities.\nSo let’s bring them back (using the CA function in the package FactoMineRand transform them into membership vectors (using dummy coding):\n\n    library(FactoMineR)\n    ca.res <- CA(A, graph = FALSE)\n    eig.vec.p <- ca.res$row$coord[, 1] #CA scores for persons in first dim.\n    eig.vec.g <- ca.res$col$coord[, 1] #CA scores for groups in first dim\n    u1 <- rep(0, n)\n    u2 <- rep(0, n)\n    d <- c(eig.vec.p, eig.vec.g) #original CA scores\n    d <- Re(d)\n    u1[which(d > 0)] <- 1\n    u2[which(d <= 0)] <- 1\n    U <- cbind(u1, u2)\n    rownames(U) <- rownames(B2)\n    U\n\n          u1 u2\nEVELYN     0  1\nLAURA      0  1\nTHERESA    0  1\nBRENDA     0  1\nCHARLOTTE  0  1\nFRANCES    0  1\nELEANOR    0  1\nPEARL      0  1\nRUTH       0  1\nVERNE      1  0\nMYRNA      1  0\nKATHERINE  1  0\nSYLVIA     1  0\nNORA       1  0\nHELEN      1  0\nDOROTHY    1  0\nOLIVIA     1  0\nFLORA      1  0\n6/27       0  1\n3/2        0  1\n4/12       0  1\n9/26       0  1\n2/25       0  1\n5/19       0  1\n3/15       0  1\n9/16       0  1\n4/8        1  0\n6/10       1  0\n2/23       1  0\n4/7        1  0\n11/21      1  0\n8/3        1  0\n\n\nRecall that we can check the modularity of a partition coded in a dummy matrix like U using the formula:\n\\[\n\\frac{tr(U^TBU)}{\\sum_i\\sum_ja_{ij}}\n\\]\nWhere \\(tr\\) is the trace matrix operation (sum of the diagonals).\nLet’s check it out:\n\n   A2 <- as.matrix(as_adjacency_matrix(g))\n   round(sum(diag(t(U) %*% B2 %*% U))/sum(A2), 3)\n\n[1] 0.318\n\n\nWhich looks pretty good!\nHere’s a plot of the bipartite graph with nodes colored by the CA induced community bipartition:\n\n\n\n\n\nSouthern Women’s bipartite graph by the best two-community partition according to CA.\n\n\n\n\n\n\n\n\nReferences\n\nBarber, Michael J. 2007. “Modularity and Community Detection in Bipartite Networks.” Physical Review E—Statistical, Nonlinear, and Soft Matter Physics 76 (6): 066102."
  },
  {
    "objectID": "tm-duality.html",
    "href": "tm-duality.html",
    "title": "The Duality of Persons and Groups",
    "section": "",
    "text": "Recall that in the one-mode case, multiplying the adjacency matrix times its transpose yields the common neighbors matrix \\(\\mathbf{M}\\):\n\\[\n\\mathbf{M} = \\mathbf{A}\\mathbf{A}^T\n\\]\nAs famously noted by Breiger (1974), doing the same for the affiliation matrix of a two-mode network also returns the common-neighbors matrix, but because objects in one mode can only connect to objects in another mode, this also reveals the duality of persons and groups: The connections between people are made up of the groups they share, and the connections between groups are revealed by the groups they share.\nThus, computing the common neighbors matrix for both persons and groups (also called the projection of the two-mode network into each of its modes) produces a one-mode similarity matrix between people and groups, where the similarities are defined by the number of objects in the other mode that they share.\nFor the people the relevant projection is:\n\\[\n\\mathbf{P} = \\mathbf{A}\\mathbf{A}^T\n\\]\nAnd for the groups:\n\\[\n\\mathbf{G} = \\mathbf{A}^T\\mathbf{A}\n\\]\nLet’s see how this works out with real data by loading our usual friend the Southern Women data:\nIn this case, the above equations yield:\nThe off-diagonal entries of these square person by person (group by group) matrices is the number of groups (people) shared by each person (group) and the diagonals are the number of memberships of each person (the size of each group/event).\nIn igraph the relevant function is called bipartite_projection. It takes a graph as an input and returns a list containing igraph graph objects of both projections by default:\nIn the graph objects produced by the bipartite_projection function, the actual shared memberships and shared members are stored as an attribute of each edge called weight used in the plotting code below to set the edge.width:\nSo to get the weighted projection matrix, we need to type:\nWe can also use the weights to draw a weighted graph network plot of people and group projections. All we have to do is set the edge.with argument to the value of the edge weight attribute in the corresponding graph:\nNote that because both G.p and G.g are weighted graphs we can calculate the weighted version of degree for both persons and groups from them (sometimes called the vertex strength).\nIn igraph we can do this using the strength function, which takes a weighted graph object as input:\nInterestingly, as noted by Faust (1997, 167), there is a (dual!) mathematical connection between the strength of each vertex in the weighted projection and the centrality of the nodes from the other set they are connected to:\nWe can verify this relationship for \\(EVELYN\\):\nWhich is indeed Evelyn’s vertex strength.\nDually, the same relation applies to groups:\nWhich is indeed the vertex strength of the event held on 6/27."
  },
  {
    "objectID": "tm-duality.html#visualizing-dual-projections-using-the-minimum-spanning-tree",
    "href": "tm-duality.html#visualizing-dual-projections-using-the-minimum-spanning-tree",
    "title": "The Duality of Persons and Groups",
    "section": "Visualizing Dual Projections Using the Minimum Spanning Tree",
    "text": "Visualizing Dual Projections Using the Minimum Spanning Tree\nAs we just saw projecting the original biadjacency matrix using Breiger’s (1974) approach results in two weighted one mode networks. Because there is an edge between two persons (groups) if they share at least one group (person) the resulting graphs tend to be dense featuring high levels of connectivity between nodes as with Figure 1. This can make it hard to discern the connectivity structure of the projected networks and detect patterns.\nOne approach to simplifying the visual projections of two-mode networks is to calculate the resulting weighted graph’s minimum spanning tree. For any weighted network, the minimum spanning tree is the graph that connects all nodes using the smallest number of edges that form a tree1, which happens to be \\(N-1\\) where \\(N\\) is the number of nodes.\nTo do this, we can follow Kruskal’s Algorithm. It goes like this:\n\nFirst, we create an edgelist containing each of the edge weights of the one-mode projection graph.\nThen we sort the edgelist by weight in increasing order (smallest weights first).\nThen, we create an undirected empty graph with \\(N\\) nodes.\nNow, we go down the edgelist adding edges to the empty graph one at a time, at each step checking that:\n\nWe are not connecting nodes that have already been connected (avoiding multiedges).\nAdditional edges do not create cycles (as the resulting graph would be no longer a tree).\n\n\nWe stop when we have the desired number of edges (\\(N-1\\)), meaning all nodes are connected in the tree.\nHere’s a function that does all of this, taking the weighted projection igraph object as input and returning the minimum spanning tree:\n\n   make.mst <- function(x) {\n      E <- data.frame(as_edgelist(x), w = E(x)$weight) #creating weighted edge list\n      E <- E[order(E$w), ] #ordering by edge weight\n      Tr <- make_empty_graph(n = vcount(x), directed = FALSE) #creating empty graph\n      V(Tr)$name <- V(x)$name\n      k <- 1\n      n.e <- ecount(Tr)\n      n.v <- vcount(Tr) - 1\n      while(n.e < n.v) {\n         i <- which(V(x)$name == E[k,1])\n         j <- which(V(x)$name == E[k,2])\n         if (are_adjacent(Tr, i, j) == 0) { #checking nodes are not adjacent\n            Tr <- add_edges(Tr, c(i,j)) #add edge\n            n.e <- ecount(Tr) #new edge id\n            if (is_acyclic(Tr) == 0) { #checking new edge does not add a cycle\n               Tr <- delete_edges(Tr, n.e) # delete edge if it adds a cycle\n               }\n            }\n         n.e <- ecount(Tr)\n         k <- k + 1\n         }\n      return(Tr)\n   }\n\nWe can now build the minimum spanning tree graph for each weighted projection:\n\n   Tr.p <- make.mst(G.p)\n   Tr.g <- make.mst(G.g)\n\nAnd here’s a point and line plot the minimum spanning tree for persons and groups in the Southern Women data (we use the layout_as_tree option in igraph):\n\nset.seed(123)\n   plot(Tr.p, \n     vertex.size=8, vertex.frame.color=\"lightgray\", \n     vertex.label.dist=2, layout=layout_as_tree,\n     vertex.label.cex = 1.5, edge.color = \"lightgray\",\n     edge.width = 3, vertex.color = cluster_leading_eigen(Tr.p)$membership)\nset.seed(123)\n   plot(Tr.g, \n     vertex.size=8, vertex.frame.color=\"lightgray\", \n     vertex.label.dist=2, layout=layout_as_tree,\n     vertex.label.cex = 1.5, edge.color = \"lightgray\",\n     edge.width = 3, vertex.color = cluster_leading_eigen(Tr.g)$membership)\n\n\n\n\n\n\n\n(a) Persons\n\n\n\n\n\n\n\n(b) Groups\n\n\n\n\nFigure 2: One mode projection MST."
  },
  {
    "objectID": "tm-fitness.html",
    "href": "tm-fitness.html",
    "title": "Fitness and Complexity in Two Mode Networks",
    "section": "",
    "text": "In a highly cited piece, Tacchella et al. (2012) introduce a new prestige metric for two-mode networks that relies on the same “prismatic” model of status distribution we considered before.\nTacchella et al. (2012) called the prestige metrics they obtained using their approach “fitness” and “complexity” because they developed in the empirical context of calculating metrics for ranking nations based on their competitive advantage in exporting products, which means analyzing a two-mode country-by-product matrix (Hidalgo and Hausmann 2009).\nHowever, when considered in the more general context of two-mode network link analysis (Fouss, Saerens, and Shimbo 2016), it is clear that their approach is a prestige metric for two-mode networks that combines ideas from Bonacich Eigenvector scoring and PageRank scoring that we covered in the two-mode prestige lecture notes."
  },
  {
    "objectID": "tm-fitness.html#fitnesscomplexity-scores",
    "href": "tm-fitness.html#fitnesscomplexity-scores",
    "title": "Fitness and Complexity in Two Mode Networks",
    "section": "Fitness/Complexity Scores",
    "text": "Fitness/Complexity Scores\nTheir basic idea is that when we are (asymmetrically) interested in determining the status or prestige of nodes in one particular mode (e.g., the row-mode nodes), we should not use summaries (e.g., sums or averages) of the scores for nodes in the other (e.g., column) mode in determining their status. Instead, we should deeply discount those nodes that connect to low-status nodes in the other end.\nTo understand what they are getting at, it helps to write down the Bonacich prestige scoring in equation form, as we go through each iteration of the two-mode status distribution game:\nIf you remember from the function tm.status, each iteration \\(q\\), the vector of status scores for the row nodes \\(\\mathbf{s}^R\\) and the column nodes \\(\\mathbf{s}^C\\) is given by:\n\\[\ns^R_i(q) = \\sum_j\\mathbf{A}_{ij}s^C_j(q-1)\n\\tag{1}\\]\n\\[\ns^C_j(q) = \\sum_i\\mathbf{A}_{ij}s^R_i(q-1)\n\\tag{2}\\]\nWhere \\(\\mathbf{A}\\) is the two-mode network’s biadjacency matrix, and with the restriction that at the initial step \\(\\mathbf{s}(0)^C = \\mathbf{1}\\) where \\(\\mathbf{1}\\) is the all ones vector of length equals to the number of columns of the biadjacency matrix \\(\\mathbf{A}\\).\nAt each iteration \\(q > 0\\) we normalize both score vectors:\n\\[\n\\mathbf{s}^R(q) = \\frac{\\mathbf{s}^R(q)}{\\left\\langle\\mathbf{s}^R(q)\\right\\rangle}\n\\tag{3}\\]\n\\[\n\\mathbf{s}^C(q) = \\frac{\\mathbf{s}^C(q)}{\\left\\langle\\mathbf{s}^C(q)\\right\\rangle}\n\\tag{4}\\]\nWhere \\(\\left\\langle \\mathbf{s} \\right\\rangle\\) is the Euclidean vector norm, and the continue iterating until the differences between the vectors across successive iterations is minimal.\nSo far, this is what we covered before. What Tacchella et al. (2012) propose is to substitute Equation 2 above with:\n\\[\n   s^C_j(q) = \\left[\\sum_i\\mathbf{A}_{ij}\\left(s^R_i(q-1)\\right)^{-1}\\right]^{-1}\n\\tag{5}\\]\nWhich means that first (inner parentheses) we take the reciprocal of the row-mode nodes’ status scores, sum them across column-mode nodes (such that column-mode nodes that connect to low-status row-mode nodes get a big score), and then take the reciprocal of the reciprocal to get back to a measure of status for column-mode nodes. This non-linear transformation heavily discounts the status scores assigned to column-mode nodes whenever they connect to lower status row-mode nodes. Tacchella et al. (2012) also change the normalization step, using the mean value of the vector in denominator of Equation 3, and Equation 4."
  },
  {
    "objectID": "tm-fitness.html#beyond-status-as-popularityactivity",
    "href": "tm-fitness.html#beyond-status-as-popularityactivity",
    "title": "Fitness and Complexity in Two Mode Networks",
    "section": "Beyond Status as Popularity/Activity",
    "text": "Beyond Status as Popularity/Activity\nHow should we understand this modification? Recall that the basic principle of standard Bonacich prestige scoring is based on the equation of status/prestige and popularity/activity. In the canonical case of persons and groups (Breiger 1974), an event receives status from being attended by high-status individuals and an individual receives status from being affiliated with a high-status event; in each case, status from the point of view of the event means having highly active members, and from the point of view of the individual it means being affiliated with popular events.\nBut status may not always work this way. Consider the world-economic network linking countries to the products they have a competitive advantage in producing (Hidalgo and Hausmann 2009). Analysts noticed that the most developed countries produce both “complex” (i.e., high-status) products that only a select few of other highly developed economies produce (like semiconductors) and also less “complex” (i.e., low-status, like extractive natural resources) products that the other less developed economics produce (Tacchella et al. 2012).\nThat means that the “complexity” (i.e., status score) of a product cannot be derived simply taking a summary (e.g., sum or average) of the status score of the countries that produce it, because high-status countries engage in both high and low-status forms of production. However, knowing that a product is produced by a low-status country is more informative (and should weigh more significantly in the determination of a product’s status score) because low-status countries only produce low-status products.\nApplying the same reasoning to the aforementioned case of persons and groups (Breiger 1974), an equivalent situation would go as follows. Imagine there is a set of prestigious women and a set of prestigious events that only the prestigious women attend. However, prestigious women are also endowed with a spirit of noblesse oblige, which means that the most prestigious of them also attend low-prestige events.\nThis means that when determining the prestige of the events it is not very informative to know that prestigious women affiliate with them; rather, we should weigh more heavily whether low-status women affiliate with an event in determining an event’s status, such that as the number of low-status women who affiliate with an event increases, a given event’s status is downgraded in a non-linear way which feeds back into the computation of each woman’s prestige.\nLet’s see how this would work in the Southern Wome data. First, we load it up:\n\n   library(networkdata)\n   library(igraph)\n   g <- southern_women\n   A <- as.matrix(as_biadjacency_matrix(g))\n\nAnd here’s a function called tm.fitness that modifies the old two-mode status distribution game function we played before—which as you recall was itself based on Kleinberg’s (1999) HITS algorithm for directed one-mode networks—to compute the fitness and complexity prestige scores for persons and groups:\n\n   tm.fitness <- function(w, iter = 1000) {\n      y <- matrix(1, ncol(w), 1) #initial group status column vector set to a constant\n      z <- t(w)\n      k <- 0\n      while (k < iter) {\n         o.y <- y \n         x <- w %*% o.y #fitness status scores for people\n         x <- x/mean(x) #normalizing new people status scores \n         y <- (z %*% x^-1)^-1 #complexity status scores for groups\n         y <- y/mean(y) #normalizing new group status scores \n         k <- k + 1\n         }\n   return(list(p.s = x, g.s = y, k = k))\n   }\n\nNote that we move to a number of iterations approach to indicate convergence (governed by the iter argument) rather than a successive differences approach, due to the non-linear nature of the algorithm’s scoring.\nWe then apply the tm.fitness function to the SW data:\n\n    fc <- tm.fitness(A, iter = 50)\n\nWe also calculate the usual Bonacich eigenvector scores using the svd function for comparison purposes:\n\n   eig <- svd(A)\n   p.s <- eig$u[, 1] * -1\n   g.s <- eig$v[, 1] * -1\n   names(p.s) <- rownames(A)\n   names(g.s) <- colnames(A)\n\nAnd we put them in a table:\n\n\nTable 1: Status Scores\n\n\n\n\n(a) Persons. \n \n  \n     \n    Bonacich \n    Fitness \n  \n \n\n  \n    EVELYN \n    0.903 \n    1.000 \n  \n  \n    LAURA \n    0.834 \n    0.842 \n  \n  \n    NORA \n    0.712 \n    0.781 \n  \n  \n    SYLVIA \n    0.748 \n    0.763 \n  \n  \n    KATHERINE \n    0.594 \n    0.746 \n  \n  \n    THERESA \n    1.000 \n    0.669 \n  \n  \n    BRENDA \n    0.845 \n    0.666 \n  \n  \n    CHARLOTTE \n    0.454 \n    0.294 \n  \n  \n    FRANCES \n    0.564 \n    0.126 \n  \n  \n    HELEN \n    0.542 \n    0.111 \n  \n  \n    MYRNA \n    0.504 \n    0.091 \n  \n  \n    ELEANOR \n    0.616 \n    0.066 \n  \n  \n    VERNE \n    0.589 \n    0.055 \n  \n  \n    RUTH \n    0.637 \n    0.050 \n  \n  \n    PEARL \n    0.486 \n    0.026 \n  \n  \n    DOROTHY \n    0.355 \n    0.007 \n  \n  \n    OLIVIA \n    0.188 \n    0.006 \n  \n  \n    FLORA \n    0.188 \n    0.006 \n  \n\n\n* Scores normalized by dividing by the maximum.\n\n\n\n\n\n(b) Groups. \n \n  \n     \n    Bonacich \n    Complexity \n  \n \n\n  \n    3/2 \n    0.297 \n    1.000 \n  \n  \n    6/27 \n    0.280 \n    0.998 \n  \n  \n    11/21 \n    0.223 \n    0.937 \n  \n  \n    8/3 \n    0.223 \n    0.937 \n  \n  \n    9/26 \n    0.347 \n    0.498 \n  \n  \n    4/12 \n    0.499 \n    0.223 \n  \n  \n    6/10 \n    0.336 \n    0.154 \n  \n  \n    4/7 \n    0.400 \n    0.088 \n  \n  \n    2/25 \n    0.635 \n    0.071 \n  \n  \n    5/19 \n    0.647 \n    0.054 \n  \n  \n    3/15 \n    0.757 \n    0.051 \n  \n  \n    9/16 \n    1.000 \n    0.014 \n  \n  \n    2/23 \n    0.177 \n    0.011 \n  \n  \n    4/8 \n    0.749 \n    0.006 \n  \n\n\n* Scores normalized by dividing by the maximum.\n\n\n\n\n\nEach table sorts persons and groups according to the fitness/complexity score. We can see that the status order changes once we introduce the fitness/complexity mode of scoring. While {Theresa} is the top person according to the usual dual Bonacich prestige score, once we heavily discount the status of events that include low-status people, {Evelyn} becomes the top person, with {Theresa} dropping to the sixth spot. In the same way while {Nora} is ranked sixth by the Bonacich prestige, her standing improves to third in the fitness scoring. Meanwhile {Brenda} was number three according to the Bonacich score, but her prestige drops to seventh place in the fitness/complexity scoring.\nThe status of groups changes even more dramatically once complexity is calculated by heavily discounting the status of groups that include lower status people. While {9/16} is the top event by the usual Bonacich prestige scoring, this event has minimal status according to the complexity scoring, ending up third from the bottom. Instead, the top even by complexity is {3/2} a relatively low-status event according to the Bonacich score. In fact, all of the other top events according to the complexity scoring, were ranked minimally by the Bonacich scoring, except for event {2/23}, which is a low-status event on both accountings. This means that the Bonacich prestige and complexity scores for events have a strong negative correlation (r = -0.63). This is different from the person ranks, which agree more closely (r = 0.77).\nTo help us make sense of the differences between the Bonacich and the fitness/complexity prestige scoring, Figure 1 (a) shows the original SW biadjacency matrix with rows and columns ordered by the degree of each node. This means that highly attended events appear in on the left, and highly active women appear on the top.\nFigure 1 gives us some insight as to why event {9/16} drops so much in the fitness/complexity rankings. While it is the most prestigious (and well attended) event by the Bonacich score (leftmost in Figure 1 (a)), this also means that it is attended by a lot of low-status women (ones at the bottom of Figure 1 (b)). The same thing happens to event {4/8} which is third according to Bonacich prestige, but dead last according to complexity, given its heavy bulk of low fitness attendees (e.g., {Verne, Ruth, Pearl, Dorothy, Olivia, Flora}).\nEvent {3/2} on the other hand, while having relatively sparse attendance (only three women), and thus low Bonacich prestige (which rewards volume), has a membership composed of exclusively high-status persons (ones at the top of Figure 1 (b): {Evelyn, Laura, Theresa}). Event {2/23} gets ranked similarly by the two measures because it has both sparse attendance and the majority (two thirds) of its attendees are low-status women ({Olivia, Flora}).\nFigure 1 also helps us make sense of the {Theresa} versus {Evelyn} contrasts across prestige rankings. Both are highly active women, but Evelyn’s membership in the top two events according to fitness/complexity {3/2, 6/27} puts her towards the top. The same goes for Laura’s membership in the same events, which move her from fourth to second in the fitness/complexity ranks. Nora’s membership in third and fourth most highly ranked events according to complexity, {11/21, 8/3}, also help her improve her fitness ranks in Figure 1 (b)."
  },
  {
    "objectID": "tm-fitness.html#an-eigenvector-based-approach",
    "href": "tm-fitness.html#an-eigenvector-based-approach",
    "title": "Fitness and Complexity in Two Mode Networks",
    "section": "An Eigenvector-Based Approach",
    "text": "An Eigenvector-Based Approach\nSciarra et al. (2020, 8–9) describe an eigenvector-based (and thus linear and non-iterative) method to derive values that are close (but not exactly) as the fitness/complexity scores. Their idea is to produce a degree-normalized biadjacency matrix that equals:\n\\[\n\\tilde{\\mathbf{A}} = \\mathbf{D}_p \\mathbf{A} \\mathbf{D'}_g\n\\]\nWhere \\(\\mathbf{D}_p\\) is the diagonal matrix of people’s degrees and \\(\\mathbf{D'}_g\\) is a diagonal matrix with each entry \\(d'_{gg}\\) in the diagonal equal to:\n\\[\nd'_{gg} = \\sum_g \\frac{A_{pg}}{k_p}\n\\]\nWhere \\(k_p = \\sum_p A_{pg}\\) is the degree of each person.\nIn R we can obtain these matrices as follows:\n\n   D.p <- diag(1/rowSums(A))\n   k.g <- colSums(A/(rowSums(A)))\n   D.g <- diag(1/k.g)\n   A.n <- D.p %*% A %*% D.g\n\nOnce we have the normalized biadjacency matrix, we compute the normalized Breiger projections:\n\\[\n\\tilde{\\mathbf{P}} = \\tilde{\\mathbf{A}}\\tilde{\\mathbf{A}}^T\n\\]\n\\[\n\\tilde{\\mathbf{G}} = \\tilde{\\mathbf{A}}^T\\tilde{\\mathbf{A}}\n\\]\nThe linear fitness/complexity scores \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are then obtained as the solution to the following, now familiar, eigenvector problem:\n\\[\n\\lambda \\mathbf{x} = \\tilde{\\mathbf{P}}\\mathbf{x}\n\\]\n\\[\n\\lambda \\mathbf{y} = \\tilde{\\mathbf{G}}\\mathbf{y}\n\\]\nWhich in R we obtain as follows:\n\n   P.n <- A.n %*% t(A.n)\n   G.n <- t(A.n) %*% A.n\n   eig.p <- abs(eigen(P.n)$vectors[,1])\n   eig.g <- abs(eigen(G.n)$vectors[,1])\n   names(eig.p) <- rownames(A)\n   names(eig.g) <- colnames(A)\n\nWe can then compare the scores for both persons and groups, with the proviso that, to maximize the correlation with their linear approximations, we normalize the fitness scores for people by dividing by the person degrees \\(k_p\\), and the complexity scores for groups by multiplying by the corresponding \\(d'_{gg}\\) score.\nWe can see that they are pretty close:\n\n   cor(fc$p.s/rowSums(A), eig.p)\n\n          [,1]\n[1,] 0.9814337\n\n   cor(fc$g.s*k.g, eig.g)\n\n          [,1]\n[1,] 0.9350304\n\n\n\n\n\n\n\n\n\n(a) Rows/Columns Reordered by Bonacich Prestige.\n\n\n\n\n\n\n\n(b) Rows/Columns Reordered by Fitness/Complexity.\n\n\n\n\n\n\n\n\n\n(c) Rows/Columns Reordered by Linearized Fitness/Complexity.\n\n\n\n\nFigure 1: Southern Women Data Biadjacency Matrix.\n\n\n\n\nTable 2: Status Scores\n\n\n\n\n(a) Persons. \n \n  \n     \n    Fitness \n    Linear Fitness \n  \n \n\n  \n    EVELYN \n    1.000 \n    0.840 \n  \n  \n    KATHERINE \n    0.994 \n    1.000 \n  \n  \n    LAURA \n    0.962 \n    0.829 \n  \n  \n    SYLVIA \n    0.872 \n    0.897 \n  \n  \n    NORA \n    0.781 \n    0.836 \n  \n  \n    BRENDA \n    0.762 \n    0.690 \n  \n  \n    THERESA \n    0.669 \n    0.623 \n  \n  \n    CHARLOTTE \n    0.589 \n    0.584 \n  \n  \n    FRANCES \n    0.253 \n    0.362 \n  \n  \n    MYRNA \n    0.183 \n    0.384 \n  \n  \n    HELEN \n    0.177 \n    0.382 \n  \n  \n    ELEANOR \n    0.132 \n    0.275 \n  \n  \n    VERNE \n    0.111 \n    0.262 \n  \n  \n    RUTH \n    0.099 \n    0.216 \n  \n  \n    PEARL \n    0.069 \n    0.196 \n  \n  \n    DOROTHY \n    0.028 \n    0.122 \n  \n  \n    OLIVIA \n    0.024 \n    0.154 \n  \n  \n    FLORA \n    0.024 \n    0.154 \n  \n\n\n* Scores normalized by dividing by the maximum.\n\n\n\n\n\n(b) Groups. \n \n  \n     \n    Complexity \n    Linear Complexity \n  \n \n\n  \n    6/27 \n    1.000 \n    0.853 \n  \n  \n    11/21 \n    0.993 \n    1.000 \n  \n  \n    8/3 \n    0.993 \n    1.000 \n  \n  \n    3/2 \n    0.958 \n    0.834 \n  \n  \n    9/26 \n    0.781 \n    0.724 \n  \n  \n    4/12 \n    0.564 \n    0.669 \n  \n  \n    6/10 \n    0.332 \n    0.704 \n  \n  \n    2/25 \n    0.266 \n    0.538 \n  \n  \n    4/7 \n    0.243 \n    0.611 \n  \n  \n    3/15 \n    0.232 \n    0.544 \n  \n  \n    5/19 \n    0.197 \n    0.531 \n  \n  \n    9/16 \n    0.104 \n    0.442 \n  \n  \n    4/8 \n    0.051 \n    0.359 \n  \n  \n    2/23 \n    0.035 \n    0.275 \n  \n\n\n* Scores normalized by dividing by the maximum.\n\n\n\n\n\nTable 2 shows the linear fitness/complexity scores compared to the non-linear ones for persons and groups, and Figure 1 (c) shows the biadjacency matrix with rows and columns re-orodered according to the linearized fitness/complexity scores, which we can see is pretty close to the non-linear version in Figure 1 (b)."
  },
  {
    "objectID": "tm-null.html",
    "href": "tm-null.html",
    "title": "Graph Ensembles in Two-Mode Networks",
    "section": "",
    "text": "As we saw in the graph ensemble lesson, there are many approaches to randomizing the structure of one-mode networks when the aim is to create graph ensembles preserving selected properties. These ensembles, in turn, can be used to do null hypothesis testing in networks.\nNot surprisingly, a similar suite of techniques exist for the two-mode case, but until recently various approaches were scattered (and reduplicated) across a bunch of literatures in social network analysis, ecology, network physics, and computer science (Neal et al. 2024)."
  },
  {
    "objectID": "tm-null.html#two-mode-erdos-renyi-model",
    "href": "tm-null.html#two-mode-erdos-renyi-model",
    "title": "Graph Ensembles in Two-Mode Networks",
    "section": "Two-Mode Erdos-Renyi Model",
    "text": "Two-Mode Erdos-Renyi Model\nLike with one-mode networks, the simplest null model for two-mode networks is one that preserves the number of nodes and the number of edges. This model, like we saw in the one-mode network case, also preserves anything that is a function of these two-quantities. In the two-mode case, this is the bipartite graph’s density and the average degrees of the nodes in each mode (recall that two-mode networks have two average degrees). This is thus a two-mode version of the Erdos-Renyi null model.\nLet’s load up the Southern Women (SW) data and see how it works:\n\n   library(igraph)\n   library(networkdata)\n   A <- as_biadjacency_matrix(southern_women)\n\nLet’s compute some basic two-mode network statistics:\n\n   d <- sum(A)/(nrow(A)*ncol(A)) #density\n   ad.p <- mean(rowSums(A)) #average degree of people\n   ad.g <- mean(colSums(A)) #average degree of groups\n   d\n\n[1] 0.3531746\n\n   ad.p\n\n[1] 4.944444\n\n   ad.g\n\n[1] 6.357143\n\n\nWe can see that the density of the SW network is \\(d=\\) 0.35, the average degree of people \\(\\bar{k_p}=\\) 4.94 and the average degree of groups \\(\\bar{k_g}=\\) 6.36.\nNow, let’s compute something on this network, like the degree correlation between people and groups, answering the question: Do people with lots of memberships tend to join larger groups?\nWe already know the answer to this question for the SW data from the two-mode network analysis lecture notes, which is negative. In this network people with a lot of memberships connect to smaller groups.\nHere’s a version of that function that takes the biadjacency matrix as input, creates the bipartite matrix from it and returns the two-mode degree correlation:\n\n   tm.deg.corr <- function(x) {\n      B <- rbind(\n                  cbind(matrix(0, nrow = nrow(x), ncol = nrow(x)), x),\n                  cbind(t(x), matrix(0, nrow = ncol(x), ncol = ncol(x)))\n                  ) #creating bipartite matrix\n      d <- data.frame(e = as.vector(B), \n                      rd = rep(rowSums(B), ncol(B)), \n                      cd = rep(colSums(B), each = nrow(B))\n                      )\n      return(cor(d[d$e == 1, ]$rd, d[d$e == 1, ]$cd))\n   }\n\n\n   tm.deg.corr(A)\n\n[1] -0.3369979\n\n\nWhich is the same number we got before.\nNow let’s see how we can generate an Erdos-Renyi two-mode network with a specified number of edges. A simple approach goes like this:\nFirst, let’s create a vectorized version of the adjacency matrix:\n\n   v.a <- as.vector(A)\n   v.a\n\n  [1] 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n [38] 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1\n [75] 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 1\n[112] 1 1 0 1 0 1 1 0 0 1 1 1 0 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0\n[149] 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0\n[186] 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0\n[223] 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0\n\n\nThese are just the biadjacency matrix entries stretched out into a long vector of length equal to the number of rows multiplied by the number of columns of the matrix (18 \\(\\times\\) 14 \\(=\\) 252).\nThen we just reshuffle the values of this vector by reassigning vector positions across the entire length of the vector at random:\n\n   v.shuff <- v.a[sample(1:length(v.a))]\n\nNow we can just generate a new biadjacency matrix A.perm from the reshuffled vector v.shuff:\n\n   A.perm <- matrix(v.shuff, nrow = nrow(A)) #creating permuted biadjacency matrix\n   rownames(A.perm) <- rownames(A)\n   colnames(A.perm) <- colnames(A)\n   A.perm\n\n          6/27 3/2 4/12 9/26 2/25 5/19 3/15 9/16 4/8 6/10 2/23 4/7 11/21 8/3\nEVELYN       0   1    1    1    0    0    1    0   1    0    1   0     0   1\nLAURA        0   0    0    0    1    0    0    1   1    1    0   0     0   0\nTHERESA      0   0    0    1    0    1    0    0   0    1    1   1     0   0\nBRENDA       1   0    0    1    1    1    1    0   0    0    0   0     0   1\nCHARLOTTE    0   0    1    1    0    0    0    1   0    0    1   1     0   1\nFRANCES      0   0    1    1    0    0    0    1   0    1    1   1     1   0\nELEANOR      1   1    1    0    0    1    0    1   1    0    0   1     0   0\nPEARL        0   1    0    1    1    1    1    0   0    0    0   1     0   0\nRUTH         0   0    0    0    0    0    0    1   0    1    0   1     0   0\nVERNE        1   0    1    0    1    1    1    0   0    1    0   1     0   0\nMYRNA        1   0    1    1    0    1    0    0   0    1    0   0     1   1\nKATHERINE    1   0    0    0    0    0    0    0   1    0    0   1     0   0\nSYLVIA       0   0    0    0    0    0    0    0   0    0    0   0     1   1\nNORA         0   0    1    0    0    0    0    0   1    1    1   0     0   0\nHELEN        1   0    0    0    0    0    0    0   0    0    0   1     1   0\nDOROTHY      0   1    0    0    0    0    0    0   0    0    0   0     1   0\nOLIVIA       0   1    1    0    0    1    1    0   0    0    1   0     0   0\nFLORA        0   0    1    1    1    0    0    0   0    1    0   1     0   0\n\n\nWe can verify that A.perm has the same basic network statistics as A:\n\n   d <- sum(A.perm)/(nrow(A.perm)*ncol(A.perm)) #density\n   ad.p <- mean(rowSums(A.perm)) #average degree of people\n   ad.g <- mean(colSums(A.perm)) #average degree of groups\n   d\n\n[1] 0.3531746\n\n   ad.p\n\n[1] 4.944444\n\n   ad.g\n\n[1] 6.357143\n\n\nBut not the same degree distributions:\n\n   rowSums(A)\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n        8         7         8         7         4         4         4         3 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n        4         4         4         6         7         8         5         2 \n   OLIVIA     FLORA \n        2         2 \n\n   rowSums(A.perm)\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n        7         4         5         6         6         7         7         6 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n        3         7         7         3         2         4         3         2 \n   OLIVIA     FLORA \n        5         5 \n\n   colSums(A)\n\n 6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10  2/23   4/7 11/21 \n    3     3     6     4     8     8    10    14    12     5     4     6     3 \n  8/3 \n    3 \n\n   colSums(A.perm)\n\n 6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10  2/23   4/7 11/21 \n    6     5     9     8     5     7     5     5     5     8     6    10     5 \n  8/3 \n    5 \n\n\nWe can now package the two-mode permutation steps into a function called tm.perm:\n\n   tm.perm <- function(x) {\n      w <- matrix(as.vector(x)[sample(1:length(v.a))], nrow = nrow(x))\n      rownames(w) <- rownames(x)\n      colnames(w) <- colnames(x)\n      return(w)\n   }\n\nAnd generate a 500 strong two-mode Erdos-Renyi graph ensemble for the SW data:\n\n   set.seed(4567)\n   G <- replicate(500, tm.perm(A), simplify = FALSE)\n\nWe can now sapply the tm.deg.corr function from before across our ensemble to get 500 degree correlations:\n\n   corrs <- sapply(G, tm.deg.corr)\n   corrs[1:100] #first hundred entries\n\n  [1] -0.344356903 -0.134295344 -0.133596070 -0.267644558 -0.171674275\n  [6] -0.275663672 -0.283254133 -0.193806406 -0.337994690 -0.212437024\n [11] -0.257590223 -0.118158698 -0.242565641 -0.166337214 -0.173132238\n [16] -0.274868693 -0.217969624 -0.216451292 -0.291344341 -0.283195324\n [21] -0.207689408 -0.248837917 -0.198871612 -0.240695741 -0.221265615\n [26] -0.185705284 -0.289478802 -0.159541270 -0.171820823 -0.186560960\n [31] -0.215368270 -0.254480771 -0.177874085 -0.151141815 -0.289125269\n [36] -0.224790425 -0.201960650 -0.270071352 -0.116296228 -0.185726631\n [41] -0.303763988 -0.259510523 -0.218542183 -0.112107029 -0.243725759\n [46] -0.005737813 -0.198127012 -0.229914646 -0.204889016 -0.079508887\n [51]  0.027440505 -0.202949007 -0.218607373 -0.025492884 -0.133046441\n [56] -0.160869565 -0.185500601 -0.213562420 -0.155718555 -0.250021152\n [61] -0.053023663 -0.137790380 -0.236702431 -0.248327843 -0.131984233\n [66] -0.205270847 -0.198270225 -0.235000000 -0.286357283 -0.144811901\n [71] -0.294190720 -0.203126837 -0.116331339 -0.253407599 -0.191774271\n [76] -0.145264282 -0.202891718 -0.184967788 -0.141816921 -0.149469250\n [81] -0.218688180 -0.218279156 -0.012786560 -0.197629515 -0.148183895\n [86] -0.138678154 -0.277539621 -0.266943121 -0.304491726 -0.118560460\n [91] -0.129700324  0.004097715 -0.252852092 -0.239662227 -0.264175504\n [96] -0.219322074 -0.201478868 -0.206365547 -0.296350444 -0.136214570\n\n\nSo let’s see how our observed value stacks up in the grand scheme:\n\n   library(ggplot2)\n   p <- ggplot(data = data.frame(round(corrs, 2)), aes(x = corrs))\n   p <- p + geom_histogram(binwidth = 0.015, stat = \"bin\", fill = \"darkblue\")\n   p <- p + geom_vline(xintercept = tm.deg.corr(A), \n                       color = \"red\", linetype = 1, linewidth = 1.5)\n   p <- p + geom_vline(xintercept = 0, linetype = 1, \n                       color = \"purple\", linewidth = 1.5)\n   p <- p + theme_minimal() + labs(x = \"Degree Correlation\", y = \"Freq.\")\n   p <- p + theme(axis.text = element_text(size = 12))\n   p <- p + annotate(\"text\", x=-0.04, y=45, label= \"Zero Point\", color = \"purple\")\n   p <- p + annotate(\"text\", x=-0.3, y=45, label= \"Obs. Value\", color = \"red\")\n   p\n\n\n\n\nIt looks like our observed value is close to the tail end of the negative spectrum, suggesting it is statistically improbable to have been obvserved by chance. We can compute the value that corresponds to the 1st percentile of the assortativity distribution from the ensemble and then see if what observe is below that value (\\(p < 0.01\\)).\n\n   quantile(corrs, probs = 0.01)\n\n        1% \n-0.3587599 \n\n   tm.deg.corr(A) < quantile(corrs, probs = 0.01)\n\n   1% \nFALSE \n\n\nWhoops. Looks like the observed value is not extreme enough using a \\(p <0.01\\) criterion of statistical significance. Let’s try a less stringent one:\n\n   quantile(corrs, probs = 0.05)\n\n        5% \n-0.3141873 \n\n   tm.deg.corr(A) < quantile(corrs, probs = 0.05)\n\n  5% \nTRUE \n\n\nAha! A test of the hypothesis that the observed value is smaller than the value at the 95th percentile of the distribution of values in this null graph ensemble returns a positive answer, suggesting that degree anti-correlation is present in the SW data, at statistically significant levels, net of density.\nAs before, if we wanted a more stringent two-tailed test we would need to create a vector with the absolute value of the two-mode degree correlation:\n\n   1 - ecdf(abs(corrs))(abs(tm.deg.corr(A)))\n\n[1] 0.032\n\n\nWhich is still statistically significant at conventional levels (\\(p <0.05\\))."
  },
  {
    "objectID": "tm-null.html#fixed-degree-models",
    "href": "tm-null.html#fixed-degree-models",
    "title": "Graph Ensembles in Two-Mode Networks",
    "section": "Fixed Degree Models",
    "text": "Fixed Degree Models\nAs we already noted, the two-mode Erdos-Renyi model fixes the number of edges (and thus the density and average degrees) in the network, but does not preserve the original degree distributions. We might want to test our hypotheses by using a two-mode graph ensemble that “controls for” the node degrees.\nHow do we do that? One complication is that we have two sets of degrees so we have more options than in the one mode case. We can fix the row (person) degree, or the column (group) degree or both degrees.\nLet’s begin with the simplest case, in which we fix either the row or column degree but not both.\n\nFixing Row Degrees\nTo fix the row degrees, we need to randomize the entries in each row of the biadjacency matrix, while preserving the number of ones in that row. One way to do this is to write a function that takes an observed row of the matrix, randomizes it and then substitutes it for the observed row:\n\n   rand.row <- function(r) {\n      return(r[sample(1:length(r))])\n      }\n\nNow we can just apply the rand.row function to each row of the biadjacency matrix A to generate a new matrix A.r:\n\n   A.r <- apply(A, 1, rand.row)\n   A.r <- t(A.r)\n   rownames(A.r) <- rownames(A)\n   colnames(A.r) <- colnames(A)\n\nHere’s the original matrix A:\n\n   A\n\n          6/27 3/2 4/12 9/26 2/25 5/19 3/15 9/16 4/8 6/10 2/23 4/7 11/21 8/3\nEVELYN       1   1    1    1    1    1    0    1   1    0    0   0     0   0\nLAURA        1   1    1    0    1    1    1    1   0    0    0   0     0   0\nTHERESA      0   1    1    1    1    1    1    1   1    0    0   0     0   0\nBRENDA       1   0    1    1    1    1    1    1   0    0    0   0     0   0\nCHARLOTTE    0   0    1    1    1    0    1    0   0    0    0   0     0   0\nFRANCES      0   0    1    0    1    1    0    1   0    0    0   0     0   0\nELEANOR      0   0    0    0    1    1    1    1   0    0    0   0     0   0\nPEARL        0   0    0    0    0    1    0    1   1    0    0   0     0   0\nRUTH         0   0    0    0    1    0    1    1   1    0    0   0     0   0\nVERNE        0   0    0    0    0    0    1    1   1    0    0   1     0   0\nMYRNA        0   0    0    0    0    0    0    1   1    1    0   1     0   0\nKATHERINE    0   0    0    0    0    0    0    1   1    1    0   1     1   1\nSYLVIA       0   0    0    0    0    0    1    1   1    1    0   1     1   1\nNORA         0   0    0    0    0    1    1    0   1    1    1   1     1   1\nHELEN        0   0    0    0    0    0    1    1   0    1    1   1     0   0\nDOROTHY      0   0    0    0    0    0    0    1   1    0    0   0     0   0\nOLIVIA       0   0    0    0    0    0    0    0   1    0    1   0     0   0\nFLORA        0   0    0    0    0    0    0    0   1    0    1   0     0   0\n\n\nAnd the reshuffled matrix A.r\n\n   A.r\n\n          6/27 3/2 4/12 9/26 2/25 5/19 3/15 9/16 4/8 6/10 2/23 4/7 11/21 8/3\nEVELYN       1   0    0    1    0    1    1    1   1    0    1   0     0   1\nLAURA        0   0    1    1    1    0    0    0   1    0    1   1     0   1\nTHERESA      1   0    1    0    0    1    0    1   1    0    1   1     0   1\nBRENDA       0   1    1    0    1    0    1    0   0    1    0   0     1   1\nCHARLOTTE    0   0    1    0    0    0    0    0   1    1    1   0     0   0\nFRANCES      0   1    0    1    0    0    1    1   0    0    0   0     0   0\nELEANOR      0   0    0    0    0    0    1    0   0    1    1   0     0   1\nPEARL        0   0    0    0    1    0    0    0   1    1    0   0     0   0\nRUTH         0   0    0    0    0    1    0    1   0    1    0   0     0   1\nVERNE        0   0    0    0    1    1    0    1   0    0    0   0     1   0\nMYRNA        1   1    0    0    0    0    1    1   0    0    0   0     0   0\nKATHERINE    0   1    1    0    0    1    0    0   0    0    1   0     1   1\nSYLVIA       1   0    0    0    1    0    0    1   1    1    1   0     0   1\nNORA         1   1    0    1    1    1    0    1   0    1    0   0     1   0\nHELEN        0   0    1    0    1    1    0    0   0    1    0   0     0   1\nDOROTHY      1   0    0    0    0    0    0    0   0    1    0   0     0   0\nOLIVIA       0   1    0    0    0    0    0    0   0    1    0   0     0   0\nFLORA        0   0    0    0    0    0    0    0   0    0    0   1     0   1\n\n\nNote that the new matrix A.r preserves the person degrees of the original:\n\n   rowSums(A)\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n        8         7         8         7         4         4         4         3 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n        4         4         4         6         7         8         5         2 \n   OLIVIA     FLORA \n        2         2 \n\n   rowSums(A.r)\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n        8         7         8         7         4         4         4         3 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n        4         4         4         6         7         8         5         2 \n   OLIVIA     FLORA \n        2         2 \n\n\nBut not the group degrees, because each person’s memberships are randomly distributed across groups:\n\n   colSums(A)\n\n 6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10  2/23   4/7 11/21 \n    3     3     6     4     8     8    10    14    12     5     4     6     3 \n  8/3 \n    3 \n\n   colSums(A.r)\n\n 6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10  2/23   4/7 11/21 \n    6     6     6     4     7     7     5     8     6    10     7     3     4 \n  8/3 \n   10 \n\n\nAll the other lower order statistics like density are preserved:\n\n   sum(A.r)/(nrow(A.r)*ncol(A.r))\n\n[1] 0.3531746\n\n\nNow we package everything into a function:\n\n   fix.deg <- function(x, mode = 1) {\n      w <- apply(x, mode, function(r) {r[sample(1:length(r))]})\n      if (mode == 1) {\n         w <- t(w)\n         }\n      rownames(w) <- rownames(x)\n      colnames(w) <- colnames(x)\n      return(w)\n      }\n\nWe then generate a graph ensemble of reshuffled matrices that preserve the person degrees, and compute our degree correlations in that set of networks:\n\n   set.seed(4567)\n   G <- replicate(500, fix.deg(A), simplify = FALSE)\n   corrs <- sapply(G, tm.deg.corr)\n\nLet’s compute the degree correlations across this ensemble and see how our observed value stacks up in the grand scheme:\n\n\n\n\n\nLooking pretty good! Let’s check the p-value:\n\n   quantile(corrs, probs = 0.05)\n\n        5% \n-0.2756015 \n\n   tm.deg.corr(A) < quantile(corrs, probs = 0.05)\n\n  5% \nTRUE \n\n   1 - ecdf(abs(corrs))(abs(tm.deg.corr(A)))\n\n[1] 0.016\n\n\nNeat! Our result can continue to be defended at the \\(p < 0.05\\) level. Still a chance of getting published.\n\n\nFixing Column Degrees\nWe can fix the column degrees using the same fix.deg function as earlier, but this time, we just change the mode argument to equal 2, to apply the function to the columns and not the rows of the matrix A.\nFor instance:\n\n   A.c <- fix.deg(A, mode = 2)\n   A.c\n\n          6/27 3/2 4/12 9/26 2/25 5/19 3/15 9/16 4/8 6/10 2/23 4/7 11/21 8/3\nEVELYN       0   1    0    1    1    1    1    0   1    0    1   1     0   0\nLAURA        0   0    1    0    0    0    1    1   1    0    0   0     0   0\nTHERESA      0   1    0    1    1    0    0    1   1    1    0   0     0   0\nBRENDA       0   0    1    0    0    1    0    1   0    0    1   1     1   0\nCHARLOTTE    0   0    0    0    0    1    0    1   1    0    0   0     0   0\nFRANCES      0   1    0    0    1    0    1    1   1    1    0   0     1   0\nELEANOR      0   0    0    0    0    0    1    0   0    1    1   0     0   0\nPEARL        0   0    0    0    0    0    0    0   1    1    0   0     0   0\nRUTH         0   0    0    0    1    0    0    1   1    0    0   0     0   0\nVERNE        1   0    1    0    1    1    1    1   1    0    0   1     0   1\nMYRNA        0   0    1    0    1    1    1    1   0    0    0   0     0   0\nKATHERINE    1   0    0    1    0    0    1    1   1    0    0   1     1   0\nSYLVIA       0   0    0    0    1    0    1    1   1    1    0   0     0   0\nNORA         1   0    1    0    1    0    1    1   0    0    0   0     0   0\nHELEN        0   0    0    0    0    1    0    1   1    0    0   1     0   1\nDOROTHY      0   0    0    0    0    1    0    0   0    0    1   0     0   0\nOLIVIA       0   0    0    0    0    1    1    1   1    0    0   0     0   0\nFLORA        0   0    1    1    0    0    0    1   0    0    0   1     0   1\n\n\nWhich generates a reshuffled adjacency matrix that preserves the group degrees:\n\n   colSums(A)\n\n 6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10  2/23   4/7 11/21 \n    3     3     6     4     8     8    10    14    12     5     4     6     3 \n  8/3 \n    3 \n\n   colSums(A.c)\n\n 6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10  2/23   4/7 11/21 \n    3     3     6     4     8     8    10    14    12     5     4     6     3 \n  8/3 \n    3 \n\n\nBut not the person degrees:\n\n   rowSums(A)\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n        8         7         8         7         4         4         4         3 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n        4         4         4         6         7         8         5         2 \n   OLIVIA     FLORA \n        2         2 \n\n   rowSums(A.c)\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n        8         4         6         6         3         7         3         2 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n        3         9         5         7         5         5         5         2 \n   OLIVIA     FLORA \n        4         5 \n\n\nAnd now we test our hypothesis that there is degree anti-correlation in the SW data on an ensemble of graphs with fixed group degrees:\n\n   set.seed(4567)\n   G <- replicate(500, fix.deg(A, mode = 2), simplify = FALSE)\n   corrs <- sapply(G, tm.deg.corr)\n\nLet’s see how things look:\n\n\n\n\n\nUh oh, adjusting for group degrees seems to have made our conclusions a bit more shaky. All of the estimated degree correlations are negative (below zero) and our observed value does not seem to be as extreme as before.\nLet’ see what the p-values say:\n\n   quantile(corrs, probs = 0.05)\n\n        5% \n-0.3316684 \n\n   tm.deg.corr(A) < quantile(corrs, probs = 0.05)\n\n  5% \nTRUE \n\n   1 - ecdf(abs(corrs))(abs(tm.deg.corr(A)))\n\n[1] 0.036\n\n\nStill significant at \\(p < 0.05\\)!\nRegardless, it is clear that our earlier conclusions from the Erdos-Renyi model were a bit too optimistic. What happens when we try to fix both the row and column degrees?\n\n\nFixing Row and Column Degrees\nTo fix row and column degrees, we play a swapping game. At each round, we select two random persons \\(a\\) and \\(b\\). Each person collects their memberships and calculates the memberships they have that the other person does not have (and vice versa). Thus, \\(a\\) has a set of memberships that \\(b\\) does not have, and \\(b\\) has a set of memberships that \\(a\\) does not have. Then they trade memberships \\(n\\) times where \\(n\\) is a number between zero and the minimum of the size of the two sets of memberships that the other person does not have. We repeat this trading game for \\(k\\) number of times.\nBelow is a function called make.swap that implements this algorithm, called “curveball” (Neal et al. 2024). The function takes the biadjacency matrix as input and repeats the swapping process described above 100 times:\n\n   make.swap <- function(x, k = 100) {\n      z <- 1 #initializing counter\n      while(z <= k) {\n         n <- sample(rownames(x), 2) #sampling two people at random\n         a <- n[1] #person a\n         b <- n[2] #person b\n         a.m <- names(which(x[a, ] == 1)) #person a's memberships\n         b.m <- names(which(x[b, ] == 1)) #person b's memberships\n         ab <- setdiff(a.m, b.m) #memberships that a has that b does not\n         ba <- setdiff(b.m, a.m) #memberships that b has that a does not\n         w <- sample(0:min(length(ab), length(ba)), size = 1) #number of rounds of trade\n         if (w > 0) {\n            for (i in 1:w) {\n               ab.m <- sample(ab, 1) #membership that a will transfer to b\n               ba.m <- sample(ba, 1) #membership that b will transfer to a\n               x[a, ab.m] <- 0 #a loses membership\n               x[b, ab.m] <- 1 #b gains membership\n               x[b, ba.m] <- 0 #b loses membership\n               x[a, ba.m] <- 1 #a gains membership\n               a.m <- names(which(x[a, ] == 1)) #updating a's memberships\n               b.m <- names(which(x[b, ] == 1)) #updating b's memberships\n               ab <- setdiff(a.m, b.m) #updating differences between a and b's memberships\n               ba <- setdiff(b.m, a.m) #updating differences between b and a's memberships\n               }\n            }\n         z <- z + 1 #incrementing counter\n         }\n      return(x)\n   }\n\nHere’s an example:\n\n   A.s <- make.swap(A)\n   A.s\n\n          6/27 3/2 4/12 9/26 2/25 5/19 3/15 9/16 4/8 6/10 2/23 4/7 11/21 8/3\nEVELYN       0   0    1    0    0    0    1    1   1    0    1   1     1   1\nLAURA        0   1    1    0    1    1    0    1   1    0    0   1     0   0\nTHERESA      1   0    0    1    0    1    1    1   1    1    0   0     1   0\nBRENDA       0   0    1    0    0    1    1    1   1    1    1   0     0   0\nCHARLOTTE    0   1    0    0    0    0    1    1   0    0    0   0     0   1\nFRANCES      0   0    1    0    1    1    0    0   1    0    0   0     0   0\nELEANOR      0   0    0    0    1    0    1    1   1    0    0   0     0   0\nPEARL        0   0    0    0    0    0    1    0   1    0    0   1     0   0\nRUTH         0   0    0    1    1    0    1    1   0    0    0   0     0   0\nVERNE        1   0    0    0    0    0    0    1   0    0    0   1     0   1\nMYRNA        0   0    0    0    0    1    1    1   0    1    0   0     0   0\nKATHERINE    0   0    1    1    0    1    1    1   1    0    0   0     0   0\nSYLVIA       0   1    1    0    1    1    0    0   1    0    1   0     1   0\nNORA         1   0    0    1    1    0    1    1   1    1    0   1     0   0\nHELEN        0   0    0    0    1    0    0    1   1    1    0   1     0   0\nDOROTHY      0   0    0    0    1    0    0    0   1    0    0   0     0   0\nOLIVIA       0   0    0    0    0    0    0    1   0    0    1   0     0   0\nFLORA        0   0    0    0    0    1    0    1   0    0    0   0     0   0\n\n\nWe can see that while the specifc entries of the swapped biadjacency matrix A.s are different from those of the original matrix A both the row and column degrees are preserved:\n\n   rowSums(A.s)\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n        8         7         8         7         4         4         4         3 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n        4         4         4         6         7         8         5         2 \n   OLIVIA     FLORA \n        2         2 \n\n   rowSums(A)\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n        8         7         8         7         4         4         4         3 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n        4         4         4         6         7         8         5         2 \n   OLIVIA     FLORA \n        2         2 \n\n   colSums(A.s)\n\n 6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10  2/23   4/7 11/21 \n    3     3     6     4     8     8    10    14    12     5     4     6     3 \n  8/3 \n    3 \n\n   colSums(A)\n\n 6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10  2/23   4/7 11/21 \n    3     3     6     4     8     8    10    14    12     5     4     6     3 \n  8/3 \n    3 \n\n\nAnd now we create our graph ensemble with fixed row and column degrees, and test our degree anti-correlation hypothesis:\n\n   set.seed(4567)\n   G <- replicate(500, make.swap(A), simplify = FALSE)\n   corrs <- sapply(G, tm.deg.corr)\n\nPlotting the results gives us:\n\n\n\n\n\nLike before, all of the estimated degree correlations are below zero, and also like before, the observed value does not look as extreme as with the Erdos-Renyi or fixed degree model for either rows or columns separately.\nLet’s check the p-value at \\(p < 0.05\\):\n\n   quantile(corrs, probs = 0.05)\n\n        5% \n-0.3425058 \n\n   tm.deg.corr(A) < quantile(corrs, probs = 0.05)\n\n   5% \nFALSE \n\n\nNo longer significant! What’s the actual p-value?\n\n   1 - ecdf(abs(corrs))(abs(tm.deg.corr(A)))\n\n[1] 0.062\n\n\nAs suspected, the two-tailed test indicates that the hypothesis of degree anti-correlation is no longer justified in these data even using a criterion of \\(p < 0.05\\).\nLooks like our paper won’t be published after all :("
  },
  {
    "objectID": "tm-prestige.html",
    "href": "tm-prestige.html",
    "title": "Status and Prestige in Two Mode Networks",
    "section": "",
    "text": "We can use the same “prismatic” mode of status distribution to rank nodes in two-mode networks that we used in the standard one-mode case. The difference is that in the one-mode case you gain status by connecting to entities of the same “type” as you (e.g., other people). In the two-mode case, your status reflects the status of the “other type” entities that you connect to. And vice versa.\nFor instance, if the two entities are persons and groups (Breiger 1974), then people gain status by connecting to high status groups and groups gain status by connecting to high-status people. In other words, people distribute status points to the groups they belong to and groups distribute status points to the people that belong to them.\nWhen it comes to eigenvector-style measures, the neat idea is that people are central if they belong to central groups and groups and central if their members are central people (with people centrality defined by membership in central groups) can be effectively captured by these metrics (Bonacich 1991).\nFor this reason, measures of status and prestige are particularly applicable to two-mode networks. The reason is that the reflective principle behind these measures interacts nicely with the duality principle."
  },
  {
    "objectID": "tm-prestige.html#two-mode-bonacich-prestige",
    "href": "tm-prestige.html#two-mode-bonacich-prestige",
    "title": "Status and Prestige in Two Mode Networks",
    "section": "Two-Mode Bonacich Prestige",
    "text": "Two-Mode Bonacich Prestige\nLet’s see an example using the classic Southern Women dataset. We can load it from the trusty networkdata package, and extract the bi-adjacency matrix from the igraph object:\n\n   library(networkdata)\n   library(igraph)\n   g <- southern_women\n   A <- as.matrix(as_biadjacency_matrix(g))\n\nBelow is a quick function that plays a two-mode version of the status distribution game that we described in the status and prestige lesson, and which is really just a modification of the one-mode HITS algorithm:\n\n   tm.status <- function(w, z) {\n      y <- matrix(1/ncol(w), ncol(w), 1) #initial group status column vector set to a constant\n      delta <- 1 \n      k <- 0\n      while (delta > 1e-10) {\n         o.y <- y #old group status scores\n         x <- w %*% o.y #new people scores a function of people matrix and old group scores \n         x <- x/norm(x, type = \"E\") #normalizing new people status scores \n         y <- z %*% x #new group scores a function of group matrix and new people scores\n         y <- y/norm(y, type = \"E\") #normalizing new group status scores \n         if (k > 1) {\n            delta <- abs(sum(abs(y) - abs(o.y))) #diff. between new and old group status scores\n            }\n         k <- k + 1\n         }\n   return(list(p.s = x, g.s = y))\n   }\n\nThe function takes the two-mode biadjacency matrix \\(\\mathbf{A}\\) and its transpose \\(\\mathbf{A}^T\\) as input and returns a vector of row-mode nodes (e.g., persons) scores p.s, a vector of column-mode (e.g., groups) status scores g.s.\nThe function works like this:\n\nLine 2 initializes the original status scores for each group stored in the y object, which are just set to \\(|G|^{-1}\\) where \\(|G|\\) is the number of groups (the number of columns of the bi-adjacency matrix \\(\\mathbf{A}\\)).\nLine 3 initializes the \\(\\delta\\) value, which determines when the while loop stops. Then, inside the while loop starting on line 5, we assign status scores to the people equal to the sum of the status scores of the groups they belong to in line 7, which is just \\(\\mathbf{A}\\) (w) post-multiplied by the group status vector \\(\\mathbf{y}\\), then we normalize the people status score vector in line 8 using the Euclidean vector norm.\nIn line 9 we calculate the new group status vector (y), which, for each group, is given by the sum of the status scores of the people that belong them that we computed in line 7 (the transpose of the matrix z post-multiplied by the vector x).\nFinally, we normalize the new group status scores in line 10 and compute the difference between these new group scores and the old one to calculate \\(\\delta\\). When \\(\\delta\\) is small (\\(\\delta \\leq 10^{-10}\\)) the while loop stops as the old and new scores have achieved convergence.\n\nWe can now use the tm.status function to estimate the status scores for people and groups:\n\n   tm.hits <- tm.status(A, t(A))\n\nHere are the scores for the people:\n\n   round(tm.hits$p.s/max(tm.hits$p.s), 3)\n\n           [,1]\nEVELYN    0.903\nLAURA     0.834\nTHERESA   1.000\nBRENDA    0.845\nCHARLOTTE 0.454\nFRANCES   0.564\nELEANOR   0.616\nPEARL     0.486\nRUTH      0.637\nVERNE     0.589\nMYRNA     0.504\nKATHERINE 0.594\nSYLVIA    0.748\nNORA      0.712\nHELEN     0.542\nDOROTHY   0.355\nOLIVIA    0.188\nFLORA     0.188\n\n\nAnd for the groups:\n\n   round(tm.hits$g.s/max(tm.hits$g.s), 3)\n\n       [,1]\n6/27  0.280\n3/2   0.297\n4/12  0.499\n9/26  0.347\n2/25  0.635\n5/19  0.647\n3/15  0.757\n9/16  1.000\n4/8   0.749\n6/10  0.336\n2/23  0.177\n4/7   0.400\n11/21 0.223\n8/3   0.223\n\n\nHere, we can see that Theresa is the top person (attending the most central events) closely followed by Evelyn, with Flora and Nora toward the bottom. The event held at 9/16 is the top event (attended by the most central people).\nWhen we use the usual affiliation matrix A and its transpose t(A) as inputs, the tm.status function implements the biHITS algorithm described in Liao et al. (2014). One thing to note in this regard is that the biHITS algorithm is just a rediscovery of the two-mode prestige scoring described much earlier by Bonacich (1991).\n\nThe Dual Projection Approach\nInterestingly as Bonacich (1991) noted in that paper, the eigenvector status scores can also be obtained by playing the one-mode version of the status game over the Breiger-style one-mode projections of the two-mode network.\n\n   status1 <- function(w) {\n      x <- rep(1, nrow(w)) #initial status vector set to all ones of length equal to the number of nodes\n      d <- 1 #initial delta\n      k <- 0 #initializing counter\n      while (d > 1e-10) {\n          o.x <- x #old status scores\n          x <- w %*% o.x #new scores a function of old scores and adjacency matrix\n          x <- x/norm(x, type = \"E\") #normalizing new status scores\n          d <- abs(sum(abs(x) - abs(o.x))) #delta between new and old scores\n          k <- k + 1 #incrementing while counter\n      }\n   return(as.vector(x))\n   }\n\nTo see this, let’s play the usual status game defined by the function status1 over the one mode projections P and G. For people this would be:\n\n   P <- A %*% t(A)\n   p.s <- status1(P)\n   names(p.s) <- rownames(A)\n   round(p.s/max(p.s), 3)\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n    0.903     0.834     1.000     0.845     0.454     0.564     0.616     0.486 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n    0.637     0.589     0.504     0.594     0.748     0.712     0.542     0.355 \n   OLIVIA     FLORA \n    0.188     0.188 \n\n\nAnd for groups:\n\n   G <- t(A) %*% A \n   g.s <- status1(G)\n   names(g.s) <- colnames(A)\n   round(g.s/max(g.s), 3)\n\n 6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10  2/23   4/7 11/21 \n0.280 0.297 0.499 0.347 0.635 0.647 0.757 1.000 0.749 0.336 0.177 0.400 0.223 \n  8/3 \n0.223 \n\n\nLo and behold, these are the same status scores we obtained via the biHITS approach.\n\n\nThe Eigenvector Approach\nOf course, these scores are usually called two-mode “Eigenvector Centralities” because they can be obtained by solving the linear equation system:\n\\[\n\\lambda \\mathbf{x} = \\mathbf{A}\\mathbf{y}\n\\]\n\\[\n\\lambda \\mathbf{y} = \\mathbf{A}^T\\mathbf{x}\n\\]\nWhich says that the status scores of persons \\(\\mathbf{x}\\) are just the sums of the status scores of the groups each person is connected to, and the status scores of groups \\(\\mathbf{y}\\) are just the sum of the scores of the people that join them.\nJust like we saw in the discussion of prestige scoring for one-mode networks, this is the usual cat chasing its own tail circular equation problem. To know the person status scores \\(\\mathbf{x}\\) we need to know the group status scores \\(\\mathbf{y}\\) and vice versa.\nThis is also a classic eigenvector/eigenvaue problem, which means that we can find the person and groups status scores using the eigendecomposition approach.\nSince we have a rectangular biadjacency matrix, we can proceed in two ways. We can obtain the eigenvector/eigenvalue pairs via singular value decomposition of the biadjacency matrix itself, or via standard eigendecomposition of each of its projections separately.\nLet’s see the first approach in action first, using the R function svd:\n\n   eig <- svd(A) #singular value decomposition of biadjacency matrix\n   eig.p <- eig$u[, 1] *-1  #vector of person prestige scores\n   eig.g <- eig$v[, 1] *-1 #vector of group prestige scores\n   names(eig.p) <- rownames(A)\n   names(eig.g) <- colnames(A)\n\nAnd for the big reveal:\n\n   round(eig.p/max(eig.p), 3)\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n    0.903     0.834     1.000     0.845     0.454     0.564     0.616     0.486 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n    0.637     0.589     0.504     0.594     0.748     0.712     0.542     0.355 \n   OLIVIA     FLORA \n    0.188     0.188 \n\n   round(eig.g/max(eig.g), 3)\n\n 6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10  2/23   4/7 11/21 \n0.280 0.297 0.499 0.347 0.635 0.647 0.757 1.000 0.749 0.336 0.177 0.400 0.223 \n  8/3 \n0.223 \n\n\nWhich are the same scores we obtained earlier.\nFor the second approach, as Bonacich (1991) also noted, it turns out the person and group prestige scores can be computed by figuring out the leading eigenvector of the Breiger (1974) projection matrices:\n\\[\n\\lambda \\mathbf{x} = (\\mathbf{A}\\mathbf{A}^T)\\mathbf{x}\n\\]\n\\[\n\\lambda \\mathbf{y} = (\\mathbf{A}^T\\mathbf{A})\\mathbf{y}\n\\]\nIn R we can do this using the eigen function:\n\n   eig.p <- eigen(P)\n   eig.g <- eigen(G)\n   p.s <- eig.p$vector[, 1] * -1\n   g.s <- eig.g$vector[, 1] * -1\n   names(p.s) <- rownames(A)\n   names(g.s) <- colnames(A)\n\nAnd for the big reveal:\n\n   round(p.s/max(p.s), 3)\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n    0.903     0.834     1.000     0.845     0.454     0.564     0.616     0.486 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n    0.637     0.589     0.504     0.594     0.748     0.712     0.542     0.355 \n   OLIVIA     FLORA \n    0.188     0.188 \n\n   round(g.s/max(g.s), 3)\n\n 6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10  2/23   4/7 11/21 \n0.280 0.297 0.499 0.347 0.635 0.647 0.757 1.000 0.749 0.336 0.177 0.400 0.223 \n  8/3 \n0.223 \n\n\nNeat! These are the same scores we obtained by playing our status game on the affiliation matrix (biHITS) using tm.status, playing the same game on the one-mode projection (Bonacich) using the status1 function, or using svd to obtain the leading eigenvector of the biadjacency matrix.\nThe scores are also readily interpretable: The most central people belong to the most central (largest membership) groups and the most central groups are the ones that attract the most central (highest activity) members.\n\n\nThe Duality of Bonacich Prestige\nAnother way of thinking of the eigenvector status score of each node in this context is as a weighted sum of the eigenvector centralities on the nodes in the other mode they are connected to (Faust 1997, 170).1\nFor instance the unnormalized eigenvector status scores for persons and groups are:\n\n   p.s\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n0.3347340 0.3092247 0.3705636 0.3130088 0.1682137 0.2089673 0.2283461 0.1800298 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n0.2360402 0.2183986 0.1867900 0.2202912 0.2771746 0.2639043 0.2006669 0.1314354 \n   OLIVIA     FLORA \n0.0695706 0.0695706 \n\n   g.s\n\n      6/27        3/2       4/12       9/26       2/25       5/19       3/15 \n0.14194312 0.15047999 0.25285306 0.17599173 0.32173361 0.32761921 0.38350293 \n      9/16        4/8       6/10       2/23        4/7      11/21        8/3 \n0.50663271 0.37949240 0.17040087 0.08954622 0.20279505 0.11293095 0.11293095 \n\n\nSo for any person, let’s say {EVELYN}, their eigenvector centrality is equal to:\n\n   sum(A[\"EVELYN\", ] * g.s) * 1/sqrt(eig.p$values[1])\n\n[1] 0.334734\n\n\nWhich is indeed Evelyn’s Eigenvector score.\nThe same goes for the eigenvector score of groups, which are just a weighted sum of the Eigenvector centralities of the people who belong to them:\n\n   sum(A[, \"6/27\"] * p.s) * 1/sqrt(eig.p$values[1])\n\n[1] 0.1419431\n\n\nWhich is indeed the eigenvector score for the event held on 6/27.\nDuality at work!"
  },
  {
    "objectID": "tm-prestige.html#two-mode-pagerank",
    "href": "tm-prestige.html#two-mode-pagerank",
    "title": "Status and Prestige in Two Mode Networks",
    "section": "Two-Mode PageRank",
    "text": "Two-Mode PageRank\nLike in the case of the usual eigenvector approach to calculating status, the distribution game in the function tm.status assumes that persons and groups distribute the same amount of status regardless of their own degree (which you recall for people is the number of memberships and for groups is the number of members).\nBut just like with regular PageRank for one-mode networks, we can change this assumption in the two-mode case by presuming that people get more status points to distribute when they belong to exclusive (smaller) groups, and groups get more status to distribute when their members are more selective in their affiliation behavior (they have a smaller number of memberships).\nTo do that, we create new versions of the affiliation matrix \\(\\mathbf{A}\\) and its transpose \\(\\mathbf{A}^T\\) normalized by the group and persons degrees (respectively).\nTo do this, imagine that \\(\\mathbf{D}_p\\) is a matrix containing the inverse of the degrees of each person (number of memberships) along the diagonals. This matrix can be created in R as follows:\n\n   D.p <- diag(1/rowSums(A))\n   round(D.p, 2)\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]\n [1,] 0.12 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.00  0.00  0.00  0.00\n [2,] 0.00 0.14 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.00  0.00  0.00  0.00\n [3,] 0.00 0.00 0.12 0.00 0.00 0.00 0.00 0.00 0.00  0.00  0.00  0.00  0.00\n [4,] 0.00 0.00 0.00 0.14 0.00 0.00 0.00 0.00 0.00  0.00  0.00  0.00  0.00\n [5,] 0.00 0.00 0.00 0.00 0.25 0.00 0.00 0.00 0.00  0.00  0.00  0.00  0.00\n [6,] 0.00 0.00 0.00 0.00 0.00 0.25 0.00 0.00 0.00  0.00  0.00  0.00  0.00\n [7,] 0.00 0.00 0.00 0.00 0.00 0.00 0.25 0.00 0.00  0.00  0.00  0.00  0.00\n [8,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.33 0.00  0.00  0.00  0.00  0.00\n [9,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.25  0.00  0.00  0.00  0.00\n[10,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.25  0.00  0.00  0.00\n[11,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.00  0.25  0.00  0.00\n[12,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.00  0.00  0.17  0.00\n[13,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.00  0.00  0.00  0.14\n[14,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.00  0.00  0.00  0.00\n[15,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.00  0.00  0.00  0.00\n[16,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.00  0.00  0.00  0.00\n[17,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.00  0.00  0.00  0.00\n[18,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.00  0.00  0.00  0.00\n      [,14] [,15] [,16] [,17] [,18]\n [1,]  0.00   0.0   0.0   0.0   0.0\n [2,]  0.00   0.0   0.0   0.0   0.0\n [3,]  0.00   0.0   0.0   0.0   0.0\n [4,]  0.00   0.0   0.0   0.0   0.0\n [5,]  0.00   0.0   0.0   0.0   0.0\n [6,]  0.00   0.0   0.0   0.0   0.0\n [7,]  0.00   0.0   0.0   0.0   0.0\n [8,]  0.00   0.0   0.0   0.0   0.0\n [9,]  0.00   0.0   0.0   0.0   0.0\n[10,]  0.00   0.0   0.0   0.0   0.0\n[11,]  0.00   0.0   0.0   0.0   0.0\n[12,]  0.00   0.0   0.0   0.0   0.0\n[13,]  0.00   0.0   0.0   0.0   0.0\n[14,]  0.12   0.0   0.0   0.0   0.0\n[15,]  0.00   0.2   0.0   0.0   0.0\n[16,]  0.00   0.0   0.5   0.0   0.0\n[17,]  0.00   0.0   0.0   0.5   0.0\n[18,]  0.00   0.0   0.0   0.0   0.5\n\n\nRecalling that the row sums of \\(\\mathbf{A}\\) give us the person degrees.\nAnd then we do the same for groups by creating a matrix \\(\\mathbf{D}_g\\) containing the inverse of the degrees of each group (number of members) along the diagonals.\n\n   D.g <- diag(1/colSums(A))\n   round(D.g, 2)\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]\n [1,] 0.33 0.00 0.00 0.00 0.00 0.00  0.0 0.00 0.00   0.0  0.00  0.00  0.00\n [2,] 0.00 0.33 0.00 0.00 0.00 0.00  0.0 0.00 0.00   0.0  0.00  0.00  0.00\n [3,] 0.00 0.00 0.17 0.00 0.00 0.00  0.0 0.00 0.00   0.0  0.00  0.00  0.00\n [4,] 0.00 0.00 0.00 0.25 0.00 0.00  0.0 0.00 0.00   0.0  0.00  0.00  0.00\n [5,] 0.00 0.00 0.00 0.00 0.12 0.00  0.0 0.00 0.00   0.0  0.00  0.00  0.00\n [6,] 0.00 0.00 0.00 0.00 0.00 0.12  0.0 0.00 0.00   0.0  0.00  0.00  0.00\n [7,] 0.00 0.00 0.00 0.00 0.00 0.00  0.1 0.00 0.00   0.0  0.00  0.00  0.00\n [8,] 0.00 0.00 0.00 0.00 0.00 0.00  0.0 0.07 0.00   0.0  0.00  0.00  0.00\n [9,] 0.00 0.00 0.00 0.00 0.00 0.00  0.0 0.00 0.08   0.0  0.00  0.00  0.00\n[10,] 0.00 0.00 0.00 0.00 0.00 0.00  0.0 0.00 0.00   0.2  0.00  0.00  0.00\n[11,] 0.00 0.00 0.00 0.00 0.00 0.00  0.0 0.00 0.00   0.0  0.25  0.00  0.00\n[12,] 0.00 0.00 0.00 0.00 0.00 0.00  0.0 0.00 0.00   0.0  0.00  0.17  0.00\n[13,] 0.00 0.00 0.00 0.00 0.00 0.00  0.0 0.00 0.00   0.0  0.00  0.00  0.33\n[14,] 0.00 0.00 0.00 0.00 0.00 0.00  0.0 0.00 0.00   0.0  0.00  0.00  0.00\n      [,14]\n [1,]  0.00\n [2,]  0.00\n [3,]  0.00\n [4,]  0.00\n [5,]  0.00\n [6,]  0.00\n [7,]  0.00\n [8,]  0.00\n [9,]  0.00\n[10,]  0.00\n[11,]  0.00\n[12,]  0.00\n[13,]  0.00\n[14,]  0.33\n\n\nRecalling that the column sums of \\(\\mathbf{A}\\) give us the group degrees.\nNow we can construct degree-normalized versions of the original bi-adjacency matrix and its transpose like this:\n\n   P.n <- A %*% D.g \n   colnames(P.n) <- colnames(A)\n   G.n <- t(A) %*% D.p\n   colnames(G.n) <- rownames(A)\n\nLet’s see what’s in the matrix P.n:\n\n   round(P.n, 2)\n\n          6/27  3/2 4/12 9/26 2/25 5/19 3/15 9/16  4/8 6/10 2/23  4/7 11/21\nEVELYN    0.33 0.33 0.17 0.25 0.12 0.12  0.0 0.07 0.08  0.0 0.00 0.00  0.00\nLAURA     0.33 0.33 0.17 0.00 0.12 0.12  0.1 0.07 0.00  0.0 0.00 0.00  0.00\nTHERESA   0.00 0.33 0.17 0.25 0.12 0.12  0.1 0.07 0.08  0.0 0.00 0.00  0.00\nBRENDA    0.33 0.00 0.17 0.25 0.12 0.12  0.1 0.07 0.00  0.0 0.00 0.00  0.00\nCHARLOTTE 0.00 0.00 0.17 0.25 0.12 0.00  0.1 0.00 0.00  0.0 0.00 0.00  0.00\nFRANCES   0.00 0.00 0.17 0.00 0.12 0.12  0.0 0.07 0.00  0.0 0.00 0.00  0.00\nELEANOR   0.00 0.00 0.00 0.00 0.12 0.12  0.1 0.07 0.00  0.0 0.00 0.00  0.00\nPEARL     0.00 0.00 0.00 0.00 0.00 0.12  0.0 0.07 0.08  0.0 0.00 0.00  0.00\nRUTH      0.00 0.00 0.00 0.00 0.12 0.00  0.1 0.07 0.08  0.0 0.00 0.00  0.00\nVERNE     0.00 0.00 0.00 0.00 0.00 0.00  0.1 0.07 0.08  0.0 0.00 0.17  0.00\nMYRNA     0.00 0.00 0.00 0.00 0.00 0.00  0.0 0.07 0.08  0.2 0.00 0.17  0.00\nKATHERINE 0.00 0.00 0.00 0.00 0.00 0.00  0.0 0.07 0.08  0.2 0.00 0.17  0.33\nSYLVIA    0.00 0.00 0.00 0.00 0.00 0.00  0.1 0.07 0.08  0.2 0.00 0.17  0.33\nNORA      0.00 0.00 0.00 0.00 0.00 0.12  0.1 0.00 0.08  0.2 0.25 0.17  0.33\nHELEN     0.00 0.00 0.00 0.00 0.00 0.00  0.1 0.07 0.00  0.2 0.25 0.17  0.00\nDOROTHY   0.00 0.00 0.00 0.00 0.00 0.00  0.0 0.07 0.08  0.0 0.00 0.00  0.00\nOLIVIA    0.00 0.00 0.00 0.00 0.00 0.00  0.0 0.00 0.08  0.0 0.25 0.00  0.00\nFLORA     0.00 0.00 0.00 0.00 0.00 0.00  0.0 0.00 0.08  0.0 0.25 0.00  0.00\n           8/3\nEVELYN    0.00\nLAURA     0.00\nTHERESA   0.00\nBRENDA    0.00\nCHARLOTTE 0.00\nFRANCES   0.00\nELEANOR   0.00\nPEARL     0.00\nRUTH      0.00\nVERNE     0.00\nMYRNA     0.00\nKATHERINE 0.33\nSYLVIA    0.33\nNORA      0.33\nHELEN     0.00\nDOROTHY   0.00\nOLIVIA    0.00\nFLORA     0.00\n\n\nThe P.n matrix is of the same dimensions as the original affiliation matrix A. However, the \\({ij}^{th}\\) cell is now equal to \\(1/k_j\\) where \\(k_j\\) is the number of members of group \\(j\\). That means that if group \\(j\\) has a lot of members then the \\({ij}^{th}\\) cell will contain a small number (as in column in the non-zero entries in the column corresponding to event 9-16) but when the group is small, the \\({ij}^{th}\\) cell will contain a bigger number (as in the non-zero entries in the column corresponding to event 11-21). This means the people who belong to smaller groups will have more centrality points to distribute.\nThe same goes for groups, as we can see by checking out the G.n matrix:\n\n   round(G.n, 2)\n\n      EVELYN LAURA THERESA BRENDA CHARLOTTE FRANCES ELEANOR PEARL RUTH VERNE\n6/27    0.12  0.14    0.00   0.14      0.00    0.00    0.00  0.00 0.00  0.00\n3/2     0.12  0.14    0.12   0.00      0.00    0.00    0.00  0.00 0.00  0.00\n4/12    0.12  0.14    0.12   0.14      0.25    0.25    0.00  0.00 0.00  0.00\n9/26    0.12  0.00    0.12   0.14      0.25    0.00    0.00  0.00 0.00  0.00\n2/25    0.12  0.14    0.12   0.14      0.25    0.25    0.25  0.00 0.25  0.00\n5/19    0.12  0.14    0.12   0.14      0.00    0.25    0.25  0.33 0.00  0.00\n3/15    0.00  0.14    0.12   0.14      0.25    0.00    0.25  0.00 0.25  0.25\n9/16    0.12  0.14    0.12   0.14      0.00    0.25    0.25  0.33 0.25  0.25\n4/8     0.12  0.00    0.12   0.00      0.00    0.00    0.00  0.33 0.25  0.25\n6/10    0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00 0.00  0.00\n2/23    0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00 0.00  0.00\n4/7     0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00 0.00  0.25\n11/21   0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00 0.00  0.00\n8/3     0.00  0.00    0.00   0.00      0.00    0.00    0.00  0.00 0.00  0.00\n      MYRNA KATHERINE SYLVIA NORA HELEN DOROTHY OLIVIA FLORA\n6/27   0.00      0.00   0.00 0.00   0.0     0.0    0.0   0.0\n3/2    0.00      0.00   0.00 0.00   0.0     0.0    0.0   0.0\n4/12   0.00      0.00   0.00 0.00   0.0     0.0    0.0   0.0\n9/26   0.00      0.00   0.00 0.00   0.0     0.0    0.0   0.0\n2/25   0.00      0.00   0.00 0.00   0.0     0.0    0.0   0.0\n5/19   0.00      0.00   0.00 0.12   0.0     0.0    0.0   0.0\n3/15   0.00      0.00   0.14 0.12   0.2     0.0    0.0   0.0\n9/16   0.25      0.17   0.14 0.00   0.2     0.5    0.0   0.0\n4/8    0.25      0.17   0.14 0.12   0.0     0.5    0.5   0.5\n6/10   0.25      0.17   0.14 0.12   0.2     0.0    0.0   0.0\n2/23   0.00      0.00   0.00 0.12   0.2     0.0    0.5   0.5\n4/7    0.25      0.17   0.14 0.12   0.2     0.0    0.0   0.0\n11/21  0.00      0.17   0.14 0.12   0.0     0.0    0.0   0.0\n8/3    0.00      0.17   0.14 0.12   0.0     0.0    0.0   0.0\n\n\nNote that both matrices P.n and G.n are column stochastic which means that its columns sum to one. We can check for this property as follows:\n\n   colSums(P.n)\n\n 6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10  2/23   4/7 11/21 \n    1     1     1     1     1     1     1     1     1     1     1     1     1 \n  8/3 \n    1 \n\n   colSums(G.n)\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n        1         1         1         1         1         1         1         1 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n        1         1         1         1         1         1         1         1 \n   OLIVIA     FLORA \n        1         1 \n\n\nSo now, to calculate the two-mode PageRank status score, we just play our two-mode status game on these matrices:\n\n   tm.pr <- tm.status(P.n, G.n)\n\nNote that because the bi-adjacency matrices that go into the tm.status function are already normalized, we can set the norm argument of the function to FALSE.\nHere are the resulting scores for the people:\n\n   round(tm.pr$p.s/max(tm.pr$p.s), 3)\n\n           [,1]\nEVELYN    1.000\nLAURA     0.875\nTHERESA   1.000\nBRENDA    0.875\nCHARLOTTE 0.500\nFRANCES   0.500\nELEANOR   0.500\nPEARL     0.375\nRUTH      0.500\nVERNE     0.500\nMYRNA     0.500\nKATHERINE 0.750\nSYLVIA    0.875\nNORA      1.000\nHELEN     0.625\nDOROTHY   0.250\nOLIVIA    0.250\nFLORA     0.250\n\n\nAnd for the groups:\n\n   round(tm.pr$g.s/max(tm.pr$g.s), 3)\n\n       [,1]\n6/27  0.214\n3/2   0.214\n4/12  0.429\n9/26  0.286\n2/25  0.571\n5/19  0.571\n3/15  0.714\n9/16  1.000\n4/8   0.857\n6/10  0.357\n2/23  0.286\n4/7   0.429\n11/21 0.214\n8/3   0.214\n\n\nThe resulting scores are the same ones returned by the “co-HITS” algorithm of Deng, Lyu, and King (2009).\nOn this ranking, the top persons are Theresa and Nora (who wasn’t as highly ranked according to the Bonacich prestige) and Dorothy, Olivia, and Flora are the least prestigious. The top ranked event is the one that took place on 9/16.\nWe can of course obtain the same two-mode PageRank scores using different approaches. One alternative is to use the “PageRank” projections and play the regular (one-mode) status game on them:\n\n   P.pr <- P.n %*% G.n\n   G.pr <- G.n %*% P.n\n   pr.p <- status1(P.pr)\n   pr.g <- status1(G.pr)\n   names(pr.p) <- rownames(A)\n   names(pr.g) <- colnames(A)\n   round(pr.p/max(pr.p), 3)\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n    1.000     0.875     1.000     0.875     0.500     0.500     0.500     0.375 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n    0.500     0.500     0.500     0.750     0.875     1.000     0.625     0.250 \n   OLIVIA     FLORA \n    0.250     0.250 \n\n   round(pr.p/max(pr.g), 3)\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n    0.680     0.595     0.680     0.595     0.340     0.340     0.340     0.255 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n    0.340     0.340     0.340     0.510     0.595     0.680     0.425     0.170 \n   OLIVIA     FLORA \n    0.170     0.170 \n\n\nNeat!\n\nThe Eigenvector Approach for Two-Mode PageRank\nAnother approach is to compute the leading eigenvector of the PageRank normalized projections using the eigen function:\n\n   pr.p <- abs(Re(eigen(P.pr)$vectors[, 1]))\n   names(pr.p) <- rownames(A)\n   round(pr.p/max(pr.p), 3)\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n    1.000     0.875     1.000     0.875     0.500     0.500     0.500     0.375 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n    0.500     0.500     0.500     0.750     0.875     1.000     0.625     0.250 \n   OLIVIA     FLORA \n    0.250     0.250 \n\n   pr.g <- abs(Re(eigen(G.pr)$vectors[, 1]))\n   names(pr.g) <- colnames(A)\n   round(pr.g/max(pr.g), 3)\n\n 6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10  2/23   4/7 11/21 \n0.214 0.214 0.429 0.286 0.571 0.571 0.714 1.000 0.857 0.357 0.286 0.429 0.214 \n  8/3 \n0.214 \n\n\nWhich gives us the same solutions as before.\n\n\nDouble Normalized Two-Mode PageRank\nRecall that the main reason for using the PageRank normalization in the two-mode case (and preferring the PageRank scoring over the eigenvector scoring) is to give more status points to discerning people, so that people who belong to more exclusive clubs distribute more centrality in the system. So to do that we divided each person’s centrality points according to the size of the group they belong to, so that evenly gets 0.33 centrality points for attending event 6/27 because that event was attended by only three people, which is more than she gets for attending event 4/8 which was attended by twelve people (and the same for events).\nHowever, the PageRank centrality rankings can still be affected by the sheer number of events attended by a person (or the sheer number of members of a group) even if that person is not particularly selective. Thus, a person who attends a whole bunch of not-very selective (well-attended) events still contributes as much centrality to the system as a person who attends fewer but more selective events.\nTo do that, it would be useful to come up with versions of P.n and G.n that normalize (for persons) by the exclusivity of the groups they attend and by the number of events they go to, so as not to give an advantage to those who attend a lot of events. In the same way, it would be also be useful normalize (for groups) by the discernment of the people who are its members and by their number of members they go to, so as not to give an advantage to well-attended events.\nIn the case of P.n we can do that by pre-multiplying this matrix, which already adjusts for group size by D.p which is the diagonal matrix containing each person’s number of memberships:\n\n   P.n2 <- D.p %*% P.n\n   rownames(P.n2) <- rownames(A)\n\nLet’s see what’s inside the new P.n2 matrix:\n\n   round(P.n2, 2)\n\n          6/27  3/2 4/12 9/26 2/25 5/19 3/15 9/16  4/8 6/10 2/23  4/7 11/21\nEVELYN    0.04 0.04 0.02 0.03 0.02 0.02 0.00 0.01 0.01 0.00 0.00 0.00  0.00\nLAURA     0.05 0.05 0.02 0.00 0.02 0.02 0.01 0.01 0.00 0.00 0.00 0.00  0.00\nTHERESA   0.00 0.04 0.02 0.03 0.02 0.02 0.01 0.01 0.01 0.00 0.00 0.00  0.00\nBRENDA    0.05 0.00 0.02 0.04 0.02 0.02 0.01 0.01 0.00 0.00 0.00 0.00  0.00\nCHARLOTTE 0.00 0.00 0.04 0.06 0.03 0.00 0.03 0.00 0.00 0.00 0.00 0.00  0.00\nFRANCES   0.00 0.00 0.04 0.00 0.03 0.03 0.00 0.02 0.00 0.00 0.00 0.00  0.00\nELEANOR   0.00 0.00 0.00 0.00 0.03 0.03 0.03 0.02 0.00 0.00 0.00 0.00  0.00\nPEARL     0.00 0.00 0.00 0.00 0.00 0.04 0.00 0.02 0.03 0.00 0.00 0.00  0.00\nRUTH      0.00 0.00 0.00 0.00 0.03 0.00 0.03 0.02 0.02 0.00 0.00 0.00  0.00\nVERNE     0.00 0.00 0.00 0.00 0.00 0.00 0.03 0.02 0.02 0.00 0.00 0.04  0.00\nMYRNA     0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.02 0.02 0.05 0.00 0.04  0.00\nKATHERINE 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.01 0.03 0.00 0.03  0.06\nSYLVIA    0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.01 0.01 0.03 0.00 0.02  0.05\nNORA      0.00 0.00 0.00 0.00 0.00 0.02 0.01 0.00 0.01 0.03 0.03 0.02  0.04\nHELEN     0.00 0.00 0.00 0.00 0.00 0.00 0.02 0.01 0.00 0.04 0.05 0.03  0.00\nDOROTHY   0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.04 0.04 0.00 0.00 0.00  0.00\nOLIVIA    0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.04 0.00 0.12 0.00  0.00\nFLORA     0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.04 0.00 0.12 0.00  0.00\n           8/3\nEVELYN    0.00\nLAURA     0.00\nTHERESA   0.00\nBRENDA    0.00\nCHARLOTTE 0.00\nFRANCES   0.00\nELEANOR   0.00\nPEARL     0.00\nRUTH      0.00\nVERNE     0.00\nMYRNA     0.00\nKATHERINE 0.06\nSYLVIA    0.05\nNORA      0.04\nHELEN     0.00\nDOROTHY   0.00\nOLIVIA    0.00\nFLORA     0.00\n\n\nWe can see that now, people with lots of memberships (like Evelyn) get their centrality points reduced relative to people with not that many memberships like Charlotte.\nThis is easy to see if we compare the row sums of the two matrices, which give us the total amount of centrality points each person has to distribute:\n\n   rowSums(P.n)\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n1.4880952 1.2547619 1.2547619 1.1714286 0.6416667 0.4880952 0.4214286 0.2797619 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n0.3797619 0.4214286 0.5214286 1.1880952 1.2880952 1.5916667 0.7880952 0.1547619 \n   OLIVIA     FLORA \n0.3333333 0.3333333 \n\n   rowSums(P.n2)\n\n    EVELYN      LAURA    THERESA     BRENDA  CHARLOTTE    FRANCES    ELEANOR \n0.18601190 0.17925170 0.15684524 0.16734694 0.16041667 0.12202381 0.10535714 \n     PEARL       RUTH      VERNE      MYRNA  KATHERINE     SYLVIA       NORA \n0.09325397 0.09494048 0.10535714 0.13035714 0.19801587 0.18401361 0.19895833 \n     HELEN    DOROTHY     OLIVIA      FLORA \n0.15761905 0.07738095 0.16666667 0.16666667 \n\n\nNote that while in the original P.n matrix Evelyn has way more centrality points to distribute than Charlotte (a ratio of 2.32), this discrepancy is much smaller in the new P.n2 double-normalized matrix (a ratio of 1.16).\nNow to compute our double-normalized PageRank status scores, we just play the status game on this new matrix and its transpose:\n\n   tm.bgrm <- tm.status(P.n2, t(P.n2))\n\nWhich implements the “BGRM” algorithm of Rui et al. (2007, 589, eq. 8).2\nHere are the resulting scores for the people:\n\n   round(tm.bgrm$p.s/max(tm.bgrm$p.s), 3)\n\n           [,1]\nEVELYN    0.081\nLAURA     0.061\nTHERESA   0.081\nBRENDA    0.059\nCHARLOTTE 0.055\nFRANCES   0.050\nELEANOR   0.057\nPEARL     0.116\nRUTH      0.100\nVERNE     0.141\nMYRNA     0.188\nKATHERINE 0.235\nSYLVIA    0.212\nNORA      0.407\nHELEN     0.471\nDOROTHY   0.145\nOLIVIA    1.000\nFLORA     1.000\n\n\nAnd for the groups:\n\n   round(tm.bgrm$g.s/max(tm.bgrm$g.s), 3)\n\n       [,1]\n6/27  0.032\n3/2   0.034\n4/12  0.037\n9/26  0.037\n2/25  0.045\n5/19  0.067\n3/15  0.102\n9/16  0.111\n4/8   0.396\n6/10  0.183\n2/23  1.000\n4/7   0.173\n11/21 0.140\n8/3   0.140\n\n\nWe can see that the most highly ranked women are now Olivia and Flora (in contrast to Evelyn, Theresa, and Nora in the standard PageRank scores), who used to be toward the bottom when using the traditional Bonacich prestige or PageRank scoring. In the same way, the top event is now the one held on 2/23, which wasn’t particularly distinguished using the other ranking algorithms.\nA summary of the three ranking models goes as follows:\n\n\n\n\nPerson Prestige Scores Ordered by Degree\n \n  \n      \n    hits \n    pr \n    bgrm \n    k \n  \n \n\n  \n    EVELYN \n    0.903 \n    1.000 \n    0.081 \n    8 \n  \n  \n    THERESA \n    1.000 \n    1.000 \n    0.081 \n    8 \n  \n  \n    NORA \n    0.712 \n    1.000 \n    0.407 \n    8 \n  \n  \n    LAURA \n    0.834 \n    0.875 \n    0.061 \n    7 \n  \n  \n    BRENDA \n    0.845 \n    0.875 \n    0.059 \n    7 \n  \n  \n    SYLVIA \n    0.748 \n    0.875 \n    0.212 \n    7 \n  \n  \n    KATHERINE \n    0.594 \n    0.750 \n    0.235 \n    6 \n  \n  \n    HELEN \n    0.542 \n    0.625 \n    0.471 \n    5 \n  \n  \n    CHARLOTTE \n    0.454 \n    0.500 \n    0.055 \n    4 \n  \n  \n    FRANCES \n    0.564 \n    0.500 \n    0.050 \n    4 \n  \n  \n    ELEANOR \n    0.616 \n    0.500 \n    0.057 \n    4 \n  \n  \n    RUTH \n    0.637 \n    0.500 \n    0.100 \n    4 \n  \n  \n    VERNE \n    0.589 \n    0.500 \n    0.141 \n    4 \n  \n  \n    MYRNA \n    0.504 \n    0.500 \n    0.188 \n    4 \n  \n  \n    PEARL \n    0.486 \n    0.375 \n    0.116 \n    3 \n  \n  \n    DOROTHY \n    0.355 \n    0.250 \n    0.145 \n    2 \n  \n  \n    OLIVIA \n    0.188 \n    0.250 \n    1.000 \n    2 \n  \n  \n    FLORA \n    0.188 \n    0.250 \n    1.000 \n    2 \n  \n\n\n\n\n\n\n\nGroup Prestige Scores Ordered by Degree\n \n  \n      \n    hits \n    pr \n    bgrm \n    k \n  \n \n\n  \n    9/16 \n    1.000 \n    1.000 \n    0.111 \n    14 \n  \n  \n    4/8 \n    0.749 \n    0.857 \n    0.396 \n    12 \n  \n  \n    3/15 \n    0.757 \n    0.714 \n    0.102 \n    10 \n  \n  \n    2/25 \n    0.635 \n    0.571 \n    0.045 \n    8 \n  \n  \n    5/19 \n    0.647 \n    0.571 \n    0.067 \n    8 \n  \n  \n    4/12 \n    0.499 \n    0.429 \n    0.037 \n    6 \n  \n  \n    4/7 \n    0.400 \n    0.429 \n    0.173 \n    6 \n  \n  \n    6/10 \n    0.336 \n    0.357 \n    0.183 \n    5 \n  \n  \n    9/26 \n    0.347 \n    0.286 \n    0.037 \n    4 \n  \n  \n    2/23 \n    0.177 \n    0.286 \n    1.000 \n    4 \n  \n  \n    6/27 \n    0.280 \n    0.214 \n    0.032 \n    3 \n  \n  \n    3/2 \n    0.297 \n    0.214 \n    0.034 \n    3 \n  \n  \n    11/21 \n    0.223 \n    0.214 \n    0.140 \n    3 \n  \n  \n    8/3 \n    0.223 \n    0.214 \n    0.140 \n    3"
  },
  {
    "objectID": "tm-reflections.html",
    "href": "tm-reflections.html",
    "title": "The Method of Reflections in Two-Mode Networks",
    "section": "",
    "text": "In a highly cited piece, (Hidalgo and Hausmann 2009) (HH) motivated what they saw at the time as a novel approach to assigning meaningful scores to nodes in a two-mode network, using what they called a “reflective” approach. HH’s original empirical application was to the two-mode country-by-product networks. Hence, they referred to their approach as a way to extract the “economic complexity” of countries in the world system (and dually the complexity of given products).\nOf course, just as with the fitness/complexity approach, there is no logical connection between the formal method of iterative finding scores for nodes in two-mode networks and the particular case of economic trade since the approach proposed can be used to analyze any two-mode data matrix. As will be obvious, the HH approach belongs to the extensive line of approaches to derive prestige scores for nodes in two-mode networks that we have reviewed, which included Bonacich and PageRank-style approaches.\nHow does the HH reflective scoring work? Let’s define a two-mode network composed of a set of people \\(P\\) and their affiliation relations to a set of groups \\(G\\) can be represented by an affiliation matrix \\(\\mathbf{A}\\) of dimensions \\(|P| \\times |G|\\) with people along the rows and groups across the columns, where \\(|P|\\) is the cardinality of the people set and \\(|G|\\) is the cardinality of the group set, with cell entries \\(a_{pg}= 1\\) if person \\(p\\) is affiliated with group \\(g\\) and \\(a_{pg}= 0\\) otherwise.\nGiven this, the degree-centrality of people is given by:\n\\[\n    C^R_p(1) = \\sum_g a_{pg}\n    \\label{eq:R1_p}\n\\tag{1}\\]\nAnd for groups:\n\\[\n   C^R_g(1) = \\sum_p a_{pg}\n  \\label{eq:R1_g}\n\\tag{2}\\]\nThat is, the first-order centrality of people is the row sum of the entries in the affiliation matrix \\(\\mathbf{A}\\), and the column sums of the same matrix give the first-order centrality of groups.\nAs noted by Hidalgo and Hausmann (2009), the key to the reflective approach to computing the “complexity” of countries and products is the observation that, once we have these first-order quantities, it is possible to compute second-order quantities \\(C^R(2)\\) for both people and groups using the averaged first-order centralities of the entities in the other mode they are connected to.\nFor people, these are given by:\n\\[\n    C^R_p(2) = \\frac{1}{C^R_p(1)}\\sum_g a_{pg}C^R_g(1)\n    \\label{eq:R2_p}\n\\tag{3}\\]\nAnd for groups:\n\\[\n   C^R_g(2) = \\frac{1}{C^R_g(1)}\\sum_p a_{pg}C^R_p(1)\n    \\label{eq:R2_g}\n\\tag{4}\\]\nWhile Equation Equation 1 assigns a high value to people who belong to a lot of groups, Equation 3 assigns a high value to people who, on average, belong to large groups (e.g., whenever \\(a_{pg*= 1\\) and \\(C^R_g(1)\\) is a big number). In the same way, while Equation 2 assigns a high value to groups that have lots of members, Equation 4 assigns a high value to groups that, on average, have members who themselves have lots of memberships (e.g., whenever \\(a_{pg*= 1\\) and \\(C^R_p(1)\\) is a big number).\nOf course, we can keep on going and define “third-order” reflections; for the people, these are given by:\n\\[\n   C^R_p(3) = \\frac{1}{C^R_p(1)}\\sum_g a_{pg}C^R_g(2)\n   \\label{eq:R3_p}\n\\tag{5}\\]\nAnd for groups:\n\\[\n   C^R_g(3) = \\frac{1}{C^R_g(1)}\\sum_p a_{pg}C^R_p(2)\n   \\label{eq:R3_g}\n\\tag{6}\\]\nAs we saw, while for people, Equation 3 measured the average size of the groups they join, Equation 5 assigns a high value to people who, on average, belong to groups who are themselves attended by highly active members (e.g., whenever \\(a_{pg*= 1\\) and \\(C^R_g(2)\\) is a big number).\nIn the same way, while Equation 4 assigns a high value to groups whose members have lots of memberships, Equation 6 assigns a high value to groups that, on average, have members who themselves (also on average) belong to large groups (e.g., \\(a_{pg*= 1\\) and \\(C^R_p(2)\\) is a big number).\nNote that for people, the even-numbered reflection \\(C^R_p(2)\\) assigns scores based on a formal feature of the groups they belong to (in this case, the group sizes). On the other hand, the odd-numbered reflection \\(C^R_p(3)\\) assigns scores based on a formal feature of the members of the groups they belong to (in this case, the average size of the groups they belong to).\nIn the same way, for the groups, the even-numbered reflection \\(C^R_g(2)\\) assigns scores based on a formal feature of the people who belong to them (in this case, their activity). On the other hand, the odd-numbered reflection \\(C^R_g(3)\\) assigns scores based on a formal feature of the other groups their members belong to(in this case, their average group size).\nWhile these are distinct metrics in principle, in practice, the ordering of the nodes in each mode ends up being identical across even and odd scores after their rank ordering “freezes” past a given number of iterations (proportional to the network size).\nMore generally, Hidalgo and Hausmann (2009) show that we can define a series of reflective quantities for people and groups (whose verbal and substantive interpretation becomes more complex as the number of iterations increases).\nFor people, these are given by:\n\\[\n    C^R_p(q) = \\frac{1}{C^R_p(1)}\\sum_g a_{pg}C^R_g(q-1)\n   \\label{eq:Rq_p}\n\\tag{7}\\]\nAnd for groups:\n\\[\n   C^R_g(q) = \\frac{1}{C^R_g(1)}\\sum_p a_{pg}C^R_p(q-1)\n   \\label{eq:Rq_g}\n\\tag{8}\\]\nEquation 7 says that the reflective score of a person pat iteration qis the sum of the reflective scores the groups they belong to at the \\(q-1\\) iteration (\\(C^R_g(q-1)\\)) divided by their number of memberships \\(C^{R}_p(1)\\). Equation 8 says that the \\(q^{th}\\) group reflective score is the sum of the reflective scores of their members at the \\(q-1\\) iteration \\(C^R_p(q-1)\\), divided by the number of group members \\(C^R_g(1)\\)."
  },
  {
    "objectID": "tm-reflections.html#method-of-reflections-in-the-southern-women-data",
    "href": "tm-reflections.html#method-of-reflections-in-the-southern-women-data",
    "title": "The Method of Reflections in Two-Mode Networks",
    "section": "Method of Reflections in the Southern Women Data",
    "text": "Method of Reflections in the Southern Women Data\nLet’s see how this would work in a real two-mode network. To do that, we load up our trusty Southern Women (SW) data:\n\n   library(networkdata)\n   library(igraph)\n   g <- southern_women\n   A <- as.matrix(as_biadjacency_matrix(g))\n\nNow, we need a function that does all of the reflections that we just spoke about. Here’s one that accomplishes that:\n\n    reflections <- function(x, iter = 20) { #x is a matrix with people as rows and groups as columns iters is number of reflections\n    p <- nrow(x)\n    g <- ncol(x)\n    p.c <- matrix(0, p, iter) #initialize person centralities\n    g.c <- matrix(0, g, iter) #initialize group centralities trajectory matrix\n    rownames(p.c) <- rownames(x)\n    rownames(g.c) <- colnames(x)\n    colnames(p.c) <- paste(\"Ref_\", c(1:iter), sep = \"\")\n    colnames(g.c) <- paste(\"Ref_\", c(1:iter), sep = \"\")\n    p.c[, 1] <- rowSums(x) #person degree centrality \n    g.c[, 1] <- colSums(x) #group degree centrality \n    k <- 1 #initializing counter\n    while (k < iter) {\n        m <- k + 1\n        for(i in 1:p) {\n            p.c[i, m] <- sum(x[i, ] * g.c[, k]) * (1/p.c[i, 1]) #assign person average centrality of groups they belong to\n            } #end person loop\n        for(j in 1:g) {\n            g.c[j, m] <- sum(x[, j] * p.c[, k]) * (1/g.c[j, 1]) #assign group average centrality of people in the group\n            } #end group loop\n        k <- k + 1 #increase counter\n        } #end while loop\n    return(list(p.c = p.c, g.c = g.c))\n    } #end function\n\nThe function above takes the biadjacency matrix \\(\\mathbf{A}\\) as input and returns two objects:\n\nA matrix p.c with people as the rows and the as many columns as there are reflections, containing the reflective scores for persons in each column.\nA matrix g.c with groups as the rows and the as many columns as there are reflections, containing the reflective scores for groups in each column.\n\nAnd let’s try it out:\n\n    ref.res <- reflections(A)\n\nLet’s check out the person scores at the 18th reflection:\n\n    ref.res$p.c[, 18]\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n 8.237399  8.237471  8.237136  8.237493  8.237705  8.237390  8.236873  8.236036 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n 8.236263  8.235172  8.234456  8.234045  8.234358  8.234379  8.234508  8.235424 \n   OLIVIA     FLORA \n 8.234058  8.234058 \n\n\nThis all look like slightly different versions of the same number! The reason for this is as Hidalgo and Hausmann (2009) note, is that the scores begin to converge as the number of reflections increase. To get noticeable differences, we need to scale the higher reflection scores using the standard normalization:\n\\[\n\\tilde{C}^R =\\frac{C^R-\\bar{C}^R}{\\sigma_{C^R}}\n\\tag{9}\\]\nWhere \\(\\bar{C}^R\\) is the mean reflective score and \\(\\sigma_{C^R}\\) is the standard deviation of the reflective scores. The resulting scores will have a mean of zero and variance of 1.0.\nIn R we can do this using the native scale function:\n\n    scale(ref.res$p.c[, 18])\n\n                [,1]\nEVELYN     1.1235228\nLAURA      1.1743094\nTHERESA    0.9398214\nBRENDA     1.1895381\nCHARLOTTE  1.3377969\nFRANCES    1.1176566\nELEANOR    0.7563908\nPEARL      0.1715174\nRUTH       0.3304853\nVERNE     -0.4321409\nMYRNA     -0.9320071\nKATHERINE -1.2190721\nSYLVIA    -1.0001263\nNORA      -0.9855713\nHELEN     -0.8958773\nDOROTHY   -0.2556010\nOLIVIA    -1.2103213\nFLORA     -1.2103213\nattr(,\"scaled:center\")\n[1] 8.23579\nattr(,\"scaled:scale\")\n[1] 0.001431558\n\n\nBetter!\nRecall also that the Hidalgo and Hausmann (2009) reflections iterate between odd and even scores. We want only one of those. So let’s choose the even columns for persons and groups:\n\n    HH.p <- scale(ref.res$p.c[, seq(2, 20, by = 2)])\n    HH.g <- scale(ref.res$g.c[, seq(2, 20, by = 2)])\n\nAnd now we can put them in fancy tables, as in Table 1:\n\n\nTable 1: Reflective Scores\n\n\n\n\n(a) Persons. \n \n  \n      \n    Ref_2 \n    Ref_4 \n    Ref_6 \n    Ref_8 \n    Ref_10 \n    Ref_12 \n    Ref_14 \n    Ref_16 \n    Ref_18 \n    Ref_20 \n  \n \n\n  \n    CHARLOTTE \n    -0.94 \n    -1.03 \n    -0.73 \n    0.27 \n    1.03 \n    1.26 \n    1.32 \n    1.33 \n    1.34 \n    1.34 \n  \n  \n    BRENDA \n    -0.62 \n    -0.62 \n    -0.31 \n    0.47 \n    1.00 \n    1.15 \n    1.18 \n    1.19 \n    1.19 \n    1.19 \n  \n  \n    LAURA \n    -0.70 \n    -0.57 \n    -0.19 \n    0.57 \n    1.03 \n    1.14 \n    1.17 \n    1.17 \n    1.17 \n    1.17 \n  \n  \n    EVELYN \n    -0.80 \n    -0.58 \n    -0.18 \n    0.58 \n    1.02 \n    1.11 \n    1.13 \n    1.13 \n    1.12 \n    1.12 \n  \n  \n    FRANCES \n    0.17 \n    0.27 \n    0.51 \n    0.94 \n    1.11 \n    1.12 \n    1.12 \n    1.12 \n    1.12 \n    1.12 \n  \n  \n    THERESA \n    -0.31 \n    -0.21 \n    0.05 \n    0.58 \n    0.88 \n    0.93 \n    0.94 \n    0.94 \n    0.94 \n    0.94 \n  \n  \n    ELEANOR \n    0.73 \n    0.75 \n    0.79 \n    0.86 \n    0.80 \n    0.76 \n    0.75 \n    0.76 \n    0.76 \n    0.76 \n  \n  \n    RUTH \n    1.28 \n    1.21 \n    1.11 \n    0.85 \n    0.51 \n    0.38 \n    0.34 \n    0.33 \n    0.33 \n    0.33 \n  \n  \n    PEARL \n    1.47 \n    1.55 \n    1.53 \n    1.08 \n    0.50 \n    0.27 \n    0.20 \n    0.18 \n    0.17 \n    0.17 \n  \n  \n    DOROTHY \n    2.40 \n    2.22 \n    2.00 \n    1.13 \n    0.22 \n    -0.12 \n    -0.22 \n    -0.25 \n    -0.26 \n    -0.26 \n  \n  \n    VERNE \n    1.01 \n    0.96 \n    0.68 \n    0.07 \n    -0.34 \n    -0.44 \n    -0.44 \n    -0.44 \n    -0.43 \n    -0.43 \n  \n  \n    HELEN \n    -0.49 \n    -0.57 \n    -0.74 \n    -0.99 \n    -0.99 \n    -0.94 \n    -0.91 \n    -0.90 \n    -0.90 \n    -0.89 \n  \n  \n    MYRNA \n    0.31 \n    0.40 \n    0.07 \n    -0.63 \n    -0.97 \n    -1.00 \n    -0.97 \n    -0.95 \n    -0.93 \n    -0.92 \n  \n  \n    NORA \n    -1.28 \n    -1.38 \n    -1.62 \n    -1.68 \n    -1.34 \n    -1.12 \n    -1.04 \n    -1.00 \n    -0.99 \n    -0.98 \n  \n  \n    SYLVIA \n    -0.62 \n    -0.92 \n    -1.34 \n    -1.67 \n    -1.43 \n    -1.20 \n    -1.08 \n    -1.03 \n    -1.00 \n    -0.99 \n  \n  \n    KATHERINE \n    -0.84 \n    -1.08 \n    -1.55 \n    -1.94 \n    -1.69 \n    -1.44 \n    -1.31 \n    -1.25 \n    -1.22 \n    -1.20 \n  \n  \n    OLIVIA \n    -0.38 \n    -0.20 \n    -0.05 \n    -0.25 \n    -0.67 \n    -0.94 \n    -1.09 \n    -1.17 \n    -1.21 \n    -1.23 \n  \n  \n    FLORA \n    -0.38 \n    -0.20 \n    -0.05 \n    -0.25 \n    -0.67 \n    -0.94 \n    -1.09 \n    -1.17 \n    -1.21 \n    -1.23 \n  \n\n\n* Scores normalized to unit variance.\n\n\n\n\n\n\n\n(b) Groups. \n \n  \n      \n    Ref_2 \n    Ref_4 \n    Ref_6 \n    Ref_8 \n    Ref_10 \n    Ref_12 \n    Ref_14 \n    Ref_16 \n    Ref_18 \n    Ref_20 \n  \n \n\n  \n    6/27 \n    1.24 \n    1.24 \n    1.17 \n    1.14 \n    1.13 \n    1.13 \n    1.13 \n    1.13 \n    1.13 \n    1.13 \n  \n  \n    9/26 \n    0.64 \n    0.96 \n    1.09 \n    1.11 \n    1.11 \n    1.11 \n    1.11 \n    1.11 \n    1.11 \n    1.11 \n  \n  \n    4/12 \n    0.22 \n    0.88 \n    1.07 \n    1.11 \n    1.11 \n    1.11 \n    1.11 \n    1.11 \n    1.11 \n    1.11 \n  \n  \n    3/2 \n    1.59 \n    1.10 \n    1.02 \n    1.02 \n    1.03 \n    1.03 \n    1.04 \n    1.04 \n    1.04 \n    1.04 \n  \n  \n    2/25 \n    -0.38 \n    0.43 \n    0.81 \n    0.92 \n    0.95 \n    0.95 \n    0.95 \n    0.95 \n    0.95 \n    0.95 \n  \n  \n    5/19 \n    0.00 \n    0.30 \n    0.52 \n    0.60 \n    0.62 \n    0.63 \n    0.63 \n    0.63 \n    0.63 \n    0.63 \n  \n  \n    3/15 \n    -0.33 \n    -0.06 \n    0.13 \n    0.18 \n    0.18 \n    0.18 \n    0.17 \n    0.17 \n    0.17 \n    0.17 \n  \n  \n    9/16 \n    -0.93 \n    -0.39 \n    -0.05 \n    0.05 \n    0.07 \n    0.08 \n    0.08 \n    0.08 \n    0.07 \n    0.07 \n  \n  \n    4/8 \n    -1.33 \n    -1.19 \n    -0.87 \n    -0.67 \n    -0.58 \n    -0.54 \n    -0.51 \n    -0.50 \n    -0.50 \n    -0.50 \n  \n  \n    4/7 \n    -0.47 \n    -0.65 \n    -0.81 \n    -0.91 \n    -0.96 \n    -0.99 \n    -1.01 \n    -1.01 \n    -1.02 \n    -1.02 \n  \n  \n    6/10 \n    -0.13 \n    -0.47 \n    -0.80 \n    -0.97 \n    -1.05 \n    -1.09 \n    -1.10 \n    -1.11 \n    -1.12 \n    -1.12 \n  \n  \n    11/21 \n    0.90 \n    0.15 \n    -0.56 \n    -0.91 \n    -1.05 \n    -1.12 \n    -1.15 \n    -1.16 \n    -1.17 \n    -1.18 \n  \n  \n    8/3 \n    0.90 \n    0.15 \n    -0.56 \n    -0.91 \n    -1.05 \n    -1.12 \n    -1.15 \n    -1.16 \n    -1.17 \n    -1.18 \n  \n  \n    2/23 \n    -1.93 \n    -2.45 \n    -2.15 \n    -1.76 \n    -1.51 \n    -1.38 \n    -1.30 \n    -1.27 \n    -1.25 \n    -1.24 \n  \n\n\n* Scores normalized to unit variance.\n\n\n\n\n\nWe can see in Table 1 that in higher order reflections, the scores have converged to a steady value. Some of persons/groups receive big positive scores, while others receive negative scores, suggestive of strong differentiation in HH reflective scores for the SW data.\nA neat way to visualize data like that in Table 1 is using a bump chart. We can do this in R using the program ggbump. First, we need to transform the data in Table 1 to “long” format. We use dplyr and tidyr to do that:\n\n    library(dplyr)\n    library(tidyr)\n     HH.p.long <- data.frame(HH.p) %>% \n          mutate(person = rownames(HH.p)) %>% \n          pivot_longer(\n               cols = 1:10,\n               names_to = \"Ref\",\n               values_to = \"k\"\n          ) %>%\n       mutate(Ref = factor(Ref, ordered = TRUE, levels = colnames(HH.p))) \n    head(HH.p.long)\n\n# A tibble: 6 × 3\n  person Ref         k\n  <chr>  <ord>   <dbl>\n1 EVELYN Ref_2  -0.799\n2 EVELYN Ref_4  -0.580\n3 EVELYN Ref_6  -0.178\n4 EVELYN Ref_8   0.585\n5 EVELYN Ref_10  1.02 \n6 EVELYN Ref_12  1.11 \n\n     HH.g.long <- data.frame(HH.g) %>% \n          mutate(group = rownames(HH.g)) %>% \n          pivot_longer(\n               cols = 1:10,\n               names_to = \"Ref\",\n               values_to = \"k\"\n          ) %>%\n       mutate(Ref = factor(Ref, ordered = TRUE, levels = colnames(HH.g))) \n    head(HH.g.long)\n\n# A tibble: 6 × 3\n  group Ref        k\n  <chr> <ord>  <dbl>\n1 6/27  Ref_2   1.24\n2 6/27  Ref_4   1.24\n3 6/27  Ref_6   1.17\n4 6/27  Ref_8   1.14\n5 6/27  Ref_10  1.13\n6 6/27  Ref_12  1.13\n\n\nNow we can plot the persons and groups in their reflective score trajectory across the even reflections. The result looks like Figure 1.\n\n\n\n\n\n\n\n(a) Persons\n\n\n\n\n\n\n\n\n\n(b) Groups\n\n\n\n\nFigure 1: Reflections\n\n\nAs Figure 1 (a) shows, {Dorothy} begins pretty high on the list according to the second reflection (the average size of the groups she belongs to is big) but loses that advantage as we move to further reflections, ending up closer to the bottom of the middle. Meanwhile, {Charlotte, Brenda} begin with pretty low scores on the second reflection, but end up at the top by the 20th.\nAs Figure 1 (b) shows, there’s less volatility across reflections in the group ranks, but there is still some non-trivial reshuffling by the end."
  },
  {
    "objectID": "tm-similarity.html",
    "href": "tm-similarity.html",
    "title": "Similarity and Equivalence in Two-Mode Networks",
    "section": "",
    "text": "Note that the one-mode projections can be considered unnormalized similarity matrices just like in the case of regular networks. That means that if we have the degrees of nodes in each mode, we can transform this matrix into any of the normalized vertex similarity metrics we discussed before, including Jaccard, Cosine, Dice, LHN, and so on.\nLet’s see how this would work in our trusty Southern Women dataset:\n\n   library(igraph)\n   library(networkdata)\n   g <- southern_women\n   A <- as_biadjacency_matrix(g)\n\nRepackaging our vertex similarity function for the two-mode case, we have:\n\n   vertex.sim <- function(x) {\n      A <- as.matrix(as_biadjacency_matrix(x))\n      M <- nrow(A) #number of persons\n      N <- ncol(A) #number of groups\n      p.d <- rowSums(A) #person degrees\n      g.d <- colSums(A) #group degrees\n      P <- A %*% t(A) #person projection\n      G <- t(A) %*% A #group projection\n      J.p <- diag(1, M, M)\n      J.g <- diag(1, N, N)\n      C.p <- diag(1, M, M)\n      C.g <- diag(1, N, N)\n      D.p <- diag(1, M, M)\n      D.g <- diag(1, N, N)\n      L.p <- diag(1, M, M)\n      L.g <- diag(1, N, N)\n      for (i in 1:M) {\n         for (j in 1:M) {\n            if (i < j) {\n               J.p[i,j] <- P[i,j]/(P[i,j] + p.d[i] + p.d[j])\n               J.p[j,i] <- P[i,j]/(P[i,j] + p.d[i] + p.d[j])\n               C.p[i,j] <- P[i,j]/(sqrt(p.d[i] * p.d[j]))\n               C.p[j,i] <- P[i,j]/(sqrt(p.d[i] * p.d[j]))\n               D.p[i,j] <- (2*P[i,j])/(2*P[i,j] + p.d[i] + p.d[j])\n               D.p[j,i] <- (2*P[i,j])/(2*P[i,j] + p.d[i] + p.d[j])\n               L.p[i,j] <- P[i,j]/(p.d[i] * p.d[j])\n               L.p[j,i] <- P[i,j]/(p.d[i] * p.d[j])\n               }\n            }\n         }\n      for (i in 1:N) {\n         for (j in 1:N) {\n            if (i < j) {\n               J.g[i,j] <- G[i,j]/(G[i,j] + g.d[i] + g.d[j])\n               J.g[j,i] <- G[i,j]/(G[i,j] + g.d[i] + g.d[j])\n               C.g[i,j] <- G[i,j]/(sqrt(g.d[i] * g.d[j]))\n               C.g[j,i] <- G[i,j]/(sqrt(g.d[i] * g.d[j]))\n               D.g[i,j] <- (2*G[i,j])/(2*G[i,j] + g.d[i] + g.d[j])\n               D.g[j,i] <- (2*G[i,j])/(2*G[i,j] + g.d[i] + g.d[j])\n               L.g[i,j] <- G[i,j]/(g.d[i] * g.d[j])\n               L.g[j,i] <- G[i,j]/(g.d[i] * g.d[j])\n               }\n            }\n         }\n      return(list(J.p = J.p, C.p = C.p, D.p = D.p, L.p = L.p,\n                  J.g = J.g, C.g = C.g, D.g = D.g, L.g = L.g))\n      }\n\nUsing this function to compute the Jaccard similarity between people yields:\n\n   J.p <- vertex.sim(g)$J.p\n   rownames(J.p) <- rownames(A)\n   colnames(J.p) <- rownames(A)\n   round(J.p, 2)\n\n          EVELYN LAURA THERESA BRENDA CHARLOTTE FRANCES ELEANOR PEARL RUTH\nEVELYN      1.00  0.29    0.30   0.29      0.20    0.25    0.20  0.21 0.20\nLAURA       0.29  1.00    0.29   0.30      0.21    0.27    0.27  0.17 0.21\nTHERESA     0.30  0.29    1.00   0.29      0.25    0.25    0.25  0.21 0.25\nBRENDA      0.29  0.30    0.29   1.00      0.27    0.27    0.27  0.17 0.21\nCHARLOTTE   0.20  0.21    0.25   0.27      1.00    0.20    0.20  0.00 0.20\nFRANCES     0.25  0.27    0.25   0.27      0.20    1.00    0.27  0.22 0.20\nELEANOR     0.20  0.27    0.25   0.27      0.20    0.27    1.00  0.22 0.27\nPEARL       0.21  0.17    0.21   0.17      0.00    0.22    0.22  1.00 0.22\nRUTH        0.20  0.21    0.25   0.21      0.20    0.20    0.27  0.22 1.00\nVERNE       0.14  0.15    0.20   0.15      0.11    0.11    0.20  0.22 0.27\nMYRNA       0.14  0.08    0.14   0.08      0.00    0.11    0.11  0.22 0.20\nKATHERINE   0.12  0.07    0.12   0.07      0.00    0.09    0.09  0.18 0.17\nSYLVIA      0.12  0.12    0.17   0.12      0.08    0.08    0.15  0.17 0.21\nNORA        0.11  0.12    0.16   0.12      0.08    0.08    0.14  0.15 0.14\nHELEN       0.07  0.14    0.13   0.14      0.10    0.10    0.18  0.11 0.18\nDOROTHY     0.17  0.10    0.17   0.10      0.00    0.14    0.14  0.29 0.25\nOLIVIA      0.09  0.00    0.09   0.00      0.00    0.00    0.00  0.17 0.14\nFLORA       0.09  0.00    0.09   0.00      0.00    0.00    0.00  0.17 0.14\n          VERNE MYRNA KATHERINE SYLVIA NORA HELEN DOROTHY OLIVIA FLORA\nEVELYN     0.14  0.14      0.12   0.12 0.11  0.07    0.17   0.09  0.09\nLAURA      0.15  0.08      0.07   0.12 0.12  0.14    0.10   0.00  0.00\nTHERESA    0.20  0.14      0.12   0.17 0.16  0.13    0.17   0.09  0.09\nBRENDA     0.15  0.08      0.07   0.12 0.12  0.14    0.10   0.00  0.00\nCHARLOTTE  0.11  0.00      0.00   0.08 0.08  0.10    0.00   0.00  0.00\nFRANCES    0.11  0.11      0.09   0.08 0.08  0.10    0.14   0.00  0.00\nELEANOR    0.20  0.11      0.09   0.15 0.14  0.18    0.14   0.00  0.00\nPEARL      0.22  0.22      0.18   0.17 0.15  0.11    0.29   0.17  0.17\nRUTH       0.27  0.20      0.17   0.21 0.14  0.18    0.25   0.14  0.14\nVERNE      1.00  0.27      0.23   0.27 0.20  0.25    0.25   0.14  0.14\nMYRNA      0.27  1.00      0.29   0.27 0.20  0.25    0.25   0.14  0.14\nKATHERINE  0.23  0.29      1.00   0.32 0.26  0.21    0.20   0.11  0.11\nSYLVIA     0.27  0.27      0.32   1.00 0.29  0.25    0.18   0.10  0.10\nNORA       0.20  0.20      0.26   0.29 1.00  0.24    0.09   0.17  0.17\nHELEN      0.25  0.25      0.21   0.25 0.24  1.00    0.12   0.12  0.12\nDOROTHY    0.25  0.25      0.20   0.18 0.09  0.12    1.00   0.20  0.20\nOLIVIA     0.14  0.14      0.11   0.10 0.17  0.12    0.20   1.00  0.33\nFLORA      0.14  0.14      0.11   0.10 0.17  0.12    0.20   0.33  1.00"
  },
  {
    "objectID": "tm-similarity.html#structural-equivalence",
    "href": "tm-similarity.html#structural-equivalence",
    "title": "Similarity and Equivalence in Two-Mode Networks",
    "section": "Structural Equivalence",
    "text": "Structural Equivalence\nAnd, of course, once we have a similarity we can cluster nodes based on approximate structural equivalence by transforming proximities to distances:\n\n   D <- as.dist(1- J.p)\n   hc.p <- hclust(D, method = \"ward.D2\")\n   plot(hc.p)\n\n\n\n\nAnd for events:\n\n   J.g <- vertex.sim(g)$J.g\n   rownames(J.g) <- colnames(A)\n   colnames(J.g) <- colnames(A)\n   D <- as.dist(1- J.g)\n   hc.g <- hclust(D, method = \"ward.D2\")\n   plot(hc.g)\n\n\n\n\nWe can then derive cluster memberships for people and groups from the hclust object:\n\n   library(dendextend)\n   clus.p <- sort(cutree(hc.p, 4)) #selecting four clusters for people\n   clus.p\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n        1         1         1         1         1         1         1         2 \n     RUTH     VERNE   DOROTHY     MYRNA KATHERINE    SYLVIA      NORA     HELEN \n        2         2         2         3         3         3         3         3 \n   OLIVIA     FLORA \n        4         4 \n\n   clus.g <- sort(cutree(hc.g, 3)) #selecting three clusters for groups\n   clus.g\n\n 6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10  2/23   4/7 11/21 \n    1     1     1     1     1     1     2     2     2     3     3     3     3 \n  8/3 \n    3 \n\n\nAnd finally we can block the original affiliation matrix, as recommended by Everett and Borgatti (2013, 210, table 5):\n\n   library(ggcorrplot)\n   p <- ggcorrplot(t(A[names(clus.p), names(clus.g)]), \n                   colors = c(\"white\", \"white\", \"red\")) \n   p <- p + theme(legend.position = \"none\", \n                  axis.text.y = element_text(size = 8),\n                  axis.text.x = element_text(size = 8, angle = 0),\n                  )\n   p <- p + scale_x_discrete(position = \"top\") \n   p <- p + geom_hline(yintercept = 7.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_hline(yintercept = 11.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_hline(yintercept = 16.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_vline(xintercept = 6.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_vline(xintercept = 9.5, linewidth = 2, color = \"blue\")\n   p\n\n\n\n\nWhich reveals a number of almost complete (one-blocks) and almost null (zero-blocks) in the social structure, with a reduced image matrix that looks like:\n\n   library(kableExtra)\n   IM <- matrix(0, 4, 3)\n   IM[1, ] <- c(0, 1, 0)\n   IM[2, ] <- c(0, 1, 1)\n   IM[3, ] <- c(0, 1, 0)\n   IM[4, ] <- c(1, 1, 0)\n   rownames(IM) <- c(\"P.Block1\", \"P.Block2\", \"P.Block3\", \"P.Block4\")\n   colnames(IM) <- c(\"E.Block1\", \"E.Block2\", \"E.Block3\")\n   kbl(IM, format = \"html\", , align = \"c\") %>% \n      column_spec(1, bold = TRUE) %>% \n      kable_styling(full_width = TRUE,\n                     bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\n \n  \n      \n    E.Block1 \n    E.Block2 \n    E.Block3 \n  \n \n\n  \n    P.Block1 \n    0 \n    1 \n    0 \n  \n  \n    P.Block2 \n    0 \n    1 \n    1 \n  \n  \n    P.Block3 \n    0 \n    1 \n    0 \n  \n  \n    P.Block4 \n    1 \n    1 \n    0"
  },
  {
    "objectID": "tm-similarity.html#generalized-vertex-similarity",
    "href": "tm-similarity.html#generalized-vertex-similarity",
    "title": "Similarity and Equivalence in Two-Mode Networks",
    "section": "Generalized Vertex Similarity",
    "text": "Generalized Vertex Similarity\nRecall that vertex similarity works using the principle of structural equivalence: Two people are similar if the choose the same objects (groups), and two objects (groups) are similar if they are chosen by the same people.\nWe can, like we did in the one mode case, be after a more general version of similarity, which says that: Two people are similar if they choose similar (not necessarily the same) objects, and two objects are similar if they are chosen by similar (not necessarily the same) people.\nThis leads to the same problem setup that inspired the SimRank approach (Jeh and Widom 2002).\nA (longish) function to compute the SimRank similarity between nodes in a two mode network goes as follows:\n\n   TM.SimRank <- function(A, C = 0.8, iter = 10) {\n        nr <- nrow(A)\n        nc <- ncol(A)\n        dr <- rowSums(A)\n        dc <- colSums(A)\n        Sr <- diag(1, nr, nr) #baseline similarity: every node maximally similar to themselves\n        Sc <- diag(1, nc, nc) #baseline similarity: every node maximally similar to themselves\n        rn <- rownames(A)\n        cn <- colnames(A)\n        rownames(Sr) <- rn\n        colnames(Sr) <- rn\n        rownames(Sc) <- cn\n        colnames(Sc) <- cn\n        m <- 1\n        while(m < iter) {\n             Sr.pre <- Sr\n             Sc.pre <- Sc\n             for(i in 1:nr) {\n                  for(j in 1:nr) {\n                       if (i != j) {\n                            a <- names(which(A[i, ] == 1)) #objects chosen by i\n                            b <- names(which(A[j, ] == 1)) #objects chosen by j\n                            Scij <- 0\n                            for (k in a) {\n                                 for (l in b) {\n                                      Scij <- Scij + Sc[k, l] #i's similarity to j\n                                 }\n                            }\n                            Sr[i, j] <- C/(dr[i] * dr[j]) * Scij\n                       }\n                  }\n             }\n             for(i in 1:nc) {\n                  for(j in 1:nc) {\n                       if (i != j) {\n                            a <- names(which(A[, i] == 1)) #people who chose object i\n                            b <- names(which(A[, j] == 1)) #people who chose object j\n                            Srij <- 0\n                            for (k in a) {\n                                 for (l in b) {\n                                      Srij <- Srij + Sr[k, l] #i's similarity to j\n                                 }\n                            }\n                            Sc[i, j] <- C/(dc[i] * dc[j]) * Srij\n                       }\n                  }\n             }\n             m <- m + 1\n        }\n        return(list(Sr = Sr, Sc = Sc))\n   }\n\nThis function takes the biadjacency matrix \\(\\mathbf{A}\\) as input and returns two generalized relational similarity matrices: One for the people (row objects) and the other one for the groups (column objects).\nHere’s how that would work in the SW data. First we compute the SimRank scores:\n\n   sim.res <- TM.SimRank(A)\n\nThen we peek inside the people similarity matrix:\n\n   round(sim.res$Sr[1:10, 1:10], 3)\n\n          EVELYN LAURA THERESA BRENDA CHARLOTTE FRANCES ELEANOR PEARL  RUTH\nEVELYN     1.000 0.267   0.262  0.266     0.259   0.275   0.248 0.255 0.237\nLAURA      0.267 1.000   0.262  0.277     0.270   0.287   0.280 0.237 0.247\nTHERESA    0.262 0.262   1.000  0.262     0.273   0.270   0.264 0.254 0.256\nBRENDA     0.266 0.277   0.262  1.000     0.290   0.287   0.279 0.235 0.246\nCHARLOTTE  0.259 0.270   0.273  0.290     1.000   0.276   0.269 0.175 0.256\nFRANCES    0.275 0.287   0.270  0.287     0.276   1.000   0.305 0.280 0.256\nELEANOR    0.248 0.280   0.264  0.279     0.269   0.305   1.000 0.279 0.294\nPEARL      0.255 0.237   0.254  0.235     0.175   0.280   0.279 1.000 0.279\nRUTH       0.237 0.247   0.256  0.246     0.256   0.256   0.294 0.279 1.000\nVERNE      0.201 0.207   0.222  0.206     0.198   0.202   0.246 0.276 0.288\n          VERNE\nEVELYN    0.201\nLAURA     0.207\nTHERESA   0.222\nBRENDA    0.206\nCHARLOTTE 0.198\nFRANCES   0.202\nELEANOR   0.246\nPEARL     0.276\nRUTH      0.288\nVERNE     1.000\n\n\nAnd the group similarity matrix:\n\n   round(sim.res$Sc[1:10, 1:10], 3)\n\n      6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10\n6/27 1.000 0.343 0.314 0.312 0.287 0.277 0.224 0.226 0.178 0.137\n3/2  0.343 1.000 0.312 0.311 0.285 0.276 0.224 0.228 0.200 0.141\n4/12 0.314 0.312 1.000 0.314 0.288 0.265 0.226 0.220 0.179 0.138\n9/26 0.312 0.311 0.314 1.000 0.287 0.256 0.230 0.214 0.186 0.137\n2/25 0.287 0.285 0.288 0.287 1.000 0.260 0.235 0.226 0.187 0.146\n5/19 0.277 0.276 0.265 0.256 0.260 1.000 0.224 0.226 0.200 0.171\n3/15 0.224 0.224 0.226 0.230 0.235 0.224 1.000 0.221 0.204 0.209\n9/16 0.226 0.228 0.220 0.214 0.226 0.226 0.221 1.000 0.221 0.214\n4/8  0.178 0.200 0.179 0.186 0.187 0.200 0.204 0.221 1.000 0.234\n6/10 0.137 0.141 0.138 0.137 0.146 0.171 0.209 0.214 0.234 1.000\n\n\nLike before we can use these results to define two sets of distances:\n\n   D.p <- as.dist(1 - sim.res$Sr)\n   D.g <- as.dist(1 - sim.res$Sc)\n\nSubject to hierarchical clustering:\n\n   hc.p <- hclust(D.p, method = \"ward.D2\")\n   hc.g <- hclust(D.g, method = \"ward.D2\")\n\nAnd plot:\n\n   plot(hc.p)\n\n\n\n   plot(hc.g)\n\n\n\n\nGet cluster memberships for people and groups from the hclust object:\n\n   clus.p <- sort(cutree(hc.p, 4)) #selecting four clusters for people\n   clus.p\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n        1         1         1         1         1         1         1         2 \n     RUTH     VERNE   DOROTHY     MYRNA KATHERINE    SYLVIA      NORA     HELEN \n        2         2         2         3         3         3         3         3 \n   OLIVIA     FLORA \n        4         4 \n\n   clus.g <- sort(cutree(hc.g, 3)) #selecting three clusters for groups\n   clus.g\n\n 6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  2/23  6/10   4/7 11/21 \n    1     1     1     1     1     1     2     2     2     2     3     3     3 \n  8/3 \n    3 \n\n\nAnd block the biadjacency matrix:\n\n   p <- ggcorrplot(t(A[names(clus.p), names(clus.g)]), \n                   colors = c(\"white\", \"white\", \"red\")) \n   p <- p + theme(legend.position = \"none\", \n                  axis.text.y = element_text(size = 8),\n                  axis.text.x = element_text(size = 8, angle = 0),\n                  )\n   p <- p + scale_x_discrete(position = \"top\") \n   p <- p + geom_hline(yintercept = 7.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_hline(yintercept = 11.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_hline(yintercept = 16.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_vline(xintercept = 6.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_vline(xintercept = 10.5, linewidth = 2, color = \"blue\")\n   p\n\n\n\n\nNote that this block solution is similar (pun intended) but not exactly the same as the one based on structural equivalence we obtained earlier, although it would lead to the same reduced image matrix for the blocks."
  },
  {
    "objectID": "tm-spectral.html",
    "href": "tm-spectral.html",
    "title": "Spectral Clustering of Two-Mode Networks",
    "section": "",
    "text": "We can use a variant of the spectral clustering approach to find communities in two-mode networks (Wu, Gu, and Yang 2022). This approach combines Correspondence Analysis (CA) and k-means clustering on multiple dimensions.\nSo let’s bring back our old friend, the Southern Women data:\nLet’s also compute the bi-adjacency modularity matrix:\nGreat! Now, from this information we can compute a version of the bipartite modularity matrix:\nAnd now let’s find the CA scores. This time will use the canned function CA from the the package FactoMineR\nWhich computes CA directly on the bi-adjacency matrix (the argument ncp asks to keep the first ten dimensions).\nWe can now extract the CA scores for persons and groups from the resulting object:\nGreat! You can verify that these are the same scores we obtained in two-mode CA lecture via a more elaborate route.\nNow, we can just create our U matrix by stacking the person and group scores using the first three dimensions:\nNice! Now we can just feed U to our k.cuts function to place persons and groups into cluster assignments beginning with two and ending with ten:\nOf course, we can’t use the mod.check function we used before because that relies on the standard method for checking the modularity in one-mode networks and doesn’t take into account the structural zeros in the bipartite graph.\nSo we need to come up with a custom method to check the modularity for the bipartite case.\nFirst, we need a function that takes a cluster assignment vector containing numbers for each cluster \\(k = \\{1, 2, 3, \\ldots C\\}\\) and turns it into a dummy coded cluster assignment matrix:\nLet’s test it out:\nGreat! Looks like it works.\nFinally, we need to write a custom function for bipartite modularity checking across different assignments:\nThe function mod.check2 needs three inputs: The bipartite modularity matrix, a list with different assignments of the nodes in the bipartite graph to different clusters, and the bipartite adjacency matrix. It returns a vector m with the modularities of each of the partitions in the list c.\nAnd, now, for the big reveal:\nLooks like the spectral clustering results favor a four-community partition although the more parsimonious three and binary community partitions also look pretty good.\nFigure 1 (a) and Figure 1 (b) show a plot of the three and four community solutions according to the CA dimensions (since we already saw the binary partition in the CA handout).\nOf course, just like we did with one-mode networks, we can also obtain a spectral clustering directly from the eigenvectors of the bipartite modularity matrix in just a couple of lines:\nHere we create the U matrix from the first two dimensions of the eigendecomposition of the bipartite modularity matrix. The results suggest that the three community partition is optimal, although the two-community one also does well. The corresponding splits are shown in Figure 1 (c) and Figure 1 (d).\nNote that the main difference between CA and modularity based clustering in two-mode networks is that modularity seems to prefer evenly balanced communities (in terms of number of nodes), while CA does not mind grouping nodes into small communities (like \\(\\{Flora, Nora, 2/23\\}\\))"
  },
  {
    "objectID": "tm-spectral.html#bipartite-modularity-allowing-people-and-groups-to-belong-to-different-number-of-communities",
    "href": "tm-spectral.html#bipartite-modularity-allowing-people-and-groups-to-belong-to-different-number-of-communities",
    "title": "Spectral Clustering of Two-Mode Networks",
    "section": "Bipartite Modularity Allowing People and Groups to Belong to Different Number of Communities",
    "text": "Bipartite Modularity Allowing People and Groups to Belong to Different Number of Communities\nOne limitation of Barber’s (2007) approach to computing the modularity we have been using to analyze community structure in two-mode networks is that it can only be used under the assumption that the number of communities is the same for both persons and groups.\nHowever, it could be that the optimal partition is actually one in which the people node set is split into a different number of clusters than the group node set.\nSo we need a way to evaluate the modularity of a partition when we have different number of communities on the people and group side.\nHere’s how to do it.\nFirst, we generate two separate candidate community assignments for people and groups via spectral clustering from CA using the first six eigenvectors:\n\n   k.cuts.p <- k.cuts(CA.res$row$coord[, 1:6])\n   k.cuts.g <- k.cuts(CA.res$col$coord[, 1:6])\n\nAs an example, let’s pick the solution that partitions people into four communities and the groups into three communities:\n\n   C.p <- k.cuts.p[[3]]\n   C.g <- k.cuts.g[[2]]\n\nGiven this information, we can create a \\(4 \\times 3\\) matrix recording the proportion of ties in the network that go from person-community \\(l\\) to group-community \\(m\\):\n\n   e <- matrix(0, 4, 3)\n   for (l in 1:4) {\n      for (m in 1:3) {\n         e[l, m] = sum(A[C.p == l, C.g == m]) * 1/sum(A)\n      }\n   }\n   round(e, 2)\n\n     [,1] [,2] [,3]\n[1,] 0.19 0.02 0.17\n[2,] 0.00 0.00 0.04\n[3,] 0.00 0.02 0.02\n[4,] 0.00 0.00 0.53\n\n\nFor instance, this matrix says that 53% of the ties in the Southern women data go from people in the fourth community to groups in the third community, according to the CA spectral partition.\nSuzuki and Wakita (2009), building on work by Murata (2009), suggest using the e matrix above to compute the modularity of any pair of person/group community assignment according to the following formula:\n\\[\nQ = \\frac{1}{2}\\sum_{l, m}\\frac{e_{lm}}{e_{l+}}\\left(e_{lm} - e_{l+}e_{+m}\\right)\n\\]\nWhere \\(e_{lm}\\) is the proportion of edges connecting people in the \\(l^{th}\\) person-community to groups in the \\(m^{th}\\) event-community, \\(e_{l+}\\) is the proportion of edges originating from person-community \\(i\\) (the corresponding entry of the row sum of e), and \\(e_{+m}\\) is the proportion of edges originating from nodes in group-community \\(j\\) (the corresponding entry of the column sum of e).\nSo the idea is that given a partition of the person nodes into \\(L\\) communities and a partition of the group nodes into \\(M\\) communities, we can generate an e matrix like the one above and compute the corresponding modularity of that person/group partition using the above equation.\nHere’s a function that computes the e matrix from a pair of person/group partitions and then returns the \\(Q\\) value corresponding to that matrix using the above formula:\n\n   find.mod <- function(w, x, y) {\n      Cx <- max(x)\n      Cy <- max(y)\n      M <- sum(w)\n      e <- matrix(0, Cx, Cy)\n      for (l in 1:Cx) {\n         for (m in 1:Cy) {\n            e[l, m] = sum(w[x == l, y == m]) * 1/M\n         }\n      }\n      Q <- 0\n      a <- rowSums(e)\n      b <- colSums(e)\n      for (l in 1:Cx) {\n         for (m in 1:Cy) {\n            Q <- Q + (e[l, m]/a[l] * (e[l, m] - a[l]*b[m]))\n         }\n      }\n   return(Q/2)\n   }\n\nSo for the e matrix from the above example, \\(Q\\) would be:\n\n   find.mod(A,  k.cuts.p[[3]],  k.cuts.g[[2]])\n\n[1] 0.07220939\n\n\nWhich looks like a positive number. But is it the biggest of all the possible community partition combinations between people and groups?\nTo answer this question, we can use the find.mod function to compute the modularity between every pair of partitions between people and groups that we calculated earlier. Since we computed eight different partitions for people and groups this leads to \\(8 \\times 8 = 64\\) pairs.\nHere’s a wrapper function over find.mod that computes the corresponding modularity values for each partition pair:\n\n   mod.mat <- function(d) {\n     q <- matrix(0, d, d)\n      for (i in 1:d) {\n         for (j in 1:d) {\n               q[i, j] <- find.mod(A,  k.cuts.p[[i]],  k.cuts.g[[j]])\n         }\n      }\n    return(q)\n   }\n   Q <- round(mod.mat(8), 3)\n   Q\n\n      [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]\n[1,] 0.102 0.058 0.042 0.031 0.032 0.033 0.029 0.024\n[2,] 0.106 0.066 0.050 0.038 0.039 0.041 0.035 0.033\n[3,] 0.104 0.072 0.056 0.042 0.045 0.043 0.037 0.039\n[4,] 0.112 0.074 0.069 0.055 0.058 0.056 0.050 0.046\n[5,] 0.112 0.075 0.069 0.058 0.061 0.060 0.054 0.050\n[6,] 0.110 0.074 0.075 0.064 0.061 0.063 0.058 0.054\n[7,] 0.111 0.077 0.077 0.066 0.067 0.068 0.062 0.059\n[8,] 0.111 0.077 0.078 0.067 0.067 0.070 0.064 0.060\n\n\nInterestingly, the analysis suggests that the maximum modularity \\(Q = 0.112\\) is obtained with a partition of people into five communities and groups into two communities corresponding to cells \\((4, 1)\\) of the above matrix.\nHere’s what this community assignment looks like in the Southern Women data:\n\n\n\n\n\nSpectral Clustering of Nodes in the Southern Women Data with Optimal Community Assignment Obtained via the Suzuki Modularity, with Five Communities for People, Two Communities for Groups.\n\n\n\n\nThe analysis separates two groups of densely connected actors of size six and five, respectively, namely, {Brenda, Theresa, Laura, Frances, Evelyn, Eleanor} and {Katherine, Nora, Sylvia, Myrna, Helen} along with their corresponding events from the one another. In the same way, {Pearl, Dorothy, Ruth, Verne} form a community of more peripheral actors who selectively attend the more popular events; {Flora, Olivia} are a two-actor community occupying the most peripheral position. Among the core set of actors, {Charlotte} occupies a singleton-community of her own.\nEvents are partitioned into two broad groups: One the one hand, we have those selectively attended by the larger community of densely connected actors along with the most popular events; on the other hand, we have the events selectively attended by the smaller group of densely connected actors."
  },
  {
    "objectID": "tm-reflections.html#the-method-of-reflections-as-correspondence-analysis",
    "href": "tm-reflections.html#the-method-of-reflections-as-correspondence-analysis",
    "title": "The Method of Reflections in Two-Mode Networks",
    "section": "The Method of Reflections as Correspondence Analysis",
    "text": "The Method of Reflections as Correspondence Analysis\nAs Dam et al. (2021) show, it turns out that the equilibrium scores computed by the method of reflections are none other than our old friend Correspondence Analysis! Let’s see how this works.\nIf you recall from the CA for two-mode networks lecture notes, in a two-mode network the first dimension of the CA scores (for persons) are equivalent to the second eigenvector of a matrix \\(\\mathbf{W}\\) which is obtained from the information in the biadjacency matrix \\(\\mathbf{A}\\) like this:\n\n    W.p <- diag(1/rowSums(A)) %*% A %*% diag(1/colSums(A)) %*% t(A)\n\nAnd we get the (real part of the) second eigenvector for the matrix like this:\n\n    ca.p <- Re(eigen(W.p)$vectors[, 2])\n    ca.p\n\n [1] -0.24086413 -0.25389472 -0.19699941 -0.25768209 -0.29332258 -0.24023748\n [7] -0.15396144 -0.01170823 -0.05064337  0.13065696  0.24914639  0.31499509\n[13]  0.26300900  0.26263739  0.24339931  0.09130661  0.33019362  0.33019362\n\n\nFor groups, the relevant CA matrix W.g is:\n\n    W.g <- diag(1/colSums(A)) %*% t(A) %*% diag(1/rowSums(A)) %*% A\n\nAnd we get the second eigenvector of that matrix:\n\n    ca.g <- Re(eigen(W.p)$vectors[, 2])\n    ca.g\n\n [1] -0.24086413 -0.25389472 -0.19699941 -0.25768209 -0.29332258 -0.24023748\n [7] -0.15396144 -0.01170823 -0.05064337  0.13065696  0.24914639  0.31499509\n[13]  0.26300900  0.26263739  0.24339931  0.09130661  0.33019362  0.33019362\n\n\nAnd we can see that these scores are perfectly correlated with the equilibrium reflection scores:\n\n    abs(round(cor(ca.p, HH.p[, 10]), 2))\n\n[1] 1\n\n    abs(round(cor(ca.g, HH.p[, 10]), 2))\n\n[1] 1"
  }
]