{
  "hash": "aa85da0dbe152073da6ae76d105e7c14",
  "result": {
    "markdown": "---\ntitle: \"The Eigendecomposition of a Square Matrix\"\nexecute: \n  eval: true\n  echo: true\n  output: true\n  warning: false\n  message: false\nformat: \n   html:\n      code-line-numbers: true\n---\n\n\n## The Asymmetric Matrix Case\n\nIn this lecture, we review the idea of an **eigendecomposition** of a square matrix. Let's say we have the following matrix $\\mathbf{B}$ of dimensions $3 \\times 3$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   set.seed(567)\n   B <- matrix(round(runif(9), 2), nrow = 3, ncol = 3)\n   B\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,] 0.74 0.49 0.07\n[2,] 0.88 0.26 0.51\n[3,] 0.63 0.24 0.59\n```\n:::\n:::\n\n\nWe use the `R` function `runif` (which stands for \"random uniform\") to populate the matrix with random numbers in the zero to one interval.\n\nMost matrices like this can be decomposed into two other matrices $\\mathbf{U}$ and $\\mathbf{\\lambda}$, such that the following matrix multiplication equation is true:\n\n$$\n\\mathbf{B} = \\mathbf{U}\\mathbf{\\lambda}\\mathbf{U}^{-1}\n$$\n\nBoth $\\mathbf{U}$ and $\\mathbf{\\lambda}$ are of the same dimensions as the original, with $\\mathbf{U}$ having numbers in each cell and $\\mathbf{\\lambda}$ being a matrix with values along the diagonals and zeros everywhere else. \n\nThe column values of $\\mathbf{U}$ are called the **eigenvectors** of $\\mathbf{B}$ and the diagonal values of $\\mathbf{\\lambda}$ are called the **eigenvalues** of $\\mathbf{B}$.\n\nIn `R` you can find the values that yield the eigendecomposition of any square matrix (if one exists) using the function `eigen`. \n\nSo in our case this would be:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   eig.res <- eigen(B)\n   eig.res\n```\n\n::: {.cell-output .cell-output-stdout}\n```\neigen() decomposition\n$values\n[1]  1.4256541  0.3195604 -0.1552145\n\n$vectors\n           [,1]       [,2]       [,3]\n[1,] -0.5148954 -0.4669390  0.4838633\n[2,] -0.6388257  0.2808667 -0.8653808\n[3,] -0.5716507  0.8384997 -0.1303551\n```\n:::\n:::\n\n\nThe function `eigen` returns a `list` with two components, one called `values` are the diagonal values of $\\mathbf{\\lambda}$, and the other one called `vectors` is the eigenvector matrix $\\mathbf{U}$.\n\nWe can check that these two elements can help us reconstruct the original matrix as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   lambda <- diag(eig.res$values)\n   U <- eig.res$vectors\n   B.rec <- U %*% lambda %*% solve(U)\n   B.rec\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,] 0.74 0.49 0.07\n[2,] 0.88 0.26 0.51\n[3,] 0.63 0.24 0.59\n```\n:::\n:::\n\n\nWhich are indeed the original values of $\\mathbf{B}$!\n\n## The Symmetric Matrix Case\n\nNow imagine that the matrix $\\mathbf{B}$ is *symmetric*:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   B[upper.tri(B)] <- B[lower.tri(B)]\n   B\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,] 0.74 0.88 0.63\n[2,] 0.88 0.26 0.24\n[3,] 0.63 0.24 0.59\n```\n:::\n:::\n\n\nAnd let's do the eigendecomposition of this matrix:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   eig.res <- eigen(B)\n   eig.res\n```\n\n::: {.cell-output .cell-output-stdout}\n```\neigen() decomposition\n$values\n[1]  1.7709909  0.2757779 -0.4567688\n\n$vectors\n           [,1]       [,2]       [,3]\n[1,] -0.7200185 -0.2389854  0.6515054\n[2,] -0.4963684 -0.4787344 -0.7241766\n[3,] -0.4849657  0.8448073 -0.2260728\n```\n:::\n:::\n\n\nThe interesting thing here is that now the reconstruction equation boils down to:\n\n$$\n\\mathbf{B} = \\mathbf{U}\\mathbf{\\lambda}\\mathbf{U}^T\n$$\n\nNote that now we just need to post-multiply $\\mathbf{U}\\mathbf{\\lambda}$ by the *transpose* of $\\mathbf{U}$ rather than the inverse, which is a much simpler matrix operation.\n\nWe can check that this is true as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   lambda <- diag(eig.res$values)\n   U <- eig.res$vectors\n   B.rec <- U %*% lambda %*% t(U)\n   B.rec\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,] 0.74 0.88 0.63\n[2,] 0.88 0.26 0.24\n[3,] 0.63 0.24 0.59\n```\n:::\n:::\n\n\nWhich are indeed the original values of the symmetric version of $\\mathbf{B}$!\n\nNow, the idea is that we can perform the asymmetric or symmetric eigendecomposition with any matrix, including a network adjacency matrix or a proximity matrix derived from it. \n\nIn fact, we have already done a partial version of the matrix eigendecomposition many times before, because the reflective status game is a way to compute the first column (leading eigenvector) of the $\\mathbf{U}$ matrix for any proximity or adjacency matrix you feed into it.\n\n## Low Rank Approximation of the Original Matrix\n\nThe more important thing is that, once you have the eigendecomposition of a matrix, and the full set of eigenvectors stored in $\\mathbf{U}$, the first few columns of $\\mathbf{U}$, gives us the best *low dimensional approximation* of the original matrix. \n\nFor instance, in the above case, the one-dimensional (also called \"rank one\") approximation of the original matrix is given by:\n\n$$\n\\mathbf{B}_{1-dim} = u_1\\lambda_1u_1^T\n$$\n\nWhere $u$ is just the first column (eigenvector) of $\\mathbf{U}$, and $\\lambda_1$ is just the first eigenvalue. \n\nIn `R` we can do this approximation as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   u.1 <- as.matrix(U[, 1]) #column vector\n   B.1dim <- u.1 %*% lambda[1, 1] %*% t(u.1)\n   round(B.1dim, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,] 0.92 0.63 0.62\n[2,] 0.63 0.44 0.43\n[3,] 0.62 0.43 0.42\n```\n:::\n:::\n\n\nWhich are not quite the same as the original values of $\\mathbf{B}$, but they are not wildly far off either. \n\nIf we wanted to be more accurate, however, we would use a *two-dimensional* approximation (rank two) and thus use more of the information:\n\n$$\n\\mathbf{B}_{2-dim} = u_1\\lambda_1u_1^T + u_2\\lambda_2u_2^T\n$$\n\nIn `R`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   u.2 <- as.matrix(U[, 2])\n   B.2dim <- u.1 %*% lambda[1, 1] %*% t(u.1) + u.2 %*% lambda[2, 2] %*% t(u.2)\n   round(B.2dim, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,] 0.93 0.66 0.56\n[2,] 0.66 0.50 0.31\n[3,] 0.56 0.31 0.61\n```\n:::\n:::\n\n\nWhich are not the same as the original, but are now a bit closer!\n\nOf course, if we wanted to reconstruct the original matrix, all we have to do is add the product of the eigenvector, eigenvalue, and transpose of the eigenvector across all three dimensions of the matrix:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   u.3 <- as.matrix(U[, 3])\n   B.3dim <- \n      u.1 %*% lambda[1, 1] %*% t(u.1) + \n      u.2 %*% lambda[2, 2] %*% t(u.2) + \n      u.3 %*% lambda[3, 3] %*% t(u.3)\n   round(B.3dim, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,] 0.74 0.88 0.63\n[2,] 0.88 0.26 0.24\n[3,] 0.63 0.24 0.59\n```\n:::\n:::\n\n\nWhich reconstructs the original values. \n\nSo in general, if you have a symmetric square matrix $\\mathbf{B}$ of dimensions $k \\times k$, and you obtain an eigenvalue decomposition of $\\mathbf{B}$ with eigenvectors stored in the columns of $\\mathbf{U}$ and eigenvalues in $\\lambda$, then the rank-$p$ approximation of the original matrix is given by:\n \n$$\n\\mathbf{B}_{rank-p} = \\sum_{m = 1}^p u_{m}\\lambda_mu_m^T\n$$\n\nWhen $p = k$, the equation above gives you the original matrix back. When $p<k$ you get the best guess as to what the original was, given $p$ dimensions.",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}