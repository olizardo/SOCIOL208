{
  "hash": "d785ac28b7929f7edc3ac01d11b03134",
  "result": {
    "markdown": "---\ntitle: \"Global Vertex Similarities\"\nexecute: \n  eval: true\n  echo: true\n  output: true\n  warning: false\n  message: false\nformat: \n   html:\n      code-line-numbers: true\n---\n\n\n## Global Similarity Indices\n\nAs we saw in the lecture notes on [local similarity metrics](locsim.qmd), the most important ingredient of structural similarity measures between pairs of nodes is the number of common of neighbors (followed by the degrees of each node), and this quantity is given by the square of the adjacency matrix $\\mathbf{A}^2$. So we can say that this matrix gives us a basic similarity measure between nodes, namely, the common neighbors similarity:\n\n$$\n\\mathbf{S} = \\mathbf{A}^2\n$$\n\nAnother way of seeing this is that a common neighbor defines a *path of length two* between a pair of nodes. So the number of common neighbors between two nodes is equivalent to the number of paths of length two between them. We are thus saying that the *similarity between two nodes increases as the number of paths of length two between them increases*, and that info is also recorded in the $\\mathbf{A}^2$ matrix. \n\n### The Local Path Similarity Index\n\nBut if the similarity between node pairs increases in the number of paths of length two between them, wouldn't nodes be even more similar if they also have a bunch of paths of length *three* between them? \n\n@lu_etal09 asked themselves the same question and proposed the following as a similarity metric based on paths:\n\n$$\n\\mathbf{S} = \\mathbf{A}^2 + \\alpha \\mathbf{A}^3\n$$ {#eq-locpath}\n\nThis is the so-called **local path similarity index** [@lu_etal09]. Obviously, structurally equivalent nodes will be counted as similar by this metric (lots of paths of length two between them) but also nodes indirectly connected by many paths of length three, but to a lesser extent given the discount parameter $\\alpha$ (a number between zero and one).\n\nA function that does this is:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   local.path <- function(w, alpha = 0.5) {\n      w2 <- w %*% w\n      s <- w2 + alpha*(w2 %*% w)\n   return(s)\n   }\n```\n:::\n\n\nHere's how the local path similarity looks in the *Flintstones* network:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   library(networkdata)\n   library(igraph)\n   library(stringr) #using stringr to change names from all caps to title case\n   g <- movie_267\n   V(g)$name <- str_to_title(V(g)$name)\n   g <- delete_vertices(g, degree(g) <= 3) #deleting low degree vertices\n   A <- as.matrix(as_adjacency_matrix(g))\n   S <- local.path(A)\n   S[1:10, 1:10]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             Bam-Bam Barney Betty Feldspar Fred Headmistress Herdmaster Lava\nBam-Bam         20.0   29.5  30.5      6.5 31.5         12.5        9.5 25.0\nBarney          29.5   50.0  51.0     13.0 52.0         21.0       21.0 46.5\nBetty           30.5   51.0  52.0     11.0 53.0         24.0       18.0 46.0\nFeldspar         6.5   13.0  11.0      3.0 13.5          4.5        5.0 10.0\nFred            31.5   52.0  53.0     13.5 55.0         21.5       21.5 47.5\nHeadmistress    12.5   21.0  24.0      4.5 21.5         13.0        6.5 21.5\nHerdmaster       9.5   21.0  18.0      5.0 21.5          6.5       10.0 17.0\nLava            25.0   46.5  46.0     10.0 47.5         21.5       17.0 42.0\nLeach            9.5   15.5  16.0      3.5 17.5          7.5        5.5 15.5\nMorris           8.5   13.0  15.5      2.5 13.0          8.0        3.5 12.5\n             Leach Morris\nBam-Bam        9.5    8.5\nBarney        15.5   13.0\nBetty         16.0   15.5\nFeldspar       3.5    2.5\nFred          17.5   13.0\nHeadmistress   7.5    8.0\nHerdmaster     5.5    3.5\nLava          15.5   12.5\nLeach          6.0    4.0\nMorris         4.0    6.0\n```\n:::\n:::\n\n\nOf course as $\\alpha$ approaches zero, then the local path measure reduces to the number of common neighbors, while numbers closer to one count paths of length three more. \n\nLet us now cluster the nodes in the graph according to the local path similarity. To do that, we need to transform the similarities into proper **distances** that we can feed to the usual hierarchical clustering algorithm. We can do by computing $\\mathbf{D} = 1-\\mathbf{S}$.^[Note that this does not return a *metric* distance in the technical sense, since the triangle inequality ($d(x, z) \\leq d{x, y} + d{y, z}$ is not guaranteed to be respected.)]\n\nIn `R` we can do this as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   D <- dist(1- S)\n   h.res <- hclust(D, method = \"ward.D2\") #hierarchical clustering\n   plot(h.res)\n```\n\n::: {.cell-output-display}\n![](globsim_files/figure-html/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n\n\"Eyeballing\" the dendrogram, it seems like there are three similarity blocks in the network. We can obtain them like this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   blocks  <- cutree(h.res, k = 3) #requesting three blocks\n   blocks\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        Bam-Bam          Barney           Betty        Feldspar            Fred \n              1               2               2               3               2 \n   Headmistress      Herdmaster            Lava           Leach          Morris \n              3               3               2               3               3 \n      Mrs Slate         Pebbles Pebbles Bam-Bam        Piltdown      Poindexter \n              1               1               1               1               1 \n         Pyrite           Slate           Wilma \n              1               2               2 \n```\n:::\n:::\n\n\nWe can see that Betty, Barney, Fred, and Wilma (along with Slate and Lava) end up in the same block.\n\n### The Katz Similarity\n\nThe local path similarity considers number of paths of lengths two and three linking two nodes in computing their similarity. Naturally, people may wonder why we can't keep going and add paths of length four:\n\n$$\n\\mathbf{S} = \\mathbf{A}^2 + \\alpha \\mathbf{A}^3 + \\alpha^2 \\mathbf{A}^4\n$$\n\nOr paths of length whatever:\n\n$$\n\\mathbf{S} = \\mathbf{A}^2 + \\alpha \\mathbf{A}^3 + \\alpha^2 \\mathbf{A}^4 ... + \\alpha^{k-2} \\mathbf{A}^k\n$$\n\nWhere $k$ is the length of the maximum path considered. @lu_etal09 argue that these higher order paths don't matter, so maybe we don't have to worry about them.\n\nAnother issue is that there was already an all-paths similarity measure in existence, one developed by the mathematical social scientist Leo @katz53 in the 1950s (!). \n\nThe basic idea was to use linear algebra tricks to solve:\n\n$$\n\\mathbf{S} = \\sum_{k=1}^{\\infty} \\alpha^k \\mathbf{A}^k\n$$\n\nWhich would theoretically count the *all* the paths of *all possible lengths* between two nodes while discounting the contribution of the longer paths in proportion to their length (as $k$ gets bigger, with $\\alpha$ a number between zero and one, $\\alpha^k$ gets smaller and smaller).\n\nLinear algebra hocus-pocus (non-technical explainer [here](https://olizardo.github.io/networks-textbook/lesson-sna-status.html#a-mathy-interlude)) turns the above infinite sum into the more tractable:\n\n$$\n\\mathbf{S} = (\\mathbf{I} - \\alpha \\mathbf{A})^{-1}\n$$ {#eq-katz}\n\nWhere $\\mathbf{I}$ is the **identity matrix** (a matrix of all zeros except that it has the number one in each diagonal cell) of the same dimensions as the original. Raising the result of the subtraction in parentheses to minus one indicates the **matrix inverse** operation (most matrices are invertible, unless they are weird).\n\nA function to compute the **Katz similarity** between all node pairs looks like:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   katz.sim <- function(w) {\n      I <- diag(nrow(w)) #creating identity matrix\n      alpha <- 1/eigen(w)$values[1] - 1e-10 #reciprocal of first eigenvalue of adjacency matrix minus a tiny number\n      S <- solve(I - alpha * w) \n      return(S)\n   }\n```\n:::\n\n\nThis function takes the network's adjacency matrix (`w`) as input and returns the **Katz similarity matrix** (`S`) as output. \n\nFor technical reasons (e.g., guarantee that the infinite sum converges) we need to choose $\\alpha$ to be a number larger than zero but smaller than the reciprocal of the first eigenvalue of the matrix. Here we just pick the reciprocal of the largest (first) eigenvalue minus a very small number ($10^{-10}$). \n\nLine 4 computes the actual Katz similarity using the native `R` function `solve` to find the relevant matrix inverse.^[See [here](https://cran.r-project.org/web/packages/matlib/vignettes/a3-inv-ex1.html) for an explainer of the matrix inverse in general and its computation in `R`.] \n\nIn the *Flintstones* network the Katz similarity looks like:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   set.seed(456)\n   S <- katz.sim(A)\n   round(S[1:10, 1:10], 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              Bam-Bam    Barney     Betty Feldspar      Fred Headmistress\nBam-Bam      43912268  75530407  76630126 17269355  78596698     34072232\nBarney       75530407 129914551 131806098 29703805 135188661     58605253\nBetty        76630126 131806098 133725188 30136290 137157000     59458542\nFeldspar     17269355  29703805  30136290  6791511  30909683     13399569\nFred         78596698 135188661 137157000 30909683 140676885     60984437\nHeadmistress 34072232  58605253  59458542 13399569  60984437     26437192\nHerdmaster   28235263  48565494  49272605 11104068  50537096     21908193\nLava         68595891 117986974 119704858 26976671 122776865     53224651\nLeach        24107266  41465215  42068946  9480652  43148569     18705214\nMorris       21013980  36144672  36670936  8264157  37612029     16305084\n             Herdmaster      Lava    Leach   Morris\nBam-Bam        28235263  68595891 24107266 21013980\nBarney         48565494 117986974 41465215 36144672\nBetty          49272605 119704858 42068946 36670936\nFeldspar       11104068  26976671  9480652  8264157\nFred           50537096 122776865 43148569 37612029\nHeadmistress   21908193  53224651 18705214 16305084\nHerdmaster     18155067  44106651 15500794 13511834\nLava           44106651 107154482 37658255 32826196\nLeach          15500794  37658255 13234577 11536403\nMorris         13511834  32826196 11536403 10056129\n```\n:::\n:::\n\n\nWhich are pretty big numbers! Which makes sense, since these are estimates of *all* the paths of *any* length between every pair of nodes in the network. We can get more tractable figures by normalizing the matrix by its maximum:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   S <- S/max(S)\n   round(S[1:10, 1:10], 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             Bam-Bam Barney Betty Feldspar Fred Headmistress Herdmaster Lava\nBam-Bam         0.31   0.54  0.54     0.12 0.56         0.24       0.20 0.49\nBarney          0.54   0.92  0.94     0.21 0.96         0.42       0.35 0.84\nBetty           0.54   0.94  0.95     0.21 0.97         0.42       0.35 0.85\nFeldspar        0.12   0.21  0.21     0.05 0.22         0.10       0.08 0.19\nFred            0.56   0.96  0.97     0.22 1.00         0.43       0.36 0.87\nHeadmistress    0.24   0.42  0.42     0.10 0.43         0.19       0.16 0.38\nHerdmaster      0.20   0.35  0.35     0.08 0.36         0.16       0.13 0.31\nLava            0.49   0.84  0.85     0.19 0.87         0.38       0.31 0.76\nLeach           0.17   0.29  0.30     0.07 0.31         0.13       0.11 0.27\nMorris          0.15   0.26  0.26     0.06 0.27         0.12       0.10 0.23\n             Leach Morris\nBam-Bam       0.17   0.15\nBarney        0.29   0.26\nBetty         0.30   0.26\nFeldspar      0.07   0.06\nFred          0.31   0.27\nHeadmistress  0.13   0.12\nHerdmaster    0.11   0.10\nLava          0.27   0.23\nLeach         0.09   0.08\nMorris        0.08   0.07\n```\n:::\n:::\n\n\nWhich looks better. As we would expect, Fred is very similar to Barney and Betty. We can then do the standard hierarchical clustering to see how this similarity measure groups nodes:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   D <- dist(1- S)\n   h.res <- hclust(D, method = \"ward.D2\") #hierarchical clustering\n   plot(h.res)\n```\n\n::: {.cell-output-display}\n![](globsim_files/figure-html/unnamed-chunk-8-1.png){width=960}\n:::\n\n```{.r .cell-code}\n   blocks  <- cutree(h.res, k = 3)\n   blocks\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        Bam-Bam          Barney           Betty        Feldspar            Fred \n              1               2               2               3               2 \n   Headmistress      Herdmaster            Lava           Leach          Morris \n              3               3               2               3               3 \n      Mrs Slate         Pebbles Pebbles Bam-Bam        Piltdown      Poindexter \n              1               1               1               1               1 \n         Pyrite           Slate           Wilma \n              1               2               2 \n```\n:::\n:::\n\n\nThe Katz similarity, like the local paths one, assigns Fred, Barney, Betty, Wilma (along with Slate and Lava) to the same group, and puts the kids in a separate block. \n\n### Degree-Weighted Katz Similarity (AKA Leicht-Holme-Newman)\n\n@leicht_etal06 argue that the Katz approach is fine and dandy as a similarity measure, but note that is an unweighted index (like the raw number of common neighbors). This means that nodes with large degree will end up being \"similar\" to a bunch of other nodes in the graph, just because they have lots of paths of length whatever between them and those nodes.\n\n@leicht_etal06 propose a \"fix\" for this weakness in the Katz similarity, resulting in the matrix linear algebra equivalent of a degree-normalized similarity measure like the Jaccard or Cosine. \n\nSo instead of Katz they suggest we compute:\n\n$$\n\\mathbf{S} = \\mathbf{D}^{-1} \\left( \\mathbf{I} - \\alpha \\mathbf{A} \\right)^{-1} \\mathbf{D}^{-1}\n$$\n\nWhere $\\mathbf{D}$ is the **degree matrix** containing the degrees of each node along the diagonals, meaning that $\\mathbf{D}^{-1}$ will  contain the inverse of each node's degree $\\frac{1}{k_i}$ along the diagonals. \n\nSo, the **LHN Similarity** is just the Katz similarity (the term in parentheses) weighted by the degree of the sender and receiver node along each path, further discounting paths featuring high-degree nodes at either or both ends. \n\nWhich leads to the function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   LHN.sim <- function(w) {\n      I <- diag(nrow(w)) #creating identity matrix\n      d <- rowSums(w) #degree vector\n      D <- diag(d) #creating degree matrix from degree vector\n      alpha <- 1/eigen(w)$values[1] - 1e-10 #reciprocal of first eigenvalue of adjacency matrix minus a tiny number\n      z <- solve(D) %*% solve(I - alpha * w)  %*% solve(D) #LHN index\n      rownames(z) <- rownames(w)\n      colnames(z) <- colnames(w)\n      return(z)\n   }\n```\n:::\n\n\nAnd the *Flintstones* result:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   S <- LHN.sim(A)\n   S <- S/max(S)\n   round(S[, 1:5], 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                Bam-Bam Barney Betty Feldspar Fred\nBam-Bam            0.72   0.57  0.58     0.85 0.55\nBarney             0.57   0.45  0.46     0.67 0.44\nBetty              0.58   0.46  0.47     0.68 0.44\nFeldspar           0.85   0.67  0.68     1.00 0.65\nFred               0.55   0.44  0.44     0.65 0.42\nHeadmistress       0.67   0.53  0.54     0.79 0.51\nHerdmaster         0.69   0.55  0.56     0.82 0.53\nLava               0.61   0.49  0.49     0.72 0.47\nLeach              0.79   0.63  0.64     0.93 0.61\nMorris             0.69   0.55  0.55     0.81 0.53\nMrs Slate          0.60   0.48  0.49     0.71 0.46\nPebbles            0.72   0.57  0.58     0.85 0.55\nPebbles Bam-Bam    0.69   0.55  0.56     0.82 0.53\nPiltdown           0.69   0.54  0.55     0.81 0.53\nPoindexter         0.67   0.53  0.54     0.79 0.52\nPyrite             0.69   0.54  0.55     0.81 0.53\nSlate              0.66   0.52  0.53     0.77 0.50\nWilma              0.56   0.44  0.45     0.66 0.43\n```\n:::\n:::\n\n\nHere we can see that, compared to the number we got for the Katz similarity, Barney and Fred are actually not that similar to one another after we take into account their very high degree, and that Barney is actually most similar to Feldspar.\n\nLet's block the actors in this network according to the LHN similarities:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   D <- dist(1 - S)\n   h.res <- hclust(D, method = \"ward.D2\")\n   plot(h.res)\n```\n\n::: {.cell-output-display}\n![](globsim_files/figure-html/unnamed-chunk-11-1.png){width=960}\n:::\n\n```{.r .cell-code}\n   blocks <- cutree(h.res, k = 3)\n   blocks\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        Bam-Bam          Barney           Betty        Feldspar            Fred \n              1               2               2               3               2 \n   Headmistress      Herdmaster            Lava           Leach          Morris \n              1               1               2               3               1 \n      Mrs Slate         Pebbles Pebbles Bam-Bam        Piltdown      Poindexter \n              2               1               1               1               1 \n         Pyrite           Slate           Wilma \n              1               1               2 \n```\n:::\n:::\n\n\nIt seems like this approach is geared towards finding smaller, more fine-grained clusters (e.g., Feldspar and Leach) of similar actors in the data, although it still correctly assigns Wilma, Fred, Barney, and Betty to the same cluster. \n\n## The Matrix Exponential Similarity\n\nAnother approach to computing global similarities between nodes, similar to the Katz similarities, is to use an operation called the **matrix exponential** [@fouss_etal16, p. 87]. \n\nFor the adjacency matrix, the matrix exponential similarity is defined as:\n\n$$\n\\mathbf{S} = \\sum_{k=1}^{\\infty} \\frac{\\alpha^k\\mathbf{A}^k}{k!}\n$$\n\nWhere, the denominator is the factorial of the $k^{th}$ matrix power $(k \\times (k-1) \\times (k-2) \\ldots \\times 1)$.\n\nRecall that the **powers of the adjacency matrix** $\\mathbf{A}^k$ produces a matrix counting the number of paths of length $k$ between every pair of nodes (and cycles of length $k$ in the diagonals). So this global measure of similarity, like the Katz similarity, counts nodes as similar if they are connected by indirect paths of all lengths, discounted by their length, as given by the denominator $k!$.\n\nMatrix algebra magic turns the above infinite sum into the more tractable:\n\n$$\n\\mathbf{S} = expm(\\alpha\\mathbf{A})\n$$\n\nWhere $expm$ refers to the [matrix exponential](https://en.wikipedia.org/wiki/Matrix_exponential) operation. \n\nIn `R` we can calculate the matrix exponential similarity using the package `expm`. Here's a quick function that does that:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   #install.packages(c(\"expm\", \"Matrix\"))\n   me.sim <- function(w, alpha) {\n      library(expm)\n      s <- expm(alpha * w)\n      return(s)\n      }\n```\n:::\n\n\nAnd now to test it out:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   S <- me.sim(A, alpha = 1.5)\n   round(S[1:10, 1:10], 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             Bam-Bam  Barney   Betty Feldspar    Fred Headmistress Herdmaster\nBam-Bam      25550.4 43929.1 44572.7  10044.3 45715.2      19816.1    16417.6\nBarney       43929.1 75565.6 76661.1  17279.0 78632.2      34082.8    28252.7\nBetty        44572.7 76661.1 77781.2  17526.6 79772.8      34585.3    28655.0\nFeldspar     10044.3 17279.0 17526.6   3953.4 17980.9       7789.8     6462.2\nFred         45715.2 78632.2 79772.8  17980.9 81827.1      35463.6    29399.8\nHeadmistress 19816.1 34082.8 34585.3   7789.8 35463.6      15388.7    12732.9\nHerdmaster   16417.6 28252.7 28655.0   6462.2 29399.8      12732.9    10572.9\nLava         39893.6 68624.8 69622.9  15690.0 71411.0      30958.1    25655.1\nLeach        14022.3 24115.9 24468.0   5514.0 25097.1      10880.7     9013.7\nMorris       12224.2 21019.7 21332.3   4803.7 21870.9       9491.2     7850.8\n                Lava   Leach  Morris\nBam-Bam      39893.6 14022.3 12224.2\nBarney       68624.8 24115.9 21019.7\nBetty        69622.9 24468.0 21332.3\nFeldspar     15690.0  5514.0  4803.7\nFred         71411.0 25097.1 21870.9\nHeadmistress 30958.1 10880.7  9491.2\nHerdmaster   25655.1  9013.7  7850.8\nLava         62327.6 21904.4 19091.1\nLeach        21904.4  7701.0  6710.2\nMorris       19091.1  6710.2  5857.4\n```\n:::\n:::\n\n\nNote that we set the alpha parameter to $\\alpha = 1.5$. When $\\alpha < 1$ it functions as a discount parameter on longer paths (counting shorter paths more) when $\\alpha > 1$ it switches the weight toward longer paths. When $\\alpha = 1$ it counts shorter and longer paths equally.^[This last is what @estrada08 calls the communicability between nodes.]\n\nAs we can see, the resulting matrix has pretty giant numbers as entries, which we would expect since we decide to put the weight on longer paths. Dividing by the maximum gives us the similarities we seek:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   S <- S/max(S)\n   round(S[1:10, 1:10], 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             Bam-Bam Barney Betty Feldspar Fred Headmistress Herdmaster Lava\nBam-Bam         0.31   0.54  0.54     0.12 0.56         0.24       0.20 0.49\nBarney          0.54   0.92  0.94     0.21 0.96         0.42       0.35 0.84\nBetty           0.54   0.94  0.95     0.21 0.97         0.42       0.35 0.85\nFeldspar        0.12   0.21  0.21     0.05 0.22         0.10       0.08 0.19\nFred            0.56   0.96  0.97     0.22 1.00         0.43       0.36 0.87\nHeadmistress    0.24   0.42  0.42     0.10 0.43         0.19       0.16 0.38\nHerdmaster      0.20   0.35  0.35     0.08 0.36         0.16       0.13 0.31\nLava            0.49   0.84  0.85     0.19 0.87         0.38       0.31 0.76\nLeach           0.17   0.29  0.30     0.07 0.31         0.13       0.11 0.27\nMorris          0.15   0.26  0.26     0.06 0.27         0.12       0.10 0.23\n             Leach Morris\nBam-Bam       0.17   0.15\nBarney        0.29   0.26\nBetty         0.30   0.26\nFeldspar      0.07   0.06\nFred          0.31   0.27\nHeadmistress  0.13   0.12\nHerdmaster    0.11   0.10\nLava          0.27   0.23\nLeach         0.09   0.08\nMorris        0.08   0.07\n```\n:::\n:::\n\n\nAnd the resulting clustering solution looks like:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   D <- dist(1-S)\n   h.res <- hclust(D, method = \"ward.D2\")\n   plot(h.res)\n```\n\n::: {.cell-output-display}\n![](globsim_files/figure-html/unnamed-chunk-15-1.png){width=960}\n:::\n:::\n\n\nWhich is substantively identical to the three cluster solution obtained using the Katz similarity, which makes sense given the similarity (pun intended) between the two metrics. \n\n",
    "supporting": [
      "globsim_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}