{
  "hash": "ce6622b5c23674a0836fe4044936ccf7",
  "result": {
    "markdown": "---\ntitle: \"Generalized Vertex Similarities\"\nexecute: \n  eval: true\n  echo: true\n  output: true\n  warning: false\n  message: false\nformat: \n   html:\n      code-line-numbers: true\n---\n\n\n## Generalized Similarities\n\nSo far, we have defined distances and similarities mainly as a function of the number of shared neighbors between two nodes. Structural equivalence is an idealized version of this, obtaining whenever people share exactly the same set of neighbors.\n\nYet, similarities based on local neighborhood information only have been criticized (e.g., by @borgatti_everett92) for not quite capturing the sociological intuition behind the idea of a *role* which is usually what they are deployed for. \nThat is, two doctors don't occupy the same role because they treat the *same* set of patients (in fact, this would be weird); instead, the set of doctors occupy the same role to the extent they treat a set of actors in the same (or similar) role: Namely, patients, regardless of whether those actors are literally the same or not. \n\nThis worry led social network analysts to try to generalize the concept of structural equivalence using less stringent algebraic definition, resulting in something of a proliferation of different equivalences such as **automorphic** or **regular equivalence** [@borgatti_everett92]. \n\nUnfortunately, none of this work led (unlike the work on structural equivalence) to useful or actionable tools for data analysis, with some even concluding that some definitions of equivalence (like regular equivalence) are unlikely to be found in any interesting substantive setting. \n\nA better approach here is to use the more relaxed notion of similarity like we did above to come up with a more general definition that can capture our role-based intuitions. Note that all the similarity notions studied earlier have structural equivalence as the limiting case of maximum similarity. So they are still based on the idea that *nodes are similar to the extent they connect to the same others*. \n\nWe can generalize this idea (to deal with the doctor/patient role problem) in the following way: *nodes are similar to the extent they connect to similar others*, with the restriction that we can only use endogenous (structural connectivity) information---like with structural equivalence or common-neighbor approaches---to define everyone's similarity (no exogenous attribute stuff).\n\n## Jeh and Widom's SimRank\n\nAs @jeh_widom02 note, this approach does lead to a coherent generalization of the idea of similarity for nodes in graphs because we can just define similarity *recursively* and iterate through the graph until the similarity scores stop changing.^[Note the similarity (heh, heh) between this idea and the various status ranking algorithms for computing [prestige scores](tm-prestige.qmd).] \n\nMore specifically, they propose to measure the similarity between two nodes $i$ and $j$ at each iteration-step $t$ ($s_{ij}(t)$) using the formula:\n\n\n$$\n   s_{ij}(t) = \\frac{\\alpha}{d_i d_j} \\sum_{k \\in N(i)} \\sum_{l \\in N(j)} s_{kl}(t-1)\n$$ {#eq-simrank}\n\nSo the similarity between two nodes at step $t$ is just the sum of the pairwise similarities between each of their neighbors (computed in the previous step, $t-1$), weighted by the ratio of a free parameter $\\alpha$ (a number between zero and one) to the product of their degrees (to take a weighted average). \n\nThis measure nicely captures the idea that nodes are similar to the extent they both connect to people who are themselves similar. Note that it doesn't matter whether these neighbors are shared between the two nodes (the summation occurs over each pair of nodes formed by crossing the set $p$ + $q$ against $p$ + $r$ as defined in the [local similarity](locsim.qmd) lecture), whether they are themselves neighbors, which deals with the doctor/patient problem we referred to earlier. \n\nA function that implements this idea looks like:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   SimRank <- function(w, alpha = 0.85) {\n      d <- rowSums(w) #degree vector\n      n <- nrow(w) #number of nodes in the graph\n      z <- diag(1, n, n) #diagonal matrix (nodes are maximally similar to themselves)\n      if(is.null(rownames(w)) == TRUE) {rownames(w) <- 1:nrow(w)} #checking if matrix has row names\n      if(is.null(colnames(w)) == TRUE) {colnames(w) <- 1:nrow(w)} #checking if matrix has column names\n      rownames(z) <- rownames(w) #naming similarity matrix rows\n      colnames(z) <- colnames(w) #naming similarity matrix columns\n      nei <- apply(w, 1, function(x){which(x == 1)}) #neighbors list\n      delta <- 1\n      while(delta > 1e-10) { #run while delta is bigger than a tiny number\n         z.o <- z #old similarity matrix at t-1\n         for(i in 1:n) {\n            for(j in 1:n) {\n               if (i != j) { \n                  sigma <- 0 #initialize ij sum for iteration t\n                  for (k in nei[[i]]) { #loop through i's neighbors\n                     for (l in nei[[j]]) { #loop through j's neighbors\n                        sigma <- sigma + z[k, l] #i's similarity to j is equal to sum of neighbors' similarity\n                        } #end l for loop\n                     } #end k for loop\n                  z[i,j] <- alpha/(d[i] * d[j]) * sigma #updating ij similarity score at t\n                  } #end if\n               } #end j for loop\n            } #end i for loop\n         delta <- abs(sum(abs(z)) - sum(abs(z.o))) #difference between similarity matrices at t and t-1\n         } #end while loop\n      return(z) #return similarity matrix after convergence\n      } #end function\n```\n:::\n\n\nThe `SimRank` function takes an adjacency matrix as input (`w`) and returns a generalized similarity matrix between nodes (`z`) as output.\n\nLet's try it out in the *Pulp Fiction* graph:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   library(networkdata)\n   library(igraph)\n   library(stringr) #using stringr to change names from all caps to title case\n   g <- movie_559\n   V(g)$name <- str_to_title(V(g)$name)\n   V(g)$name[which(V(g)$name == \"Esmarelda\")] <- \"Esmeralda\" #fixing misspelled name\n   g <- delete_vertices(g, degree(g) < 2) #deleting low degree vertices\n   A <- as.matrix(as_adjacency_matrix(g))\n   S <- SimRank(A)\n   round(S[1:5, 1:5], 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Brett Buddy Butch Capt Koons Ed Sullivan\nBrett        1.00  0.22  0.22       0.22        0.22\nBuddy        0.22  1.00  0.22       0.32        0.52\nButch        0.22  0.22  1.00       0.22        0.22\nCapt Koons   0.22  0.32  0.22       1.00        0.32\nEd Sullivan  0.22  0.52  0.22       0.32        1.00\n```\n:::\n:::\n\n\nWe can transform the generalized similarities to distances and plot:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   D <- dist(1- S)\n   h.res <- hclust(D, method = \"ward.D2\") #hierarchical clustering\n   plot(h.res)\n```\n\n::: {.cell-output-display}\n![](gensim_files/figure-html/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n\nNote that the lowest levels of the dendrogram hierarchy some of the pairings make a lot of sense: The Gimp is most similar to Zed, Pumpkin is most similar to Honey Bunny (and other characters in the diner scene), Jimmie is most similar ot the Wolf, and Jules, Marsellus, Butch, Mia, and Vincent (the main characters) are most similar to one another. \n\nThe plot suggests a division into five major node clusters. We can obtain that as follows using the `cutree` function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   blocks  <- cutree(h.res, k = 5)\n   blocks\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          Brett           Buddy           Butch      Capt Koons     Ed Sullivan \n              1               2               1               2               2 \n   English Dave        Fabienne      Fourth Man       Gawker #2     Honey Bunny \n              2               1               3               1               4 \n         Jimmie            Jody           Jules           Lance         Manager \n              3               2               1               2               4 \n      Marsellus          Marvin         Maynard             Mia          Mother \n              1               1               5               1               2 \n         Patron      Pedestrian        Preacher         Pumpkin          Raquel \n              4               1               2               4               3 \n          Roger Sportscaster #1        The Gimp        The Wolf         Vincent \n              1               1               5               3               1 \n       Waitress         Winston           Woman       Young Man     Young Woman \n              4               3               2               4               4 \n            Zed \n              5 \n```\n:::\n:::\n\n \n## The LHN Approach\n\n@leicht_etal06 provide a twist on the SimRank approach. Their idea is that a node $i$ is similar to another node $j$ just in case $i$'s neighbors are similar to $j$:\n\n$$\n   s_{ij}(t) = \\delta_{ij} + \\alpha \\sum_{k \\in N(i)} s_{kj}(t-1)\n$$ {#eq-lhn}\n\nWhere $\\delta_{ij}$ is the Kronecker function that equals one when $i = j$ and zero otherwise, and $\\alpha$ is a number between zero and one with the restriction: $\\alpha < \\frac{1}{\\lambda_1}$, where $\\lambda_1$ is the largest eigenvalue of the adjacency matrix (also called the **spectral radius** of the adjacency matrix). This is required for the series to converge, just like the global Katz similarity [we considered before](globsim.qmd).\n\nMeanwhile, the $\\delta_{ij}$ can be interpreted as \"free similarity points\" that each node gets for being similar to itself. In this setup, the pairwise similarity between $i$ and $j$ is just the sum of the previously calculated similarities between the set of neighbors of $i$ and the focal node $j$ weighted by the parameter $\\alpha$.\n\nHere's a function that does that:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   LHN.sim <- function(w, alpha = 0.25) {\n      n <- nrow(w) #number of nodes in the graph\n      z <- diag(1, n, n) #diagonal matrix (nodes are maximally similar to themselves)\n      if(is.null(rownames(w)) == TRUE) {rownames(w) <- 1:nrow(w)} #checking if matrix has row names\n      if(is.null(colnames(w)) == TRUE) {colnames(w) <- 1:nrow(w)} #checking if matrix has column names\n      rownames(z) <- rownames(w) #naming similarity matrix rows\n      colnames(z) <- colnames(w) #naming similarity matrix columns\n      nei <- apply(w, 1, function(x){which(x == 1)}) #neighbors list\n      rho <- (eigen(w)$values[1]^-1) - 1e-02  #inverse of spectral radius of adjacency matrix minus a tiny number\n      if(alpha > rho) {alpha <- rho} #if alpha is larger than rho make it rho\n      delta <- 1\n      while(delta > 1e-10) { #run while delta is bigger than a tiny number\n         z.o <- z #old similarity matrix at t-1\n         for(i in 1:n) {\n            for(j in 1:n) {\n               sigma <- 0 #initialize ij sum for iteration t\n               for (k in nei[[i]]) { #loop through i's neighbors\n                  sigma <- sigma + z[k,j] #ij similarity equal to sum of i's neighbors' k similarity to j\n                  } #end k for loop\n               z[i,j] <- as.numeric(i == j) + (alpha * sigma) #updating ij similarity score at t\n               } #end j for loop\n            } #end i for loop\n         delta <- abs(sum(abs(z)) - sum(abs(z.o))) #difference between similarity matrices at t and t-1\n         } #end while loop\n      return(z) #return similarity matrix after convergence\n      } #end function\n```\n:::\n\n\nAnd here are the results for the *Pulp Fiction* network:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   S <- LHN.sim(A)\n   round(S[, 1:5], 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                Brett Buddy Butch Capt Koons Ed Sullivan\nBrett            1.51  0.16  0.82       0.31        0.16\nBuddy            0.16  1.08  0.25       0.13        0.08\nButch            0.82  0.25  2.16       0.61        0.25\nCapt Koons       0.31  0.13  0.61       1.27        0.13\nEd Sullivan      0.16  0.08  0.25       0.13        1.08\nEnglish Dave     0.32  0.13  0.58       0.24        0.13\nFabienne         0.33  0.07  0.43       0.15        0.07\nFourth Man       0.20  0.07  0.28       0.13        0.07\nGawker #2        0.18  0.05  0.37       0.12        0.05\nHoney Bunny      0.32  0.12  0.46       0.21        0.12\nJimmie           0.22  0.08  0.32       0.14        0.08\nJody             0.20  0.09  0.31       0.16        0.09\nJules            0.83  0.24  1.14       0.46        0.24\nLance            0.20  0.09  0.31       0.16        0.09\nManager          0.30  0.11  0.42       0.19        0.11\nMarsellus        0.69  0.20  0.99       0.39        0.20\nMarvin           0.58  0.15  0.78       0.30        0.15\nMaynard          0.09  0.03  0.23       0.07        0.03\nMia              0.48  0.29  0.87       0.49        0.29\nMother           0.31  0.13  0.61       0.37        0.13\nPatron           0.30  0.11  0.42       0.19        0.11\nPedestrian       0.18  0.05  0.37       0.12        0.05\nPreacher         0.15  0.07  0.23       0.12        0.07\nPumpkin          0.32  0.12  0.46       0.21        0.12\nRaquel           0.22  0.08  0.32       0.14        0.08\nRoger            0.58  0.15  0.78       0.30        0.15\nSportscaster #1  0.09  0.03  0.23       0.06        0.03\nThe Gimp         0.01  0.00  0.03       0.01        0.00\nThe Wolf         0.22  0.08  0.32       0.14        0.08\nVincent          1.04  0.45  1.54       0.77        0.45\nWaitress         0.09  0.03  0.12       0.06        0.03\nWinston          0.22  0.08  0.32       0.14        0.08\nWoman            0.31  0.13  0.61       0.37        0.13\nYoung Man        0.09  0.03  0.12       0.06        0.03\nYoung Woman      0.09  0.03  0.12       0.06        0.03\nZed              0.01  0.00  0.03       0.01        0.00\n```\n:::\n:::\n\n\nNote that in the LHN framework, nodes are always maximally similar to themselves. \n\nWe can transform the LHN generalized similarities to distances by dividing by the maximum and then clustering:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   S <- S/max(S)\n   D <- dist(1- S)\n   h.res <- hclust(D, method = \"ward.D2\") #hierarchical clustering\n   plot(h.res)\n```\n\n::: {.cell-output-display}\n![](gensim_files/figure-html/unnamed-chunk-7-1.png){width=960}\n:::\n:::\n\n\nWhich looks like another five-block partition. Cutting the dendrogram at the requisite height, we get:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   blocks  <- cutree(h.res, k = 5)\n   blocks\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          Brett           Buddy           Butch      Capt Koons     Ed Sullivan \n              1               2               3               4               2 \n   English Dave        Fabienne      Fourth Man       Gawker #2     Honey Bunny \n              4               2               2               2               4 \n         Jimmie            Jody           Jules           Lance         Manager \n              2               2               3               2               4 \n      Marsellus          Marvin         Maynard             Mia          Mother \n              1               1               5               1               4 \n         Patron      Pedestrian        Preacher         Pumpkin          Raquel \n              4               2               2               4               2 \n          Roger Sportscaster #1        The Gimp        The Wolf         Vincent \n              1               5               5               2               3 \n       Waitress         Winston           Woman       Young Man     Young Woman \n              5               2               4               5               5 \n            Zed \n              5 \n```\n:::\n:::\n\n\nWhich is a bit different from the SimRank clustering. Here, the three main characters $\\{$ Vincent, Jules, Butch $\\}$ appear as maximally similar in their own cluster, while $\\{$ Brett, Marvin, Roger, Marsellus, Mia $\\}$ form their own similarity group. \n\n## The Melnik et al. Approach\n\n@melnik_etal02 provide yet another variation on the theme of generalized similarity scoring. Their approach is a bit different that SimRank or the LHN strategy. The idea is to start from state in which *all* pairs of nodes have the same similarity score (e.g., $s_{ij} = 1$) at $t = 0$ and then update the subsequent similarities according to the rule:\n\n$$\ns_{ij}(t) = s_{ij}(t-1) + \\left[\\sum_{k \\in N(i)} \\sum_{l \\in N(j)} \\frac{1}{d_k d_l} s_{kl}(t-1)\\right]\n$${#eq-melnik}\n\nSo the similarity between $i$ and $j$ is equal to their previous similarity $s_{ij}(t-1)$ plus the sum of their neighbors similarity (just like SimRank), weighted by their respective degrees (unlike SimRank).\n\nIn order for the iterations to converge, we need to normalize the similarity matrix $\\mathbf{S}$ at each iteration step $t$ according to:\n\n$$\n\\mathbf{S}(t) = \\frac{\\mathbf{S}(t)}{||\\mathbf{S}(t)||_2}\n$$\n\nWhere $||\\mathbf{S}||_2$ is the matrix Frobenius (Euclidean) norm, or the square root of the sum of the square entries: \n\n$$\n||\\mathbf{S}(t)||_2 = \\sqrt{\\sum_i \\sum_j s_{ij}(t)^2}\n$$\n\nHere's a function that does that:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   melnik.sim <- function(w) {\n      d <- rowSums(w) #degree vector\n      n <- nrow(w) #number of nodes in the graph\n      z <- matrix(1, n, n) #all pairs start as equally similar (sij = 1)\n      if(is.null(rownames(w)) == TRUE) {rownames(w) <- 1:nrow(w)} #checking if matrix has row names\n      if(is.null(colnames(w)) == TRUE) {colnames(w) <- 1:nrow(w)} #checking if matrix has column names\n      rownames(z) <- rownames(w) #naming similarity matrix rows\n      colnames(z) <- colnames(w) #naming similarity matrix columns\n      nei <- apply(w, 1, function(x){which(x == 1)}) #neighbors list\n      delta <- 1\n      while(delta > 1e-10) { #run while delta is bigger than a tiny number\n         z.o <- z #old similarity matrix at t-1\n         for(i in 1:n) {\n            for(j in 1:n) {\n               sigma <- 0 #initialize ij sum for iteration t\n               for (k in nei[[i]]) { #loop through i's neighbors\n                  for (l in nei[[j]]) { #loop through j's neighbors\n                     sigma <- sigma + (d[k]*d[l])^-1 * z[k,l] #i's similarity to j is equal to sum of neighbors' similarity weighted by their degree \n                     } #end l for loop\n                  } #end k for loop\n               z[i,j] <- z[i,j] + sigma #updating ij similarity score at t\n               } #end j for loop\n            } #end i for loop\n         z <- z/norm(z, type = \"F\") #normalizing similarity matrix at z\n         delta <- abs(sum(abs(z)) - sum(abs(z.o))) #difference between similarity matrices at t and t-1\n         } #end while loop\n      return(z) #return similarity matrix after convergence\n      } #end function\n```\n:::\n\n\nWe can now test it on the *Flintstones* network:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   S <- melnik.sim(A)\n   round(S[, 1:5], 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                Brett Buddy Butch Capt Koons Ed Sullivan\nBrett            0.01  0.00  0.03       0.01        0.00\nBuddy            0.00  0.00  0.01       0.00        0.00\nButch            0.03  0.01  0.08       0.02        0.01\nCapt Koons       0.01  0.00  0.03       0.01        0.00\nEd Sullivan      0.00  0.00  0.01       0.00        0.00\nEnglish Dave     0.01  0.00  0.02       0.01        0.00\nFabienne         0.01  0.00  0.02       0.01        0.00\nFourth Man       0.00  0.00  0.01       0.00        0.00\nGawker #2        0.01  0.00  0.02       0.01        0.00\nHoney Bunny      0.02  0.01  0.05       0.01        0.01\nJimmie           0.01  0.00  0.01       0.00        0.00\nJody             0.01  0.00  0.02       0.01        0.00\nJules            0.04  0.01  0.10       0.03        0.01\nLance            0.01  0.00  0.02       0.01        0.00\nManager          0.02  0.00  0.04       0.01        0.00\nMarsellus        0.03  0.01  0.07       0.02        0.01\nMarvin           0.02  0.01  0.05       0.01        0.01\nMaynard          0.01  0.00  0.02       0.01        0.00\nMia              0.04  0.01  0.09       0.03        0.01\nMother           0.02  0.01  0.04       0.01        0.01\nPatron           0.02  0.01  0.04       0.01        0.01\nPedestrian       0.01  0.00  0.03       0.01        0.00\nPreacher         0.01  0.00  0.02       0.01        0.00\nPumpkin          0.03  0.01  0.07       0.02        0.01\nRaquel           0.01  0.00  0.02       0.01        0.00\nRoger            0.02  0.01  0.05       0.02        0.01\nSportscaster #1  0.00  0.00  0.01       0.00        0.00\nThe Gimp         0.01  0.00  0.02       0.01        0.00\nThe Wolf         0.01  0.00  0.02       0.01        0.00\nVincent          0.11  0.03  0.26       0.08        0.03\nWaitress         0.02  0.00  0.04       0.01        0.00\nWinston          0.02  0.00  0.04       0.01        0.00\nWoman            0.03  0.01  0.06       0.02        0.01\nYoung Man        0.02  0.01  0.05       0.01        0.01\nYoung Woman      0.02  0.01  0.06       0.02        0.01\nZed              0.01  0.00  0.03       0.01        0.00\n```\n:::\n:::\n\n\nWhich yields the following distance matrix and hierarchical clustering dendrogram:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   S <- S/max(S)\n   D <- dist(1- S)\n   h.res <- hclust(D, method = \"ward.D2\") #hierarchical clustering\n   plot(h.res)\n```\n\n::: {.cell-output-display}\n![](gensim_files/figure-html/unnamed-chunk-11-1.png){width=960}\n:::\n:::\n\n\nWhich looks like yet another five-block partition. Cutting the dendrogram at the requisite height, we get:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   blocks  <- cutree(h.res, k = 5)\n   blocks\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          Brett           Buddy           Butch      Capt Koons     Ed Sullivan \n              1               1               2               1               1 \n   English Dave        Fabienne      Fourth Man       Gawker #2     Honey Bunny \n              1               1               1               1               3 \n         Jimmie            Jody           Jules           Lance         Manager \n              1               1               4               1               3 \n      Marsellus          Marvin         Maynard             Mia          Mother \n              2               3               1               4               3 \n         Patron      Pedestrian        Preacher         Pumpkin          Raquel \n              3               1               1               2               1 \n          Roger Sportscaster #1        The Gimp        The Wolf         Vincent \n              2               1               1               1               5 \n       Waitress         Winston           Woman       Young Man     Young Woman \n              3               3               2               3               2 \n            Zed \n              1 \n```\n:::\n:::\n\n\nNote that the Milnek et al. approach produces a clustering that is distinct from that obtained using either SimRank or LHN. Here, $\\{$ Vincent $\\}$ is its own singleton cluster, and $\\{$ Jules, Mia $\\}$ comprise another small similarity class. \n\n## Appendix\n\nA more general (and shorter) version of the function to compute `SimRank` looks like this [partly based on @fouss_etal16, pp. 84, algorithm 2.4]:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n   SimRank2 <- function(w, alpha = 0.85) {\n      n <- nrow(w) #number of nodes\n      d <- as.numeric(colSums(w) > 0) #degree vector\n      e <- matrix(1, n, 1) #all ones column vector\n      q <- diag(as.vector(t(e) %*% w), n, n) #q matrix\n      diag(q) <- 1/diag(q) #setting q diagonals to inverse\n      q <- w %*% q\n      q[is.nan(q)] <- 0\n      delta <- 1\n      k <- diag(1, n, n)\n      while(delta > 1e-10) {\n         k.o <- k\n         k.p <- alpha * t(q) %*% k %*% q\n         k <- k.p - diag(diag(k.p), n, n) + diag(d, n, n)\n         delta <- abs(sum(abs(k)) - sum(abs(k.o)))\n         }\n      rownames(k) <- rownames(w)\n      colnames(k) <- colnames(w)\n      return(k)\n   }\n```\n:::",
    "supporting": [
      "gensim_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}