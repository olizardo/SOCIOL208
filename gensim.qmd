---
title: "Generalized Vertex Similarities"
execute: 
  eval: true
  echo: true
  output: true
  warning: false
  message: false
format: 
   html:
      code-line-numbers: true
---

## Generalized Similarities

So far, we have defined distances and similarities mainly as a function of the number of shared neighbors between two nodes. Structural equivalence is an idealized version of this, obtaining whenever people share exactly the same set of neighbors.

Yet, similarities based on local neighborhood information only have been criticized (e.g., by @borgatti_everett92) for not quite capturing the sociological intuition behind the idea of a *role* which is usually what they are deployed for. 
That is, two doctors don't occupy the same role because they treat the *same* set of patients (in fact, this would be weird); instead, the set of doctors occupy the same role to the extent they treat a set of actors in the same (or similar) role: Namely, patients, regardless of whether those actors are literally the same or not. 

This worry led social network analysts to try to generalize the concept of structural equivalence using less stringent algebraic definition, resulting in something of a proliferation of different equivalences such as **automorphic** or **regular equivalence** [@borgatti_everett92]. 

Unfortunately, none of this work led (unlike the work on structural equivalence) to useful or actionable tools for data analysis, with some even concluding that some definitions of equivalence (like regular equivalence) are unlikely to be found in any interesting substantive setting. 

A better approach here is to use the more relaxed notion of similarity like we did above to come up with a more general definition that can capture our role-based intuitions. Note that all the similarity notions studied earlier have structural equivalence as the limiting case of maximum similarity. So they are still based on the idea that *nodes are similar to the extent they connect to the same others*. 

We can generalize this idea (to deal with the doctor/patient role problem) in the following way: *nodes are similar to the extent they connect to similar others*, with the restriction that we can only use endogenous (structural connectivity) information---like with structural equivalence or common-neighbor approaches---to define everyone's similarity (no exogenous attribute stuff).

## Jeh and Widom's SimRank

As @jeh_widom02 note, this approach does lead to a coherent generalization of the idea of similarity for nodes in graphs because we can just define similarity *recursively* and iterate through the graph until the similarity scores stop changing.^[Note the similarity (heh, heh) between this idea and the various status ranking algorithms for computing [prestige scores](tm-prestige.qmd).] 

More specifically, they propose to measure the similarity between two nodes $i$ and $j$ at each time-step $t$ ($s_{ij}(t)$) in the iteration using the formula:


$$
   s_{ij}(t) = \frac{\alpha}{d_i d_j} \sum_{k \in N(i)} \sum_{l \in N(j)} s_{kl}(t-1)
$$ {#eq-simrank}

So the similarity between two nodes at step $t$ is just the sum of the pairwise similarities between each of their neighbors (computed in the previous step, $t-1$), weighted by the ratio of a free parameter $\alpha$ (a number between zero and one) to the product of their degrees (to take a weighted average). 

This measure nicely captures the idea that nodes are similar to the extent they both connect to people who are themselves similar. Note that it doesn't matter whether these neighbors are shared between the two nodes (the summation occurs over each pair of nodes formed by crossing the set $p$ + $q$ against $p$ + $r$ as defined in the [local similarity](locsim.qmd) lecture), whether they are themselves neighbors, which deals with the doctor/patient problem we referred to earlier. 

A function that implements this idea looks like:

```{r}
   SimRank <- function(w, alpha = 0.85) {
      d <- rowSums(w) #degree vector
      n <- nrow(w) #number of nodes in the graph
      z <- diag(1, n, n) #diagonal matrix (nodes are maximally similar to themselves)
      rownames(z) <- rownames(w)
      colnames(z) <- colnames(w)
      nei <- apply(w, 1, function(x){which(x == 1)}) #neighbors list
      delta <- 1
      while(delta > 1e-10) { #run while delta is bigger than a tiny number
         z.o <- z #old similarity matrix at t-1
         for(i in 1:n) {
            for(j in 1:n) {
               if (i != j) { 
                  zij <- 0 #initialize ij similarity for iteration t
                  for (k in names(nei[[i]])) { #loop through i's neighbors
                     for (l in names(nei[[j]])) { #loop through j's neighbors
                        zij <- zij + z[k, l] #i's similarity to j is equal to sum of neighbors' similarity
                        } #end l for loop
                     } #end k for loop
                  z[i,j] <- alpha/(d[i] * d[j]) * zij #updating new similarity matrix at t
                  } #end if
               } #end j for loop
            } #end i for loop
         delta <- abs(sum(abs(z)) - sum(abs(z.o))) #difference between similarity matrices at t and t-1
         } #end while loop
      return(z) #return similarity matrix after convergence
      } #end function
```

The `SimRank` function takes an adjacency matrix as input (`w`) and returns a generalized similarity matrix between nodes (`z`) as output.

Let's try it out in the *Flintstones* graph:

```{r}
   library(networkdata)
   library(igraph)
   library(stringr) #using stringr to change names from all caps to title case
   g <- movie_267
   V(g)$name <- str_to_title(V(g)$name)
   g <- delete_vertices(g, degree(g) <= 3) #deleting low degree vertices
   A <- as.matrix(as_adjacency_matrix(g))
   S <- SimRank(A)
   round(S[1:5, 1:5], 3)
```

We can transform the generalized similarities to distances and plot:

```{r}
#| fig-height: 10
#| fig-width: 10
   D <- dist(1- S)
   h.res <- hclust(D, method = "ward.D2") #hierarchical clustering
   plot(h.res)
```

The plot suggests a division into four major node clusters. We can obtain that as follows using the `cutree` function:

```{r}
   blocks  <- cutree(h.res, k = 4)
   blocks
```

In the SimRank equivalence partition, $\{$ Fred, Barney, Betty, Wilma, Slate, Mrs. Slate, Lava $\}$ appear as a separate cluster.  

## The LHN Approach

@leicht_etal06 provide a twist on the SimRank approach. Their idea is that a node $i$ is similar to another node $j$ just in case $i$'s neighbors are similar to $j$:

$$
   s_{ij}(t) = \alpha \sum_{k \in N(i)} s_{kj}(t-1)
$$ {#eq-lhn}

Here the pairwise similarity between $i$ and $j$ is just the sum of the previously calculated similarities between the neighbors of $i$ and $j$ weighted by the parameter $\alpha$.

Here's a function that does that:

```{r}
   LHN.sim <- function(w) {
      n <- nrow(w)
      z <- diag(1, n, n) #all nodes are maximally similar to themselves
      rownames(z) <- rownames(w)
      colnames(z) <- colnames(w)
      alpha <- (eigen(w)$values[1] - 1e-05)^-1
      d <- 1 #initial delta
      while (d > 1e-10) {
         o.z <- z
         for(i in 1:n) {
            for(j in 1:n) {
                  if (i < j) {
                     a <- names(which(w[, i] == 1)) 
                     zij <- 0
                     for (k in a) {zij <- zij + z[k, j]} #i's similarity to j is equal to sum of j's similarities to i's neighbors
                     z[i, j] <- alpha * zij
                     z[j, i] <- z[i, j]
                  }
               }
            }
         d <- abs(sum(abs(z) - abs(o.z))) #delta between new and old scores
         }
    return(z)
    }
```

```{r}
   LHN.sim <- function(w) {
      n <- nrow(w) #number of nodes in the graph
      z <- diag(1, n, n) #diagonal matrix (nodes are maximally similar to themselves)
      rownames(z) <- rownames(w)
      colnames(z) <- colnames(w)
      nei <- apply(w, 1, function(x){which(x == 1)}) #neighbors list
      delta <- 1
      while(delta > 1e-10) { #run while delta is bigger than a tiny number
         z.o <- z #old similarity matrix at t-1
         for(i in 1:n) {
            for(j in 1:n) {
               if (i != j) { 
                  zij <- 0 #initialize ij similarity for iteration t
                  for (k in names(nei[[i]])) { #loop through i's neighbors
                     for (l in names(nei[[j]])) { #loop through j's neighbors
                        zij <- zij + z[k, l] #i's similarity to j is equal to sum of neighbors' similarity
                        } #end l for loop
                     } #end k for loop
                  z[i,j] <- alpha/(d[i] * d[j]) * zij #updating new similarity matrix at t
                  } #end if
               } #end j for loop
            } #end i for loop
         delta <- abs(sum(abs(z)) - sum(abs(z.o))) #difference between similarity matrices at t and t-1
         } #end while loop
      return(z) #return similarity matrix after convergence
      } #end function
```

And here are the results for the Flintstones network:

```{r}
   S <- LHN.sim(A)
   round(S[, 1:5], 2)
```

Note that for the series to converge, we need to pick a value of $\alpha$ that is less than the reciprocal  of the largest eigenvalue of the adjacency matrix, just like the global Katz similarity [we considered before](globsim.qmd).

We can transform the similarities to distances by dividing by maximum and then clustering:

```{r}
#| fig-height: 10
#| fig-width: 10
   S <- S/max(S)
   D <- dist(1- S)
   h.res <- hclust(D, method = "ward.D2") #hierarchical clustering
   plot(h.res)
```

We we partition this clustering into four blocks, we get:

```{r}
   blocks  <- cutree(h.res, k = 4)
   blocks
```

Which is different from the SimRank partition. Here, $\{$ Fred, Barney, Betty, Wilma, Lava $\}$ are their cluster, while $\{$ Wilma, Slate, Poindexter, Piltdown, Pyrite $\}$ constitute a separate similarity class. 

## The Blondel Approach

## Appendix

A more general (and shorter) version of the function to compute `SimRank` looks like this [partly based on @fouss_etal16, pp. 84, algorithm 2.4]:

```{r}
   SimRank2 <- function(w, alpha = 0.85) {
      n <- nrow(w) #number of nodes
      d <- as.numeric(colSums(w) > 0) #degree vector
      e <- matrix(1, n, 1) #all ones column vector
      q <- diag(as.vector(t(e) %*% w), n, n) #q matrix
      diag(q) <- 1/diag(q) #setting q diagonals to inverse
      q <- w %*% q
      q[is.nan(q)] <- 0
      delta <- 1
      k <- diag(1, n, n)
      while(delta > 1e-10) {
         k.o <- k
         k.p <- alpha * t(q) %*% k %*% q
         k <- k.p - diag(diag(k.p), n, n) + diag(d, n, n)
         delta <- abs(sum(abs(k)) - sum(abs(k.o)))
         }
      rownames(k) <- rownames(w)
      colnames(k) <- colnames(w)
      return(k)
   }
```