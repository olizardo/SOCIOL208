[
  {
    "objectID": "tm-ca.html",
    "href": "tm-ca.html",
    "title": "Correspondence Analysis (CA) of Two-Mode Networks",
    "section": "",
    "text": "Correspondence Analysis (CA) a relatively simple way to analyze and visualize two-mode data. As you might have already guessed, two-mode CA boils down to the eigendecomposition of a suitable matrix, derived from the original affiliation (bi-adjacency) matrix of a two-mode network.\nThe goal is to come up with a low-rank (usually two-dimensional) approximation of the original affiliation network using the eigenvectors and eigenvalues obtained from the decomposition, as we did above with our toy example.\nSo which matrix should be use for CA?\nLet’s find out:\nFirst we need to create row stochastic versions of the affiliation matrix and its transpose \\(\\mathbf{A}\\) and \\(\\mathbf{A}^T\\). Recall that a matrix is row stochastic if their rows sum to one.\nFor the people, we can do this by taking the original affiliation matrix, and pre-multiplying it by a diagonal square matrix \\(\\mathbf{D}_P^{-1}\\) of dimensions \\(M \\times M\\) containing the inverse of the degrees of each person in the affiliation network along the diagonals and zeros everywhere else, yielding the row-stochastic matrix \\(\\mathbf{P}_{PG}\\) of dimensions \\(M \\times N\\):\n\\[\n\\mathbf{P}_{PG} = \\mathbf{D}_P^{-1}\\mathbf{A}\n\\]\nAnd we can do the same with the groups, except that we pre-multiply the transpose of the original affiliation matrix by \\(\\mathbf{D}_G^{-1}\\) which is an \\(N \\times N\\) matrix containing the inverse of the size of each group along the diagonals and zero everywhere else, this yields the matrix \\(\\mathbf{P}_{GP}\\) of dimensions \\(N \\times M\\):\n\\[\n\\mathbf{P}_{GP} = \\mathbf{D}_G^{-1}\\mathbf{A}^T\n\\]\nIn R can compute \\(\\mathbf{P}_{PG}\\) and \\(\\mathbf{P}_{GP}\\), using the classic Southern Women two-mode data, as follows:\nAnd we can check that both \\(\\mathbf{P}_{PG}\\) and \\(\\mathbf{P}_{GP}\\) are indeed row stochastic:\nAnd that they are of the predicted dimensions:\nGreat! Now, we can obtain the degree-normalized projections for people by multiplying \\(\\mathbf{P}_{PG}\\) times \\(\\mathbf{P}_{GP}\\):\n\\[\n\\mathbf{P}_{PP} = \\mathbf{P}_{PG}\\mathbf{P}_{GP}\n\\]\nWhich produces the matrix \\(\\mathbf{P}_{PP}\\) a square \\(M \\times M\\) matrix containing the degree-normalized similarities between each pair of people.\nWe then do the same for groups:\n\\[\n\\mathbf{P}_{GG} = \\mathbf{P}_{GP}\\mathbf{P}_{PG}\n\\]\nWhich produces the matrix \\(\\mathbf{P}_{GG}\\) a square \\(N \\times N\\) matrix containing the degree-normalized similarities between each pair of groups.\nIn R we obtain these matrices as follows:\nWhich are still row stochastic–but now square–matrices:\nLet’s peek inside one of these matrices:\nWhat are these numbers? Well, they can be interpreted as probabilities that a random walker starting at the row node and, following any sequence of \\(person-group-person'-group'\\) hops, will reach the column person. Thus, higher values indicate an affinity or proximity between the people (and the groups in the corresponding matrix)."
  },
  {
    "objectID": "tm-ca.html#performing-ca",
    "href": "tm-ca.html#performing-ca",
    "title": "Correspondence Analysis (CA) of Two-Mode Networks",
    "section": "Performing CA",
    "text": "Performing CA\nWe went through all these steps because CA is equivalent to the eigendecomposition of the last two square matrices we obtained, namely, \\(\\mathbf{P_{PP}}\\) and \\(\\mathbf{P_{GG}}\\):\n\n   CA.p <- lapply(eigen(P.pp), Re)\n   CA.g <- lapply(eigen(P.gg), Re)\n\nLet’s see what we have here:\n\n   round(CA.p$values, 2)\n\n [1] 1.00 0.63 0.32 0.18 0.14 0.11 0.10 0.06 0.04 0.04 0.02 0.01 0.01 0.00 0.00\n[16] 0.00 0.00 0.00\n\n   round(CA.g$values, 2)\n\n [1] 1.00 0.63 0.32 0.18 0.14 0.11 0.10 0.06 0.04 0.04 0.02 0.01 0.01 0.00\n\n\nSo the two matrices have identical eigenvalues, and the first one is 1.0.\nLet’s check out the first three eigenvectors:\n\n   rownames(CA.p$vectors) <- rownames(A)\n   rownames(CA.g$vectors) <- colnames(A)\n   round(CA.p$vectors[, 1:3], 2)\n\n           [,1]  [,2]  [,3]\nEVELYN    -0.24 -0.24  0.03\nLAURA     -0.24 -0.25 -0.01\nTHERESA   -0.24 -0.20  0.02\nBRENDA    -0.24 -0.26 -0.01\nCHARLOTTE -0.24 -0.29 -0.01\nFRANCES   -0.24 -0.24 -0.02\nELEANOR   -0.24 -0.15 -0.03\nPEARL     -0.24 -0.01  0.06\nRUTH      -0.24 -0.05  0.03\nVERNE     -0.24  0.13 -0.04\nMYRNA     -0.24  0.25 -0.10\nKATHERINE -0.24  0.31 -0.22\nSYLVIA    -0.24  0.26 -0.20\nNORA      -0.24  0.26 -0.03\nHELEN     -0.24  0.24  0.07\nDOROTHY   -0.24  0.09  0.09\nOLIVIA    -0.24  0.33  0.66\nFLORA     -0.24  0.33  0.66\n\n   round(CA.g$vectors[, 1:3], 2)\n\n      [,1]  [,2]  [,3]\n6/27  0.27 -0.30 -0.01\n3/2   0.27 -0.28 -0.04\n4/12  0.27 -0.30  0.00\n9/26  0.27 -0.30 -0.02\n2/25  0.27 -0.25  0.00\n5/19  0.27 -0.16  0.00\n3/15  0.27 -0.04  0.05\n9/16  0.27 -0.01  0.05\n4/8   0.27  0.15 -0.19\n6/10  0.27  0.32  0.22\n2/23  0.27  0.35 -0.79\n4/7   0.27  0.29  0.20\n11/21 0.27  0.34  0.35\n8/3   0.27  0.34  0.35\n\n\nSo this is interesting. The first eigenvector of the decomposition of both \\(\\mathbf{P_{PP}}\\) and \\(\\mathbf{P_{GG}}\\) is just the same number for each person and group. Note that this is the eigenvector that is associated with the first eigenvalue which happens to be \\(\\lambda_1 = 1.0\\).\nSo it looks like the first eigenvector is a pretty useless quantity (a constant) so we can discard it, keeping all the other ones. Now the old second eigenvector is the first, the old third is the second, and so on:\n\n   eig.vec.p <- CA.p$vectors[, 2:ncol(CA.p$vectors)]\n   eig.vec.g <- CA.g$vectors[, 2:ncol(CA.g$vectors)]\n\nNote that the rest of the eigenvalues (discarding the 1.0 one) are arranged in descending order:\n\n   eig.vals <- CA.p$values[2:length(CA.p$values)]\n   round(eig.vals, 3)\n\n [1] 0.627 0.319 0.179 0.138 0.107 0.099 0.064 0.044 0.036 0.021 0.012 0.005\n[13] 0.000 0.000 0.000 0.000 0.000\n\n\nThe magnitude of the eigenvalue tells us how important is the related eigenvector in containing information about the original matrix. It looks like here, the first two eigenvectors contain a good chunk of the info.\nWe can check how much exactly by computing the ratio between the sum of the first two eigenvalues over the sum of all the eigenvalues:\n\n   round(sum(eig.vals[1:2])/sum(eig.vals), 2)\n\n[1] 0.57\n\n\nWhich tells us that the first two eigenvectors account for about 57% of the action (or more precisely we could reconstruct the original matrix with 57% accuracy using just these two eigenvectors and associated eigenvalues).\nBecause the magnitude of the CA eigenvectors don’t have a natural scale, it is common to normalize them to have a variance of 1.0 (Fouss, Saerens, and Shimbo 2016, 399), and the multiplying them by the square root of the eigenvalue corresponding to that dimension, so that the new variance is scaled to the importance of that dimension.\nWe can perform the normalization of the raw CA scores using the following function, which performs CA on the affiliation matrix:\n\n   norm.CA.vec <- function(x) {\n      D.p <- diag(rowSums(x)) #degree matrix for persons\n      D.g <- diag(colSums(x)) #degree matrix for groups\n      i.Dp <- solve(D.p) #inverse of degree matrix for persons\n      i.Dg <- solve(D.g) #inverse of degree matrix for groups\n      CA.p <- lapply(eigen(i.Dp %*% x %*% i.Dg %*% t(x)), Re) #person CA\n      CA.g <- lapply(eigen(i.Dg %*% t(x) %*% i.Dp %*% x), Re) #group CA\n      ev <- CA.p$values[2:length(CA.g$values)]\n      m <- length(ev)\n      CA.p <- CA.p$vectors[, 2:ncol(CA.p$vectors)]\n      CA.g <- CA.g$vectors[, 2:ncol(CA.g$vectors)]\n      rownames(CA.p) <- rownames(A)\n      rownames(CA.g) <- colnames(A)\n      Z.u.p <- matrix(0, nrow(x), m)\n      Z.u.g <- matrix(0, ncol(x), m)\n      Z.v.p <- matrix(0, nrow(x), m)\n      Z.v.g <- matrix(0, ncol(x), m)\n      rownames(Z.u.p) <- rownames(x)\n      rownames(Z.u.g) <- colnames(x)\n      rownames(Z.v.p) <- rownames(x)\n      rownames(Z.v.g) <- colnames(x)\n      for (i in 1:m) {\n         ev.p <- as.matrix(CA.p[, i])\n         ev.g <- as.matrix(CA.g[, i])\n         norm.p <- as.numeric(t(ev.p) %*% D.p %*% ev.p) #person norm\n         Z.u.p[, i] <- ev.p * sqrt(sum(A)/norm.p) #normalizing to unit variance\n         Z.v.p[, i] <- Z.u.p[, i] * sqrt(ev[i]) #normalizing to square root of eigenvalue\n         norm.g <- as.numeric(t(ev.g) %*% D.g %*% ev.g) #group norm\n         Z.u.g[, i] <- ev.g * sqrt(sum(A)/norm.g) #normalizing to unit variance\n         Z.v.g[, i] <- Z.u.g[, i] * sqrt(ev[i]) #normalizing to square root of eigenvalue\n      }\n   return(list(Z.u.p = Z.u.p, Z.u.g = Z.u.g,\n               Z.v.p = Z.v.p, Z.v.g = Z.v.g))\n   }\n\nThis function takes the bi-adjacency matrix as input and returns two new set of normalized CA scores for both persons and groups as output. The normalized CA scores are stored in four separate matrices: \\(\\mathbf{Z_P^U}, \\mathbf{Z_G^U}, \\mathbf{Z_P^V}, \\mathbf{Z_G^V}\\).\nOne person-group set of scores is normalized to unit variance (Z.u.p and Z.u.g) and the other person-group set of scores is normalized to the scale of the eigenvalue corresponding to each CA dimension for both persons and groups (Z.v.p and Z.v.g).\nLet’s see the normalization function at work, extracting the first two dimensions for persons and groups (the first two columns of each \\(\\mathbf{Z}\\) matrix):\n\n   CA.res <- norm.CA.vec(A)\n   uni.p <- CA.res$Z.u.p[, 1:2]\n   uni.g <- CA.res$Z.u.g[, 1:2]\n   val.p <- CA.res$Z.v.p[, 1:2]\n   val.g <- CA.res$Z.v.g[, 1:2]\n   round(uni.p, 2)\n\n           [,1]  [,2]\nEVELYN    -1.01  0.20\nLAURA     -1.06 -0.07\nTHERESA   -0.83  0.14\nBRENDA    -1.08 -0.09\nCHARLOTTE -1.23 -0.07\nFRANCES   -1.01 -0.10\nELEANOR   -0.65 -0.21\nPEARL     -0.05  0.37\nRUTH      -0.21  0.17\nVERNE      0.55 -0.23\nMYRNA      1.04 -0.58\nKATHERINE  1.32 -1.33\nSYLVIA     1.10 -1.20\nNORA       1.10 -0.19\nHELEN      1.02  0.44\nDOROTHY    0.38  0.55\nOLIVIA     1.38  3.98\nFLORA      1.38  3.98\n\n   round(uni.g, 2)\n\n       [,1]  [,2]\n6/27  -1.33 -0.02\n3/2   -1.22 -0.16\n4/12  -1.31  0.00\n9/26  -1.31 -0.08\n2/25  -1.12  0.00\n5/19  -0.72 -0.01\n3/15  -0.16  0.23\n9/16  -0.04  0.24\n4/8    0.65 -0.87\n6/10   1.41  1.01\n2/23   1.54 -3.64\n4/7    1.29  0.91\n11/21  1.48  1.61\n8/3    1.48  1.61\n\n   round(val.p, 2)\n\n           [,1]  [,2]\nEVELYN    -0.80  0.11\nLAURA     -0.84 -0.04\nTHERESA   -0.65  0.08\nBRENDA    -0.86 -0.05\nCHARLOTTE -0.97 -0.04\nFRANCES   -0.80 -0.06\nELEANOR   -0.51 -0.12\nPEARL     -0.04  0.21\nRUTH      -0.17  0.10\nVERNE      0.43 -0.13\nMYRNA      0.83 -0.33\nKATHERINE  1.05 -0.75\nSYLVIA     0.87 -0.68\nNORA       0.87 -0.11\nHELEN      0.81  0.25\nDOROTHY    0.30  0.31\nOLIVIA     1.10  2.25\nFLORA      1.10  2.25\n\n   round(val.g, 2)\n\n       [,1]  [,2]\n6/27  -1.05 -0.01\n3/2   -0.97 -0.09\n4/12  -1.04  0.00\n9/26  -1.04 -0.05\n2/25  -0.88  0.00\n5/19  -0.57 -0.01\n3/15  -0.13  0.13\n9/16  -0.03  0.14\n4/8    0.51 -0.49\n6/10   1.12  0.57\n2/23   1.22 -2.05\n4/7    1.02  0.52\n11/21  1.17  0.91\n8/3    1.17  0.91\n\n\nGreat! Now we have two sets (unit variance versus eigenvalue variance) of normalized CA scores for persons and groups on the first two dimensions."
  },
  {
    "objectID": "tm-ca.html#the-duality-of-ca-scores-between-persons-and-groups",
    "href": "tm-ca.html#the-duality-of-ca-scores-between-persons-and-groups",
    "title": "Correspondence Analysis (CA) of Two-Mode Networks",
    "section": "The Duality of CA Scores Between Persons and Groups",
    "text": "The Duality of CA Scores Between Persons and Groups\nJust like as we saw when discussing the Bonacich Eigenvector centrality in the two-mode case, there is a duality between the CA scores assigned to the person and the groups on each dimension, such that the scores for each person are a weighted sum of the scores assigned to each group on that dimension and vice versa (Faust 1997, 171).\nThe main difference is that this time we sum scores across the \\(\\mathbf{P_P}\\) and \\(\\mathbf{P_G}\\) matrices rather than the original affiliation matrix and its transpose, resulting in degree-weighted sums of scores for both persons and groups.\nSo for any given person, on any given dimension, let’s say \\(EVELYN\\), her CA score is given by the sum of the (unit variance normalized) CA scores of the groups she belongs to weighted by her degree (done by multiplying each CA score by the relevant cell in Evelyn’s row of the \\(\\mathbf{P}_{PG}\\) matrix):\n\n   sum(P.pg[\"EVELYN\", ] * uni.g[, 1]) \n\n[1] -0.7994396\n\n\nWhich is the same as the (eigenvalue variance) normalized score we obtained via CA for \\(EVELYN\\):\n\n   val.p[\"EVELYN\", 1]\n\n    EVELYN \n-0.7994396 \n\n\nA similar story applies to groups. Each group score is the group-size-weighted sum of the (unit variance normalized) CA scores of the people who join it:\n\n   sum(P.gp[\"6/27\", ] * uni.p[, 1])\n\n[1] -1.051052\n\n\nWhich is the same as the (eigenvalue variance normalized) score we obtained via CA:\n\n    val.g[\"6/27\", 1]  \n\n     6/27 \n-1.051052 \n\n\nNeat! Duality at work."
  },
  {
    "objectID": "tm-ca.html#visualizing-two-mode-networks-using-ca",
    "href": "tm-ca.html#visualizing-two-mode-networks-using-ca",
    "title": "Correspondence Analysis (CA) of Two-Mode Networks",
    "section": "Visualizing Two-Mode Networks Using CA",
    "text": "Visualizing Two-Mode Networks Using CA\nAnd, finally, we can use the first two (eigenvalue variance) normalized CA scores to plot the persons and groups in a common two-dimensional space:\n\n   val.g[, 2] <- val.g[, 2]*-1 #flippling sign of group scores on second dimension for plotting purposes\n   plot.dat <- data.frame(rbind(uni.p, val.g)) %>% \n      cbind(type = as.factor(c(rep(1, 18), rep(2, 14))))\n   library(ggplot2)\n   # install.packages(\"ggrepel\")\n   library(ggrepel)\n   p <- ggplot(data = plot.dat, aes(x = X1, y = X2, color = type))\n   p <- p + geom_hline(aes(yintercept = 0), color = \"gray\")\n   p <- p + geom_vline(aes(xintercept = 0), color = \"gray\")\n   p <- p + geom_text_repel(aes(label = rownames(plot.dat)), \n                            max.overlaps = 20, size = 2.75)\n   p <- p + theme_minimal()\n   p <- p + theme(legend.position = \"none\",\n                  axis.title = element_text(size = 14),\n                  axis.text = element_text(size = 12))\n   p <- p + scale_color_manual(values = c(\"red\", \"blue\"))\n   p <- p + labs(x = \"First Dimension\", y = \"Second Dimension\") \n   p <- p + xlim(-2, 2) + ylim(-2, 4)\n   p\n\n\n\n\nIn this space, people with the most similar patterns of memberships to the most similar groups are placed close to one another. In the same way, groups with the most similar members are placed closed to one another.\nAlso like before, we can use the scores obtained from the CA analysis to re-arrange the rows and columns of the original matrix to reveal blocks of maximally similar persons and events:\n\n   library(ggcorrplot)\n   p <- ggcorrplot(t(A[order(val.p[,1]), order(val.g[,1])]), \n                   colors = c(\"white\", \"white\", \"red\")) \n   p <- p + theme(legend.position = \"none\", \n                  axis.text.y = element_text(size = 8),\n                  axis.text.x = element_text(size = 8, angle = 0),\n                  )\n   p <- p + scale_x_discrete(position = \"top\") \n   p <- p + geom_hline(yintercept = 6.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_hline(yintercept = 11.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_hline(yintercept = 16.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_vline(xintercept = 9.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_vline(xintercept = 6.5, linewidth = 2, color = \"blue\")\n   p\n\n\n\n\nHere CA seems to have detected two separate clusters of actors who preferentially attend two distinct clusters of events!\nThe three events in the middle {3/15, 9/16, 4/8} don’t seem to differentiate between participants in each cluster (everyone attends)–they thus appear near the origin in the CA diagram, indicating a weak association with either dimension.\nHowever, the events to the left (with clusters of participants in the lower-left) and to the right of the x-axis (with clusters of participants in the upper-right) are attended preferentially by distinct groups of participants; they thus appear at the extreme left and right positions of the first dimension of the CA diagram.\nIn the same way, the four people in the middle {Ruth, Dorothy, Pearl, Verne} only attend the undifferentiated, popular events, so that means that they are not strongly associated with either cluster of actors (and thus appear near the origin in the CA diagram). The top and bottom participants, by contrast, appear to the extreme right and left in the CA diagram, indicating a strong association with the underlying dimensions.\nNote the similarity between this blocking and that obtained from the generalized vertex similarity analysis of the same network."
  },
  {
    "objectID": "tm-spectral.html",
    "href": "tm-spectral.html",
    "title": "Spectral Clustering of Two-Mode Networks",
    "section": "",
    "text": "We can use a variant of the spectral clustering approach to find communities in two-mode networks (Wu, Gu, and Yang 2022). This approach combines Correspondence Analysis (CA) and k-means clustering on multiple dimensions.\nSo let’s bring back our old friend, the Southern Women data:\nLet’s also compute the bi-adjacency modularity matrix:\nGreat! Now, from this information we can compute a version of the bipartite modularity matrix:\nAnd now let’s find the CA scores. This time will use the canned function CA from the the package FactoMineR\nWhich computes CA directly on the bi-adjacency matrix (the argument ncp asks to keep the first ten dimensions).\nWe can now extract the CA scores for persons and groups from the resulting object:\nGreat! You can verify that these are the same scores we obtained in two-mode CA lecture via a more elaborate route.\nNow, we can just create our U matrix by stacking the person and group scores using the first three dimensions:\nNice! Now we can just feed U to our k.cuts function to place persons and groups into cluster assignments beginning with two and ending with ten:\nOf course, we can’t use the mod.check function we used before because that relies on the standard method for checking the modularity in one-mode networks and doesn’t take into account the structural zeros in the bipartite graph.\nSo we need to come up with a custom method to check the modularity for the bipartite case.\nFirst, we need a function that takes a cluster assignment vector containing numbers for each cluster \\(k = \\{1, 2, 3, \\ldots C\\}\\) and turns it into a dummy coded cluster assignment matrix:\nLet’s test it out:\nGreat! Looks like it works.\nFinally, we need to write a custom function for bipartite modularity checking across different assignments:\nThe function mod.check2 needs three inputs: The bipartite modularity matrix, a list with different assignments of the nodes in the bipartite graph to different clusters, and the bipartite adjacency matrix. It returns a vector m with the modularities of each of the partitions in the list c.\nAnd, now, for the big reveal:\nLooks like the spectral clustering results favor a four-community partition although the more parsimonious three and binary community partitions also look pretty good.\nFigure 1 (a) and Figure 1 (b) show a plot of the three and four community solutions according to the CA dimensions (since we already saw the binary partition in the CA handout).\nOf course, just like we did with one-mode networks, we can also obtain a spectral clustering directly from the eigenvectors of the bipartite modularity matrix in just a couple of lines:\nHere we create the U matrix from the first two dimensions of the eigendecomposition of the bipartite modularity matrix. The results suggest that the three community partition is optimal, although the two-community one also does well. The corresponding splits are shown in Figure 1 (c) and Figure 1 (d).\nNote that the main difference between CA and modularity based clustering in two-mode networks is that modularity seems to prefer evenly balanced communities (in terms of number of nodes), while CA does not mind grouping nodes into small communities (like \\(\\{Flora, Nora, 2/23\\}\\))"
  },
  {
    "objectID": "tm-spectral.html#bipartite-modularity-allowing-people-and-groups-to-belong-to-different-number-of-communities",
    "href": "tm-spectral.html#bipartite-modularity-allowing-people-and-groups-to-belong-to-different-number-of-communities",
    "title": "Spectral Clustering of Two-Mode Networks",
    "section": "Bipartite Modularity Allowing People and Groups to Belong to Different Number of Communities",
    "text": "Bipartite Modularity Allowing People and Groups to Belong to Different Number of Communities\nOne limitation of Barber’s (2007) approach to computing the modularity we have been using to analyze community structure in two-mode networks is that it can only be used under the assumption that the number of communities is the same for both persons and groups.\nHowever, it could be that the optimal partition is actually one in which the people node set is split into a different number of clusters than the group node set.\nSo we need a way to evaluate the modularity of a partition when we have different number of communities on the people and group side.\nHere’s how to do it.\nFirst, we generate two separate candidate community assignments for people and groups via spectral clustering from CA using the first six eigenvectors:\n\n   k.cuts.p <- k.cuts(CA.res$row$coord[, 1:6])\n   k.cuts.g <- k.cuts(CA.res$col$coord[, 1:6])\n\nAs an example, let’s pick the solution that partitions people into four communities and the groups into three communities:\n\n   C.p <- k.cuts.p[[3]]\n   C.g <- k.cuts.g[[2]]\n\nGiven this information, we can create a \\(4 \\times 3\\) matrix recording the proportion of ties in the network that go from person-community \\(l\\) to group-community \\(m\\):\n\n   e <- matrix(0, 4, 3)\n   for (l in 1:4) {\n      for (m in 1:3) {\n         e[l, m] = sum(A[C.p == l, C.g == m]) * 1/sum(A)\n      }\n   }\n   round(e, 2)\n\n     [,1] [,2] [,3]\n[1,] 0.19 0.02 0.17\n[2,] 0.00 0.00 0.04\n[3,] 0.00 0.02 0.02\n[4,] 0.00 0.00 0.53\n\n\nFor instance, this matrix says that 53% of the ties in the Southern women data go from people in the fourth community to groups in the third community, according to the CA spectral partition.\nSuzuki and Wakita (2009), building on work by Murata (2009), suggest using the e matrix above to compute the modularity of any pair of person/group community assignment according to the following formula:\n\\[\nQ = \\frac{1}{2}\\sum_{l, m}\\frac{e_{lm}}{e_{l+}}\\left(e_{lm} - e_{l+}e_{+m}\\right)\n\\]\nWhere \\(e_{lm}\\) is the proportion of edges connecting people in the \\(l^{th}\\) person-community to groups in the \\(m^{th}\\) event-community, \\(e_{l+}\\) is the proportion of edges originating from person-community \\(i\\) (the corresponding entry of the row sum of e), and \\(e_{+m}\\) is the proportion of edges originating from nodes in group-community \\(j\\) (the corresponding entry of the column sum of e).\nSo the idea is that given a partition of the person nodes into \\(L\\) communities and a partition of the group nodes into \\(M\\) communities, we can generate an e matrix like the one above and compute the corresponding modularity of that person/group partition using the above equation.\nHere’s a function that computes the e matrix from a pair of person/group partitions and then returns the \\(Q\\) value corresponding to that matrix using the above formula:\n\n   find.mod <- function(w, x, y) {\n      Cx <- max(x)\n      Cy <- max(y)\n      M <- sum(w)\n      e <- matrix(0, Cx, Cy)\n      for (l in 1:Cx) {\n         for (m in 1:Cy) {\n            e[l, m] = sum(w[x == l, y == m]) * 1/M\n         }\n      }\n      Q <- 0\n      a <- rowSums(e)\n      b <- colSums(e)\n      for (l in 1:Cx) {\n         for (m in 1:Cy) {\n            Q <- Q + (e[l, m]/a[l] * (e[l, m] - a[l]*b[m]))\n         }\n      }\n   return(Q/2)\n   }\n\nSo for the e matrix from the above example, \\(Q\\) would be:\n\n   find.mod(A,  k.cuts.p[[3]],  k.cuts.g[[2]])\n\n[1] 0.07220939\n\n\nWhich looks like a positive number. But is it the biggest of all the possible community partition combinations between people and groups?\nTo answer this question, we can use the find.mod function to compute the modularity between every pair of partitions between people and groups that we calculated earlier. Since we computed eight different partitions for people and groups this leads to \\(8 \\times 8 = 64\\) pairs.\nHere’s a wrapper function over find.mod that computes the corresponding modularity values for each partition pair:\n\n   mod.mat <- function(d) {\n     q <- matrix(0, d, d)\n      for (i in 1:d) {\n         for (j in 1:d) {\n               q[i, j] <- find.mod(A,  k.cuts.p[[i]],  k.cuts.g[[j]])\n         }\n      }\n    return(q)\n   }\n   Q <- round(mod.mat(8), 3)\n   Q\n\n      [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]\n[1,] 0.102 0.058 0.042 0.031 0.032 0.033 0.029 0.024\n[2,] 0.106 0.066 0.050 0.038 0.039 0.041 0.035 0.033\n[3,] 0.104 0.072 0.056 0.042 0.045 0.043 0.037 0.039\n[4,] 0.112 0.074 0.069 0.055 0.058 0.056 0.050 0.046\n[5,] 0.112 0.075 0.069 0.058 0.061 0.060 0.054 0.050\n[6,] 0.110 0.074 0.075 0.064 0.061 0.063 0.058 0.054\n[7,] 0.111 0.077 0.077 0.066 0.067 0.068 0.062 0.059\n[8,] 0.111 0.077 0.078 0.067 0.067 0.070 0.064 0.060\n\n\nInterestingly, the analysis suggests that the maximum modularity \\(Q = 0.112\\) is obtained with a partition of people into five communities and groups into two communities corresponding to cells \\((4, 1)\\) of the above matrix.\nHere’s what this community assignment looks like in the Southern Women data:\n\n\n\n\n\nSpectral Clustering of Nodes in the Southern Women Data with Optimal Community Assignment Obtained via the Suzuki Modularity, with Five Communities for People, Two Communities for Groups.\n\n\n\n\nThe analysis separates two groups of densely connected actors of size six and five, respectively, namely, {Brenda, Theresa, Laura, Frances, Evelyn, Eleanor} and {Katherine, Nora, Sylvia, Myrna, Helen} along with their corresponding events from the one another. In the same way, {Pearl, Dorothy, Ruth, Verne} form a community of more peripheral actors who selectively attend the more popular events; {Flora, Olivia} are a two-actor community occupying the most peripheral position. Among the core set of actors, {Charlotte} occupies a singleton-community of her own.\nEvents are partitioned into two broad groups: One the one hand, we have those selectively attended by the larger community of densely connected actors along with the most popular events; on the other hand, we have the events selectively attended by the smaller group of densely connected actors."
  },
  {
    "objectID": "tm-ca.html#another-way-of-performing-ca-in-a-two-mode-network",
    "href": "tm-ca.html#another-way-of-performing-ca-in-a-two-mode-network",
    "title": "Correspondence Analysis (CA) of Two-Mode Networks",
    "section": "Another way of Performing CA in a Two-Mode Network",
    "text": "Another way of Performing CA in a Two-Mode Network\nSciarra et al. (2020) suggest an alternative way of computing CA for two-mode networks that avoids the issue of having to do the eigendecomposition of an asymmetric matrix (like the \\(\\mathbf{P}_{PP}\\) and \\(\\mathbf{P}_{GG}\\)) above. Their suggestion is to normalize the biadjacency matrix \\(\\mathbf{A}\\), before computing the degree-weighted projections, according to the formula:\n\\[\n\\tilde{\\mathbf{A}} = \\mathbf{D^{-2}_P}\\mathbf{A}\\mathbf{D^{-2}_G}\n\\]\nWhere \\(\\mathbf{D^{-2}}\\) is a matrix containing the inverse of the square root of the degrees in diagonals and zero everywhere else.\nWe can then obtain symmetric versions of the projection matrices, by doing the usual formula:\n\\[\n\\mathbf{P} = \\tilde{\\mathbf{A}}\\tilde{\\mathbf{A}}^T\n\\]\n\\[\n\\mathbf{G} = \\tilde{\\mathbf{A}}^T\\tilde{\\mathbf{A}}\n\\]\nThe matrices \\(\\mathbf{P}\\) and \\(\\mathbf{G}\\) no longer have the nice interpretation of being Markov transition matrices that the \\(\\mathbf{P}_{pp}\\) \\(\\mathbf{P}_{gg}\\) matrices have, but the fact that they are symmetric can be an advantage in faciliating the eigendecomposition computation as they are guaranteed to have real-valued solutions.\nLet’s see how this would work:\n\n   D2.p <- diag(1/sqrt(rowSums(A)))\n   D2.g <- diag(1/sqrt(colSums(A)))\n   A2 <- D2.p %*% A %*% D2.g\n   P <- A2 %*% t(A2)\n   G <- t(A2) %*% A2\n\nWe can check that indeed \\(\\mathbf{P}\\) and \\(\\mathbf{G}\\) are symmetric:\n\n   isSymmetric(P)\n\n[1] TRUE\n\n   isSymmetric(G)\n\n[1] TRUE\n\n\nAnd now we can obtain the row and column CA scores via eigendecomposition of these matrices:\n\n   eig.vec.p2 <- eigen(P)$vectors\n   eig.vec.g2 <- eigen(G)$vectors\n\nAs Sciarra et al. (2020, 3) also note, to have the eigenvectors obtained via this procedure match the CA scores, we need to rescale the standard CA scores by multiplying them by the square root of the degrees of the row and column nodes:\n\n   cor(CA.res$Z.u.p[, 1] * sqrt(rowSums(A)), eig.vec.p2[, 2])\n\n[1] 1\n\n   cor(CA.res$Z.u.g[, 1] * sqrt(colSums(A)), eig.vec.g2[, 2])\n\n[1] 1\n\n\nWhich as we can see, results in perfectly correlated scores across the two methods."
  },
  {
    "objectID": "tm-ca.html#the-duality-of-persons-and-groups-ca-scores",
    "href": "tm-ca.html#the-duality-of-persons-and-groups-ca-scores",
    "title": "Correspondence Analysis (CA) of Two-Mode Networks",
    "section": "The Duality of Persons and Groups CA Scores",
    "text": "The Duality of Persons and Groups CA Scores\nJust like as we saw when discussing the Bonacich Eigenvector centrality in the two-mode case, there is a duality between the CA scores assigned to the person and the groups on each dimension, such that the scores for each person are a weighted sum of the scores assigned to each group on that dimension and vice versa (Faust 1997, 171).\nThe main difference is that this time we sum scores across the \\(\\mathbf{P_P}\\) and \\(\\mathbf{P_G}\\) matrices rather than the original affiliation matrix and its transpose, resulting in degree-weighted sums of scores for both persons and groups.\nSo for any given person, on any given dimension, let’s say \\(EVELYN\\), her CA score is given by the sum of the (unit variance normalized) CA scores of the groups she belongs to weighted by her degree (done by multiplying each CA score by the relevant cell in Evelyn’s row of the \\(\\mathbf{P}_{PG}\\) matrix):\n\n   sum(P.pg[\"EVELYN\", ] * uni.g[, 1]) \n\n[1] -0.7994396\n\n\nWhich is the same as the (eigenvalue variance) normalized score we obtained via CA for \\(EVELYN\\):\n\n   val.p[\"EVELYN\", 1]\n\n    EVELYN \n-0.7994396 \n\n\nA similar story applies to groups. Each group score is the group-size-weighted sum of the (unit variance normalized) CA scores of the people who join it:\n\n   sum(P.gp[\"6/27\", ] * uni.p[, 1])\n\n[1] -1.051052\n\n\nWhich is the same as the (eigenvalue variance normalized) score we obtained via CA:\n\n    val.g[\"6/27\", 1]  \n\n     6/27 \n-1.051052 \n\n\nNeat! Duality at work."
  }
]