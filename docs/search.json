[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Social Networks Sequence",
    "section": "",
    "text": "Website containing syllabi, reading schedules, and other instructional materials for the courses in the Social Network Analysis computational sequence (208A & 208B) at UCLA Sociology."
  },
  {
    "objectID": "structequiv.html",
    "href": "structequiv.html",
    "title": "Role Equivalence and Structural Similarity",
    "section": "",
    "text": "One of the earlier “proofs of concept” of the power of social network analysis came from demonstrating that you could formalize the fuzzy idea of “role” central to functionalist sociology and British social anthropology using the combined tools of graph theoretical and matrix representations of networks (White, Boorman, and Breiger 1976).\nThis and other contemporaneous work (Breiger, Boorman, and Arabie 1975) set off an entire sub-tradition of data analysis of networks focused on the idea that one could partition the set of vertices in a graph into meaningful classes based on some mathematical (e.g., graph theoretic) criterion.\nThese classes would in turn would be isomorphic with the concept of role as social position and the classes thereby derived as indicating the number of such positions in the social structure under investigation as well as which actors belonged to which positions."
  },
  {
    "objectID": "structequiv.html#structural-equivalence",
    "href": "structequiv.html#structural-equivalence",
    "title": "Role Equivalence and Structural Similarity",
    "section": "Structural Equivalence",
    "text": "Structural Equivalence\nThe earliest work pursued simultaneously by analysts at Harvard (White, Boorman, and Breiger 1976) and Chicago (Burt 1976) relied on the idea of structural equivalence.\nIn a graph \\(G = \\{E, V\\}\\) two nodes \\(v_i, v_j\\) are structurally equivalent if they are connected to the same others in the network; that is, if \\(N(v_i)\\) is the set of nodes adjacent to node \\(v_i\\) and \\(N(v_j)\\) is the set of nodes adjacent to node \\(v_j\\), then:\n\\[\n   v_i \\equiv v_j \\iff N(v_i) = N(v_j)\n\\]\nIn a graph, an equivalence class \\(C\\) is just a set of nodes that are structurally equivalent, such that if \\(v_i \\in C_i\\) and \\(v_j \\in C_i\\) then \\(v_i \\equiv v_j\\) for all pairs \\((v_i, v_j) \\in C_i\\).\nThe partitioning of the vertex set into a set of equivalence classes \\(\\{C_1, C_2 \\ldots C_k\\}\\) as well as the adjacency relations between nodes in the same class and nodes in different classes defines the role structure of the network."
  },
  {
    "objectID": "structequiv.html#structural-equivalence-in-an-ideal-world",
    "href": "structequiv.html#structural-equivalence-in-an-ideal-world",
    "title": "Role Equivalence and Structural Similarity",
    "section": "Structural Equivalence in an Ideal World",
    "text": "Structural Equivalence in an Ideal World\nLet us illustrate these concepts. Consider the following toy graph:\n\n\n\n\n\nFigure 1: A toy graph demonstrating structural equivalence.\n\n\n\n\nWith associated adjacency matrix:\n\n\n\n\n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n  \n \n\n  \n    A \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    B \n    1 \n    0 \n    0 \n    1 \n    1 \n    1 \n    1 \n    0 \n    0 \n  \n  \n    C \n    1 \n    0 \n    0 \n    1 \n    1 \n    1 \n    1 \n    0 \n    0 \n  \n  \n    D \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n  \n  \n    E \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n  \n  \n    F \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n  \n  \n    G \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    H \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n    0 \n    0 \n    1 \n  \n  \n    I \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n    0 \n    1 \n    0 \n  \n\n\n\n\n\nA simple function to check for structural equivalence in the graph, relying on the native R function setequal would be:\n\n   check.equiv <- function(x) {\n      n <- vcount(x)\n      v <- V(x)$name\n      E <- matrix(0, n, n)\n      for (i in v) {\n         for (j in v) {\n            if (i != j & E[which(v == j), which(v == i)] != 1) {\n               N.i <- neighbors(x, i)\n               N.j <- neighbors(x, j)\n               if (are_adjacent(x, i, j) == TRUE) {\n                  N.i <- c(names(N.i), i)\n                  N.j <- c(names(N.j), j)\n                  } #end sub-if\n               if (setequal(N.i, N.j) == TRUE) {\n                  E[which(v == i), which(v == j)] <- 1\n                  E[which(v == j), which(v == i)] <- 1\n                  } #end sub-if\n               } #end main if\n            } #end j loop\n         } #end i loop\n      rownames(E) <- v\n      colnames(E) <- v\n   return(E)\n   }\n\nThis function creates an empty “equivalence” matrix \\(\\mathbf{E}\\) in line 4, loops through each pair of nodes in the graph in lines 5-20. The main condition restricts the checking to nodes that are not the same or have not yet to be found to be equivalent (line 7). Lines 8-9 extract the node neighborhoods using the igraph function neighbors.\nLines 10-13 check to see if the pair of nodes that are being checked for equivalence are themselves adjacent. If they are indeed adjacent (the conditional in line 10 is TRUE) then we need to use the so-called closed neighborhood of \\(v_i\\) and \\(v_j\\), written \\(N[v_i], N[v_j]\\), to do the equivalence check, or otherwise we get the wrong answer.1\nThe equivalence check is done in line 14 using the native R function setequal. This function takes two inputs (e.g., two vectors) and will return a value of TRUE if the elements in the first vector are the same as the elements in the second vector. In that case we update the matrix \\(\\mathbf{E}\\) accordingly.\nAfter writing our function, we can then type:\n\n   Equiv <- check.equiv(g)\n\nAnd the resulting equivalence matrix \\(\\mathbf{E}\\) corresponding to the graph in Figure 1 is:\n\n\n\n\n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n  \n \n\n  \n    A \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    B \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    C \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    D \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    E \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    F \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    G \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    H \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n  \n  \n    I \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n  \n\n\n\n\n\nIn this matrix, there is a 1 in the corresponding cell if the row node is structurally equivalent to the column node.\nOne thing we can do with this matrix is re-order the rows and columns, so that rows(columns) corresponding to nodes that are “adjacent” in the equivalence relation appear next to one another in the matrix.\nTo do that we can use the corrMatOrder function from the corrplot package, designed to work with correlation matrices, but works with any matrix of values:\n\n   #install.packages(\"corrplot\")\n   library(corrplot)\n   SE.ord <- corrMatOrder(Equiv, order = \"hclust\", hclust.method = \"ward.D2\")\n   SE.ord\n\n[1] 6 4 5 8 9 1 7 2 3\n\n\nThe corrplot function corrMatorder takes a matrix as input and returns a vector of reordered values of the rows(columns) as output. We use a hierarchical clustering algorithm using Ward’s method to do the job.\nWe can see that the new re-ordered vector has the previous row(column) 6 in fist position, 4 at second, five at third, 8 at fourth, and so forth.;\nWe can then re-order rows and columns of the old equivalence matrix using this new ordering by typing:\n\n   Equiv <- Equiv[SE.ord, SE.ord]\n\nThe resulting re-ordered matrix looks like:\n\n\n\n\n \n  \n      \n    F \n    D \n    E \n    H \n    I \n    A \n    G \n    B \n    C \n  \n \n\n  \n    F \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    D \n    1 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    E \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    H \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    I \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    A \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    G \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    B \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n  \n  \n    C \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n  \n\n\n\n\n\nOnce the equivalence matrix is re-ordered we can see that sets of structurally equivalent nodes in Figure 1, appear clustered along the diagonals. This type of re-arranged matrix is said to be in block-diagonal form (e.g., non-zero entries clustered along the diagonals).\nEven more interestingly, we can do the same re-arranging on the original adjacency matrix, to reveal:\n\n\n\n\n \n  \n      \n    F \n    D \n    E \n    H \n    I \n    A \n    G \n    B \n    C \n  \n \n\n  \n    F \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    1 \n    1 \n  \n  \n    D \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    1 \n    1 \n  \n  \n    E \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    1 \n    1 \n  \n  \n    H \n    1 \n    1 \n    1 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    I \n    1 \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    A \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n  \n  \n    G \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    1 \n  \n  \n    B \n    1 \n    1 \n    1 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n  \n  \n    C \n    1 \n    1 \n    1 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n  \n\n\n\n\n\nThis is called a blocked adjacency matrix. As you can see, once the structural equivalence relations in the network are revealed by permuting the rows and columns, the adjacency matrix shows an orderly pattern.\nThe way to interpret the blocked adjacency matrix is as follows:\n\nThe block diagonals of the matrix reveal the intra-block relations between sets of structurally equivalent nodes. If the block diagonal is empty–called a zero block–it means that set of structurally equivalent nodes does not connect with one another directly. If it has ones–called a one block–it it means that members of that set of structurally equivalent nodes are also neighbors.\nThe off diagonal blocks reveals the inter-block adjacency relations between different clusters of structurally equivalent nodes. If an off-diagonal block is a one-block, it means that members of block \\(C_i\\) send ties to members of block \\(C_j\\). If and off diagonal block is a zero-block, it means that members of block \\(C_i\\) avoid associating with members of block \\(C_j\\).\n\nSo if:\n\\[\nC_1 = \\{D, E, F\\}\n\\]\n\\[\nC_2 = \\{H, I\\}\n\\]\n\\[\nC_3 = \\{A, G\\}\n\\]\n\\[\nC_4 = \\{B, C\\}\n\\]\nThen we can see that:\n\nMembers of \\(C_1\\) connect with members of \\(C_2\\) and \\(C_4\\) but not among themselves.\nMembers of \\(C_2\\) connect among themselves and with \\(C_1\\).\nMembers of \\(C_3\\) connect among themselves and with \\(C_4\\).\nMembers of \\(C_4\\) connect with \\(C_1\\) and \\(C_3\\) but avoid associating with their own block.\n\nThese intra and inter-block relations can then be represented in the reduced image matrix:\n\n\n\n\n \n  \n      \n    C_1 \n    C_2 \n    C_3 \n    C_4 \n  \n \n\n  \n    C_1 \n    0 \n    1 \n    0 \n    1 \n  \n  \n    C_2 \n    1 \n    1 \n    0 \n    0 \n  \n  \n    C_3 \n    0 \n    0 \n    1 \n    1 \n  \n  \n    C_4 \n    1 \n    0 \n    1 \n    0 \n  \n\n\n\n\n\nWhich reveals a more economical representation of the system based on structural equivalence."
  },
  {
    "objectID": "structequiv.html#structural-equivalence-in-the-real-world",
    "href": "structequiv.html#structural-equivalence-in-the-real-world",
    "title": "Role Equivalence and Structural Similarity",
    "section": "Structural Equivalence in the Real World",
    "text": "Structural Equivalence in the Real World\nOf course, in real data, it is very unlikely that two nodes will meet the exact mathematical criterion of structural equivalence. They might have three out of five, or eight out of nine common neighbors but would still get a big zero in the \\(\\mathbf{E}\\) defined using our strict function.\nSo, a lot of role analysis in real networks follows instead searches for a near cousin to structural equivalence. This leads us to the large class of distance and similarity metrics, and the task is to pick one such that structural equivalence falls off as a special case of the given metric.\nFor random reasons, early work in social network analysis focused on distance metrics, while more recent work inspired by network science focuses on similarity metrics. The end goal is the same though; to cluster nodes in a graph such that those in the same class are the most structurally similar to one another.\nLet us, therefore, begin with the distance approach. Here the goal is simply to pick a distance metric \\(d\\) with a well defined minimum \\(d_{min}\\) or maximum value \\(d_{max}\\), such that:\n\\[\n   v_i \\equiv v_j \\iff d(v_i, v_j) = d_{min} \\lor d(v_i, v_j) = d_{max}\n\\]\nWhere whether we pick the maximum or minimum value depends on the particularities of the measure \\(d\\).\nWe then populate the \\(\\mathbf{E}\\) matrix with the values of \\(d\\) for each pair of nodes \\((v_i, v_j)\\), do some kind of clustering on the matrix, and use our clusters assignments to re-arrange the original adjacency matrix to find our blocks, and so forth.\nA very obvious candidate for \\(d\\) is the Euclidean Distance (Burt 1976):\n\\[\n   d_{i,j} = \\sqrt{\\sum_{k \\neq i,j} (a_{ik} - a_{jk})^2}\n\\]\nWhere \\(a_{ik}\\) and \\(a_{jk}\\) are the corresponding entries in the graph’s adjacency matrix \\(\\mathbf{A}\\). The minimum for this measure is \\(d_{min} = 0\\), so this is the value we should find for structurally equivalent nodes.\nA function that does this for any graph is:\n\n   d.euclid <- function(x) {\n      A <- as.matrix(as_adjacency_matrix(x))\n      n <- nrow(A)\n      E <- matrix(0, n, n)\n      for (i in 1:n) {\n         for (j in 1:n) {\n            if (i < j & i != j) {\n               d.ij <- 0\n               for (k in 1:n) {\n                  if (k != i & k != j) {\n                     d.ij <- d.ij + (A[i,k] - A[j,k])^2\n                     }\n                  }\n               E[i,j] <- sqrt(d.ij)\n               E[j,i] <- sqrt(d.ij)\n            }\n         }\n      }\n   rownames(E) <- rownames(A)\n   colnames(E) <- colnames(A)\n   return(E)\n   }\n\nAnd we can try it out with our toy graph:\n\n   E <- d.euclid(g)\n   round(E, 1)\n\n    A   B   C   D   E   F   G   H   I\nA 0.0 2.0 2.0 1.7 1.7 1.7 0.0 2.6 2.6\nB 2.0 0.0 0.0 2.6 2.6 2.6 2.0 1.7 1.7\nC 2.0 0.0 0.0 2.6 2.6 2.6 2.0 1.7 1.7\nD 1.7 2.6 2.6 0.0 0.0 0.0 1.7 2.0 2.0\nE 1.7 2.6 2.6 0.0 0.0 0.0 1.7 2.0 2.0\nF 1.7 2.6 2.6 0.0 0.0 0.0 1.7 2.0 2.0\nG 0.0 2.0 2.0 1.7 1.7 1.7 0.0 2.6 2.6\nH 2.6 1.7 1.7 2.0 2.0 2.0 2.6 0.0 0.0\nI 2.6 1.7 1.7 2.0 2.0 2.0 2.6 0.0 0.0\n\n\nAnd it looks like indeed it detected the structurally equivalent nodes in the graph. We can see it clearly by re-ordering the rows and columns according to our known ordering:\n\n   E <- E[SE.ord, SE.ord]\n   round(E, 1)\n\n    F   D   E   H   I   A   G   B   C\nF 0.0 0.0 0.0 2.0 2.0 1.7 1.7 2.6 2.6\nD 0.0 0.0 0.0 2.0 2.0 1.7 1.7 2.6 2.6\nE 0.0 0.0 0.0 2.0 2.0 1.7 1.7 2.6 2.6\nH 2.0 2.0 2.0 0.0 0.0 2.6 2.6 1.7 1.7\nI 2.0 2.0 2.0 0.0 0.0 2.6 2.6 1.7 1.7\nA 1.7 1.7 1.7 2.6 2.6 0.0 0.0 2.0 2.0\nG 1.7 1.7 1.7 2.6 2.6 0.0 0.0 2.0 2.0\nB 2.6 2.6 2.6 1.7 1.7 2.0 2.0 0.0 0.0\nC 2.6 2.6 2.6 1.7 1.7 2.0 2.0 0.0 0.0\n\n\nHere the block-diagonals of the matrix contain zeroes because the \\(d\\) is a distance function with a minimum of zero. If we wanted it to contain ones instead we would normalize:\n\\[\n   d^* = 1-\\left(\\frac{d}{max(d)}\\right)\n\\]\nWhich would give us:\n\n   E.norm <- 1 - E/max(E)\n   round(E.norm, 1)\n\n    F   D   E   H   I   A   G   B   C\nF 1.0 1.0 1.0 0.2 0.2 0.3 0.3 0.0 0.0\nD 1.0 1.0 1.0 0.2 0.2 0.3 0.3 0.0 0.0\nE 1.0 1.0 1.0 0.2 0.2 0.3 0.3 0.0 0.0\nH 0.2 0.2 0.2 1.0 1.0 0.0 0.0 0.3 0.3\nI 0.2 0.2 0.2 1.0 1.0 0.0 0.0 0.3 0.3\nA 0.3 0.3 0.3 0.0 0.0 1.0 1.0 0.2 0.2\nG 0.3 0.3 0.3 0.0 0.0 1.0 1.0 0.2 0.2\nB 0.0 0.0 0.0 0.3 0.3 0.2 0.2 1.0 1.0\nC 0.0 0.0 0.0 0.3 0.3 0.2 0.2 1.0 1.0\n\n\nAs noted, a distance function defines a clustering on the nodes, and the results of the clustering generate our blocks. In R we can use the functions dist and hclust to do the job:\n\n   E <- dist(E) #transforming E to a dist object\n   h.res <- hclust(E, method = \"ward.D2\")\n   plot(h.res)\n\n\n\n\nWhich are our original clusters of structurally equivalent nodes!\nLet’s see how this would work in real data. Let’s take the Flintstones (film) co-appearance network as an example:\n\n   library(networkdata)\n   g.flint <- movie_267\n   g.flint <- delete_vertices(g.flint, degree(g.flint) <= 3)\n   E <- d.euclid(g.flint)\n   E <- dist(E) #transforming E to a dist object\n   h.res <- hclust(E, method = \"ward.D2\") #hierarchical clustering\n   #install.packages(\"dendextend\")\n   library(dendextend) #nice dendrogram plotting\n   dend <- as.dendrogram(h.res) %>% \n      color_branches(k = 8) %>% \n      color_labels(k = 8) %>% \n      set(\"labels_cex\", 0.65) %>% \n      set(\"branches_lwd\", 2) %>% \n      plot\n\n\n\n\nThe Flintstones is a film based on a family, and families are the prototypes of social roles in networks, so if structural equivalence gets at roles, then it should recover known kin roles here, and indeed it does to some extent. One cluster is the focal parents separated into “moms” and “dads” roles, and another has the kids.\nAfter we have our clustering, we may wish to extract the structurally equivalent blocks from the hierarchical clustering results. To do that, we need to cut the dendrogram at a height that will produce a given number of clusters. Of course because hierarchical clustering is agglomerative, it begins with all nodes in the same cluster and ends with all nodes in a single cluster. So a reasonable solution is some (relatively) small number of clusters \\(k\\) such that \\(1 > k < n\\) that is, some number larger than one but smaller than the number of nodes in the graph.\nChoosing the number of clusters after a hierarchical clustering is not a well-defined problem, so you have to use a combination of pragmatic and domain-specific knowledge criteria to decide. Here, it looks like four blocks provides enough resolution and substantive interpretability so let’s do that. To do the job we use a function from the dendextend package we used above to draw our pretty colored dendrogram data viz, called cutree which as its name implies cuts the dendrogram at a height that produces the required number of classes:\n\n   blocks  <- cutree(h.res, k = 4)\n   blocks\n\n        BAM-BAM          BARNEY           BETTY        FELDSPAR            FRED \n              1               2               2               3               2 \n   HEADMISTRESS      HERDMASTER            LAVA           LEACH          MORRIS \n              4               3               2               1               4 \n      MRS SLATE         PEBBLES PEBBLES BAM-BAM        PILTDOWN      POINDEXTER \n              4               1               1               3               4 \n         PYRITE           SLATE           WILMA \n              3               2               2 \n\n\nNote that the result is a named vector with the node labels as the names and a value of \\(k\\) for each node, where \\(k\\) indicates the class of that node. For instance, \\(\\{Barney, Betty, Fred, Lava, Slate, Wilma\\}\\) all belong to \\(k = 2\\). Remember this is a purely nominal classification so the order of the numbers doesn’t matter."
  },
  {
    "objectID": "structequiv.html#concor",
    "href": "structequiv.html#concor",
    "title": "Role Equivalence and Structural Similarity",
    "section": "CONCOR",
    "text": "CONCOR\nThe other (perhaps less obvious) way of defining a distance between nodes in the network based on their connectivity patterns to other nodes is the correlation distance:\n\\[\n    d_{i,j} =\n    \\frac{\n    \\sum_{i \\neq k}\n    (a_{ki} - \\overline{a}_{i}) \\times\n    \\sum_{j \\neq k}\n    (a_{kj} - \\overline{a}_{j})\n    }\n    {\n    \\sqrt{\n    \\sum_{i \\neq k}\n    (a_{ki} - \\overline{a}_{i})^2 \\times\n    \\sum_{j \\neq k}\n    (a_{kj} - \\overline{a}_{j})^2\n        }\n    }\n\\]\nA more involved but still meaningful formula compared of the Euclidean distance. Here \\(\\overline{a}_{i}\\) is the column mean of the entries of node \\(i\\) in the affiliation matrix.\nSo the correlation distance is the ratio of the covariance of the column vectors corresponding to each node in the adjacency matrix and the product of their standard deviations.\nThe correlation distance between nodes in our toy network is given by simply typing:\n\n   m <- as.matrix(as_adjacency_matrix(g))\n   C <- cor(m)\n   round(C, 2)\n\n      A     B     C     D     E     F     G     H     I\nA  1.00 -0.32 -0.32  0.32  0.32  0.32  0.50 -0.63 -0.63\nB -0.32  1.00  1.00 -1.00 -1.00 -1.00 -0.32  0.35  0.35\nC -0.32  1.00  1.00 -1.00 -1.00 -1.00 -0.32  0.35  0.35\nD  0.32 -1.00 -1.00  1.00  1.00  1.00  0.32 -0.35 -0.35\nE  0.32 -1.00 -1.00  1.00  1.00  1.00  0.32 -0.35 -0.35\nF  0.32 -1.00 -1.00  1.00  1.00  1.00  0.32 -0.35 -0.35\nG  0.50 -0.32 -0.32  0.32  0.32  0.32  1.00 -0.63 -0.63\nH -0.63  0.35  0.35 -0.35 -0.35 -0.35 -0.63  1.00  0.55\nI -0.63  0.35  0.35 -0.35 -0.35 -0.35 -0.63  0.55  1.00\n\n\nWhich gives the Pearson product moment correlation of each pair of columns in the adjacency matrix.\nThe key thing that was noticed by Breiger, Boorman, and Arabie (1975) is that we can iterate this process, and compute correlation distances of correlation distances between nodes in the graph. If we do this for our toy network a few (e.g., three) times we get:\n\n   C1 <- cor(C)\n   C2 <- cor(C1)\n   C3 <- cor(C2)\n   round(C3, 2)\n\n   A  B  C  D  E  F  G  H  I\nA  1 -1 -1  1  1  1  1 -1 -1\nB -1  1  1 -1 -1 -1 -1  1  1\nC -1  1  1 -1 -1 -1 -1  1  1\nD  1 -1 -1  1  1  1  1 -1 -1\nE  1 -1 -1  1  1  1  1 -1 -1\nF  1 -1 -1  1  1  1  1 -1 -1\nG  1 -1 -1  1  1  1  1 -1 -1\nH -1  1  1 -1 -1 -1 -1  1  1\nI -1  1  1 -1 -1 -1 -1  1  1\n\n\nInterestingly the positive correlations converge to 1.0 and the negative correlations converge to -1.0!\nIf we sort the rows and columns of the new matrix according to these values, we get:\n\n   C.ord <- corrMatOrder(C3, order = \"hclust\", hclust.method = \"ward.D2\")\n   C3 <- C3[C.ord, C.ord]\n   round(C3, 2)\n\n   B  C  H  I  F  D  E  A  G\nB  1  1  1  1 -1 -1 -1 -1 -1\nC  1  1  1  1 -1 -1 -1 -1 -1\nH  1  1  1  1 -1 -1 -1 -1 -1\nI  1  1  1  1 -1 -1 -1 -1 -1\nF -1 -1 -1 -1  1  1  1  1  1\nD -1 -1 -1 -1  1  1  1  1  1\nE -1 -1 -1 -1  1  1  1  1  1\nA -1 -1 -1 -1  1  1  1  1  1\nG -1 -1 -1 -1  1  1  1  1  1\n\n\nAha! The iterated correlations seems to have split the matrix into two blocks \\(C_1 = \\{G, F, D, A, G\\}\\) and \\(C_2 = \\{I, H, B, C\\}\\). Each of the blocks is composed of two sub-blocks that we know are structurally equivalent from our previous analysis.\nA function implementing this method of iterated correlation distances until convergence looks like:\n\n   con.cor <- function(x) {\n      C <- x\n      while (mean(abs(C)) != 1) {\n         C <- cor(C)\n         }\n      b1 <- C[, 1] > 0\n      b2 <- !b1\n      return(list(x[, b1, drop = FALSE], x[, b2, drop = FALSE]))\n      }\n\nThis function takes a graph’s adjacency matrix as input, creates a copy of the adjacency matrix in line 2 (to be put through the iterated correlations meat grinder). The three-line (3-5) while loop goes through the iterated correlations (stopping when the matrix is full of ones). Then the resulting two blocks are returned as the columns of a couple of matrices stored in a list in line 8.\nFor instance, to use our example above:\n\n   con.cor(m)\n\n[[1]]\n  A D E F G\nA 0 0 0 0 1\nB 1 1 1 1 1\nC 1 1 1 1 1\nD 0 0 0 0 0\nE 0 0 0 0 0\nF 0 0 0 0 0\nG 1 0 0 0 0\nH 0 1 1 1 0\nI 0 1 1 1 0\n\n[[2]]\n  B C H I\nA 1 1 0 0\nB 0 0 0 0\nC 0 0 0 0\nD 1 1 1 1\nE 1 1 1 1\nF 1 1 1 1\nG 1 1 0 0\nH 0 0 0 1\nI 0 0 1 0\n\n\nThe columns of these two matrices are the two blocks we found before. Of course, to implement this method as a divisive clustering algorithm, what we want is to split these two blocks into two finer grained blocks, by iterative correlations of the columns of these two sub-matrices (to reveal two further sub-matrices each) and thus find our original four structurally equivalent groups.\nThe following function–simplified and adapted from Adam Slez’s work—which includes the con.cor function shown earlier inside of it, will do it:\n\n  blocks <- function(g, s = 2) {\n     A <- as.matrix(as_adjacency_matrix(g))\n     B <- list(A)\n     for(i in 1:s) {\n       B <- unlist(lapply(B, con.cor), recursive = FALSE)\n       }\n     return(lapply(B, colnames))\n   }\n\nThis function takes the graph as input and returns a list of column names containing the structurally equivalent blocks as output:\n\n   blocks(g)\n\n[[1]]\n[1] \"A\" \"G\"\n\n[[2]]\n[1] \"D\" \"E\" \"F\"\n\n[[3]]\n[1] \"B\" \"C\"\n\n[[4]]\n[1] \"H\" \"I\"\n\n\nWhich are the original structurally equivalent classes. The argument s controls the number of splits. When it is equal to one, the function produces two blocks, and when it is equal to two it produces four blocks, when it is equal to three, six blocks, and so on.\nThis is the algorithm called CONCOR (Breiger, Boorman, and Arabie 1975), short for convergence of iterate correlations, and can be used to cluster the rows(columns) of any valued square data matrix.\nFor instance, if we wanted to split the Flintstones network into four blocks we would proceed as follows:\n\n   g.flint <- movie_267\n   g.flint <- delete_vertices(g.flint, degree(g.flint) <= 3)\n   blocks(g.flint)\n\n[[1]]\n[1] \"BAM-BAM\"    \"BETTY\"      \"PEBBLES\"    \"POINDEXTER\" \"WILMA\"     \n\n[[2]]\n[1] \"FELDSPAR\"   \"HERDMASTER\" \"LAVA\"       \"PILTDOWN\"   \"PYRITE\"    \n[6] \"SLATE\"     \n\n[[3]]\n[1] \"BARNEY\"          \"FRED\"            \"LEACH\"           \"PEBBLES BAM-BAM\"\n\n[[4]]\n[1] \"HEADMISTRESS\" \"MORRIS\"       \"MRS SLATE\"   \n\n\nWhich is similar to the results we got from the Euclidean distance method, except that now the children are put in the same blocks as the moms.\nWe could then visualize the results as follows:\n\n   #install.packages(\"ggcorrplot\")\n   library(ggcorrplot)\n   A <- as.matrix(as_adjacency_matrix(g.flint))\n   ord <- unlist(blocks(g.flint, 2))\n   A <- A[ord, ord]\n   p <- ggcorrplot(A, colors = c(\"white\", \"white\", \"black\")) \n   p <- p + theme(legend.position = \"none\", \n                  axis.text.y = element_text(size = 8),\n                  axis.text.x = element_text(size = 8),\n                  )\n   p <- p + geom_hline(yintercept = 5.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_vline(xintercept = 5.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_hline(yintercept = 11.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_vline(xintercept = 11.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_hline(yintercept = 15.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_vline(xintercept = 15.5, linewidth = 2, color = \"blue\")\n   p\n\n\n\n\nWhich is just a tile plot of the original adjacency matrix (adjacent cells in black) with rows and columns re-ordered according to the four-block solution and ggplot vertical and horizontal lines highlighting the boundaries of the blocked matrix.\nAs you can see, characters with similar patterns of scene co-appearances (like Barney and Fred) are drawn next to one another, revealing the larger “roles” in the network."
  },
  {
    "objectID": "structequiv.html#structural-equivalence-in-directed-graphs",
    "href": "structequiv.html#structural-equivalence-in-directed-graphs",
    "title": "Role Equivalence and Structural Similarity",
    "section": "Structural Equivalence in Directed Graphs",
    "text": "Structural Equivalence in Directed Graphs\nLike before, the main complication introduced by the directed case is the “doubling” of the relations considered.\nIn the Euclidean distance case, we have to decide whether we want to compute two set of distances between nodes, one based on the in-distance vectors and the other on the out-distance vectors, and the two sets of hierarchical clustering partitions.\nAnother approach is simply to combine both according to the formula:\n\\[\n   d_{i,j} = \\sqrt{\n                  \\sum_{k \\neq i,j} (a_{ik} - a_{jk})^2 +\n                  \\sum_{k \\neq i,j} (a_{ki} - a_{kj})^2\n                  }\n\\]\nWhich just computes the Euclidean distances between nodes using both in and out neighbors. Here nodes would be structurally equivalent only if they have the same set of in and out-neighbors. This would mean changing line 11 in the function d.euclid above with the following:\n\n   d.ij <- d.ij + ((A[i,k] - A[j,k])^2 + (A[k,i] - A[k,j])^2)\n\nThis way, distances are computed on both the row and columns of the directed graph’s adjacency matrix.\nIf we are using the correlation distance approach in a directed graph, then the main trick is to stack the original adjacency matrix against its transpose, and then compute the correlation distance on the columns of the stacked matrices, which by definition combines information in incoming and outgoing ties.\nLet’s see a brief example. Let’s load up the Krackhardt’s high-tech managers data on advice relations and look at the adjacency matrix:\n\n   g <- ht_advice\n   A <- as.matrix(as_adjacency_matrix(g))\n   A\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]\n [1,]    0    1    0    1    0    0    0    1    0     0     0     0     0\n [2,]    0    0    0    0    0    1    1    0    0     0     0     0     0\n [3,]    1    1    0    1    0    1    1    1    1     1     1     1     0\n [4,]    1    1    0    0    0    1    0    1    0     1     1     1     0\n [5,]    1    1    0    0    0    1    1    1    0     1     1     0     1\n [6,]    0    0    0    0    0    0    0    0    0     0     0     0     0\n [7,]    0    1    0    0    0    1    0    0    0     0     1     1     0\n [8,]    0    1    0    1    0    1    1    0    0     1     1     0     0\n [9,]    1    1    0    0    0    1    1    1    0     1     1     1     0\n[10,]    1    1    1    1    1    0    0    1    0     0     1     0     1\n[11,]    1    1    0    0    0    0    1    0    0     0     0     0     0\n[12,]    0    0    0    0    0    0    1    0    0     0     0     0     0\n[13,]    1    1    0    0    1    0    0    0    1     0     0     0     0\n[14,]    0    1    0    0    0    0    1    0    0     0     0     0     0\n[15,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[16,]    1    1    0    0    0    0    0    0    0     1     0     0     0\n[17,]    1    1    0    1    0    0    1    0    0     0     0     0     0\n[18,]    1    1    1    1    1    0    1    1    1     1     1     0     1\n[19,]    1    1    1    0    1    0    1    0    0     1     1     0     0\n[20,]    1    1    0    0    0    1    0    1    0     0     1     1     0\n[21,]    0    1    1    1    0    1    1    1    0     0     0     1     0\n      [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21]\n [1,]     0     0     1     0     1     0     0     1\n [2,]     0     0     0     0     0     0     0     1\n [3,]     1     0     0     1     1     0     1     1\n [4,]     0     0     1     1     1     0     1     1\n [5,]     1     0     1     1     1     1     1     1\n [6,]     0     0     0     0     0     0     0     1\n [7,]     1     0     0     1     1     0     0     1\n [8,]     0     0     0     0     1     0     0     1\n [9,]     1     0     1     1     1     0     0     1\n[10,]     0     1     1     1     1     1     1     0\n[11,]     0     0     0     0     0     0     0     0\n[12,]     0     0     0     0     0     0     0     1\n[13,]     1     0     0     0     1     0     0     0\n[14,]     0     0     0     0     1     0     0     1\n[15,]     1     0     1     1     1     1     1     1\n[16,]     0     0     0     0     1     0     0     0\n[17,]     0     0     0     0     0     0     0     1\n[18,]     1     1     1     0     0     1     1     1\n[19,]     1     1     0     0     1     0     1     0\n[20,]     1     1     1     1     1     0     0     1\n[21,]     1     0     0     1     1     0     1     0\n\n\nRecall that in these data a tie goes from an advice seeker to an advisor. So the standard correlation distance on the columns computes the in-correlation, or structural equivalence based on incoming ties (two managers are equivalent if they are nominated as advisors by the same others).\nWe may also be interested in the out-correlation, that is structural equivalence based on out-going ties. Here two managers are structurally equivalent is they seek advice from the same others. This information is contained in the transpose of the original adjacency matrix:\n\n   A.t <- t(A)\n   A.t\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]\n [1,]    0    0    1    1    1    0    0    0    1     1     1     0     1\n [2,]    1    0    1    1    1    0    1    1    1     1     1     0     1\n [3,]    0    0    0    0    0    0    0    0    0     1     0     0     0\n [4,]    1    0    1    0    0    0    0    1    0     1     0     0     0\n [5,]    0    0    0    0    0    0    0    0    0     1     0     0     1\n [6,]    0    1    1    1    1    0    1    1    1     0     0     0     0\n [7,]    0    1    1    0    1    0    0    1    1     0     1     1     0\n [8,]    1    0    1    1    1    0    0    0    1     1     0     0     0\n [9,]    0    0    1    0    0    0    0    0    0     0     0     0     1\n[10,]    0    0    1    1    1    0    0    1    1     0     0     0     0\n[11,]    0    0    1    1    1    0    1    1    1     1     0     0     0\n[12,]    0    0    1    1    0    0    1    0    1     0     0     0     0\n[13,]    0    0    0    0    1    0    0    0    0     1     0     0     0\n[14,]    0    0    1    0    1    0    1    0    1     0     0     0     1\n[15,]    0    0    0    0    0    0    0    0    0     1     0     0     0\n[16,]    1    0    0    1    1    0    0    0    1     1     0     0     0\n[17,]    0    0    1    1    1    0    1    0    1     1     0     0     0\n[18,]    1    0    1    1    1    0    1    1    1     1     0     0     1\n[19,]    0    0    0    0    1    0    0    0    0     1     0     0     0\n[20,]    0    0    1    1    1    0    0    0    0     1     0     0     0\n[21,]    1    1    1    1    1    1    1    1    1     0     0     1     0\n      [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21]\n [1,]     0     1     1     1     1     1     1     0\n [2,]     1     1     1     1     1     1     1     1\n [3,]     0     1     0     0     1     1     0     1\n [4,]     0     1     0     1     1     0     0     1\n [5,]     0     1     0     0     1     1     0     0\n [6,]     0     1     0     0     0     0     1     1\n [7,]     1     1     0     1     1     1     0     1\n [8,]     0     1     0     0     1     0     1     1\n [9,]     0     1     0     0     1     0     0     0\n[10,]     0     1     1     0     1     1     0     0\n[11,]     0     1     0     0     1     1     1     0\n[12,]     0     1     0     0     0     0     1     1\n[13,]     0     1     0     0     1     0     0     0\n[14,]     0     1     0     0     1     1     1     1\n[15,]     0     0     0     0     1     1     1     0\n[16,]     0     1     0     0     1     0     1     0\n[17,]     0     1     0     0     0     0     1     1\n[18,]     1     1     1     0     0     1     1     1\n[19,]     0     1     0     0     1     0     0     0\n[20,]     0     1     0     0     1     1     0     1\n[21,]     1     1     0     1     1     0     1     0\n\n\nCorrelating the columns of this matrix would thus give us the out-correlation distance based on advice seeking relations.\n“Stacking” is a way to combine both in and out-going ties and compute a single distance based on both. It just means that we literally bind the rows of the firs matrix and its transpose:\n\n   A.stack <- rbind(A, A.t)\n   A.stack\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]\n [1,]    0    1    0    1    0    0    0    1    0     0     0     0     0\n [2,]    0    0    0    0    0    1    1    0    0     0     0     0     0\n [3,]    1    1    0    1    0    1    1    1    1     1     1     1     0\n [4,]    1    1    0    0    0    1    0    1    0     1     1     1     0\n [5,]    1    1    0    0    0    1    1    1    0     1     1     0     1\n [6,]    0    0    0    0    0    0    0    0    0     0     0     0     0\n [7,]    0    1    0    0    0    1    0    0    0     0     1     1     0\n [8,]    0    1    0    1    0    1    1    0    0     1     1     0     0\n [9,]    1    1    0    0    0    1    1    1    0     1     1     1     0\n[10,]    1    1    1    1    1    0    0    1    0     0     1     0     1\n[11,]    1    1    0    0    0    0    1    0    0     0     0     0     0\n[12,]    0    0    0    0    0    0    1    0    0     0     0     0     0\n[13,]    1    1    0    0    1    0    0    0    1     0     0     0     0\n[14,]    0    1    0    0    0    0    1    0    0     0     0     0     0\n[15,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[16,]    1    1    0    0    0    0    0    0    0     1     0     0     0\n[17,]    1    1    0    1    0    0    1    0    0     0     0     0     0\n[18,]    1    1    1    1    1    0    1    1    1     1     1     0     1\n[19,]    1    1    1    0    1    0    1    0    0     1     1     0     0\n[20,]    1    1    0    0    0    1    0    1    0     0     1     1     0\n[21,]    0    1    1    1    0    1    1    1    0     0     0     1     0\n[22,]    0    0    1    1    1    0    0    0    1     1     1     0     1\n[23,]    1    0    1    1    1    0    1    1    1     1     1     0     1\n[24,]    0    0    0    0    0    0    0    0    0     1     0     0     0\n[25,]    1    0    1    0    0    0    0    1    0     1     0     0     0\n[26,]    0    0    0    0    0    0    0    0    0     1     0     0     1\n[27,]    0    1    1    1    1    0    1    1    1     0     0     0     0\n[28,]    0    1    1    0    1    0    0    1    1     0     1     1     0\n[29,]    1    0    1    1    1    0    0    0    1     1     0     0     0\n[30,]    0    0    1    0    0    0    0    0    0     0     0     0     1\n[31,]    0    0    1    1    1    0    0    1    1     0     0     0     0\n[32,]    0    0    1    1    1    0    1    1    1     1     0     0     0\n[33,]    0    0    1    1    0    0    1    0    1     0     0     0     0\n[34,]    0    0    0    0    1    0    0    0    0     1     0     0     0\n[35,]    0    0    1    0    1    0    1    0    1     0     0     0     1\n[36,]    0    0    0    0    0    0    0    0    0     1     0     0     0\n[37,]    1    0    0    1    1    0    0    0    1     1     0     0     0\n[38,]    0    0    1    1    1    0    1    0    1     1     0     0     0\n[39,]    1    0    1    1    1    0    1    1    1     1     0     0     1\n[40,]    0    0    0    0    1    0    0    0    0     1     0     0     0\n[41,]    0    0    1    1    1    0    0    0    0     1     0     0     0\n[42,]    1    1    1    1    1    1    1    1    1     0     0     1     0\n      [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21]\n [1,]     0     0     1     0     1     0     0     1\n [2,]     0     0     0     0     0     0     0     1\n [3,]     1     0     0     1     1     0     1     1\n [4,]     0     0     1     1     1     0     1     1\n [5,]     1     0     1     1     1     1     1     1\n [6,]     0     0     0     0     0     0     0     1\n [7,]     1     0     0     1     1     0     0     1\n [8,]     0     0     0     0     1     0     0     1\n [9,]     1     0     1     1     1     0     0     1\n[10,]     0     1     1     1     1     1     1     0\n[11,]     0     0     0     0     0     0     0     0\n[12,]     0     0     0     0     0     0     0     1\n[13,]     1     0     0     0     1     0     0     0\n[14,]     0     0     0     0     1     0     0     1\n[15,]     1     0     1     1     1     1     1     1\n[16,]     0     0     0     0     1     0     0     0\n[17,]     0     0     0     0     0     0     0     1\n[18,]     1     1     1     0     0     1     1     1\n[19,]     1     1     0     0     1     0     1     0\n[20,]     1     1     1     1     1     0     0     1\n[21,]     1     0     0     1     1     0     1     0\n[22,]     0     1     1     1     1     1     1     0\n[23,]     1     1     1     1     1     1     1     1\n[24,]     0     1     0     0     1     1     0     1\n[25,]     0     1     0     1     1     0     0     1\n[26,]     0     1     0     0     1     1     0     0\n[27,]     0     1     0     0     0     0     1     1\n[28,]     1     1     0     1     1     1     0     1\n[29,]     0     1     0     0     1     0     1     1\n[30,]     0     1     0     0     1     0     0     0\n[31,]     0     1     1     0     1     1     0     0\n[32,]     0     1     0     0     1     1     1     0\n[33,]     0     1     0     0     0     0     1     1\n[34,]     0     1     0     0     1     0     0     0\n[35,]     0     1     0     0     1     1     1     1\n[36,]     0     0     0     0     1     1     1     0\n[37,]     0     1     0     0     1     0     1     0\n[38,]     0     1     0     0     0     0     1     1\n[39,]     1     1     1     0     0     1     1     1\n[40,]     0     1     0     0     1     0     0     0\n[41,]     0     1     0     0     1     1     0     1\n[42,]     1     1     0     1     1     0     1     0\n\n\nNote that this matrix has the same number of columns as the original adjacency matrix and double the number of rows. This doesn’t matter since the correlation distance works on the columns, meaning that it will return a matrix of the same dimensions as the original:\n\n   round(cor(A.stack), 2)\n\n       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7] [,8]  [,9] [,10] [,11] [,12]\n [1,]  1.00  0.43  0.00  0.09  0.09  0.22  0.14 0.37  0.13  0.25  0.37  0.22\n [2,]  0.43  1.00 -0.19  0.00 -0.19  0.49  0.24 0.38 -0.15 -0.24  0.51  0.52\n [3,]  0.00 -0.19  1.00  0.52  0.62 -0.24  0.19 0.33  0.57  0.00  0.03 -0.03\n [4,]  0.09  0.00  0.52  1.00  0.43 -0.03  0.29 0.33  0.57  0.10  0.03 -0.03\n [5,]  0.09 -0.19  0.62  0.43  1.00 -0.35  0.00 0.14  0.67  0.20  0.03 -0.15\n [6,]  0.22  0.49 -0.24 -0.03 -0.35  1.00  0.27 0.36 -0.16  0.00  0.50  0.74\n [7,]  0.14  0.24  0.19  0.29  0.00  0.27  1.00 0.19  0.24 -0.05  0.10  0.06\n [8,]  0.37  0.38  0.33  0.33  0.14  0.36  0.19 1.00  0.27  0.01  0.41  0.49\n [9,]  0.13 -0.15  0.57  0.57  0.67 -0.16  0.24 0.27  1.00  0.07  0.03  0.04\n[10,]  0.25 -0.24  0.00  0.10  0.20  0.00 -0.05 0.01  0.07  1.00  0.24 -0.11\n[11,]  0.37  0.51  0.03  0.03  0.03  0.50  0.10 0.41  0.03  0.24  1.00  0.49\n[12,]  0.22  0.52 -0.03 -0.03 -0.15  0.74  0.06 0.49  0.04 -0.11  0.49  1.00\n[13,]  0.17 -0.11  0.36  0.14  0.25 -0.08  0.11 0.19  0.22  0.17  0.32 -0.16\n[14,]  0.47  0.51  0.13  0.03  0.13  0.50  0.30 0.51  0.24  0.03  0.57  0.62\n[15,] -0.08 -0.48  0.63  0.25  0.63 -0.47 -0.19 0.07  0.42  0.18 -0.10 -0.25\n[16,]  0.38  0.21  0.14  0.24  0.14  0.22  0.00 0.62  0.12  0.15  0.56  0.18\n[17,]  0.37  0.40  0.13  0.03 -0.07  0.61  0.00 0.61  0.03  0.03  0.68  0.74\n[18,]  0.06  0.11 -0.03 -0.14  0.09  0.21 -0.45 0.15 -0.11  0.28  0.28  0.29\n[19,] -0.08 -0.25  0.38  0.18  0.38 -0.22 -0.05 0.26  0.30  0.28  0.21 -0.15\n[20,]  0.28  0.00  0.52  0.52  0.43  0.08  0.38 0.33  0.57  0.29  0.24  0.08\n[21,]  0.02  0.10 -0.04  0.06 -0.23  0.24  0.29 0.18  0.05 -0.02  0.24  0.17\n      [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21]\n [1,]  0.17  0.47 -0.08  0.38  0.37  0.06 -0.08  0.28  0.02\n [2,] -0.11  0.51 -0.48  0.21  0.40  0.11 -0.25  0.00  0.10\n [3,]  0.36  0.13  0.63  0.14  0.13 -0.03  0.38  0.52 -0.04\n [4,]  0.14  0.03  0.25  0.24  0.03 -0.14  0.18  0.52  0.06\n [5,]  0.25  0.13  0.63  0.14 -0.07  0.09  0.38  0.43 -0.23\n [6,] -0.08  0.50 -0.47  0.22  0.61  0.21 -0.22  0.08  0.24\n [7,]  0.11  0.30 -0.19  0.00  0.00 -0.45 -0.05  0.38  0.29\n [8,]  0.19  0.51  0.07  0.62  0.61  0.15  0.26  0.33  0.18\n [9,]  0.22  0.24  0.42  0.12  0.03 -0.11  0.30  0.57  0.05\n[10,]  0.17  0.03  0.18  0.15  0.03  0.28  0.28  0.29 -0.02\n[11,]  0.32  0.57 -0.10  0.56  0.68  0.28  0.21  0.24  0.24\n[12,] -0.16  0.62 -0.25  0.18  0.74  0.29 -0.15  0.08  0.17\n[13,]  1.00  0.20  0.26  0.51  0.20  0.05  0.63  0.36 -0.02\n[14,]  0.20  1.00 -0.10  0.34  0.57  0.16  0.11  0.24  0.14\n[15,]  0.26 -0.10  1.00  0.02 -0.10  0.08  0.34  0.25 -0.18\n[16,]  0.51  0.34  0.02  1.00  0.45  0.11  0.41  0.24  0.17\n[17,]  0.20  0.57 -0.10  0.45  1.00  0.40  0.11  0.24  0.14\n[18,]  0.05  0.16  0.08  0.11  0.40  1.00  0.18 -0.03 -0.32\n[19,]  0.63  0.11  0.34  0.41  0.11  0.18  1.00  0.28 -0.03\n[20,]  0.36  0.24  0.25  0.24  0.24 -0.03  0.28  1.00 -0.04\n[21,] -0.02  0.14 -0.18  0.17  0.14 -0.32 -0.03 -0.04  1.00\n\n\nAnd we would then do a blockmodel based on these distances:\n\n   blocks2 <- function(A, s = 2) {\n     colnames(A) <- 1:ncol(A) #use only if the original matrix has no names\n     B <- list(A)\n     for(i in 1:s) {\n       B <- unlist(lapply(B, con.cor), recursive = FALSE)\n       }\n     return(lapply(B, colnames))\n   }\n   blocks2(A.stack)\n\n[[1]]\n [1] \"1\"  \"2\"  \"6\"  \"8\"  \"11\" \"12\" \"14\" \"16\" \"17\" \"18\"\n\n[[2]]\n[1] \"7\"  \"21\"\n\n[[3]]\n[1] \"3\"  \"4\"  \"5\"  \"9\"  \"15\" \"20\"\n\n[[4]]\n[1] \"10\" \"13\" \"19\""
  },
  {
    "objectID": "structequiv.html#structural-equivalence-in-multigraphs",
    "href": "structequiv.html#structural-equivalence-in-multigraphs",
    "title": "Role Equivalence and Structural Similarity",
    "section": "Structural Equivalence in Multigraphs",
    "text": "Structural Equivalence in Multigraphs\nNote that we would apply the same trick if we wanted to do a blockmodel based on multiple relations like friendship and advice. Here’s a blockmodel based on the stacked matrices of incoming ties of both types:\n\n   A.f <- as.matrix(as_adjacency_matrix(ht_friends))\n   A.stack <- rbind(A, A.f)\n   blocks2(A.stack)\n\n[[1]]\n[1] \"1\"  \"3\"  \"5\"  \"14\" \"15\" \"20\"\n\n[[2]]\n[1] \"4\"  \"9\"  \"13\" \"19\"\n\n[[3]]\n[1] \"2\"  \"8\"  \"12\" \"16\" \"17\" \"18\"\n\n[[4]]\n[1] \"6\"  \"7\"  \"10\" \"11\" \"21\"\n\n\nAnd one combining incoming and outgoing friendship and advice ties:\n\n   A.stack <- rbind(A, A.t, A.f, t(A.f))\n   blocks2(A.stack)\n\n[[1]]\n[1] \"1\"  \"2\"  \"8\"  \"12\" \"16\" \"21\"\n\n[[2]]\n[1] \"6\"  \"11\" \"14\" \"17\"\n\n[[3]]\n[1] \"3\"  \"4\"  \"5\"  \"7\"  \"9\"  \"20\"\n\n[[4]]\n[1] \"10\" \"13\" \"15\" \"18\" \"19\"\n\n\nNote that here the stacked matrix has four sub-matrices: (1) Incoming advice, (2) outgoing advice, (3) incoming friendship, and (4) Outgoing friendship."
  },
  {
    "objectID": "syllabus-208A.html",
    "href": "syllabus-208A.html",
    "title": "208A Syllabus",
    "section": "",
    "text": "THIS IS A LIVE DOCUMENT CHECK REGULARLY FOR CHANGES"
  },
  {
    "objectID": "syllabus-208A.html#class-description",
    "href": "syllabus-208A.html#class-description",
    "title": "208A Syllabus",
    "section": "Class Description",
    "text": "Class Description\nThis class is an introductory graduate-level seminar focused on techniques in Social Network Analysis (SNA). The seminar covers the most common data analytic tasks that people engage in when analyzing “network data.” What is network data? What counts as network data is itself a point of contention—as we will see, for some people all data is network data—but let us say for the sake of this class that network data is data in which the unit of analysis is the relation or the interaction between at least two actors or objects, and the data come typically arranged in this “dyadic” form. At the end of the course, you will be familiar with (and will have acquired some practice) the basic techniques used to analyze social network data."
  },
  {
    "objectID": "syllabus-208A.html#course-content",
    "href": "syllabus-208A.html#course-content",
    "title": "208A Syllabus",
    "section": "Course Content",
    "text": "Course Content\n\nBasic SNA\nSo, what are the things that people usually do when they have network data? Well, they typically want to figure out basic statistics about the interaction system formed by the set of dyads in the data, where a dyad is any two pairs of actors (whether they are connected or not). This task requires computing basic network quantities like the number of nodes and the number of links between entities as well as more advanced statistics based on representing the network as a graph (like the average path length, number of components, etc., all notions we will cover in the first week of class).\n\n\nCentrality and Prestige\nThen come the various things that almost everyone is interested in computing when using network data to answer social science questions. Primarily, this includes measures and indices of a node’s position in the network (e.g., differentiating between more or less central or more or less prestigious nodes), which we will cover in weeks 2 and 3.\n\n\nClasses and Communities\nAfter taking a break in Week 4, we move to the common case of people wanting to see if the nodes in the network fall into definable clusters or classes, where the criterion for being in the same cluster is based on how they connect to other nodes. Here, we want to find clusters of nodes that are similar to one another by some graph theoretic criterion and partition the graph into clusters based on that criterion.\nWeek 6 is dedicated to the next thing we may want to do, and that is to see if we can uncover clumps of densely connected nodes in the network indicating some natural partition into subgroups or communities, defined as nodes that interact more among themselves than they do among those outside the group, leading to the myriad of group and community detection techniques designed to partition a graph into clusters based on the underlying connectivity structure.\n\n\nTwo-Mode and Ego Networks\nThe next two weeks are dedicated to the analysis of some pretty common “non-standard” types of network data (e.g., data that doesn’t use the dyadic relation between objects of the same type as the analytic unit). The first is ego networks, where we first sample a set of units (egos), and then within each ego, we sample a subset of their contacts (e.g., by asking the people who are their most important friends or figuring out the most frequent interaction partners). These data come closest to the traditional data in social science (a rectangular matrix of cases by variables), so various standard techniques—like regression—apply (with some twists).\nThe second type of non-standard network data comes in a two-mode network form, in which some sets of objects are linked to objects of a different set, but there is no data on the links between objects of the same set. Standard cases by variables data in surveys are two-mode data (people connect to variables), as is any web or archival data collecting memberships or interactions between persons and objects (like attendance at events or people buying books on Amazon). We will see that due to a neat mathematical trick, we can transform two-mode into standard dyadic network data and thus deploy the whole panoply of techniques we learned in weeks 1-6 (which means that we can do SNA on all types of data, not just network data, and therefore all data is network data).\n\n\nProbabilistic Models of Networks\nThe bulk of SNA assumes that the ties exist as recorded in the data. Recently (e.g., over the last two decades or so) developed approaches to social network analysis make the ties the dependent variable and thus see the observed network data as a realization of some stochastic process governing the probability that two objects will be linked and thus one that can be modeled statistically. We analyze the theory and methods behind this approach to thinking of network structure from the bottom up and also cover some models designed to treat networks as composed of “relational events” and thus model how events that link entities in networks evolve."
  },
  {
    "objectID": "syllabus-208A.html#requirements",
    "href": "syllabus-208A.html#requirements",
    "title": "208A Syllabus",
    "section": "Requirements",
    "text": "Requirements\nThere are three main requirements in the class. Participation (mainly attendance and contributions made during our seminar meetings), a short weekly data exercise, and a longer data analysis paper due at the end of the quarter.\n\nClass Attendance and Class Discussion (25% of grade)\nAttendance is required not optional. If you need to miss a class meeting please let me know beforehand. It is part of your professional socialization to commit to attending class meetings and to inform me when that’s not possible (if only as a point of courtesy). The informal part of participation will be gauged by your contribution to our class discussion in the form of questions, comments, suggestions, wonderings, problems.\n\n\nWeekly Data Analysis Exercises (25% of grade)\nThese will be short weekly assignments in which I will ask you to take a (small) social network data set of your choice and compute some of the basic statistics or implement some of the techniques that we covered the week before. They will be due on Sunday at the end of each week. What you submit will take the form of a file containing the code and results from your analysis (typically an R Markdown file). These will not be graded, but will just be counted as submitted or not submitted.\n\n\nFinal Data Analysis Paper (50% of grade)\nThe basic goal here is for you to end up with something that could be useful to you at the end of the day. Hopefully a basic data exercise that can be the basis of a longer substantive paper or as a standalone research note.\nThis will be a 2500 to 5000 word (single-spaced, Times New Roman Font, 12pt, 1in margins) write-up of a data analysis, including some type of network data and/or some kind of network analytic technique (to be discussed and cleared by me). The data source can be obtained from a public repository of network data, or it could be network data that you already have access to or collected yourself.\nIn the paper, you will describe the data, provide key network metrics, describe the data-analytic approach that you will use, and provide a summary of the key empirical patterns that you found, along with a brief conclusion. The paper should be written in the style of a “research note” focused on key empirical findings (not long theory windup or literature review).\nYou will submit an extended abstract of your final project, outlining your main research idea (e.g., data source and type of analysis) due on Sunday of week 6. This will be a one-page, single-spaced document with 12pt Times New Roman Font and 1in margins."
  },
  {
    "objectID": "syllabus-208B.html",
    "href": "syllabus-208B.html",
    "title": "208B Syllabus",
    "section": "",
    "text": "THIS IS A LIVE DOCUMENT CHECK REGULARLY FOR CHANGES"
  },
  {
    "objectID": "syllabus-208B.html#participation-50-of-grade",
    "href": "syllabus-208B.html#participation-50-of-grade",
    "title": "208B Syllabus",
    "section": "Participation (50% of grade)",
    "text": "Participation (50% of grade)\n\nWeekly Analytic Memo\nThe formal side of participation will come in the form of you submitting a short memo (500 to 1000 words) where you try to put two or more of the readings for that week in conversation with one another. The first memo will be due starting on week 2.\nMemo specifications:\n\nYou should pick at least one reading from the Tuesday meeting and at least one reading from the Thursday meeting as the focus of your memo.\nIn your memo you should feel free to raise questions or issues the readings brought up for you as well as any questions, problems, or weaknesses you identify in the argument or the analyses.\nYou may also feel free to connect the readings to other work you are familiar with, pointing to key points of commonality and difference. The main point of the memo is for me to see evidence of you thinking thought the material, as well as providing fodder for discussion during our meeting.\nThe memo will be due by 5p the day before our first class meeting of the week (that’s Monday) so that I have a chance to read it and comment on it. You will submit it via the assignments tool on Canvas.\n\n\n\nClass Attendance\nAttendance is required not optional. If you need to miss a class meeting please let me know before hand. It is part of your professional socialization to commit to attending class meetings and to inform me when that’s not possible (if only as a point of courtesy).\n\n\nClass Discussion\nThe informal part of participation will be gauged by your contribution to our class discussion. You can use the thoughts developed in your analytic memo as a take-off point for framing your contribution."
  },
  {
    "objectID": "syllabus-208B.html#paper-50-of-grade",
    "href": "syllabus-208B.html#paper-50-of-grade",
    "title": "208B Syllabus",
    "section": "Paper (50% of grade)",
    "text": "Paper (50% of grade)\nThe basic goal here is for you to end up with something that could be useful to you at the end of the day. As such, I’ll give you a set of options here, but if you none of these work, we can talk about something that can be customized for your needs and goals. So by the end of the course you will submit one of the following:\n\nDraft of a research paper.- This will be a 3500 to 9000 word (double-spaced, Times New Roman Font, 12pt, 1in margins) draft of a research paper. This paper will contain some kind of data analysis, involving networks broadly defined. It will include an introduction reviewing literature and setting up a research problem or question. It will then move on to a methods section describing your data and analytic approach, and will close with a discussion section summarizing key findings, outlining implications for substantive research and theory, and describing potential future work and extensions. The goal here is to come up with a draft of a paper that could one day be submitted to the various social science and sociology-oriented journal focused on substantive research, whether “generalist” (e.g., American Sociological Review) or “specialist” (e.g., Social Networks).\nDraft of a Conceptual Paper.- This will be a 5000 to 10000 word page (double-spaced, Times New Roman Font, 12pt, 1in margins) draft of a research paper. The paper will focus on a set of concepts, theoretical ideas, or overall perspectives for approaching the study of social life that are based, inspired, extend, or incorporate network ideas, network thinking, or network concepts and techniques (broadly defined). The goal here is to come up with a draft of a paper that could one day be submitted to the various social science and sociology-oriented journal focused on “theory.”\nExtended Literature Review Draft.- This will be a 10-15 page (double-spaced, Times New Roman Font, 12pt, 1in Margins) draft of a literature review of work done from a social network perspective on a topic of your interest. The paper will cover what has been done in the field so far, what the strengths and limitations of previous is, and will note gaps or opportunities for future work addressing those limitations or extending the literature to new substantive domains, perhaps linking previous work to the some of the stuff we will be reading in class.\nDraft of a Research Proposal.- This will be a 10 page (single-spaced, Times New Roman Font, 12pt, 1in margins, including references) draft of a research proposal for a project incorporating either network thinking, theories, or techniques that you plan to start in the near future. The proposal will include a background section reviewing previous work, noting their strengths and limitations, and pointing to gaps in the literature. It will also include an “approach” section describing your research project, your main research questions, the data gathering procedures you will use, and the data-analytic techniques you plan to implement once your data is collected. It will close with an implication sections describing what the contributions of your project will be and why it is relevant and important.\nData Analysis Exercise.- This will be a 2500 to 5000 word (single-spaced, Times New Roman Font, 12pt, 1in margins) write-up of a data analysis, including some type of network data and/or some kind of network analytic technique (to be discussed and cleared by me). The data source can be obtained from a public repository of network data, or it could be network data that you already have access to or collected yourself. In the paper, you will describe the data, provide key network metrics, describe the data-analytic approach that you will use, and provide a summary of the key empirical patterns that you found, along with a brief conclusion. The paper should be written in the style of a “research note” focused on key empirical findings (not long theory windup or literature review).\n\nWhatever you decide, you will submit an extended abstract of your final project, due on Friday of week 6. This will be a one-page, single-spaced document with 12pt Times New Roman Font and 1in margins."
  },
  {
    "objectID": "two-mode.html",
    "href": "two-mode.html",
    "title": "Analyzing Two-Mode Networks",
    "section": "",
    "text": "This handout deals with the network analysis of two-mode networks. Note that in the literature there is some terminological slippage. Two-mode networks are a type of social network. By definition two-mode networks can be represented using rectangular adjacency matrices (sometimes called affiliation matrices in sociology).\nIn this case, two-mode networks fall under the general category of “two-mode data.” Any data set that has information on two types of objects (e.g., people and variables) is two-mode data so two-mode networks are just a special case of two-mode data.\nIn this sense, a useful distinction, due to Borgatti & Everett, is useful. This is that between the “modes” and the “ways” of a data matrix. So most data matrices are two-ways, in that they have at least two dimensions (e.g., the row and column dimensions).\nBut some data matrices (like the usual adjacency matrix in regular network data) only collect information on a single type of entity, so they are “one mode, two ways.” But sometimes we have network data on two sets of objects, in which case, we use a data matrix that has “two-modes” (sets of nodes) and “two ways” (rows and columns).\nSo what makes a network a “two-mode network”? Well, a two-mode network is different from a regular network, because it has two sets of nodes not just one. So instead of \\(V\\) now we have \\(V_1\\) and \\(V_2\\). Moreover, the edges in a two-mode network only go from nodes in one set to nodes in the other set; there are no within-node-set edges."
  },
  {
    "objectID": "two-mode.html#bipartite-graphs",
    "href": "two-mode.html#bipartite-graphs",
    "title": "Analyzing Two-Mode Networks",
    "section": "Bipartite Graphs",
    "text": "Bipartite Graphs\nThis restriction makes the graph that represents a two-mode network a special kind of graph called a bipartite graph. A graph is bipartite if the set of nodes in the graph can be divided into two groups, such that relations go from nodes in one set to nodes in the other set.\nNote that bipartite graphs can be be used to represent both two-mode and regular one mode networks, as long as the above condition holds. For instance, a dating network with 100% heterosexual people in it will yield a bipartite graph based on the dating relation, with men in one set and women on the other node set, even though it’s a one-mode network.\nSo whether or not a graph is bipartite is something you can check for.\nLet’s see how that works. Let us load the most famous two-mode network data set (kind of the Drosophila of two-mode network analysis; one of the most repeatedly analyzed social structures in history: For a classic sampling of such analyses see here) a network composed of eighteen women from the social elite of a tiny town in the south in the 1930s who attended fourteen social events (Breiger 1974):\n\n   library(igraph)\n   library(networkdata)\n   g <- southern_women\n\nNow we already know this is a bipartite graph. However, let’s say you are new and you’ve never heard of these data. You can check whether the graph you loaded up is bipartite or not by using the igraph function is_bipartite:\n\n   is_bipartite(g)\n\n[1] TRUE\n\n\nWhich returns TRUE as an answer. Had we loaded up any old non-bipartite graph, the answer would have been:\n\n   g.whatever <- movie_45\n   is_bipartite(g.whatever)\n\n[1] FALSE\n\n\nWhich makes sense because that’s just a regular old graph.\nNote that if we check the bipartite graph object, it looks like any other igraph object:\n\n   g\n\nIGRAPH 1074643 UN-B 32 89 -- \n+ attr: type (v/l), name (v/c)\n+ edges from 1074643 (vertex names):\n [1] EVELYN   --6/27 EVELYN   --3/2  EVELYN   --4/12 EVELYN   --9/26\n [5] EVELYN   --2/25 EVELYN   --5/19 EVELYN   --9/16 EVELYN   --4/8 \n [9] LAURA    --6/27 LAURA    --3/2  LAURA    --4/12 LAURA    --2/25\n[13] LAURA    --5/19 LAURA    --3/15 LAURA    --9/16 THERESA  --3/2 \n[17] THERESA  --4/12 THERESA  --9/26 THERESA  --2/25 THERESA  --5/19\n[21] THERESA  --3/15 THERESA  --9/16 THERESA  --4/8  BRENDA   --6/27\n[25] BRENDA   --4/12 BRENDA   --9/26 BRENDA   --2/25 BRENDA   --5/19\n[29] BRENDA   --3/15 BRENDA   --9/16 CHARLOTTE--4/12 CHARLOTTE--9/26\n+ ... omitted several edges\n\n\nBut we can tell that the graph is a two-mode network because we have links starting with people with old lady names from the 1930s (which are also the names of a bunch of kids in middle school in 2024) and ending with events that have dates in them. So the (undirected) edge is \\(person-event\\).\nThe graph is undirected because the “membership” or “attendance” relation between a person and an organization/event doesn’t have a natural directionality.\nAnother way of checking the “bipartiteness” of a graph in igraph is by using the bipartite_mapping function.\nLet’s see what it does:\n\n   bipartite_mapping(g)\n\n$res\n[1] TRUE\n\n$type\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n    FALSE     FALSE     FALSE     FALSE     FALSE     FALSE     FALSE     FALSE \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n    FALSE     FALSE     FALSE     FALSE     FALSE     FALSE     FALSE     FALSE \n   OLIVIA     FLORA      6/27       3/2      4/12      9/26      2/25      5/19 \n    FALSE     FALSE      TRUE      TRUE      TRUE      TRUE      TRUE      TRUE \n     3/15      9/16       4/8      6/10      2/23       4/7     11/21       8/3 \n     TRUE      TRUE      TRUE      TRUE      TRUE      TRUE      TRUE      TRUE \n\n\nThis function takes the candidate bipartite graph as input and returns to objects: res is just a check to see if the graph is actually bipartite (TRUE in this case), type is a logical vector of dimensions \\(M + N\\) (where \\(M\\) is the number of nodes in the person set and \\(N\\) is the number of nodes in the event set) dividing the nodes into two groups. Here people get FALSE and events get TRUE, but this designations are arbitrary (a kind of dummy coding; FALSE = 0 and TRUE = 1).\nWe can add this as a node attribute to our graph so that way we know which node is in which set:\n\n   V(g)$type <- bipartite_mapping(g)$type"
  },
  {
    "objectID": "two-mode.html#the-bi-adjacency-affiliation-matrix",
    "href": "two-mode.html#the-bi-adjacency-affiliation-matrix",
    "title": "Analyzing Two-Mode Networks",
    "section": "The Bi-Adjacency (Affiliation) Matrix",
    "text": "The Bi-Adjacency (Affiliation) Matrix\nOnce you have your bipartite graph loaded up, you may want (if the graph is small enough) to check out the graph’s affiliation matrix \\(A\\).\nThis works just like before, except that now we use the as_biadjacency_matrix function:\n\n   A <- as.matrix(as_biadjacency_matrix(g))\n   A\n\n          6/27 3/2 4/12 9/26 2/25 5/19 3/15 9/16 4/8 6/10 2/23 4/7 11/21 8/3\nEVELYN       1   1    1    1    1    1    0    1   1    0    0   0     0   0\nLAURA        1   1    1    0    1    1    1    1   0    0    0   0     0   0\nTHERESA      0   1    1    1    1    1    1    1   1    0    0   0     0   0\nBRENDA       1   0    1    1    1    1    1    1   0    0    0   0     0   0\nCHARLOTTE    0   0    1    1    1    0    1    0   0    0    0   0     0   0\nFRANCES      0   0    1    0    1    1    0    1   0    0    0   0     0   0\nELEANOR      0   0    0    0    1    1    1    1   0    0    0   0     0   0\nPEARL        0   0    0    0    0    1    0    1   1    0    0   0     0   0\nRUTH         0   0    0    0    1    0    1    1   1    0    0   0     0   0\nVERNE        0   0    0    0    0    0    1    1   1    0    0   1     0   0\nMYRNA        0   0    0    0    0    0    0    1   1    1    0   1     0   0\nKATHERINE    0   0    0    0    0    0    0    1   1    1    0   1     1   1\nSYLVIA       0   0    0    0    0    0    1    1   1    1    0   1     1   1\nNORA         0   0    0    0    0    1    1    0   1    1    1   1     1   1\nHELEN        0   0    0    0    0    0    1    1   0    1    1   1     0   0\nDOROTHY      0   0    0    0    0    0    0    1   1    0    0   0     0   0\nOLIVIA       0   0    0    0    0    0    0    0   1    0    1   0     0   0\nFLORA        0   0    0    0    0    0    0    0   1    0    1   0     0   0\n\n\nIn this matrix we list one set of nodes in the rows and the other set is in the columns. Each cell \\(a_{ij} = 1\\) if row node \\(i\\) is affiliated with column node \\(j\\), otherwise \\(a_{ij} = 0\\)."
  },
  {
    "objectID": "two-mode.html#the-bipartite-adjacency-matrix",
    "href": "two-mode.html#the-bipartite-adjacency-matrix",
    "title": "Analyzing Two-Mode Networks",
    "section": "The Bipartite Adjacency Matrix",
    "text": "The Bipartite Adjacency Matrix\nNote that if we were to use the regular as_adjacency_matrix function on a bipartite graph, we get a curious version of the adjacency matrix:\n\n   B <- as.matrix(as_adjacency_matrix(g))\n   B\n\n          EVELYN LAURA THERESA BRENDA CHARLOTTE FRANCES ELEANOR PEARL RUTH\nEVELYN         0     0       0      0         0       0       0     0    0\nLAURA          0     0       0      0         0       0       0     0    0\nTHERESA        0     0       0      0         0       0       0     0    0\nBRENDA         0     0       0      0         0       0       0     0    0\nCHARLOTTE      0     0       0      0         0       0       0     0    0\nFRANCES        0     0       0      0         0       0       0     0    0\nELEANOR        0     0       0      0         0       0       0     0    0\nPEARL          0     0       0      0         0       0       0     0    0\nRUTH           0     0       0      0         0       0       0     0    0\nVERNE          0     0       0      0         0       0       0     0    0\nMYRNA          0     0       0      0         0       0       0     0    0\nKATHERINE      0     0       0      0         0       0       0     0    0\nSYLVIA         0     0       0      0         0       0       0     0    0\nNORA           0     0       0      0         0       0       0     0    0\nHELEN          0     0       0      0         0       0       0     0    0\nDOROTHY        0     0       0      0         0       0       0     0    0\nOLIVIA         0     0       0      0         0       0       0     0    0\nFLORA          0     0       0      0         0       0       0     0    0\n6/27           1     1       0      1         0       0       0     0    0\n3/2            1     1       1      0         0       0       0     0    0\n4/12           1     1       1      1         1       1       0     0    0\n9/26           1     0       1      1         1       0       0     0    0\n2/25           1     1       1      1         1       1       1     0    1\n5/19           1     1       1      1         0       1       1     1    0\n3/15           0     1       1      1         1       0       1     0    1\n9/16           1     1       1      1         0       1       1     1    1\n4/8            1     0       1      0         0       0       0     1    1\n6/10           0     0       0      0         0       0       0     0    0\n2/23           0     0       0      0         0       0       0     0    0\n4/7            0     0       0      0         0       0       0     0    0\n11/21          0     0       0      0         0       0       0     0    0\n8/3            0     0       0      0         0       0       0     0    0\n          VERNE MYRNA KATHERINE SYLVIA NORA HELEN DOROTHY OLIVIA FLORA 6/27 3/2\nEVELYN        0     0         0      0    0     0       0      0     0    1   1\nLAURA         0     0         0      0    0     0       0      0     0    1   1\nTHERESA       0     0         0      0    0     0       0      0     0    0   1\nBRENDA        0     0         0      0    0     0       0      0     0    1   0\nCHARLOTTE     0     0         0      0    0     0       0      0     0    0   0\nFRANCES       0     0         0      0    0     0       0      0     0    0   0\nELEANOR       0     0         0      0    0     0       0      0     0    0   0\nPEARL         0     0         0      0    0     0       0      0     0    0   0\nRUTH          0     0         0      0    0     0       0      0     0    0   0\nVERNE         0     0         0      0    0     0       0      0     0    0   0\nMYRNA         0     0         0      0    0     0       0      0     0    0   0\nKATHERINE     0     0         0      0    0     0       0      0     0    0   0\nSYLVIA        0     0         0      0    0     0       0      0     0    0   0\nNORA          0     0         0      0    0     0       0      0     0    0   0\nHELEN         0     0         0      0    0     0       0      0     0    0   0\nDOROTHY       0     0         0      0    0     0       0      0     0    0   0\nOLIVIA        0     0         0      0    0     0       0      0     0    0   0\nFLORA         0     0         0      0    0     0       0      0     0    0   0\n6/27          0     0         0      0    0     0       0      0     0    0   0\n3/2           0     0         0      0    0     0       0      0     0    0   0\n4/12          0     0         0      0    0     0       0      0     0    0   0\n9/26          0     0         0      0    0     0       0      0     0    0   0\n2/25          0     0         0      0    0     0       0      0     0    0   0\n5/19          0     0         0      0    1     0       0      0     0    0   0\n3/15          1     0         0      1    1     1       0      0     0    0   0\n9/16          1     1         1      1    0     1       1      0     0    0   0\n4/8           1     1         1      1    1     0       1      1     1    0   0\n6/10          0     1         1      1    1     1       0      0     0    0   0\n2/23          0     0         0      0    1     1       0      1     1    0   0\n4/7           1     1         1      1    1     1       0      0     0    0   0\n11/21         0     0         1      1    1     0       0      0     0    0   0\n8/3           0     0         1      1    1     0       0      0     0    0   0\n          4/12 9/26 2/25 5/19 3/15 9/16 4/8 6/10 2/23 4/7 11/21 8/3\nEVELYN       1    1    1    1    0    1   1    0    0   0     0   0\nLAURA        1    0    1    1    1    1   0    0    0   0     0   0\nTHERESA      1    1    1    1    1    1   1    0    0   0     0   0\nBRENDA       1    1    1    1    1    1   0    0    0   0     0   0\nCHARLOTTE    1    1    1    0    1    0   0    0    0   0     0   0\nFRANCES      1    0    1    1    0    1   0    0    0   0     0   0\nELEANOR      0    0    1    1    1    1   0    0    0   0     0   0\nPEARL        0    0    0    1    0    1   1    0    0   0     0   0\nRUTH         0    0    1    0    1    1   1    0    0   0     0   0\nVERNE        0    0    0    0    1    1   1    0    0   1     0   0\nMYRNA        0    0    0    0    0    1   1    1    0   1     0   0\nKATHERINE    0    0    0    0    0    1   1    1    0   1     1   1\nSYLVIA       0    0    0    0    1    1   1    1    0   1     1   1\nNORA         0    0    0    1    1    0   1    1    1   1     1   1\nHELEN        0    0    0    0    1    1   0    1    1   1     0   0\nDOROTHY      0    0    0    0    0    1   1    0    0   0     0   0\nOLIVIA       0    0    0    0    0    0   1    0    1   0     0   0\nFLORA        0    0    0    0    0    0   1    0    1   0     0   0\n6/27         0    0    0    0    0    0   0    0    0   0     0   0\n3/2          0    0    0    0    0    0   0    0    0   0     0   0\n4/12         0    0    0    0    0    0   0    0    0   0     0   0\n9/26         0    0    0    0    0    0   0    0    0   0     0   0\n2/25         0    0    0    0    0    0   0    0    0   0     0   0\n5/19         0    0    0    0    0    0   0    0    0   0     0   0\n3/15         0    0    0    0    0    0   0    0    0   0     0   0\n9/16         0    0    0    0    0    0   0    0    0   0     0   0\n4/8          0    0    0    0    0    0   0    0    0   0     0   0\n6/10         0    0    0    0    0    0   0    0    0   0     0   0\n2/23         0    0    0    0    0    0   0    0    0   0     0   0\n4/7          0    0    0    0    0    0   0    0    0   0     0   0\n11/21        0    0    0    0    0    0   0    0    0   0     0   0\n8/3          0    0    0    0    0    0   0    0    0   0     0   0\n\n\nThis bipartite adjacency matrix \\(\\mathbf{B}\\) is of dimensions \\((M + N) \\times (M + N)\\), which is \\((18 + 14) \\times (18 + 14) = 32 \\times 32\\) in the Southern Women data; it has the following block structure (Fouss, Saerens, and Shimbo 2016, 12):\n\\[\n\\mathbf{B} = \\left[\n\\begin{matrix}\n\\mathbf{O}_{M \\times M} & \\mathbf{A}_{M \\times N} \\\\\n\\mathbf{A}^T_{N \\times M} & \\mathbf{O}_{N \\times N}\n\\end{matrix}\n\\right]\n\\]\nWhere \\(\\mathbf{O}\\) is just the all zeros matrix of the relevant dimensions, and \\(\\mathbf{A}\\) is the bi-adjacency (affiliation) matrix as defined earlier. Thus, the bipartite adjacency matrix necessarily has two big diagonal “zero blocks” in it (upper-left and lower-right) corresponding to where the links between nodes in the same set would be (but necessarily aren’t because this is a two-mode network). The non-zero blocks are just the affiliation matrix (upper-right) and its transpose(lower-left)."
  },
  {
    "objectID": "two-mode.html#bipartiteness-as-anti-community",
    "href": "two-mode.html#bipartiteness-as-anti-community",
    "title": "Analyzing Two-Mode Networks",
    "section": "Bipartiteness as “Anti-Community”",
    "text": "Bipartiteness as “Anti-Community”\nRecall from the previous handout, that community structure is defined by clusters of nodes that have more connections among themselves than they do with outsiders. If you think about it, a bipartite graph has the opposite of this going on. Nodes of the same type have zero connections among themselves, and they have all their connections with nodes of the other group!\nSo that means that bipartite structure is the mirror image of community structure (in the two group case). This also means that if we were to compute the modularity of a bipartite graph, using the node type as the grouping variable we should get the theoretical minimum of this measure (which you may recall is \\(Q = -\\frac{1}{2}\\)).\nLet’s try it out, by computing the modularity from the bipartite adjacency matrix of the Southern Women data, using node type as the grouping variable:\n\n   V(g)$comm <- as.numeric(bipartite_mapping(g)$type) + 1\n   modularity(g, V(g)$comm)\n\n[1] -0.5\n\n\nAnd indeed, we recover the theoretical minimum value of the modularity (Brandes et al. 2007, 173)! This also means that this method can be used to test whether a graph is bipartite, or whether any network approximates bipartiteness (Newman 2006, 13). Values that are close to \\(-0.5\\) would indicate that the network in question has bipartite structure."
  },
  {
    "objectID": "two-mode.html#basic-two-mode-network-statistics",
    "href": "two-mode.html#basic-two-mode-network-statistics",
    "title": "Analyzing Two-Mode Networks",
    "section": "Basic Two-Mode Network Statistics",
    "text": "Basic Two-Mode Network Statistics\nWe can calculate some basic network statistics from the affiliation (bi-adjacency) matrix. We have two number of nodes to calculate, but only one quantity for the number of edges.\nThe number of nodes on the people side \\(N\\) is just the number of rows of \\(A\\):\n\n   nrow(A)\n\n[1] 18\n\n\nAnd the number of events/groups \\(M\\) is just the number of columns:\n\n   ncol(A)\n\n[1] 14\n\n\nFinally, the number of edges \\(E\\) is just the sum of all the entries of \\(A\\):\n\n   sum(A)\n\n[1] 89\n\n\nNote that if you were to use the igraph function vcount on the original graph object, you get the wrong answer:\n\n   vcount(g)\n\n[1] 32\n\n\nThat’s because vcount is working with the \\(32 \\times 32\\) regular adjacency matrix, not the bi-adjacency matrix. Here, vcount is returning the total number of nodes in the graph summing across the two sets, which is \\(M + N\\).\nIf you wanted to get the right answer for each set of edges from the regular igraph graph object, you could use the type node attribute we defined earlier along with the subgraph function:\n\n   vcount(subgraph(g, V(g)$type == FALSE))\n\n[1] 18\n\n\nWhich gives us the number of women. For the events we do the same thing:\n\n   vcount(subgraph(g, V(g)$type == TRUE))\n\n[1] 14\n\n\nHowever, because there’s only one set of edges, ecount still gives us the right answer:\n\n   ecount(g)\n\n[1] 89\n\n\nWhich is the same as:\n\n   sum(A)\n\n[1] 89\n\n\n\nDegree Statistics\nBecause we have two sets of degrees, all the basic degree statistics in the network double up. So we have two mean degrees, two maximum degrees, and two minimum degree to take care of:\n\n   mean.d.p <- mean(rowSums(A))\n   mean.d.g <- mean(colSums(A))\n   max.d.p <- max(rowSums(A))\n   max.d.g <- max(colSums(A))\n   min.d.p <- min(rowSums(A))\n   min.d.g <- min(colSums(A))\n\nSo we have:\n\n   round(mean.d.p, 1)\n\n[1] 4.9\n\n   round(mean.d.g, 1)\n\n[1] 6.4\n\n   max.d.p\n\n[1] 8\n\n   max.d.g\n\n[1] 14\n\n   min.d.p\n\n[1] 2\n\n   min.d.g\n\n[1] 3\n\n\nHowever, note that because there’s only one set of undirected edges, the total number of edges incident to each node in each of the two sets is always going to be the same.\nThat means that there’s only one sum of degrees. So the sum of degrees for people:\n\n   sum(rowSums(A))\n\n[1] 89\n\n\nIs the same as the sum of degrees of events:\n\n   sum(colSums(A))\n\n[1] 89\n\n\nNote that in a bipartite graph, therefore, the sum of degrees of nodes in each node set is equal to the \\(|E|\\), the number of edges in the graph!\n\n\nDensity\nAs we saw in the case of one-mode networks, one of the most basic network statistics that can be derived from the above quantities is the density (observed number of edges divided by maximum possible number of edges in the graph).\nIn a two-mode network, density is given by:\n\\[\nd = \\frac{|E|}{N \\times M}\n\\]\nWhere \\(|E|\\) is the number of edges in the network. In our case we can compute the density as follows:\n\n   d <- sum(A)/(nrow(A) * ncol(A))\n   d\n\n[1] 0.3531746"
  },
  {
    "objectID": "two-mode.html#degree-centrality",
    "href": "two-mode.html#degree-centrality",
    "title": "Analyzing Two-Mode Networks",
    "section": "Degree Centrality",
    "text": "Degree Centrality\nIn a two-mode network, there are two degree sets, each corresponding to one set of nodes. For the people, in this case, their degree (centrality) is just the number of events they attend, and for the groups, it’s just the number of people that attend each event.\nAs we have already seen, we can get each from the affiliation matrix. The degree of the people are just the row sums:\n\n   rowSums(A)\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n        8         7         8         7         4         4         4         3 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n        4         4         4         6         7         8         5         2 \n   OLIVIA     FLORA \n        2         2 \n\n\nAnd the degree of the events are just the column sums:\n\n   colSums(A)\n\n 6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10  2/23   4/7 11/21 \n    3     3     6     4     8     8    10    14    12     5     4     6     3 \n  8/3 \n    3 \n\n\nThe igraph function degree will also give us the right answer, but in the form of a single vector including both people and events:\n\n   degree(g)\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n        8         7         8         7         4         4         4         3 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n        4         4         4         6         7         8         5         2 \n   OLIVIA     FLORA      6/27       3/2      4/12      9/26      2/25      5/19 \n        2         2         3         3         6         4         8         8 \n     3/15      9/16       4/8      6/10      2/23       4/7     11/21       8/3 \n       10        14        12         5         4         6         3         3 \n\n\nAs Borgatti and Everett (1997) note, if we want normalized degree centrality measures, we need to divide by either \\(M\\) (for people) or \\(N\\) (for events). That is, for people we use the number of events as the norm (as this is the theoretical maximum) and for events the number of people.\nSo for people, normalized degree is:\n\n   round(rowSums(A)/ncol(A), 3)\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n    0.571     0.500     0.571     0.500     0.286     0.286     0.286     0.214 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n    0.286     0.286     0.286     0.429     0.500     0.571     0.357     0.143 \n   OLIVIA     FLORA \n    0.143     0.143 \n\n\nAnd for events:\n\n   round(colSums(A)/nrow(A), 3)\n\n 6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10  2/23   4/7 11/21 \n0.167 0.167 0.333 0.222 0.444 0.444 0.556 0.778 0.667 0.278 0.222 0.333 0.167 \n  8/3 \n0.167 \n\n\nOr with igraph:\n\n   round(degree(g)/c(rep(14, 18), rep(18, 14)), 3)\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n    0.571     0.500     0.571     0.500     0.286     0.286     0.286     0.214 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n    0.286     0.286     0.286     0.429     0.500     0.571     0.357     0.143 \n   OLIVIA     FLORA      6/27       3/2      4/12      9/26      2/25      5/19 \n    0.143     0.143     0.167     0.167     0.333     0.222     0.444     0.444 \n     3/15      9/16       4/8      6/10      2/23       4/7     11/21       8/3 \n    0.556     0.778     0.667     0.278     0.222     0.333     0.167     0.167"
  },
  {
    "objectID": "two-mode.html#geodesic-distances",
    "href": "two-mode.html#geodesic-distances",
    "title": "Analyzing Two-Mode Networks",
    "section": "Geodesic Distances",
    "text": "Geodesic Distances\nGeodesic distances work a bit different in two-mode networks because of the only between-node-sets edges restriction.\nFor instance, the minimum geodesic distance \\(g_{ii'}\\) between two people is two (a person cannot be adjacent to another person), but it is one between a person and a group (if the person is a member of the group).\nIn the same way, a group \\(g\\) cannot be at geodesic distance less than three from a person \\(p*\\) who is not a member, because the shortest path is \\(g-p-g^*-p^*\\).\nThat is, there has to be some other group \\(g^*\\) shared between a member \\(p\\) of the focal group \\(g\\) and another person \\(p^*\\) for the shortest path between \\(g\\) and the non-member \\(p^*\\) to exist, and that involves three links at minimum: \\(g-p\\), \\(p-g^*\\), and \\(g^*-p^*\\). This means that the links in paths in two-mode networks always alternate between persons and group nodes.\nBeyond that geodesic distances work the same way. In igraph when we use the distances function on a bipartite graph, we get:\n\n   D.pg <- distances(g)\n   head(D.pg)\n\n          EVELYN LAURA THERESA BRENDA CHARLOTTE FRANCES ELEANOR PEARL RUTH\nEVELYN         0     2       2      2         2       2       2     2    2\nLAURA          2     0       2      2         2       2       2     2    2\nTHERESA        2     2       0      2         2       2       2     2    2\nBRENDA         2     2       2      0         2       2       2     2    2\nCHARLOTTE      2     2       2      2         0       2       2     4    2\nFRANCES        2     2       2      2         2       0       2     2    2\n          VERNE MYRNA KATHERINE SYLVIA NORA HELEN DOROTHY OLIVIA FLORA 6/27 3/2\nEVELYN        2     2         2      2    2     2       2      2     2    1   1\nLAURA         2     2         2      2    2     2       2      4     4    1   1\nTHERESA       2     2         2      2    2     2       2      2     2    3   1\nBRENDA        2     2         2      2    2     2       2      4     4    1   3\nCHARLOTTE     2     4         4      2    2     2       4      4     4    3   3\nFRANCES       2     2         2      2    2     2       2      4     4    3   3\n          4/12 9/26 2/25 5/19 3/15 9/16 4/8 6/10 2/23 4/7 11/21 8/3\nEVELYN       1    1    1    1    3    1   1    3    3   3     3   3\nLAURA        1    3    1    1    1    1   3    3    3   3     3   3\nTHERESA      1    1    1    1    1    1   1    3    3   3     3   3\nBRENDA       1    1    1    1    1    1   3    3    3   3     3   3\nCHARLOTTE    1    1    1    3    1    3   3    3    3   3     3   3\nFRANCES      1    3    1    1    3    1   3    3    3   3     3   3\n\n   tail(D.pg)\n\n      EVELYN LAURA THERESA BRENDA CHARLOTTE FRANCES ELEANOR PEARL RUTH VERNE\n4/8        1     3       1      3         3       3       3     1    1     1\n6/10       3     3       3      3         3       3       3     3    3     3\n2/23       3     3       3      3         3       3       3     3    3     3\n4/7        3     3       3      3         3       3       3     3    3     1\n11/21      3     3       3      3         3       3       3     3    3     3\n8/3        3     3       3      3         3       3       3     3    3     3\n      MYRNA KATHERINE SYLVIA NORA HELEN DOROTHY OLIVIA FLORA 6/27 3/2 4/12 9/26\n4/8       1         1      1    1     3       1      1     1    2   2    2    2\n6/10      1         1      1    1     1       3      3     3    4   4    4    4\n2/23      3         3      3    1     1       3      1     1    4   4    4    4\n4/7       1         1      1    1     1       3      3     3    4   4    4    4\n11/21     3         1      1    1     3       3      3     3    4   4    4    4\n8/3       3         1      1    1     3       3      3     3    4   4    4    4\n      2/25 5/19 3/15 9/16 4/8 6/10 2/23 4/7 11/21 8/3\n4/8      2    2    2    2   0    2    2   2     2   2\n6/10     4    2    2    2   2    0    2   2     2   2\n2/23     4    2    2    2   2    2    0   2     2   2\n4/7      4    2    2    2   2    2    2   0     2   2\n11/21    4    2    2    2   2    2    2   2     0   2\n8/3      4    2    2    2   2    2    2   2     2   0\n\n\nWhich is a square matrix of dimensions \\((M + N) \\times (M + N)\\); that’s \\((18 + 14) \\times (18 + 14) = 32 \\times 32\\) in our case.\nWe can check in R:\n\n   dim(D.pg)\n\n[1] 32 32\n\n\nAs we can see in the distance matrix, distances between nodes in the same set are even \\(g_{ii'|jj'} = \\{2, 4, \\ldots\\}\\) but distances in nodes in different sets are odd \\(g_{ij|ji} = \\{1, 3, \\ldots\\}\\). Beyond this hiccup, distances can be interpreted in the same way as one-mode networks."
  },
  {
    "objectID": "two-mode.html#closeness-centrality-in-two-mode-networks",
    "href": "two-mode.html#closeness-centrality-in-two-mode-networks",
    "title": "Analyzing Two-Mode Networks",
    "section": "Closeness Centrality in two-mode Networks",
    "text": "Closeness Centrality in two-mode Networks\nThis means that (unnormalized) closeness centrality works the same way as it does in regular networks:\n\n   round(closeness(g), 3)\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n    0.017     0.015     0.017     0.015     0.013     0.014     0.014     0.014 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n    0.015     0.015     0.014     0.015     0.016     0.017     0.015     0.014 \n   OLIVIA     FLORA      6/27       3/2      4/12      9/26      2/25      5/19 \n    0.012     0.012     0.012     0.012     0.013     0.012     0.014     0.016 \n     3/15      9/16       4/8      6/10      2/23       4/7     11/21       8/3 \n    0.017     0.019     0.018     0.013     0.012     0.013     0.012     0.012 \n\n\nWhich is just the inverse of the sums of the distances matrix for people and groups counting their geodesic distances to nodes of both sets.\nHowever, as Borgatti and Everett (1997) note, if we want normalized closeness centralities, we can’t use the off-the-shelf normalization for one-mode networks in igraph (\\(n-1\\)) as it will give us non-sense results because now we have two sets of nodes.\nInstead, we need to normalize the closeness score for each node set by its theoretical maximum for each node set.\nFor people, this is:\n\\[\nN + 2(M - 1)\n\\]\nAnd for groups/events this same quantity is:\n\\[\nM + 2(N - 1)\n\\]\nThe basic idea is that nodes can be at minimum geodesic distance \\(g = 1\\) from nodes of the other set (for people, groups; for groups, people) and at minimum distance \\(g = 2\\) from nodes of their own set, with their own presence eliminated by subtraction (Borgatti and Everett 1997).\nIn our case, we create a normalization vector with these quantities of length \\(M + N\\):\n\n   M <- nrow(A)\n   N <- ncol(A)\n   n.p <- N + 2 * (M - 1)\n   n.e <- M + 2 * (N - 1)\n   norm.vec <- c(rep(n.p, M), rep(n.e, N))\n\nAnd normalized closeness is:\n\n   round(norm.vec/rowSums(D.pg), 3)\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n    0.800     0.727     0.800     0.727     0.600     0.667     0.667     0.667 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n    0.706     0.706     0.686     0.727     0.774     0.800     0.727     0.649 \n   OLIVIA     FLORA      6/27       3/2      4/12      9/26      2/25      5/19 \n    0.585     0.585     0.524     0.524     0.564     0.537     0.595     0.688 \n     3/15      9/16       4/8      6/10      2/23       4/7     11/21       8/3 \n    0.733     0.846     0.786     0.550     0.537     0.564     0.524     0.524 \n\n\nWhich are the same numbers in Borgatti and Everett (1997, table 1, column 6)."
  },
  {
    "objectID": "two-mode.html#betweenness-centrality-in-two-mode-networks",
    "href": "two-mode.html#betweenness-centrality-in-two-mode-networks",
    "title": "Analyzing Two-Mode Networks",
    "section": "Betweenness Centrality in two-mode Networks",
    "text": "Betweenness Centrality in two-mode Networks\nAs Borgatti and Everett (1997) also note, the normalizations for betweenness centrality in the two-mode case are a bit more involved. This is because they depend on which node set is larger than the other.\nFor the larger node set, which in our case is the people, the normalization is:\n\\[\n2(M-1)(N-1)\n\\]\nFor the smaller node set, which in our case is the groups/events, the normalization is:\n\\[\n\\frac{1}{2}(N)(N-1)+\\frac{1}{2}(M-1)(M-2)+(M-1)(N-1)\n\\]\nRemember that you have to switch this around if you are analyzing a network with more groups than people.\nCreating the relevant vectors:\n\n   n.p <- 2*(M-1)*(N-1)\n   n.e <- (1/2)*(N*(N-1))+(1/2)*(M-1)*(M-2)+(M-1)*(N-1)\n   norm.vec <- c(rep(n.p, M), rep(n.e, N))\n\nAnd normalized betweenness is:\n\n   round(betweenness(g)/norm.vec, 4)*100\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n     9.72      5.17      8.82      4.98      1.07      1.08      0.95      0.68 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n     1.69      1.58      1.65      4.77      7.22     11.42      4.27      0.20 \n   OLIVIA     FLORA      6/27       3/2      4/12      9/26      2/25      5/19 \n     0.51      0.51      0.22      0.21      1.84      0.78      3.80      6.56 \n     3/15      9/16       4/8      6/10      2/23       4/7     11/21       8/3 \n    13.07     24.60     22.75      1.15      1.98      1.83      0.23      0.23 \n\n\nWhich are (with some slight differences and rounding errors) the same numbers in Borgatti and Everett (1997, table 2, column 3)."
  },
  {
    "objectID": "two-mode.html#the-duality-of-persons-and-groups",
    "href": "two-mode.html#the-duality-of-persons-and-groups",
    "title": "Analyzing Two-Mode Networks",
    "section": "The Duality of Persons and Groups",
    "text": "The Duality of Persons and Groups\nRemember that in the one-mode case, multiplying the adjacency matrix times its transpose yields the common neighbors matrix \\(\\mathbf{M}\\):\n\\[\n\\mathbf{M} = \\mathbf{A}\\mathbf{A}^T\n\\]\nAs famously noted by Breiger (1974), doing the same for the affiliation matrix of a two-mode network also returns the common-neighbors matrix, but because objects in one mode can only connect to objects in another mode, this also reveals the duality of persons and groups: The connections between people are made up of the groups they share, and the connections between groups are revealed by the groups they share.\nThus, computing the common neighbors matrix for both persons and groups (also called the projection of the two-mode network into each of its modes) produces a one-mode similarity matrix between people and groups, where the similarities are defined by the number of objects in the other mode that they share.\nSo for the people the relevant projection is:\n\\[\n\\mathbf{P} = \\mathbf{A}\\mathbf{A}^T\n\\]\nAnd for the groups:\n\\[\n\\mathbf{G} = \\mathbf{A}^T\\mathbf{A}\n\\]\nWhich in our case yields:\n\n   P <- A %*% t(A)\n   P\n\n          EVELYN LAURA THERESA BRENDA CHARLOTTE FRANCES ELEANOR PEARL RUTH\nEVELYN         8     6       7      6         3       4       3     3    3\nLAURA          6     7       6      6         3       4       4     2    3\nTHERESA        7     6       8      6         4       4       4     3    4\nBRENDA         6     6       6      7         4       4       4     2    3\nCHARLOTTE      3     3       4      4         4       2       2     0    2\nFRANCES        4     4       4      4         2       4       3     2    2\nELEANOR        3     4       4      4         2       3       4     2    3\nPEARL          3     2       3      2         0       2       2     3    2\nRUTH           3     3       4      3         2       2       3     2    4\nVERNE          2     2       3      2         1       1       2     2    3\nMYRNA          2     1       2      1         0       1       1     2    2\nKATHERINE      2     1       2      1         0       1       1     2    2\nSYLVIA         2     2       3      2         1       1       2     2    3\nNORA           2     2       3      2         1       1       2     2    2\nHELEN          1     2       2      2         1       1       2     1    2\nDOROTHY        2     1       2      1         0       1       1     2    2\nOLIVIA         1     0       1      0         0       0       0     1    1\nFLORA          1     0       1      0         0       0       0     1    1\n          VERNE MYRNA KATHERINE SYLVIA NORA HELEN DOROTHY OLIVIA FLORA\nEVELYN        2     2         2      2    2     1       2      1     1\nLAURA         2     1         1      2    2     2       1      0     0\nTHERESA       3     2         2      3    3     2       2      1     1\nBRENDA        2     1         1      2    2     2       1      0     0\nCHARLOTTE     1     0         0      1    1     1       0      0     0\nFRANCES       1     1         1      1    1     1       1      0     0\nELEANOR       2     1         1      2    2     2       1      0     0\nPEARL         2     2         2      2    2     1       2      1     1\nRUTH          3     2         2      3    2     2       2      1     1\nVERNE         4     3         3      4    3     3       2      1     1\nMYRNA         3     4         4      4    3     3       2      1     1\nKATHERINE     3     4         6      6    5     3       2      1     1\nSYLVIA        4     4         6      7    6     4       2      1     1\nNORA          3     3         5      6    8     4       1      2     2\nHELEN         3     3         3      4    4     5       1      1     1\nDOROTHY       2     2         2      2    1     1       2      1     1\nOLIVIA        1     1         1      1    2     1       1      2     2\nFLORA         1     1         1      1    2     1       1      2     2\n\n   G <- t(A) %*% A\n   G\n\n      6/27 3/2 4/12 9/26 2/25 5/19 3/15 9/16 4/8 6/10 2/23 4/7 11/21 8/3\n6/27     3   2    3    2    3    3    2    3   1    0    0   0     0   0\n3/2      2   3    3    2    3    3    2    3   2    0    0   0     0   0\n4/12     3   3    6    4    6    5    4    5   2    0    0   0     0   0\n9/26     2   2    4    4    4    3    3    3   2    0    0   0     0   0\n2/25     3   3    6    4    8    6    6    7   3    0    0   0     0   0\n5/19     3   3    5    3    6    8    5    7   4    1    1   1     1   1\n3/15     2   2    4    3    6    5   10    8   5    3    2   4     2   2\n9/16     3   3    5    3    7    7    8   14   9    4    1   5     2   2\n4/8      1   2    2    2    3    4    5    9  12    4    3   5     3   3\n6/10     0   0    0    0    0    1    3    4   4    5    2   5     3   3\n2/23     0   0    0    0    0    1    2    1   3    2    4   2     1   1\n4/7      0   0    0    0    0    1    4    5   5    5    2   6     3   3\n11/21    0   0    0    0    0    1    2    2   3    3    1   3     3   3\n8/3      0   0    0    0    0    1    2    2   3    3    1   3     3   3\n\n\nThe off-diagonal entries of these square person by person (group by group) matrices is the number of groups (people) shared by each person (group) and the diagonals are the number of memberships of each person (the size of each group/event).\nIn igraph the relevant function is called bipartite_projection. It takes a graph as an input and returns a list containing igraph graph objects of both projections by default:\n\n   Proj <- bipartite_projection(g)\n   G.p <- Proj[[1]]\n   G.g <- Proj[[2]]\n\nIn the graph objects produced by the bipartite_projection function, the actual shared memberships and shared members are stored as an attribute of each edge called weight used in the plotting code above to set the edge.width:\n\n   edge_attr(G.p)\n\n$weight\n  [1] 6 6 7 3 4 3 3 3 2 2 2 2 2 1 2 1 1 6 6 3 4 4 3 2 2 2 2 2 1 1 1 6 4 4 4 4 3\n [38] 3 3 3 2 2 2 2 1 1 4 4 4 3 2 2 2 2 2 1 1 1 2 2 2 1 1 1 1 3 2 2 1 1 1 1 1 1\n [75] 1 3 2 2 2 2 2 1 1 1 2 2 2 2 2 2 1 2 1 1 3 3 2 2 2 2 2 1 1 4 3 3 3 3 2 1 1\n[112] 4 4 3 2 3 1 1 6 3 2 5 1 1 6 4 2 1 1 4 1 2 2 1 1 1 1 1 2\n\n   edge_attr(G.g)\n\n$weight\n [1] 2 3 2 3 3 3 1 2 3 2 3 3 3 2 2 4 6 5 5 2 4 4 3 3 2 3 6 7 3 6 7 4 5 1 1 1 1 1\n[39] 8 5 4 3 2 2 2 9 5 4 2 2 1 5 4 3 3 3 5 3 3 2 2 1 1 3 3 3\n\n\nSo to get the weighted projection matrix, we need to type:\n\n   as.matrix(as_adjacency_matrix(G.p, attr = \"weight\"))\n\n          EVELYN LAURA THERESA BRENDA CHARLOTTE FRANCES ELEANOR PEARL RUTH\nEVELYN         0     6       7      6         3       4       3     3    3\nLAURA          6     0       6      6         3       4       4     2    3\nTHERESA        7     6       0      6         4       4       4     3    4\nBRENDA         6     6       6      0         4       4       4     2    3\nCHARLOTTE      3     3       4      4         0       2       2     0    2\nFRANCES        4     4       4      4         2       0       3     2    2\nELEANOR        3     4       4      4         2       3       0     2    3\nPEARL          3     2       3      2         0       2       2     0    2\nRUTH           3     3       4      3         2       2       3     2    0\nVERNE          2     2       3      2         1       1       2     2    3\nMYRNA          2     1       2      1         0       1       1     2    2\nKATHERINE      2     1       2      1         0       1       1     2    2\nSYLVIA         2     2       3      2         1       1       2     2    3\nNORA           2     2       3      2         1       1       2     2    2\nHELEN          1     2       2      2         1       1       2     1    2\nDOROTHY        2     1       2      1         0       1       1     2    2\nOLIVIA         1     0       1      0         0       0       0     1    1\nFLORA          1     0       1      0         0       0       0     1    1\n          VERNE MYRNA KATHERINE SYLVIA NORA HELEN DOROTHY OLIVIA FLORA\nEVELYN        2     2         2      2    2     1       2      1     1\nLAURA         2     1         1      2    2     2       1      0     0\nTHERESA       3     2         2      3    3     2       2      1     1\nBRENDA        2     1         1      2    2     2       1      0     0\nCHARLOTTE     1     0         0      1    1     1       0      0     0\nFRANCES       1     1         1      1    1     1       1      0     0\nELEANOR       2     1         1      2    2     2       1      0     0\nPEARL         2     2         2      2    2     1       2      1     1\nRUTH          3     2         2      3    2     2       2      1     1\nVERNE         0     3         3      4    3     3       2      1     1\nMYRNA         3     0         4      4    3     3       2      1     1\nKATHERINE     3     4         0      6    5     3       2      1     1\nSYLVIA        4     4         6      0    6     4       2      1     1\nNORA          3     3         5      6    0     4       1      2     2\nHELEN         3     3         3      4    4     0       1      1     1\nDOROTHY       2     2         2      2    1     1       0      1     1\nOLIVIA        1     1         1      1    2     1       1      0     2\nFLORA         1     1         1      1    2     1       1      2     0\n\n\n\n   as.matrix(as_adjacency_matrix(G.g, attr = \"weight\"))\n\n      6/27 3/2 4/12 9/26 2/25 5/19 3/15 9/16 4/8 6/10 2/23 4/7 11/21 8/3\n6/27     0   2    3    2    3    3    2    3   1    0    0   0     0   0\n3/2      2   0    3    2    3    3    2    3   2    0    0   0     0   0\n4/12     3   3    0    4    6    5    4    5   2    0    0   0     0   0\n9/26     2   2    4    0    4    3    3    3   2    0    0   0     0   0\n2/25     3   3    6    4    0    6    6    7   3    0    0   0     0   0\n5/19     3   3    5    3    6    0    5    7   4    1    1   1     1   1\n3/15     2   2    4    3    6    5    0    8   5    3    2   4     2   2\n9/16     3   3    5    3    7    7    8    0   9    4    1   5     2   2\n4/8      1   2    2    2    3    4    5    9   0    4    3   5     3   3\n6/10     0   0    0    0    0    1    3    4   4    0    2   5     3   3\n2/23     0   0    0    0    0    1    2    1   3    2    0   2     1   1\n4/7      0   0    0    0    0    1    4    5   5    5    2   0     3   3\n11/21    0   0    0    0    0    1    2    2   3    3    1   3     0   3\n8/3      0   0    0    0    0    1    2    2   3    3    1   3     3   0\n\n\nWe can also use the weights to draw a weighted graph network plot of people and group projections. All we have to do is set the edge.with argument to the value of the edge weight attribute in the corresponding graph:\n\n   set.seed(123)\n   plot(G.p, \n     vertex.size=8, vertex.frame.color=\"lightgray\", \n     vertex.label.dist=2, edge.curved=0.2, \n     vertex.label.cex = 1.25, edge.color = \"lightgray\",\n     edge.width = E(G.p)$weight)\n\n\n\n\nOne mode projection of people.\n\n\n\n\n\n   set.seed(123)\n   plot(G.g, \n     vertex.size=8, vertex.frame.color=\"lightgray\", \n     vertex.label.dist=2, edge.curved=0.2, \n     vertex.label.cex = 1.25, edge.color = \"lightgray\",\n     edge.width = E(G.g)$weight)\n\n\n\n\nOne mode projection of groups\n\n\n\n\nNote that because both G.p and G.g are weighted graphs we can calculate the weighted version of degree for both persons and groups from them (sometimes called the vertex strength).\nIn igraph we can do this using the strength function, which takes a weighted graph object as input:\n\n   strength(G.p)\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n       50        45        57        46        24        32        36        31 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n       40        38        33        37        46        43        34        24 \n   OLIVIA     FLORA \n       14        14 \n\n   strength(G.g)\n\n 6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10  2/23   4/7 11/21 \n   19    20    32    23    38    41    48    59    46    25    13    28    18 \n  8/3 \n   18 \n\n\nInterestingly, as noted by Faust (1997, 167), there is a (dual!) mathematical connection between the strength of each vertex in the weighted projection and the centrality of the nodes from the other set they are connected to:\n\nFor people, the vertex strength is equal to the sum of the sizes of the groups they belong to minus their own degree.\nFor groups, the vertex strength is equal to the sum of the memberships of the people that belong to them, minus their own size.\n\nWe can verify this relationship for \\(EVELYN\\):\n\n   sum.size.evelyn <- sum(A[\"EVELYN\", ] * degree(g)[which(V(g)$type == TRUE)]) #sum of the sizes of the groups Evelyn belongs to\n   sum.size.evelyn - degree(g)[which(V(g)$name == \"EVELYN\")]\n\nEVELYN \n    50 \n\n\nWhich is indeed Evelyn’s vertex strength.\nDually, the same relation applies to groups:\n\n   sum.mem.6.27 <- sum(A[, \"6/27\"] * degree(g)[which(V(g)$type == FALSE)]) #sum of the memberships of people in the first group\n   sum.mem.6.27 - degree(g)[which(V(g)$name == \"6/27\")]\n\n6/27 \n  19 \n\n\nWhich is indeed the vertex strength of the event held on 6/27."
  },
  {
    "objectID": "two-mode.html#normalized-vertex-similarity-metrics",
    "href": "two-mode.html#normalized-vertex-similarity-metrics",
    "title": "Analyzing Two-Mode Networks",
    "section": "Normalized Vertex Similarity Metrics",
    "text": "Normalized Vertex Similarity Metrics\nNote that the one-mode projections are unnormalized similarity matrices just like in the case of regular networks. That means that if we have the degrees of nodes in each mode, we can transform this matrix into any of the normalized vertex similarity metrics we discussed before, including Jaccard, Cosine, Dice, LHN, and so on.\nThus repackaging our vertex similarity function for the two-mode case, we have:\n\n   vertex.sim <- function(x) {\n      A <- as.matrix(as_biadjacency_matrix(x))\n      M <- nrow(A) #number of persons\n      N <- ncol(A) #number of groups\n      p.d <- rowSums(A) #person degrees\n      g.d <- colSums(A) #group degrees\n      P <- A %*% t(A) #person projection\n      G <- t(A) %*% A #group projection\n      J.p <- diag(1, M, M)\n      J.g <- diag(1, N, N)\n      C.p <- diag(1, M, M)\n      C.g <- diag(1, N, N)\n      D.p <- diag(1, M, M)\n      D.g <- diag(1, N, N)\n      L.p <- diag(1, M, M)\n      L.g <- diag(1, N, N)\n      for (i in 1:M) {\n         for (j in 1:M) {\n            if (i < j) {\n               J.p[i,j] <- P[i,j]/(P[i,j] + p.d[i] + p.d[j])\n               J.p[j,i] <- P[i,j]/(P[i,j] + p.d[i] + p.d[j])\n               C.p[i,j] <- P[i,j]/(sqrt(p.d[i] * p.d[j]))\n               C.p[j,i] <- P[i,j]/(sqrt(p.d[i] * p.d[j]))\n               D.p[i,j] <- (2*P[i,j])/(2*P[i,j] + p.d[i] + p.d[j])\n               D.p[j,i] <- (2*P[i,j])/(2*P[i,j] + p.d[i] + p.d[j])\n               L.p[i,j] <- P[i,j]/(p.d[i] * p.d[j])\n               L.p[j,i] <- P[i,j]/(p.d[i] * p.d[j])\n               }\n            }\n         }\n      for (i in 1:N) {\n         for (j in 1:N) {\n            if (i < j) {\n               J.g[i,j] <- G[i,j]/(G[i,j] + g.d[i] + g.d[j])\n               J.g[j,i] <- G[i,j]/(G[i,j] + g.d[i] + g.d[j])\n               C.g[i,j] <- G[i,j]/(sqrt(g.d[i] * g.d[j]))\n               C.g[j,i] <- G[i,j]/(sqrt(g.d[i] * g.d[j]))\n               D.g[i,j] <- (2*G[i,j])/(2*G[i,j] + g.d[i] + g.d[j])\n               D.g[j,i] <- (2*G[i,j])/(2*G[i,j] + g.d[i] + g.d[j])\n               L.g[i,j] <- G[i,j]/(g.d[i] * g.d[j])\n               L.g[j,i] <- G[i,j]/(g.d[i] * g.d[j])\n               }\n            }\n         }\n      return(list(J.p = J.p, C.p = C.p, D.p = D.p, L.p = L.p,\n                  J.g = J.g, C.g = C.g, D.g = D.g, L.g = L.g))\n      }\n\nUsing this function to compute the Jaccard similarity between people yields:\n\n   J.p <- vertex.sim(g)$J.p\n   rownames(J.p) <- rownames(A)\n   colnames(J.p) <- rownames(A)\n   round(J.p, 2)\n\n          EVELYN LAURA THERESA BRENDA CHARLOTTE FRANCES ELEANOR PEARL RUTH\nEVELYN      1.00  0.29    0.30   0.29      0.20    0.25    0.20  0.21 0.20\nLAURA       0.29  1.00    0.29   0.30      0.21    0.27    0.27  0.17 0.21\nTHERESA     0.30  0.29    1.00   0.29      0.25    0.25    0.25  0.21 0.25\nBRENDA      0.29  0.30    0.29   1.00      0.27    0.27    0.27  0.17 0.21\nCHARLOTTE   0.20  0.21    0.25   0.27      1.00    0.20    0.20  0.00 0.20\nFRANCES     0.25  0.27    0.25   0.27      0.20    1.00    0.27  0.22 0.20\nELEANOR     0.20  0.27    0.25   0.27      0.20    0.27    1.00  0.22 0.27\nPEARL       0.21  0.17    0.21   0.17      0.00    0.22    0.22  1.00 0.22\nRUTH        0.20  0.21    0.25   0.21      0.20    0.20    0.27  0.22 1.00\nVERNE       0.14  0.15    0.20   0.15      0.11    0.11    0.20  0.22 0.27\nMYRNA       0.14  0.08    0.14   0.08      0.00    0.11    0.11  0.22 0.20\nKATHERINE   0.12  0.07    0.12   0.07      0.00    0.09    0.09  0.18 0.17\nSYLVIA      0.12  0.12    0.17   0.12      0.08    0.08    0.15  0.17 0.21\nNORA        0.11  0.12    0.16   0.12      0.08    0.08    0.14  0.15 0.14\nHELEN       0.07  0.14    0.13   0.14      0.10    0.10    0.18  0.11 0.18\nDOROTHY     0.17  0.10    0.17   0.10      0.00    0.14    0.14  0.29 0.25\nOLIVIA      0.09  0.00    0.09   0.00      0.00    0.00    0.00  0.17 0.14\nFLORA       0.09  0.00    0.09   0.00      0.00    0.00    0.00  0.17 0.14\n          VERNE MYRNA KATHERINE SYLVIA NORA HELEN DOROTHY OLIVIA FLORA\nEVELYN     0.14  0.14      0.12   0.12 0.11  0.07    0.17   0.09  0.09\nLAURA      0.15  0.08      0.07   0.12 0.12  0.14    0.10   0.00  0.00\nTHERESA    0.20  0.14      0.12   0.17 0.16  0.13    0.17   0.09  0.09\nBRENDA     0.15  0.08      0.07   0.12 0.12  0.14    0.10   0.00  0.00\nCHARLOTTE  0.11  0.00      0.00   0.08 0.08  0.10    0.00   0.00  0.00\nFRANCES    0.11  0.11      0.09   0.08 0.08  0.10    0.14   0.00  0.00\nELEANOR    0.20  0.11      0.09   0.15 0.14  0.18    0.14   0.00  0.00\nPEARL      0.22  0.22      0.18   0.17 0.15  0.11    0.29   0.17  0.17\nRUTH       0.27  0.20      0.17   0.21 0.14  0.18    0.25   0.14  0.14\nVERNE      1.00  0.27      0.23   0.27 0.20  0.25    0.25   0.14  0.14\nMYRNA      0.27  1.00      0.29   0.27 0.20  0.25    0.25   0.14  0.14\nKATHERINE  0.23  0.29      1.00   0.32 0.26  0.21    0.20   0.11  0.11\nSYLVIA     0.27  0.27      0.32   1.00 0.29  0.25    0.18   0.10  0.10\nNORA       0.20  0.20      0.26   0.29 1.00  0.24    0.09   0.17  0.17\nHELEN      0.25  0.25      0.21   0.25 0.24  1.00    0.12   0.12  0.12\nDOROTHY    0.25  0.25      0.20   0.18 0.09  0.12    1.00   0.20  0.20\nOLIVIA     0.14  0.14      0.11   0.10 0.17  0.12    0.20   1.00  0.33\nFLORA      0.14  0.14      0.11   0.10 0.17  0.12    0.20   0.33  1.00"
  },
  {
    "objectID": "two-mode.html#structural-equivalence",
    "href": "two-mode.html#structural-equivalence",
    "title": "Analyzing Two-Mode Networks",
    "section": "Structural Equivalence",
    "text": "Structural Equivalence\nAnd, of course, once we have a similarity we can cluster nodes based on approximate structural equivalence by transforming proximities to distances:\n\n   D <- as.dist(1- J.p)\n   hc.p <- hclust(D, method = \"ward.D2\")\n   plot(hc.p)\n\n\n\n\nAnd for events:\n\n   J.g <- vertex.sim(g)$J.g\n   rownames(J.g) <- colnames(A)\n   colnames(J.g) <- colnames(A)\n   D <- as.dist(1- J.g)\n   hc.g <- hclust(D, method = \"ward.D2\")\n   plot(hc.g)\n\n\n\n\nWe can then derive cluster memberships for people and groups from the hclust object:\n\n   library(dendextend)\n   clus.p <- sort(cutree(hc.p, 4)) #selecting four clusters for people\n   clus.p\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n        1         1         1         1         1         1         1         2 \n     RUTH     VERNE   DOROTHY     MYRNA KATHERINE    SYLVIA      NORA     HELEN \n        2         2         2         3         3         3         3         3 \n   OLIVIA     FLORA \n        4         4 \n\n   clus.g <- sort(cutree(hc.g, 3)) #selecting three clusters for groups\n   clus.g\n\n 6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10  2/23   4/7 11/21 \n    1     1     1     1     1     1     2     2     2     3     3     3     3 \n  8/3 \n    3 \n\n\nAnd finally we can block the original affiliation matrix, as recommended by Everett and Borgatti (2013, 210, table 5):\n\n   library(ggcorrplot)\n   p <- ggcorrplot(t(A[names(clus.p), names(clus.g)]), \n                   colors = c(\"white\", \"white\", \"red\")) \n   p <- p + theme(legend.position = \"none\", \n                  axis.text.y = element_text(size = 8),\n                  axis.text.x = element_text(size = 8, angle = 0),\n                  )\n   p <- p + scale_x_discrete(position = \"top\") \n   p <- p + geom_hline(yintercept = 7.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_hline(yintercept = 11.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_hline(yintercept = 16.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_vline(xintercept = 6.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_vline(xintercept = 9.5, linewidth = 2, color = \"blue\")\n   p\n\n\n\n\nWhich reveals a number of almost complete (one-blocks) and almost null (zero-blocks) in the social structure, with a reduced image matrix that looks like:\n\n   library(kableExtra)\n   IM <- matrix(0, 4, 3)\n   IM[1, ] <- c(0, 1, 0)\n   IM[2, ] <- c(0, 1, 1)\n   IM[3, ] <- c(0, 1, 0)\n   IM[4, ] <- c(1, 1, 0)\n   rownames(IM) <- c(\"P.Block1\", \"P.Block2\", \"P.Block3\", \"P.Block4\")\n   colnames(IM) <- c(\"E.Block1\", \"E.Block2\", \"E.Block3\")\n   kbl(IM, format = \"html\", , align = \"c\") %>% \n      column_spec(1, bold = TRUE) %>% \n      kable_styling(full_width = TRUE,\n                     bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\n \n  \n      \n    E.Block1 \n    E.Block2 \n    E.Block3 \n  \n \n\n  \n    P.Block1 \n    0 \n    1 \n    0 \n  \n  \n    P.Block2 \n    0 \n    1 \n    1 \n  \n  \n    P.Block3 \n    0 \n    1 \n    0 \n  \n  \n    P.Block4 \n    1 \n    1 \n    0"
  },
  {
    "objectID": "two-mode.html#generalized-vertex-similarity",
    "href": "two-mode.html#generalized-vertex-similarity",
    "title": "Analyzing Two-Mode Networks",
    "section": "Generalized Vertex Similarity",
    "text": "Generalized Vertex Similarity\nRecall that vertex similarity works using the principle of structural equivalence: Two people are similar if the choose the same objects (groups), and two objects (groups) are similar if they are chosen by the same people.\nWe can, like we did in the one mode case, be after a more general version of similarity, which says that: Two people are similar if they choose similar (not necessarily the same) objects, and two objects are similar if they are chosen by similar (not necessarily the same) people.\nThis leads to the same problem setup that inspired the SimRank approach (Jeh and Widom 2002).\nA (longish) function to compute the SimRank similarity between nodes in a two mode network goes as follows:\n\n   TM.SimRank <- function(A, C = 0.8, iter = 10) {\n        nr <- nrow(A)\n        nc <- ncol(A)\n        dr <- rowSums(A)\n        dc <- colSums(A)\n        Sr <- diag(1, nr, nr) #baseline similarity: every node maximally similar to themselves\n        Sc <- diag(1, nc, nc) #baseline similarity: every node maximally similar to themselves\n        rn <- rownames(A)\n        cn <- colnames(A)\n        rownames(Sr) <- rn\n        colnames(Sr) <- rn\n        rownames(Sc) <- cn\n        colnames(Sc) <- cn\n        m <- 1\n        while(m < iter) {\n             Sr.pre <- Sr\n             Sc.pre <- Sc\n             for(i in 1:nr) {\n                  for(j in 1:nr) {\n                       if (i != j) {\n                            a <- names(which(A[i, ] == 1)) #objects chosen by i\n                            b <- names(which(A[j, ] == 1)) #objects chosen by j\n                            Scij <- 0\n                            for (k in a) {\n                                 for (l in b) {\n                                      Scij <- Scij + Sc[k, l] #i's similarity to j\n                                 }\n                            }\n                            Sr[i, j] <- C/(dr[i] * dr[j]) * Scij\n                       }\n                  }\n             }\n             for(i in 1:nc) {\n                  for(j in 1:nc) {\n                       if (i != j) {\n                            a <- names(which(A[, i] == 1)) #people who chose object i\n                            b <- names(which(A[, j] == 1)) #people who chose object j\n                            Srij <- 0\n                            for (k in a) {\n                                 for (l in b) {\n                                      Srij <- Srij + Sr[k, l] #i's similarity to j\n                                 }\n                            }\n                            Sc[i, j] <- C/(dc[i] * dc[j]) * Srij\n                       }\n                  }\n             }\n             m <- m + 1\n        }\n        return(list(Sr = Sr, Sc = Sc))\n   }\n\nThis function takes the bi-adjacency matrix \\(\\mathbf{A}\\) as input and returns two generalized relational similarity matrices: One for the people (row objects) and the other one for the groups (column objects).\nHere’s how that would work in the Southern Women data. First we compute the SimRank scores:\n\n   sim.res <- TM.SimRank(A)\n\nThen we peek inside the people similarity matrix:\n\n   round(sim.res$Sr[1:10, 1:10], 3)\n\n          EVELYN LAURA THERESA BRENDA CHARLOTTE FRANCES ELEANOR PEARL  RUTH\nEVELYN     1.000 0.267   0.262  0.266     0.259   0.275   0.248 0.255 0.237\nLAURA      0.267 1.000   0.262  0.277     0.270   0.287   0.280 0.237 0.247\nTHERESA    0.262 0.262   1.000  0.262     0.273   0.270   0.264 0.254 0.256\nBRENDA     0.266 0.277   0.262  1.000     0.290   0.287   0.279 0.235 0.246\nCHARLOTTE  0.259 0.270   0.273  0.290     1.000   0.276   0.269 0.175 0.256\nFRANCES    0.275 0.287   0.270  0.287     0.276   1.000   0.305 0.280 0.256\nELEANOR    0.248 0.280   0.264  0.279     0.269   0.305   1.000 0.279 0.294\nPEARL      0.255 0.237   0.254  0.235     0.175   0.280   0.279 1.000 0.279\nRUTH       0.237 0.247   0.256  0.246     0.256   0.256   0.294 0.279 1.000\nVERNE      0.201 0.207   0.222  0.206     0.198   0.202   0.246 0.276 0.288\n          VERNE\nEVELYN    0.201\nLAURA     0.207\nTHERESA   0.222\nBRENDA    0.206\nCHARLOTTE 0.198\nFRANCES   0.202\nELEANOR   0.246\nPEARL     0.276\nRUTH      0.288\nVERNE     1.000\n\n\nAnd the group similarity matrix:\n\n   round(sim.res$Sc[1:10, 1:10], 3)\n\n      6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  6/10\n6/27 1.000 0.343 0.314 0.312 0.287 0.277 0.224 0.226 0.178 0.137\n3/2  0.343 1.000 0.312 0.311 0.285 0.276 0.224 0.228 0.200 0.141\n4/12 0.314 0.312 1.000 0.314 0.288 0.265 0.226 0.220 0.179 0.138\n9/26 0.312 0.311 0.314 1.000 0.287 0.256 0.230 0.214 0.186 0.137\n2/25 0.287 0.285 0.288 0.287 1.000 0.260 0.235 0.226 0.187 0.146\n5/19 0.277 0.276 0.265 0.256 0.260 1.000 0.224 0.226 0.200 0.171\n3/15 0.224 0.224 0.226 0.230 0.235 0.224 1.000 0.221 0.204 0.209\n9/16 0.226 0.228 0.220 0.214 0.226 0.226 0.221 1.000 0.221 0.214\n4/8  0.178 0.200 0.179 0.186 0.187 0.200 0.204 0.221 1.000 0.234\n6/10 0.137 0.141 0.138 0.137 0.146 0.171 0.209 0.214 0.234 1.000\n\n\nLike before we can use these results to define two sets of distances:\n\n   D.p <- as.dist(1 - sim.res$Sr)\n   D.g <- as.dist(1 - sim.res$Sc)\n\nSubject to hierarchical clustering:\n\n   hc.p <- hclust(D.p, method = \"ward.D2\")\n   hc.g <- hclust(D.g, method = \"ward.D2\")\n\nAnd plot:\n\n   plot(hc.p)\n\n\n\n   plot(hc.g)\n\n\n\n\nGet cluster memberships for people and groups from the hclust object:\n\n   clus.p <- sort(cutree(hc.p, 4)) #selecting four clusters for people\n   clus.p\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n        1         1         1         1         1         1         1         2 \n     RUTH     VERNE   DOROTHY     MYRNA KATHERINE    SYLVIA      NORA     HELEN \n        2         2         2         3         3         3         3         3 \n   OLIVIA     FLORA \n        4         4 \n\n   clus.g <- sort(cutree(hc.g, 3)) #selecting three clusters for groups\n   clus.g\n\n 6/27   3/2  4/12  9/26  2/25  5/19  3/15  9/16   4/8  2/23  6/10   4/7 11/21 \n    1     1     1     1     1     1     2     2     2     2     3     3     3 \n  8/3 \n    3 \n\n\nAnd block the bi-adjacency matrix:\n\n   p <- ggcorrplot(t(A[names(clus.p), names(clus.g)]), \n                   colors = c(\"white\", \"white\", \"red\")) \n   p <- p + theme(legend.position = \"none\", \n                  axis.text.y = element_text(size = 8),\n                  axis.text.x = element_text(size = 8, angle = 0),\n                  )\n   p <- p + scale_x_discrete(position = \"top\") \n   p <- p + geom_hline(yintercept = 7.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_hline(yintercept = 11.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_hline(yintercept = 16.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_vline(xintercept = 6.5, linewidth = 2, color = \"blue\")\n   p <- p + geom_vline(xintercept = 10.5, linewidth = 2, color = \"blue\")\n   p\n\n\n\n\nNote that this block solution is similar (pun intended) but not exactly the same as the one based on structural equivalence we obtained earlier, although it would lead to the same reduced image matrix for the blocks."
  },
  {
    "objectID": "blondel.html",
    "href": "blondel.html",
    "title": "Role Similarity Across Graphs",
    "section": "",
    "text": "Sometimes we may want to figure out how similar a given node’s position in one social network is to that of another node in a different network. This calls for a method that could allow us to compare how similar a node in one graph is to other nodes in another graph.\nA particularly interesting version of this problem arises when we have information on the same set of nodes across different set of relations. In that case, we may be interested in answering the question as to whether nodes occupy similar or dissimilar positions across the networks defined by the different relations.\nBlondel et al. (2004) describe an approach that can help us make headway on this problem. They use a similar iterative procedure that we saw can be used to compute status scores from directed graphs (like PageRank and HITS) but this time to compute similarity scores between pairs of nodes across graphs.\nThe idea, just like with the status scores, is that the two set of nodes in each graph start with the same set of similarity scores, and then we update them as we traverse the connectivity structure of the two graphs.\nSo let’s say the adjacency matrix of the first graph is \\(\\mathbf{A}\\) and that of the second graph is \\(\\mathbf{B}\\). The first graph has \\(n_A\\) number of nodes and the corresponding quantity in the second graph is \\(n_B\\) our target similarity matrix \\(\\mathbf{Z}\\), comparing the node sets in the two graphs, will therefore be of dimensions \\(n_B \\times n_A\\).\nWe initialize \\(z_{ij}(0) = 1\\) for all \\(i\\) and \\(j\\); that is, \\(\\mathbf{Z}(0)\\) is a matrix full of ones. At each time step subsequent to that, we fill up the \\(\\mathbf{Z}\\) matrix with new values according to:\n\\[\n   \\mathbf{Z}(t + 1) = \\mathbf{B}\\mathbf{Z(t)}\\mathbf{A}^T + \\mathbf{B}^T\\mathbf{Z(t)}\\mathbf{A}\n\\]\nTo ensure convergence, we then normalize the \\(\\mathbf{Z}\\) matrix after every update using our trusty Euclidean norm:\n\\[\n\\mathbf{Z}(t > 0) = \\frac{\\mathbf{Z}}{||\\mathbf{Z}||_2}\n\\]"
  },
  {
    "objectID": "blondel.html#computing-node-similarities-across-different-graphs",
    "href": "blondel.html#computing-node-similarities-across-different-graphs",
    "title": "Role Similarity Across Graphs",
    "section": "Computing Node Similarities Across Different Graphs",
    "text": "Computing Node Similarities Across Different Graphs\nLet us see how this would work with real data. We will compare two subgraphs of the larger law_advice network (Lazega 2001) from the networkdata package. This is a directed advice-seeking network so a node goes from advisee to adviser.\nWe create two subgraphs. One composed of older male partners (aged fifty or older) and the other composed of the women in the firm (both parterns and associates). They look lik this:\n\n\n\n\n\n\nOlder Men Partners\n\n\n\n\n\n\n\n\n\nWomen Lawyers\n\n\n\n\n\nA function to compute the Blondel similarity as described earlier can be written as:\n\n   blondel.sim <- function(A, B) {\n      K <- matrix(1, nrow(B), nrow(A))\n      if (is.null(rownames(A)) == TRUE) {\n         rownames(A) <- 1:nrow(A)\n         colnames(A) <- 1:nrow(A)\n         }\n      if (is.null(rownames(B)) == TRUE) {\n         rownames(B) <- 1:nrow(B)\n         colnames(B) <- 1:nrow(B)\n         }\n      k <- 1\n      diff <- 1\n      old.diff <- 2\n      while (diff != old.diff | k %% 2 == 0) {\n         old.diff <- diff\n         K.old <- K\n         K <- (B %*% K.old %*% t(A)) + (t(B) %*% K.old %*% A)\n         K <- K/norm(K, type = \"F\")\n         diff <- abs(sum(abs(K)) - sum(abs(K.old)))\n         k <- k + 1\n      }\n   for (j in 1:ncol(K)) {\n      K[, j] <- K[, j]/max(K[, j])\n      }\n   rownames(K) <- rownames(B)\n   colnames(K) <- rownames(A)\n   return(list(K = K, k = k, diff = diff))\n   }\n\nWhich is modeled after our status game function but instead of computing a vector of scores we are populating a whole matrix!\nThe basic task is to figure out which nodes from the first matrix are most similar to which nodes from the second. That is, given these two networks can be identify actors who play similar roles in each?\nAnd here are the results presented in tabular form:\n\n   library(kableExtra)\n   A <- as.matrix(as_adjacency_matrix(g1))\n   B <- as.matrix(as_adjacency_matrix(g2))\n   K <- round(blondel.sim(A, B)[[1]], 2)\n   kbl(K, align = \"c\", format = \"html\", row.names = TRUE) %>% \n      column_spec(1, bold = TRUE) %>% \n      kable_styling(full_width = TRUE,\n                     bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\n \n  \n      \n    1 \n    2 \n    3 \n    4 \n    5 \n    6 \n    7 \n    8 \n    9 \n    10 \n    11 \n    12 \n    13 \n  \n \n\n  \n    1 \n    0.25 \n    0.22 \n    0.41 \n    0.48 \n    0.45 \n    0.11 \n    0.47 \n    0.24 \n    0.20 \n    0.30 \n    0.47 \n    0.25 \n    0.36 \n  \n  \n    2 \n    0.39 \n    0.38 \n    0.38 \n    0.27 \n    0.16 \n    0.37 \n    0.15 \n    0.38 \n    0.34 \n    0.38 \n    0.36 \n    0.34 \n    0.34 \n  \n  \n    3 \n    0.64 \n    0.63 \n    0.54 \n    0.33 \n    0.09 \n    0.67 \n    0.10 \n    0.63 \n    0.56 \n    0.58 \n    0.51 \n    0.55 \n    0.51 \n  \n  \n    4 \n    0.65 \n    0.66 \n    0.52 \n    0.26 \n    0.00 \n    0.71 \n    0.00 \n    0.63 \n    0.60 \n    0.59 \n    0.46 \n    0.58 \n    0.50 \n  \n  \n    5 \n    1.00 \n    1.00 \n    0.99 \n    0.73 \n    0.28 \n    1.00 \n    0.34 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n  \n  \n    6 \n    0.66 \n    0.67 \n    0.56 \n    0.33 \n    0.04 \n    0.71 \n    0.06 \n    0.65 \n    0.64 \n    0.62 \n    0.53 \n    0.62 \n    0.57 \n  \n  \n    7 \n    0.05 \n    0.04 \n    0.12 \n    0.14 \n    0.16 \n    0.00 \n    0.15 \n    0.04 \n    0.04 \n    0.08 \n    0.12 \n    0.05 \n    0.09 \n  \n  \n    8 \n    0.32 \n    0.31 \n    0.37 \n    0.32 \n    0.24 \n    0.26 \n    0.24 \n    0.30 \n    0.29 \n    0.34 \n    0.37 \n    0.31 \n    0.34 \n  \n  \n    9 \n    0.49 \n    0.44 \n    0.88 \n    1.00 \n    1.00 \n    0.20 \n    1.00 \n    0.47 \n    0.40 \n    0.63 \n    0.96 \n    0.50 \n    0.71 \n  \n  \n    10 \n    0.31 \n    0.28 \n    0.73 \n    0.89 \n    0.89 \n    0.04 \n    0.91 \n    0.29 \n    0.30 \n    0.47 \n    0.82 \n    0.39 \n    0.60 \n  \n  \n    11 \n    0.86 \n    0.86 \n    1.00 \n    0.82 \n    0.58 \n    0.76 \n    0.57 \n    0.86 \n    0.79 \n    0.92 \n    0.97 \n    0.82 \n    0.86 \n  \n  \n    12 \n    0.15 \n    0.15 \n    0.18 \n    0.16 \n    0.04 \n    0.15 \n    0.08 \n    0.15 \n    0.20 \n    0.17 \n    0.21 \n    0.20 \n    0.23 \n  \n  \n    13 \n    0.34 \n    0.34 \n    0.42 \n    0.37 \n    0.26 \n    0.29 \n    0.28 \n    0.33 \n    0.34 \n    0.38 \n    0.43 \n    0.36 \n    0.40 \n  \n  \n    14 \n    0.18 \n    0.17 \n    0.32 \n    0.35 \n    0.33 \n    0.09 \n    0.34 \n    0.17 \n    0.16 \n    0.23 \n    0.35 \n    0.19 \n    0.27 \n  \n  \n    15 \n    0.40 \n    0.41 \n    0.41 \n    0.28 \n    0.16 \n    0.39 \n    0.15 \n    0.40 \n    0.38 \n    0.41 \n    0.37 \n    0.38 \n    0.36 \n  \n  \n    16 \n    0.53 \n    0.51 \n    0.92 \n    0.99 \n    0.87 \n    0.29 \n    0.91 \n    0.52 \n    0.54 \n    0.69 \n    1.00 \n    0.61 \n    0.80 \n  \n  \n    17 \n    0.46 \n    0.44 \n    0.70 \n    0.74 \n    0.49 \n    0.33 \n    0.58 \n    0.46 \n    0.53 \n    0.55 \n    0.81 \n    0.57 \n    0.71 \n  \n  \n    18 \n    0.34 \n    0.31 \n    0.67 \n    0.81 \n    0.63 \n    0.15 \n    0.72 \n    0.34 \n    0.41 \n    0.46 \n    0.82 \n    0.47 \n    0.67 \n  \n\n\n\n\n\nIn the table each column is normalized by its maximum, so a 1.0 in that column tells us that that node (from the first network) is maximally similar to the corresponding row node (from the second network).\nFor instance, node 5 in the women’s lawyers graph (a highly central node in terms of being an adviser) is most similar to node 1 in the older men partner’s graph (also a highly central node in terms of being an adviser).\nNode 9 in the women lawyer’s graph, who’s mostly an advise-seeker, is most similar to node 5 in the older men partner graph who’s also an advise-seeker. So it looks like it works!"
  },
  {
    "objectID": "blondel.html#equivalence-to-hits",
    "href": "blondel.html#equivalence-to-hits",
    "title": "Role Similarity Across Graphs",
    "section": "Equivalence to HITS",
    "text": "Equivalence to HITS\nOne neat thing that Blondel et al. (2004) show is that we can also take a network and compare it to ideal-typical small graphs and get scores for how much each node in the observed network resembles each of the nodes in the hypothetical ideal-typical structure.\nMore specifically, they show that if we can run their algorithm to compare any network to the following two-node graph:\n\n   g <- make_empty_graph(2)\n   g <- add_edges(g, c(1,2))\n   V(g)$name <- c(\"Hub\", \"Authority\")\n   plot(g, \n        edge.arrow.size=1, \n        vertex.color=\"tan2\", \n        vertex.size=12, vertex.frame.color=\"lightgray\", \n        vertex.label.color=\"black\", vertex.label.cex=1.5, \n        vertex.label.dist=-3)\n\n\n\n\nIn which case, the result “similarity” scores, will be equivalent to the Hub and Authority scores!\nWe can check that this is the case for the women’s lawyers advice graph:\n\n   A <- as.matrix(as_adjacency_matrix(g))\n   B <- as.matrix(as_adjacency_matrix(g2))\n   K <- round(blondel.sim(A, B)[[1]], 4)\n   tab <- cbind(K, Hub.Score = round(hits_scores(g2)$hub, 4), \n         Auth.Score = round(hits_scores(g2)$authority, 4))\n   kbl(tab, align = \"c\", format = \"html\", row.names = TRUE) %>% \n      column_spec(1, bold = TRUE) %>% \n      kable_styling(full_width = TRUE,\n                     bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\n \n  \n      \n    Hub \n    Authority \n    Hub.Score \n    Auth.Score \n  \n \n\n  \n    1 \n    0.3810 \n    0.0000 \n    0.3810 \n    0.0000 \n  \n  \n    2 \n    0.1434 \n    0.3415 \n    0.1434 \n    0.3415 \n  \n  \n    3 \n    0.0813 \n    0.6021 \n    0.0813 \n    0.6021 \n  \n  \n    4 \n    0.0000 \n    0.7826 \n    0.0000 \n    0.7826 \n  \n  \n    5 \n    0.1654 \n    1.0000 \n    0.1654 \n    1.0000 \n  \n  \n    6 \n    0.0000 \n    0.7826 \n    0.0000 \n    0.7826 \n  \n  \n    7 \n    0.1434 \n    0.0000 \n    0.1434 \n    0.0000 \n  \n  \n    8 \n    0.2235 \n    0.2858 \n    0.2235 \n    0.2858 \n  \n  \n    9 \n    1.0000 \n    0.0386 \n    1.0000 \n    0.0386 \n  \n  \n    10 \n    0.8973 \n    0.0000 \n    0.8973 \n    0.0000 \n  \n  \n    11 \n    0.6110 \n    0.6559 \n    0.6110 \n    0.6559 \n  \n  \n    12 \n    0.0000 \n    0.2095 \n    0.0000 \n    0.2095 \n  \n  \n    13 \n    0.2239 \n    0.3364 \n    0.2239 \n    0.3364 \n  \n  \n    14 \n    0.3197 \n    0.0523 \n    0.3197 \n    0.0523 \n  \n  \n    15 \n    0.1434 \n    0.4036 \n    0.1434 \n    0.4036 \n  \n  \n    16 \n    0.8432 \n    0.2174 \n    0.8432 \n    0.2174 \n  \n  \n    17 \n    0.4089 \n    0.3188 \n    0.4089 \n    0.3188 \n  \n  \n    18 \n    0.5222 \n    0.0955 \n    0.5222 \n    0.0955 \n  \n\n\n\n\n\nThe first two columns are the scores using the function to compute the Blondel et al. similarity to each of the two nodes in the Hub/Authority micro-graph and the third and fourth columns are the scores we get from the igraph function hits scores, which as we can see, are identical."
  },
  {
    "objectID": "blondel.html#computing-a-brokerage-score",
    "href": "blondel.html#computing-a-brokerage-score",
    "title": "Role Similarity Across Graphs",
    "section": "Computing a Brokerage Score",
    "text": "Computing a Brokerage Score\nOf course in a directed graph, there are more than two ideal typical “roles.” In addition to “sender” (Hub) or “receiver” (Authority) we may also have “intermediaries” or “pass along” nodes. We can thus get an “intermediary” score for each node by comparing any network to the following three-node graph:\n\n   g <- make_empty_graph(3)\n   g <- add_edges(g, c(1,2, 2,3))\n   V(g)$name <- c(\"Hub\", \"Broker\", \"Authority\")\n   plot(g, \n        edge.arrow.size=1, \n        vertex.color=\"tan2\", \n        vertex.size=12, vertex.frame.color=\"lightgray\", \n        vertex.label.color=\"black\", vertex.label.cex=1.5, \n        vertex.label.dist=2)\n\n\n\n\nHere are the results for the women lawyers graph:\n\n   A <- as.matrix(as_adjacency_matrix(g))\n   B <- as.matrix(as_adjacency_matrix(g2))\n   K <- round(blondel.sim(A, B)[[1]], 4)\n   kbl(K, align = \"c\", format = \"html\", row.names = TRUE) %>% \n      column_spec(1, bold = TRUE) %>% \n      kable_styling(full_width = TRUE,\n                     bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\n \n  \n      \n    Hub \n    Broker \n    Authority \n  \n \n\n  \n    1 \n    0.5190 \n    0.3321 \n    0.1135 \n  \n  \n    2 \n    0.1318 \n    0.3472 \n    0.3600 \n  \n  \n    3 \n    0.0998 \n    0.4586 \n    0.6923 \n  \n  \n    4 \n    0.0000 \n    0.4888 \n    0.7547 \n  \n  \n    5 \n    0.5241 \n    0.8968 \n    1.0000 \n  \n  \n    6 \n    0.0954 \n    0.5091 \n    0.7547 \n  \n  \n    7 \n    0.1318 \n    0.1234 \n    0.0000 \n  \n  \n    8 \n    0.2501 \n    0.3584 \n    0.2755 \n  \n  \n    9 \n    1.0000 \n    0.8236 \n    0.2000 \n  \n  \n    10 \n    0.9913 \n    0.6859 \n    0.0318 \n  \n  \n    11 \n    0.5445 \n    1.0000 \n    0.7371 \n  \n  \n    12 \n    0.1971 \n    0.1427 \n    0.1530 \n  \n  \n    13 \n    0.3175 \n    0.4117 \n    0.2970 \n  \n  \n    14 \n    0.3710 \n    0.2875 \n    0.0918 \n  \n  \n    15 \n    0.1318 \n    0.4205 \n    0.3780 \n  \n  \n    16 \n    0.9998 \n    0.8747 \n    0.2453 \n  \n  \n    17 \n    0.7783 \n    0.5841 \n    0.3102 \n  \n  \n    18 \n    0.9644 \n    0.5162 \n    0.1302 \n  \n\n\n\n\n\nColumns one and three gives us versions of the Hub and Authority scores (respectively), but column two now gives us a “score” for how much the row node resembles and intermediary (or broker) in the network. We can see that the “purest” broker in the women’s advice network is node 11."
  },
  {
    "objectID": "cube.html",
    "href": "cube.html",
    "title": "The Cube",
    "section": "",
    "text": "Brandes, Borgatti, and Freeman (2016) discuss the centrality “cube,” an interesting and intuitive way to understand the way betweenness centrality works, as well as the dual connection between closeness and betweenness.\nLet us illustrate using a simple example. We begin by creating an Erdos-Renyi graph with eight nodes and connection probability \\(p = 0.5\\):\nThe resulting graph looks like:\nThe basic innovation behind the centrality cube is to store the intermediation information among every node triplet in the graph \\(s\\), \\(r\\), \\(b\\) (standing for “sender,” receiver,” and “broker”) in a three dimensional array rather than the usual two dimensional matrix.\nThe three dimensional array can be thought of as a “cube” by stacking multiple reachability matrices between every pair \\(s\\) and \\(r\\) along a three dimensional dimension \\(b\\). So each “b-slice” of the cube will contain the number of times node \\(b\\) stands in a shortest path between \\(s\\) and \\(r\\) divided by the total number of paths between \\(s\\) and \\(r\\) which as you recall computes the pairwise betweenness of \\(b\\) with respect to \\(s\\) and \\(r\\).\nLet’s see how that works."
  },
  {
    "objectID": "cube.html#building-the-cube",
    "href": "cube.html#building-the-cube",
    "title": "The Cube",
    "section": "Building the Cube",
    "text": "Building the Cube\nWe begin by writing a simple user-defined function to count the total number of shortest paths between each pair of nodes:\n\n   nsp <- function(x) {\n      n <- vcount(g)\n      S <- matrix(0, n, n)\n      for (i in 1:n) {\n            for (j in 1:n) {\n                  if (j %in% neighbors(x, i) == FALSE) {\n                     S[i, j] <- length(all_shortest_paths(x, i, j)$vpaths)\n               }\n            }\n         }\n      return(S)\n   }\n\nThe function is called nsp and takes a graph as input and returns and matrix called \\(\\mathbf{S}\\) with entries \\(s_{ij}\\) equal to the total number of shortest paths between \\(i\\) and \\(j\\). This is done by computing the length of the list returned by the all_shortest_paths function in igraph for each pair of non-adjacent nodes. This is done in two steps.\n\nFirst, we check whether \\(j\\) is a neighbor of \\(i\\) using the neighbors function in igraph. The neighbors function takes a graph and a node id as input and returns a vector of that node’s neighbors in the graph. We want the function to update the \\(S\\) matrix only when \\(i\\) and \\(j\\) are not adjacent (indirectly connected).\nSecond, we use the all_shortest_paths function to actually compute the number of shortest paths between \\(i\\) and \\(j\\). This function takes three inputs: (1) A graph object, (2) a sender node id, and (3) a receiver node id (which can be a vector of receiver nodes), and returns a list of the paths between the sender and receiver nodes in the form of vectors of node ids defining each path as elements of a list called “vpaths.”\n\nNow we are ready to write a user defined function to build the cube. Here’s a not-so-efficient (programming wise) but working example:\n\n   cube <- function(g) {\n      n <- vcount(g)\n      c <- array(rep(n^3, 0), c(n, n, n))\n      S <- nsp(g)\n      for (b in 1:n) {\n         for (s in 1:n) {\n            for (r in 1:n) {\n               if (s != r & r %in% neighbors(g, s) == FALSE) {\n                  p.sr <- all_shortest_paths(g, s, r)$vpaths\n                  b.sr <- lapply(p.sr, function(x) {x[-c(1, length(x))]}) \n                  c[s, r, b] <- sum(as.numeric(sapply(b.sr, function(x) {b %in% x})))\n                  c[s, r, b] <- c[s, r, b]/S[s, r]\n               }\n            }\n         }\n      }\n   c[is.na(c)] <- 0\n   return(c)\n   }\n\nIn line 1, we name the function cube. Line 3 initializes the array in R. It takes a string of zeros of length \\(n^3\\) where \\(n\\) is the number of nodes and crams them into an \\(n \\times n \\times n\\) array. In this case, since \\(n = 8\\), this means eight empty matrices of dimensions \\(8 \\times 8\\) stacked together to form our cube full of zeros. The \\(ijk^{th}\\) cell of the array corresponds to sender \\(i\\), receiver \\(j\\) and broker node \\(k\\). Line 4 computes the matrix \\(S\\) containing the number of shortest paths between every sender and receiver node in the graph.\nLines 5-15 populate the cube with the required information using an (inefficient) triple for loop. As noted some, useful igraph functions come into play here:\n\nIn line 8 the if conditional inside the triple loop uses the neighbors function in igraph and checks that node \\(r\\) is not a neighbor of \\(s\\) (if they are directly connected then node \\(b\\) cannot be a broker).\nAfter we check that \\(s\\) and \\(r\\) are not neighbors, we use the all_shortest_paths function in igraph to get all the shortest paths between \\(s\\) and \\(r\\) in line 9.\nLine 10 uses some lapply magic to drop the source and receiver nodes from the list of node ids vectors returned by the all_shortest_paths function.\nLine 11 uses additional sapply magic and the base R function %in% to check how many times each broker node \\(b\\) shows up in that list of shortest paths as an inner node between \\(s\\) and \\(r\\); we put that number in the \\(ijk^{th}\\) cell of the array, and loop through all triplets until we are done.\nLine 12 takes the number computed in line 11 and divides by the total number of shortest paths between \\(s\\) and \\(r\\) which is the betweenness ratio we are seeking."
  },
  {
    "objectID": "cube.html#exploring-the-cube",
    "href": "cube.html#exploring-the-cube",
    "title": "The Cube",
    "section": "Exploring the Cube",
    "text": "Exploring the Cube\nOnce we have our array, we can create all kinds of interesting sub-matrices containing the intermediation information in the graph by summing rows and columns of the array along different dimensions.\nFirst, let us see what’s in the cube. We can query specific two-dimensional sub-matrices using an extension of the usual format for querying matrices in R for three-dimensional arrays. For instance this:\n\n   srb <- cube(g)\n   round(srb[, , 2], 2)\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n[1,]  0.0    0 0.00    0 0.00  0.5    0 0.00\n[2,]  0.0    0 0.00    0 0.00  0.0    0 0.00\n[3,]  0.0    0 0.00    0 0.00  0.0    0 0.33\n[4,]  0.0    0 0.00    0 0.00  0.0    0 0.00\n[5,]  0.0    0 0.00    0 0.00  0.0    0 0.33\n[6,]  0.5    0 0.00    0 0.00  0.0    0 1.00\n[7,]  0.0    0 0.00    0 0.00  0.0    0 0.00\n[8,]  0.0    0 0.33    0 0.33  1.0    0 0.00\n\n\nCreates a three-dimensional matrix of pairwise betweenness probabilities and assigns it to the srb object in line 1, and looks at the \\(s_{\\bullet} \\times r_{\\bullet} \\times b_2\\) entry in line 2.\nEach entry in the matrix is the probability that node 2 stands on a shortest path between the row sender and the column receiver node. For instance, the 0.33 in the entry corresponding to row 5 and column 8 tells us that node 2 stands in one third of the shortest paths between nodes 5 and 8 (there are 3 distinct shortest paths between 5 and 8).\nBecause each sub-matrix in the cube is a matrix, we can do the usual matrix operations on them. For instance, let’s take the row sums of the \\(s\\) to \\(r\\) matrix corresponding to node 3 as the broker. This can be done like this:\n\n   d.3 <- rowSums(srb[ , , 3])\n   names(d.3) <- 1:vcount(g)\n   d.3\n\n  1   2   3   4   5   6   7   8 \n1.5 2.0 0.0 3.0 6.0 3.5 3.0 1.0 \n\n\nAs Brandes, Borgatti, and Freeman (2016), note this vector gives us the dependence of each node in the graph on node 3. Obviously node 3 doesn’t depend on itself so there is a zero on the third spot in the vector. As is clear from the plot, node 5 is the most dependent on 3 for intermediation with the rest of the nodes in the graph.\nWe can also pick a particular sender and receiver node and sum all their dyadic entries in the cube across the third (broker) dimension:\n\n   sum(srb[1, 6, ])\n\n[1] 2\n\n\nThis number is equivalent to the geodesic distance between the nodes minus one:\n\n   distances(g)[1, 6] - 1\n\n[1] 2"
  },
  {
    "objectID": "cube.html#betweenness-and-closeness-in-the-cube",
    "href": "cube.html#betweenness-and-closeness-in-the-cube",
    "title": "The Cube",
    "section": "Betweenness and Closeness in the Cube",
    "text": "Betweenness and Closeness in the Cube\nThe betweenness centrality of each node is encoded in the cube, because we already computed the main ratio that the measure depends on. For instance, let’s look at the matrix composed by taking the slice of cube that corresponds to node 3 as a broker:\n\n   srb[, , 3]\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n[1,]  0.0  0.0    0  0.0    1  0.5  0.0    0\n[2,]  0.0  0.0    0  0.5    1  0.0  0.5    0\n[3,]  0.0  0.0    0  0.0    0  0.0  0.0    0\n[4,]  0.0  0.5    0  0.0    1  1.0  0.5    0\n[5,]  1.0  1.0    0  1.0    0  1.0  1.0    1\n[6,]  0.5  0.0    0  1.0    1  0.0  1.0    0\n[7,]  0.0  0.5    0  0.5    1  1.0  0.0    0\n[8,]  0.0  0.0    0  0.0    1  0.0  0.0    0\n\n\nThe sum of the all the cells in this matrix (divided by two) correspond to node 3’s betweenness centrality:\n\n   sum(srb[, , 3])/2\n\n[1] 10\n\n   betweenness(g)[3]\n\n[1] 10\n\n\nSo to get each node’s betweenness we just can just sum up the entries in each of the cube’s sub-matrices:\n\n   b.cube <- round(colSums(srb, dims = 2)/2, 2)\n   b.igraph <- round(betweenness(g), 2)\n   names(b.cube) <- 1:vcount(g)\n   names(b.igraph) <- 1:vcount(g)\n   b.cube\n\n    1     2     3     4     5     6     7     8 \n 0.00  2.17 10.00  0.67  0.00  0.00  3.17  4.00 \n\n   b.igraph\n\n    1     2     3     4     5     6     7     8 \n 0.00  2.17 10.00  0.67  0.00  0.00  3.17  4.00 \n\n\nNote the neat trick of using the argument dims = 2 in the usual colSums command. This tells colSums that we are dealing with a three dimensional matrix, and that what we want is the sum of the columns across the cube’s third dimension (the brokers). Note also that we divide the cube betweenness by two because we are summing identical entries across the upper and lower triangle of the symmetric dyadic brokerage matrices inside the cube (not surprisingly, node 3 is the top betweenness centrality node).\nAs Brandes, Borgatti, and Freeman (2016) point out, using the cube info, we can build a matrix of dependencies between each pair of nodes. In this matrix, the rows correspond to a sender (or receiver) node, the columns to a broker node and the \\(sb^{th}\\) entry contains the sum of the proportion of paths containing the broker nodes that starts with the sender node and end with some other node in the graph.\nHere’s a function that uses the cube info to build the dependency matrix that Brandes, Borgatti, and Freeman (2016) talk about using the cube as input:\n\n   dep.ij <- function(c) {\n      n <- nrow(c)\n      dep.ij <- rowSums(c[, , 1])\n      for (i in 2:n) {\n         dep.ij <- cbind(dep.ij,  rowSums(c[, , i]))\n         \n         }\n      rownames(dep.ij) <- 1:n\n      colnames(dep.ij) <- 1:n\n      return(dep.ij)\n   }\n\nThis function just takes the various vectors formed by the row sums of the sender-receiver matrix across each value of the third dimension (which is just each node in the graph when playing the broker role). It then returns a regular old \\(n \\times n\\) containing the info.\nHere’s the result when applied to our little example:\n\n   library(kableExtra)\n   kbl(round(dep.ij(srb), 2), \n       format = \"html\", align = \"c\", row.names = TRUE,\n       caption = \"Dependence Matrix.\") %>% \n      column_spec(1, bold = TRUE) %>%\n   kable_styling(bootstrap_options = c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\nDependence Matrix.\n \n  \n      \n    1 \n    2 \n    3 \n    4 \n    5 \n    6 \n    7 \n    8 \n  \n \n\n  \n    1 \n    0 \n    0.50 \n    1.5 \n    0.00 \n    0 \n    0 \n    2.50 \n    2.5 \n  \n  \n    2 \n    0 \n    0.00 \n    2.0 \n    0.00 \n    0 \n    0 \n    0.00 \n    2.0 \n  \n  \n    3 \n    0 \n    0.33 \n    0.0 \n    0.33 \n    0 \n    0 \n    1.33 \n    0.0 \n  \n  \n    4 \n    0 \n    0.00 \n    3.0 \n    0.00 \n    0 \n    0 \n    0.00 \n    2.0 \n  \n  \n    5 \n    0 \n    0.33 \n    6.0 \n    0.33 \n    0 \n    0 \n    1.33 \n    0.0 \n  \n  \n    6 \n    0 \n    1.50 \n    3.5 \n    0.00 \n    0 \n    0 \n    0.50 \n    0.5 \n  \n  \n    7 \n    0 \n    0.00 \n    3.0 \n    0.00 \n    0 \n    0 \n    0.00 \n    1.0 \n  \n  \n    8 \n    0 \n    1.67 \n    1.0 \n    0.67 \n    0 \n    0 \n    0.67 \n    0.0 \n  \n\n\n\n\n\nNote that this is valued matrix that is also asymmetric. Take for instance, node 3. Every node in the graph depends on node 3 for access to other nodes, but node 3 does not depend on nodes 1, 5, 6, or 8.\nInterestingly, as Brandes, Borgatti, and Freeman (2016) also show, the betweenness centrality also can be calculated from the dependency matrix! All we need to do is compute the column sums, equivalent to in-degree in the directed dependence network:\n\n   round(colSums(dep.ij(srb))/2, 2)\n\n    1     2     3     4     5     6     7     8 \n 0.00  2.17 10.00  0.67  0.00  0.00  3.17  4.00 \n\n\nEven more interestingly, closeness centrality is also in the dependence matrix! It is given by the outdegree of each actor in the directed dependence network, corresponding to the row sums of the matrix (shifted by a constant given by \\(n-1\\)).\n\n   c.c <- rowSums(distances(g))\n   c.d <- rowSums(dep.ij(srb)) + (vcount(g) - 1)\n   names(c.c) <- 1:vcount(g)\n   names(c.d) <- 1:vcount(g)\n   round(1/c.c, 3)\n\n    1     2     3     4     5     6     7     8 \n0.071 0.091 0.111 0.083 0.067 0.077 0.091 0.091 \n\n   round(1/c.d, 3)\n\n    1     2     3     4     5     6     7     8 \n0.071 0.091 0.111 0.083 0.067 0.077 0.091 0.091 \n\n\nHere we see that node 3 is also the top in closeness, followed closely (pun intended) by nodes 2, 7, and 8. This makes sense because an actor with high closeness is one that has low dependence on key nodes to be able to reach others.\nOf course, closeness is also in the cube because of the mathematical relationship we saw earlier between the sum of entries between senders and receivers across brokers in the cube and the geodesic distance.\nFor instance, let’s get the matrix corresponding to node 3’s role as sender across all brokers and receivers:\n\n   round(srb[3, , ], 2)\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n[1,]    0 0.00    0 0.00    0    0 1.00    0\n[2,]    0 0.00    0 0.00    0    0 0.00    0\n[3,]    0 0.00    0 0.00    0    0 0.00    0\n[4,]    0 0.00    0 0.00    0    0 0.00    0\n[5,]    0 0.00    0 0.00    0    0 0.00    0\n[6,]    0 0.00    0 0.00    0    0 0.00    0\n[7,]    0 0.00    0 0.00    0    0 0.00    0\n[8,]    0 0.33    0 0.33    0    0 0.33    0\n\n\nThe entries of this matrix give us the probability that node 3 is the sender, whenever the row node is the is the broker (an inner node in the path) and the column node is the receiver.\nFor instance, the value 0.3 in row 8 and column 1 tells us that node 3 is the sender node in one third of the paths that end in node 1 and feature node 8 as a broker.\nInterestingly, the sums of the entries in this matrix are equivalent to the sum of the geodesic distances between node 3 and every other node in the graph shifted by a constant (\\(n- 1\\)):\n\n   sum(srb[3, , ]) + (vcount(g) - 1)\n\n[1] 9\n\n   sum(distances(g)[3, ])\n\n[1] 9\n\n\nSo the closeness centrality can be computed from the cube as follows:\n\n   c <- 0\n   n <- vcount(g)\n   for (i in 1:n) {\n      c[i] <- 1/(sum(srb[i, , ]) + (n - 1))\n   }\n   round(c, 3)\n\n[1] 0.071 0.091 0.111 0.083 0.067 0.077 0.091 0.091\n\n\nWhich is the same as:\n\n   round(closeness(g), 3)\n\n[1] 0.071 0.091 0.111 0.083 0.067 0.077 0.091 0.091"
  },
  {
    "objectID": "ahn.html",
    "href": "ahn.html",
    "title": "Assigning Nodes to Multiple Communities",
    "section": "",
    "text": "In the previous handout, we examined various approaches to community and dense subgraph detection. What they all have in common is they assign nodes into non-overlapping groups. That is, nodes are either in one group or another but cannot belong to multiple groups at once. While this may make sense for a lot of substantive settings, it might not make sense for other ones, where multiple group memberships are normal (e.g., think of high school)."
  },
  {
    "objectID": "ahn.html#detecting-overlapping-communities",
    "href": "ahn.html#detecting-overlapping-communities",
    "title": "Assigning Nodes to Multiple Communities",
    "section": "Detecting Overlapping Communities",
    "text": "Detecting Overlapping Communities\nMethodologically, overlapping community detection methods are not as well-developed as classical community detection methods. Here, we review one simple an intuitive approach that combines the idea of clustering nodes by computing a quantity on the links but instead of computing a rank order (like Newman and Girvan’s edge betweenness), we compute pairwise similarities between links like we did in handout 5. We then cluster the links using standard hierarchical clustering methods, which because nodes are incident to many links results in a multigroup clustering of the nodes for free. This approach is called link clustering (Ahn, Bagrow, and Lehmann 2010).\nLet’s see how it works.\nFirst we load data from an undirected graph:\n\n   library(igraph)\n   library(networkdata)\n   g <- movie_559 #Pulp Fiction\n\nAnd we plot:\n\n\n\n\n\n\nOriginal Graph.\n\n\n\n\n\n\n\nNodes Clusterered Into Communities According to the Modularity.\n\n\n\n\n\n\nThe Pulp Fiction Movie Network.\n\n\n\nThe key idea behind link clustering is that similar links should be assigned to the same clusters. How do we compute the similarity between links?"
  },
  {
    "objectID": "ahn.html#measuring-edge-similarity",
    "href": "ahn.html#measuring-edge-similarity",
    "title": "Assigning Nodes to Multiple Communities",
    "section": "Measuring Edge Similarity",
    "text": "Measuring Edge Similarity\nAccording to Ahn, Bagrow, and Lehmann (2010) two links \\(e_{ik}\\) and \\(e_{jk}\\) are similar if they share a node \\(v_k\\) and the other two nodes incident to each link (\\(v_i, v_k\\)) are themselves similar. To measure the similarity between these two nodes, we can use any of the off-the-shelf vertex similarity measures that we have seen in action, like Jaccard, cosine, or Dice.\nThe first step is thus to build a link by link similarity matrix based on this idea. The following function loops through each pair of links in the graph and computes the similarity between two links featuring a common node \\(v_k\\) based on the Jaccard vertex similarity of the two other nodes:\n\n    edge.sim <- function(x) {\n      el <- as_edgelist(x)\n      A <- as.matrix(as_adjacency_matrix(x))\n      S <- A %*% A #shared neighbors\n      d <- degree(x)\n      E <- nrow(el)\n      E.sim <- matrix(0, E, E)\n      for (e1 in 1:E) {\n        for (e2 in 1:E) {\n          if (e1 < e2 & sum(as.numeric(intersect(el[e1,], el[e2,])!=\"0\"))==1) {\n              v <- setdiff(union(el[e1,], el[e2,]), intersect(el[e1,], el[e2,]))\n              E.sim[e1, e2] <- S[v[1], v[2]]/(S[v[1], v[2]] + d[v[1]] + d[v[2]])\n              E.sim[e2, e1] <- S[v[1], v[2]]/(S[v[1], v[2]] + d[v[1]] + d[v[2]])\n            }\n          }\n        }\n    return(round(E.sim, 3))\n    }\n\nThe function takes the graph as input and returns an inter-link similarity matrix of dimensions \\(E \\times E\\) where \\(E\\) is the number of edges in the graph:\n\n    E.sim <- edge.sim(g)\n    E.sim[1:10, 1:10]\n\n       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10]\n [1,] 0.000 0.238 0.238 0.167 0.000 0.000 0.250 0.000 0.000 0.000\n [2,] 0.238 0.000 0.294 0.139 0.000 0.000 0.179 0.000 0.000 0.000\n [3,] 0.238 0.294 0.000 0.139 0.000 0.000 0.179 0.000 0.000 0.000\n [4,] 0.167 0.139 0.139 0.000 0.000 0.100 0.192 0.000 0.000 0.000\n [5,] 0.000 0.000 0.000 0.000 0.000 0.217 0.000 0.000 0.000 0.000\n [6,] 0.000 0.000 0.000 0.100 0.217 0.000 0.000 0.000 0.000 0.000\n [7,] 0.250 0.179 0.179 0.192 0.000 0.000 0.000 0.143 0.111 0.167\n [8,] 0.000 0.000 0.000 0.000 0.000 0.000 0.143 0.000 0.143 0.111\n [9,] 0.000 0.000 0.000 0.000 0.000 0.000 0.111 0.143 0.000 0.200\n[10,] 0.000 0.000 0.000 0.000 0.000 0.000 0.167 0.111 0.200 0.000\n\n\nWe can then transform the link similarities into distances, and cluster them:\n\n    D <- 1 - E.sim\n    D <- as.dist(D)\n    hc.res <- hclust(D, method = \"ward.D2\")\n\nThe resulting dendrogram looks like this:\n\n    par(cex = 0.5)\n    plot(hc.res)\n\n\n\n\nThe leaves of the dendrogram (bottom-most objects) represent each link in the graph (\\(E = 102\\) in this case), and the clusters are “link communities” (Ahn, Bagrow, and Lehmann 2010)."
  },
  {
    "objectID": "ahn.html#clustering-nodes",
    "href": "ahn.html#clustering-nodes",
    "title": "Assigning Nodes to Multiple Communities",
    "section": "Clustering Nodes",
    "text": "Clustering Nodes\nAs we noted, because it is the links that got clustered, the nodes incident to each link can go to more than one cluster (because nodes with degree \\(k>1\\) will be incident to multiple links).\nThe following function uses the dendrogram information to return a list of node assignments to multiple communities, controlled by the parameter k:\n\n    create.clus <- function(x, k) {\n      library(dendextend)\n      link.clus <- cutree(x, k = k)\n      link.dat <- data.frame(as_edgelist(g), link.clus)\n      clus.list <- list()\n      for (i in 1:max(link.clus)) {\n        sub.dat <- link.dat[link.dat$link.clus == i, ]\n        clus.list[[i]] <- unique(c(sub.dat[, 1], sub.dat[, 2]))\n        }\n    return(clus.list)\n    }\n\nLet’s see the list with twelve overlapping communities:\n\n    C <- create.clus(hc.res, k = 12)\n    C\n\n[[1]]\n[1] \"BRETT\"      \"GAWKER #2\"  \"JULES\"      \"MARSELLUS\"  \"MARVIN\"    \n[6] \"PEDESTRIAN\" \"ROGER\"      \"VINCENT\"   \n\n[[2]]\n [1] \"BRETT\"           \"FABIENNE\"        \"MARVIN\"          \"SPORTSCASTER #1\"\n [5] \"JIMMIE\"          \"RAQUEL\"          \"ROGER\"           \"JULES\"          \n [9] \"SPORTSCASTER #2\" \"THE WOLF\"        \"WINSTON\"        \n\n[[3]]\n [1] \"BRETT\"       \"FOURTH MAN\"  \"RAQUEL\"      \"THE WOLF\"    \"HONEY BUNNY\"\n [6] \"JIMMIE\"      \"JULES\"       \"MANAGER\"     \"MARVIN\"      \"PATRON\"     \n[11] \"PUMPKIN\"     \"ROGER\"       \"VINCENT\"     \"WINSTON\"    \n\n[[4]]\n [1] \"BUDDY\"        \"JODY\"         \"LANCE\"        \"BUTCH\"        \"CAPT KOONS\"  \n [6] \"ED SULLIVAN\"  \"ENGLISH DAVE\" \"MARSELLUS\"    \"MIA\"          \"MOTHER\"      \n[11] \"WOMAN\"        \"VINCENT\"     \n\n[[5]]\n [1] \"BUDDY\"        \"LANCE\"        \"PREACHER\"     \"CAPT KOONS\"   \"ED SULLIVAN\" \n [6] \"ENGLISH DAVE\" \"JODY\"         \"MOTHER\"       \"VINCENT\"      \"WOMAN\"       \n\n[[6]]\n [1] \"BRETT\"           \"BUTCH\"           \"ENGLISH DAVE\"    \"CAPT KOONS\"     \n [5] \"ESMARELDA\"       \"GAWKER #2\"       \"JULES\"           \"MARSELLUS\"      \n [9] \"PEDESTRIAN\"      \"SPORTSCASTER #1\" \"FABIENNE\"        \"MARVIN\"         \n[13] \"MAYNARD\"         \"MOTHER\"          \"ROGER\"           \"VINCENT\"        \n[17] \"WOMAN\"          \n\n[[7]]\n [1] \"FOURTH MAN\"  \"JIMMIE\"      \"BRETT\"       \"HONEY BUNNY\" \"JULES\"      \n [6] \"MANAGER\"     \"MARVIN\"      \"PATRON\"      \"PUMPKIN\"     \"RAQUEL\"     \n[11] \"ROGER\"       \"WINSTON\"     \"THE WOLF\"   \n\n[[8]]\n[1] \"HONEY BUNNY\" \"MANAGER\"     \"PATRON\"      \"PUMPKIN\"    \n\n[[9]]\n[1] \"JODY\"     \"LANCE\"    \"PREACHER\"\n\n[[10]]\n[1] \"MAYNARD\"  \"THE GIMP\" \"ZED\"     \n\n[[11]]\n[1] \"CAPT KOONS\" \"MOTHER\"     \"WOMAN\"     \n\n[[12]]\n[1] \"HONEY BUNNY\" \"PUMPKIN\"     \"WAITRESS\"    \"YOUNG MAN\"   \"YOUNG WOMAN\"\n\n\nBecause there are nodes that belong to multiple communities, the resulting actor by community ties form a two-mode network.\nWe can see re-construct this network from the list of community memberships for each node using this function:\n\n    create.two <- function(x) {\n      v <- unique(unlist(x))\n      B <- as.numeric(v %in% x[[1]])\n      for(j in 2:length(x)) {\n        B <- cbind(B, as.numeric(v %in% x[[j]]))\n      }\n      rownames(B) <- v\n      colnames(B) <- paste(\"c\", 1:12, sep = \"\")\n      return(B)\n      }\n\nHere’s the two mode matrix of characters by communities:\n\n    B <- create.two(C)\n    B\n\n                c1 c2 c3 c4 c5 c6 c7 c8 c9 c10 c11 c12\nBRETT            1  1  1  0  0  1  1  0  0   0   0   0\nGAWKER #2        1  0  0  0  0  1  0  0  0   0   0   0\nJULES            1  1  1  0  0  1  1  0  0   0   0   0\nMARSELLUS        1  0  0  1  0  1  0  0  0   0   0   0\nMARVIN           1  1  1  0  0  1  1  0  0   0   0   0\nPEDESTRIAN       1  0  0  0  0  1  0  0  0   0   0   0\nROGER            1  1  1  0  0  1  1  0  0   0   0   0\nVINCENT          1  0  1  1  1  1  0  0  0   0   0   0\nFABIENNE         0  1  0  0  0  1  0  0  0   0   0   0\nSPORTSCASTER #1  0  1  0  0  0  1  0  0  0   0   0   0\nJIMMIE           0  1  1  0  0  0  1  0  0   0   0   0\nRAQUEL           0  1  1  0  0  0  1  0  0   0   0   0\nSPORTSCASTER #2  0  1  0  0  0  0  0  0  0   0   0   0\nTHE WOLF         0  1  1  0  0  0  1  0  0   0   0   0\nWINSTON          0  1  1  0  0  0  1  0  0   0   0   0\nFOURTH MAN       0  0  1  0  0  0  1  0  0   0   0   0\nHONEY BUNNY      0  0  1  0  0  0  1  1  0   0   0   1\nMANAGER          0  0  1  0  0  0  1  1  0   0   0   0\nPATRON           0  0  1  0  0  0  1  1  0   0   0   0\nPUMPKIN          0  0  1  0  0  0  1  1  0   0   0   1\nBUDDY            0  0  0  1  1  0  0  0  0   0   0   0\nJODY             0  0  0  1  1  0  0  0  1   0   0   0\nLANCE            0  0  0  1  1  0  0  0  1   0   0   0\nBUTCH            0  0  0  1  0  1  0  0  0   0   0   0\nCAPT KOONS       0  0  0  1  1  1  0  0  0   0   1   0\nED SULLIVAN      0  0  0  1  1  0  0  0  0   0   0   0\nENGLISH DAVE     0  0  0  1  1  1  0  0  0   0   0   0\nMIA              0  0  0  1  0  0  0  0  0   0   0   0\nMOTHER           0  0  0  1  1  1  0  0  0   0   1   0\nWOMAN            0  0  0  1  1  1  0  0  0   0   1   0\nPREACHER         0  0  0  0  1  0  0  0  1   0   0   0\nESMARELDA        0  0  0  0  0  1  0  0  0   0   0   0\nMAYNARD          0  0  0  0  0  1  0  0  0   1   0   0\nTHE GIMP         0  0  0  0  0  0  0  0  0   1   0   0\nZED              0  0  0  0  0  0  0  0  0   1   0   0\nWAITRESS         0  0  0  0  0  0  0  0  0   0   0   1\nYOUNG MAN        0  0  0  0  0  0  0  0  0   0   0   1\nYOUNG WOMAN      0  0  0  0  0  0  0  0  0   0   0   1\n\n\nAnd the matrix of inter-community ties based on shared characters:\n\n    M <- t(B) %*% B\n    M\n\n    c1 c2 c3 c4 c5 c6 c7 c8 c9 c10 c11 c12\nc1   8  4  5  2  1  8  4  0  0   0   0   0\nc2   4 11  8  0  0  6  8  0  0   0   0   0\nc3   5  8 14  1  1  5 13  4  0   0   0   2\nc4   2  0  1 12  9  7  0  0  2   0   3   0\nc5   1  0  1  9 10  5  0  0  3   0   3   0\nc6   8  6  5  7  5 17  4  0  0   1   3   0\nc7   4  8 13  0  0  4 13  4  0   0   0   2\nc8   0  0  4  0  0  0  4  4  0   0   0   2\nc9   0  0  0  2  3  0  0  0  3   0   0   0\nc10  0  0  0  0  0  1  0  0  0   3   0   0\nc11  0  0  0  3  3  3  0  0  0   0   3   0\nc12  0  0  2  0  0  0  2  2  0   0   0   5\n\n\nAnd we can visualize the nodes connected to multiple communities as follows:\n\n    library(RColorBrewer)\n    set.seed(45)\n    g <- graph_from_biadjacency_matrix(B)\n    V(g)$type <- bipartite_mapping(g)$type\n    V(g)$shape <- ifelse(V(g)$type, \"square\", \"circle\")\n    V(g)$color <- c(rep(\"orange\", 38), \n                    c(brewer.pal(8, \"Paired\"), brewer.pal(4, \"Dark2\"))) \n    E(g)$color <- \"lightgray\"\n    plot(g, \n    vertex.size=5, vertex.frame.color=\"lightgray\", \n    vertex.label = V(g)$name,\n    vertex.label.dist=1, vertex.label.cex = 1)"
  },
  {
    "objectID": "prestige.html",
    "href": "prestige.html",
    "title": "Status and Prestige",
    "section": "",
    "text": "In the the centrality lecture notes, we saw how to compute the most popular centrality measures. Freeman’s “big three” have strong graph-theoretic foundation and do a good job of formalizing and quantifying the idea that a node is central if it is “well-placed” in the network, where being well-placed resolves into either being able to reach others (directly as with degree or indirectly as with closeness) or being able to intermediate between others (as with betweenness)."
  },
  {
    "objectID": "prestige.html#networks-as-prisms",
    "href": "prestige.html#networks-as-prisms",
    "title": "Status and Prestige",
    "section": "Networks as Prisms",
    "text": "Networks as Prisms\nThere is, however, another strong and well-motivated intuition as to what it means to be “well-placed” in a network. Here the ties in the network are seen less as “pipes” that transmit stuff and more like “prisms” that reflect on you (Podolny 2001).\nOne way to think about this second version of well-placedness is that what is transmitted through the network is the network itself, or more accurately, the importance, status, and prestige of the people you are connected to, preferably flowing from them (high status people) to you.\nUnder this interpretation, actors get status and prestige in the network from being connected to prestigious and high status others. Those others, in turn, get their status from being connected to high status others, and so ad infinitum.\nOne way of quantifying this idea goes like this. If \\(\\mathbf{x}\\) is a vector containing the desired status scores, then the status of actor \\(i\\) should be equal to:\n\\[\n   x_i = \\sum_{j} a_{ij}x_j\n\\tag{1}\\]\nWhere \\(a_{ij} = 1\\) if \\(i\\) is adjacent to \\(j\\) in the network. Note that this formula just sums up the status scores of all the others each actor is connected to.\nIn matrix notation, if \\(\\mathbf{x}\\) is a column vector of status scores then:\n\\[\n   \\mathbf{x} = A\\mathbf{x}\n\\]\nBecause \\(\\mathbf{x}\\) is an \\(n \\times n\\) matrix and \\(\\mathbf{x}\\) is \\(n \\times 1\\) column vector, the multiplication \\(A\\mathbf{x}\\) will return another column vector of dimensions \\(n \\times 1\\), in this case \\(\\mathbf{x}\\) itself!\nNote the problem that this formulation poses: \\(\\mathbf{x}\\) appears on both sides of the equation, which means that in order to know the status of any one node we would need to know the status of the others, but calculating the status of the others depends on knowing the status of the focal node, and so on. There’s a chicken and the egg problem here.\nNow, there is an obvious (to the math majors) mathematical solution to this problem, because there’s a class of solvable (under some mild conditions imposed on the matrix \\(\\mathbf{A}\\)) linear algebra problems that take the form:\n\\[\n   \\lambda\\mathbf{x} = A\\mathbf{x}\n\\]\nWhere \\(\\lambda\\) is just a plain old number (a scalar). Once again conditional of the aforementioned mild conditions being met, we can iteratively search for a value \\(\\lambda\\), fix it, then fill up the \\(\\mathbf{x}\\) vector with another set of values, fix those, search for a new \\(\\lambda\\), and continue until we have values of \\(\\lambda\\) and \\(\\mathbf{x}\\) that make the above equality true.\nWhen we do that successfully, we say that the value of \\(\\lambda\\) we hit upon is an eigenvalue of the matrix \\(\\mathbf{A}\\) and the values of the vector \\(\\mathbf{x}\\) we came up with are an eigenvector of the same matrix (technically in the above equation a right eigenvector).\nEigenvalues and eigenvectors, like Don Quixote and Sancho Panza, come in pairs, because you need a unique combination of both to solve the equation. Typically, a given matrix (like an adjacency matrix) will have multiple \\(\\lambda/\\mathbf{x}\\) pairs that will solve the equation. Together the whole set \\(\\lambda/\\mathbf{x}\\) pairs that make the equation true are the eigenvalues and eigenvectors of the matrix."
  },
  {
    "objectID": "prestige.html#eigenvalues-eigenvectors-oh-my",
    "href": "prestige.html#eigenvalues-eigenvectors-oh-my",
    "title": "Status and Prestige",
    "section": "Eigenvalues, Eigenvectors, Oh My!",
    "text": "Eigenvalues, Eigenvectors, Oh My!\nNote that all of this obscure talk about eigenvalues and eigenvectors is just matrix math stuff. It has nothing to do with networks and social structure.\nIn contrast, because the big three centrality measures have a direct foundation in graph theory, and graph theory is an isomorphic model of social structures (points map to actors/people and lines map to relations) the “math” we do with graph theory is directly meaningful as a model of networks (the counts of the number of edges incident to a node is the count of other actors they someone is directly connected to).\nEigenvalues and eigenvectors are not a model of social structure in the way graph theory is (their first scientific application was in Chemistry and Physics). They are just a mechanical math fix to a circular equation problem.\nThis is why it’s a mistake to introduce network measures of status and prestige by jumping directly to the machinery of linear algebra (or worse talk about the idea of eigenvector centrality which means nothing to most people, and combines two obscure terms into one even more obscure compound term).\nA better approach is to see if we can motivate the use of measures like the ones above using the simple model of the distribution of status and prestige we started with earlier. We will see that we can, and that doing that leads us back to solutions that are the mathematical equivalent of all the eigenvector stuff."
  },
  {
    "objectID": "prestige.html#distributing-centrality-to-others",
    "href": "prestige.html#distributing-centrality-to-others",
    "title": "Status and Prestige",
    "section": "Distributing Centrality to Others",
    "text": "Distributing Centrality to Others\nLet’s start with the simplest model of how people can get their status from the status of others in a network. It is the simplest because it is based on degree.\nImagine everyone has the same “quantum” of status to begin with (this can be stored in a vector containing the same number of length equals to number of actors in the network). Then, at each step, people “send” the same amount of status to all their alters in the network. At the end of each step, we compute people’s status scores using Equation 1. We stop doing this after the status scores of people stop changing across each iteration.\nLet us see a real-life example at work.\nWe will use a data set collected by David Krackhardt on the friendships of 21 managers in a high tech company in the West coast (see the description here). The data are reported as directed ties (\\(i\\) nominates \\(j\\) as a friend) but we will constrain ties to be undirected:\n\n   library(networkdata)\n   library(igraph)\n   g <- as.undirected(ht_friends, mode = \"collapse\")\n\nThis is what the network looks like:\n\n\n\n\n\nKrackhardt’s Manager Data.\n\n\n\n\nWe extract the adjacency matrix corresponding to this network:\n\n   A <- as.matrix(as_adjacency_matrix(g))\n\nAnd here’s a simple custom function using a while loop that exemplifies the process of status distribution through the network we talked about earlier:\n\n   status1 <- function(w) {\n      x <- rep(1, nrow(w)) #initial status vector set to all ones of length equal to the number of nodes\n      d <- 1 #initial delta\n      k <- 0 #initializing counter\n      while (d > 1e-15) {\n          o.x <- x #old status scores\n          x <- w %*% o.x #new scores a function of old scores and adjacency matrix\n          x <- x/norm(x, type = \"E\") #normalizing new status scores\n          d <- abs(sum(abs(x) - abs(o.x))) #delta between new and old scores\n          k <- k + 1 #incrementing while counter\n      }\n   return(as.vector(x))\n   }\n\nLines 2-4 initialize various quantities, most importantly the initial status vector for each node to just a series of ones:\n\n   rep(1, nrow(A))\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n\nThen lines 5-12 implement a little loop of how status is distributed through the network, with the most important piece of code being line 8 where the current status scores for each node are just the sum of the status scores of its neighbors computed one iteration earlier. The program stops when the difference between the old and the new scores is negligible (\\(\\delta < 10^{10}\\)) as checked in line 9.\nNote the normalization step on line 8, which is necessary to prevent the sum of status scores from getting bigger and bigger indefinitely (in mathese, this is referred to as the sum “diverging”). In base R, the type = \"E\" normalization implements the euclidean vector norm (also sometimes confusingly called the Frobenieus norm), by which we divide each value of the status scores by after each update.1\nAnd here’s the resulting (row) vector of status scores for each node:\n\n   s <- status1(A)\n   s <- s/max(s) #normalizing by maximum\n   round(s, 3)\n\n [1] 0.619 0.635 0.446 0.489 0.629 0.430 0.205 0.380 0.444 0.468 0.814 0.549\n[13] 0.162 0.401 0.613 0.360 1.000 0.247 0.680 0.360 0.392\n\n\nWhat if I told you that this vector is the same as that given by the leading (first) eigenvector of the adjacency matrix?\n\n   s.eig <- eigen(A)$vector[, 1] *-1#computing the first eigenvector\n   s.eig <- s.eig/max(s.eig) #normalizing by maximum\n   round(s.eig, 3)\n\n [1] 0.619 0.635 0.446 0.489 0.629 0.430 0.205 0.380 0.444 0.468 0.814 0.549\n[13] 0.162 0.401 0.613 0.360 1.000 0.247 0.680 0.360 0.392\n\n\nWhich is of course what is computed by the eigen_centrality function in igraph:\n\n   round(eigen_centrality(g)$vector, 3) #igraph automatically normalizes the scores\n\n [1] 0.619 0.635 0.446 0.489 0.629 0.430 0.205 0.380 0.444 0.468 0.814 0.549\n[13] 0.162 0.401 0.613 0.360 1.000 0.247 0.680 0.360 0.392\n\n\nSo, the “eigenvector centralities” are just the limit scores produced by the status distribution process implemented in the status1 function!\nWhen treated as a structural index of connectivity in a graph (i.e., a centrality measure) the eigenvector status scores induce an ordering of the nodes which we may be interested in looking at:\n\n   nodes <- 1:vcount(g)\n   eig.dat <- data.frame(Nodes = nodes, Eigen.Cent = s, Deg.Cent = degree(g))\n   eig.dat <- eig.dat[order(eig.dat$Eigen.Cent, decreasing = TRUE), ]\n   library(kableExtra)\n   kbl(eig.dat[1:10, ], \n       format = \"html\", align = \"c\", row.names = FALSE,\n       caption = \"Top Ten Eigenvector Scores.\",\n       digits = 3) %>%    \n   kable_styling(bootstrap_options = \n                    c(\"hover\", \"condensed\", \"responsive\"))\n\n\n\nTop Ten Eigenvector Scores.\n \n  \n    Nodes \n    Eigen.Cent \n    Deg.Cent \n  \n \n\n  \n    17 \n    1.000 \n    18 \n  \n  \n    11 \n    0.814 \n    14 \n  \n  \n    19 \n    0.680 \n    10 \n  \n  \n    2 \n    0.635 \n    10 \n  \n  \n    5 \n    0.629 \n    10 \n  \n  \n    1 \n    0.619 \n    9 \n  \n  \n    15 \n    0.613 \n    9 \n  \n  \n    12 \n    0.549 \n    8 \n  \n  \n    4 \n    0.489 \n    7 \n  \n  \n    10 \n    0.468 \n    8 \n  \n\n\n\n\n\nMost other measures of status in networks are constructed using similar principles. What changes is the model of how status is distributed in the system. That’s why scary and non-intuitive stuff about eigenvectors or whatever is misleading.\nOther measures are designed such that they either change the quantum of status that is distributed through the network by making it dependent on some node characteristic (like degree) or differentiate between different routes of distribution in directed graphs, by for instance, differentiating status derived from outgoing links from that derived from incoming links.\nLet’s see some examples of these alternative cases."
  },
  {
    "objectID": "prestige.html#bonacich-prestige",
    "href": "prestige.html#bonacich-prestige",
    "title": "Status and Prestige",
    "section": "Bonacich Prestige",
    "text": "Bonacich Prestige\nIn a classic paper, Philip Bonacich (1972) noted the above connection between different ways people conceptualized status and prestige in networks and the leading eigenvector of the adjacency matrix. He then noted that we can extend similar ideas to the directed case.\nHere, people get status from receiving nominations from high status others (i.e., those who receive a lot of nominations), whose partners also get status from receiving a lot of nominations from high status others, and so forth.\nThis means that in a directed system of relations, status distribution operates primarily via the indegree of each node, so that if \\(\\mathbf{A}\\) is the asymmetric adjacency matrix corresponding to the directed graph, then if we play our status game on the transpose of this matrix \\(\\mathbf{A}^T\\) we will get the scores we seek (Fouss, Saerens, and Shimbo 2016, 204).\nRecall that in transposing the matrix of a directed graph, we change it from being a from/to matrix (nodes in the rows send ties to nodes in the columns) to a to/from matrix: Nodes in the rows receive ties from nodes in the columns. So we want to play our status game in this matrix, because we want to rank nodes according to their receipt of ties from high-status others.\nLet’s see a real-life example, this time using the directed version of the Krackhardt friendship nomination network among the high-tech managers:\n\n   g <- ht_friends\n   A <- as.matrix(as_adjacency_matrix(g))\n   s <- status1(t(A))\n   s <- s/max(s)\n   round(s, 3)\n\n [1] 0.922 1.000 0.379 0.639 0.396 0.190 0.240 0.531 0.411 0.117 0.413 0.769\n[13] 0.082 0.428 0.368 0.450 0.592 0.440 0.425 0.225 0.583\n\n\nWhich are the same scores we would have gotten using the eigen_centrality function in igraph with the argument directed set to TRUE:\n\n   round(eigen_centrality(g, directed = TRUE)$vector, 3)\n\n [1] 0.922 1.000 0.379 0.639 0.396 0.190 0.240 0.531 0.411 0.117 0.413 0.769\n[13] 0.082 0.428 0.368 0.450 0.592 0.440 0.425 0.225 0.583\n\n\nAnd, like before, we can treat these scores as centrality measures and rank the nodes in the graph according to them.\nHere are the top ten nodes:\n\n\n\n\nTop Ten Eigenvector Scores for a Directed Graph.\n \n  \n    Nodes \n    Eigen.Cent \n    In.Deg.Cent \n  \n \n\n  \n    2 \n    1.000 \n    10 \n  \n  \n    1 \n    0.922 \n    8 \n  \n  \n    12 \n    0.769 \n    8 \n  \n  \n    4 \n    0.639 \n    5 \n  \n  \n    17 \n    0.592 \n    6 \n  \n  \n    21 \n    0.583 \n    5 \n  \n  \n    8 \n    0.531 \n    5 \n  \n  \n    16 \n    0.450 \n    4 \n  \n  \n    18 \n    0.440 \n    4 \n  \n  \n    14 \n    0.428 \n    5 \n  \n\n\n\n\n\nWhile the top indegree centrality node (2) also gets the top Eigenvector Centrality scores, we see many cases of nodes with equal indegree centrality that get substantively different Eigenvector scores. So who you are connected matters in addition to how many incoming connections you have."
  },
  {
    "objectID": "prestige.html#a-degree-normalized-model-of-status-aka-pagerank",
    "href": "prestige.html#a-degree-normalized-model-of-status-aka-pagerank",
    "title": "Status and Prestige",
    "section": "A Degree-Normalized Model of Status (AKA PageRank)",
    "text": "A Degree-Normalized Model of Status (AKA PageRank)\nNote that the model of status distribution implied by the Bonacich’s Eigenvector Centrality just reviewed implies that each actor distributes the same amount of status independently of the number of connections they have. Status just replicates indefinitely. Thus, a node with a 100 friends has 100 status units to distribute to each of them and a node with a 10 friends has 10 units.\nThis is why the eigenvector idea rewards nodes who are connected to popular others more. Even though everyone begins with a single unit of status, well-connected nodes by degree end up having more of it to distribute.\nBut what if status propagated in the network proportionately to the number of connections one had? For instance, if someone has 100 friends and they only had so much time or energy, they would only have a fraction of status to distribute to others than a person with 10 friends.\nIn that case, the node with a hundred friends would only have 1/100 of status units to distribute to each of their connections while the node with 10 friends would have 1/10 units. Under this formulation, being connected to discerning others, that is people who only connect to a few, is better than being connected to others who connect to everyone else indiscriminately.\nHow would we implement this model? First, let’s create a variation of the undirected friendship nomination adjacency matrix called the \\(\\mathbf{P}\\) matrix. It is defined like this:\n\\[\n\\mathbf{P} = \\mathbf{D}_{out}^{-1}\\mathbf{A}\n\\]\nWhere \\(\\mathbf{A}\\) is our old friend the adjacency matrix, and \\(\\mathbf{D}_{out}^{-1}\\) is a matrix containing the inverse of each node outdegree along the diagonals and zeroes in every other cell. In R we can create the \\(\\mathbf{D}_{out}^{-1}\\) matrix using the native diag function like this:\n\n   g <- as.undirected(ht_friends, mode = \"collapse\")\n   A <- as.matrix(as_adjacency_matrix(g))\n   D.o <- diag(1/rowSums(A))\n\nRecalling that the function rowSums gives us the row sums of the adjacency matrix, which is the same as each node’s outdegree.\nWe can check out that the \\(\\mathbf{D}_{out}^{-1}\\) indeed contains the quantities we seek by looking at its first few rows and columns:\n\n   round(D.o[1:10, 1:10], 2)\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n [1,] 0.11  0.0 0.00 0.00  0.0 0.00 0.00  0.0 0.00  0.00\n [2,] 0.00  0.1 0.00 0.00  0.0 0.00 0.00  0.0 0.00  0.00\n [3,] 0.00  0.0 0.17 0.00  0.0 0.00 0.00  0.0 0.00  0.00\n [4,] 0.00  0.0 0.00 0.14  0.0 0.00 0.00  0.0 0.00  0.00\n [5,] 0.00  0.0 0.00 0.00  0.1 0.00 0.00  0.0 0.00  0.00\n [6,] 0.00  0.0 0.00 0.00  0.0 0.14 0.00  0.0 0.00  0.00\n [7,] 0.00  0.0 0.00 0.00  0.0 0.00 0.33  0.0 0.00  0.00\n [8,] 0.00  0.0 0.00 0.00  0.0 0.00 0.00  0.2 0.00  0.00\n [9,] 0.00  0.0 0.00 0.00  0.0 0.00 0.00  0.0 0.17  0.00\n[10,] 0.00  0.0 0.00 0.00  0.0 0.00 0.00  0.0 0.00  0.12\n\n\nWe can then create the \\(\\mathbf{P}\\) matrix corresponding to the undirected version of the Krackhardt friendship network using matrix multiplication like this:\n\n   P <- D.o %*% A\n\nSo this is the original adjacency matrix, in which each non-zero entry is equal to one divided by the outdegree of the corresponding node in each row.\nHere are the first 10 rows and columns of the new matrix:\n\n   round(P[1:10, 1:10], 2)\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n [1,] 0.00 0.11 0.00 0.11 0.00 0.00 0.00 0.11 0.00  0.00\n [2,] 0.10 0.00 0.00 0.10 0.10 0.10 0.00 0.00 0.00  0.00\n [3,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.17\n [4,] 0.14 0.14 0.00 0.00 0.00 0.00 0.00 0.14 0.00  0.00\n [5,] 0.00 0.10 0.00 0.00 0.00 0.00 0.00 0.00 0.10  0.10\n [6,] 0.00 0.14 0.00 0.00 0.00 0.00 0.14 0.00 0.14  0.00\n [7,] 0.00 0.00 0.00 0.00 0.00 0.33 0.00 0.00 0.00  0.00\n [8,] 0.20 0.00 0.00 0.20 0.00 0.00 0.00 0.00 0.00  0.20\n [9,] 0.00 0.00 0.00 0.00 0.17 0.17 0.00 0.00 0.00  0.17\n[10,] 0.00 0.00 0.12 0.00 0.12 0.00 0.00 0.12 0.12  0.00\n\n\nNote that the entries are now numbers between zero and one and the matrix is asymmetric that is \\(p_{ij}\\) is not necessarily equal to \\(p_{ji}\\). In fact \\(p_{ij}\\) will only be equal to \\(p_{ji}\\) when \\(k_i = k_j\\) (nodes have the same degree).\nMoreover the rows of \\(\\mathbf{P}\\) sum to one:\n\n   rowSums(P)\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n\nWhich means that the \\(\\mathbf{P}\\) matrix is row stochastic. That is the “outdegree” of each node in the matrix is forced to sum to a fixed number. Substantively this means that we are equalizing the total amount of prestige or status that each node can distribute in the system to a fixed quantity.\nThis means that nodes with a lot of out-neighbors will dissipate this quantity by distributing it across a larger number of recipients (hence their corresponding non-zero entries in the rows of \\(\\mathbf{P}\\)) will be a small number) and nodes with a few out-neighbors will have more to distribute.\nAnother thing to note is that while the sums of the \\(\\mathbf{P}\\) matrix sum to a fixed number (1.0) the sums of the columns of the same matrix do not:\n\n   round(colSums(P), 2)\n\n [1] 1.11 1.34 0.63 0.86 1.56 1.06 0.37 0.51 0.61 1.21 2.33 0.92 0.17 0.87 1.08\n[16] 0.53 2.73 0.54 1.21 0.60 0.77\n\n\nThis means that inequalities in the system will be tied to the indegree of each node in the \\(\\mathbf{P}\\) matrix, which is given by either the column sums of the matrix (as we just saw) or the row sums of the transpose of the same matrix \\(\\mathbf{P}^T\\):\n\n   round(rowSums(t(P)), 2)\n\n [1] 1.11 1.34 0.63 0.86 1.56 1.06 0.37 0.51 0.61 1.21 2.33 0.92 0.17 0.87 1.08\n[16] 0.53 2.73 0.54 1.21 0.60 0.77\n\n\nThis will come in handy in a second.\nThe \\(\\mathbf{P}\\) matrix has many interpretations, but here it just quantifies the idea that the amount of centrality each node can distribute is proportional to their degree, and that the larger the degree, the less there is to distribute (the smaller each cell \\(p_{ij}\\) will be). Meanwhile, it is clear that nodes that are pointed to by many other nodes who themselves don’t point to many others have a larger indegree in \\(\\mathbf{P}\\).\nNow we can just adapt the the model of status distribution we used for eigenvector centrality but this time using the \\(\\mathbf{P}\\) rather than the \\(\\mathbf{A}\\) matrix. Note that because we are interested in the status that comes into each node we use the transpose of \\(\\mathbf{P}\\) rather than \\(\\mathbf{P}\\).\nSo at each step the status of a node is equivalent to the sum of the status scores of their in-neighbors, with more discerning in-neighbors passing along more status than less discerning ones:\n\n   s2 <- status1(t(P))\n   s2 <- s2/max(s2)\n   round(s2, 3)\n\n [1] 0.500 0.556 0.333 0.389 0.556 0.389 0.167 0.278 0.333 0.444 0.778 0.444\n[13] 0.111 0.333 0.500 0.278 1.000 0.222 0.556 0.278 0.333\n\n\nWhat if I told you that these numbers are the same as the leading eigenvector of \\(\\mathbf{P}^T\\)?\n\n   s2 <- abs(eigen(t(P))$vector[, 1])\n   s2 <- s2/max(s2)\n   round(s2, 3) \n\n [1] 0.500 0.556 0.333 0.389 0.556 0.389 0.167 0.278 0.333 0.444 0.778 0.444\n[13] 0.111 0.333 0.500 0.278 1.000 0.222 0.556 0.278 0.333\n\n\nAnd, of course, the (normalized) scores produced by this approach are identical to those computed by the page_rank function in igraph with “damping factor” (to be explained in a second) set to 1.0:\n\n   pr <- page_rank(g, damping = 1, algo = \"arpack\")$vector\n   round(pr/max(pr), 3)\n\n [1] 0.500 0.556 0.333 0.389 0.556 0.389 0.167 0.278 0.333 0.444 0.778 0.444\n[13] 0.111 0.333 0.500 0.278 1.000 0.222 0.556 0.278 0.333\n\n\nSo the distributional model of status is the same one implemented in the PageRank algorithm!"
  },
  {
    "objectID": "prestige.html#pagerank-with-damping-and-teleportation-in-directed-graphs",
    "href": "prestige.html#pagerank-with-damping-and-teleportation-in-directed-graphs",
    "title": "Status and Prestige",
    "section": "PageRank with Damping and Teleportation in Directed Graphs",
    "text": "PageRank with Damping and Teleportation in Directed Graphs\nPageRank of course was designed to deal with directed graphs (like the World Wide Web). So let’s load up the version of the Krackhardt’s Managers data that contains the advice network which is an unambiguously directed relation.\n\n   g <- ht_advice\n   A <- as.matrix(as_adjacency_matrix(g))\n\nWe then compute the \\(\\mathbf{P}\\) matrix:\n\n   D.o <- diag(1/rowSums(A))\n   P <- D.o %*% A \n\nRemember how we said earlier that there are multiple ways of thinking about \\(\\mathbf{P}\\)? Another way of thinking about the \\(\\mathbf{P}\\) matrix is as characterizing the behavior of a random walker in the directed graph. At any time point \\(t\\) the walker (a piece of information, a virus, or status itself) sits on a node and the with probability \\(p_{ij}\\) it jumps to one of that node’s out-neighbors. The probabilities are stored in the matrix \\(\\mathbf{P}\\).\nOne issue that arises is that there could be nodes with no out-neighbors (so-called sink nodes) or like node 6 above, who has just one out-neighbor (e.g., seeks advice from just one person), in which case the probability is 1.0 that if the random walker is at node 6 it will go to node 21.\nTo avoid this issue the original designers of the PageRank algorithm (Brin and Page 1998) added a “fudge” factor: That is, with probability \\(\\alpha\\) the random walker should hop from node to node following the directed links in the graph. But once in a while with probability \\(1-\\alpha\\) the walker should decide to “teleport” (with uniform probability) to any node in the graph whether it is an out-neighbor of the current node or not.\nHow do we do that? Well we need to “fix” the \\(\\mathbf{P}\\) matrix to allow for such behavior. So instead of \\(\\mathbf{P}\\) we estimate our distributive status model on the matrix \\(\\mathbf{G}\\) (yes, for Google):\n\\[\n   \\mathbf{G} = \\alpha \\mathbf{P} + (1 - \\alpha) \\mathbf{E}\n\\]\nWhere \\(\\mathbf{E}\\) is a matrix of the same dimensions as \\(\\mathbf{P}\\) but containing \\(1/n\\) in every cell indicating that every node has an equal chance of being “teleported” to.\nSo, fixing \\(\\alpha = 0.85\\) (the standard value chosen by Brin and Page (1998) in their original paper) our \\(\\mathbf{G}\\) matrix would be:\n\n   n <- vcount(g)\n   E <- matrix(1/n, n, n)\n   G <- (0.85 * P) + (0.15 * E)\n\nAnd then we just play our status distribution game on the transpose of \\(\\mathbf{G}\\):\n\n   s3 <- round(status1(t(G)), 3)\n   round(s3/max(s3), 3)\n\n [1] 0.302 0.573 0.165 0.282 0.100 0.437 0.635 0.255 0.092 0.180 0.237 0.242\n[13] 0.087 0.268 0.097 0.168 0.258 0.440 0.087 0.200 1.000\n\n\nWhich is the same answer you would get from the igraph function page_rank by setting the “damping” parameter to 0.85:\n\n   pr <- page_rank(g, damping = 0.85, algo = \"arpack\")$vector\n   round(pr/max(pr), 3)\n\n [1] 0.302 0.573 0.165 0.281 0.100 0.437 0.635 0.256 0.091 0.180 0.237 0.242\n[13] 0.086 0.269 0.097 0.169 0.258 0.440 0.086 0.200 1.000\n\n\nWe can see therefore that the damping parameter simply controls the extent to which the PageRank ranking is driven by the directed connectivity of the \\(\\mathbf{P}\\) matrix, versus a stochastic or random component."
  },
  {
    "objectID": "prestige.html#hubs-and-authorities",
    "href": "prestige.html#hubs-and-authorities",
    "title": "Status and Prestige",
    "section": "Hubs and Authorities",
    "text": "Hubs and Authorities\nRecall from previous discussions that everything doubles (sometimes quadruples like degree correlations) in directed graphs. The same goes for status as reflected in a distributive model through the network.\nConsider two ways of showing your status in a system governed by directed relations (like advice). You can be highly sought after by others (be an “authority”), or you can be discerning in who you seek advice from, preferably seeking out people who are also sought after (e.g., be a “hub” pointing to high-quality others).\nThese two forms of status are mutually defining. The top authorities are those who are sought after by the top hubs, and the top hubs are the ones who seek the top authorities!\nSo this leads to a doubling of Equation 1:\n\\[  \n   x^h_i = \\sum_j a_{ij} x^a_j\n\\]\n\\[\n   x^a_i = \\sum_i a_{ij} x^h_i\n\\]\nWhich says that the hub score \\(x^h\\) of a node is the sum of the authority scores \\(x^a\\) of the nodes they point to (sum over \\(j\\); the outdegree), and the authority score of a node is the sum of the hub scores of the nodes that point to it (sum over \\(i\\); the indegree).\nSo we need to make our status game a bit more complicated (but not too much) to account for this duality:\n\n   status2 <- function(w) {\n     a <- rep(1, nrow(w))  #initializing authority scores\n     d <- 1 #initializing delta\n     k <- 0 #initializing counter\n     while (d >= 1e-10) {\n         o.a <- a #old authority scores\n         h <- w %*% o.a #new hub scores a function of previous authority scores of their out-neighbors\n         h <- h/norm(h, type = \"E\")\n         a <- t(w) %*% h #new authority scores a function of current hub scores of their in-neighbors\n         a <- a/norm(a, type = \"E\")\n         d <- abs(sum(abs(o.a) - abs(a))) #delta between old and new authority scores\n         k <- k + 1\n         }\n   return(list(h = as.vector(h), a = as.vector(a), k = k))\n   }\n\nEverything is like our previous status1 function except now we are keeping track of two mutually defining scores a and h. We first initialize the authority scores by setting them to the value of \\(1/n\\) (where \\(n\\) is the number of nodes) in line 2. We then initialize the \\(\\delta\\) difference and \\(k\\) counter in lines 3-4. The while loop in lines 5-13 then update the hub scores (to be the sum of the authority scores of each out-neighbor) in line 7 normalize them in line 8 and update the new authority scores to be the sum (across each in-neighbor) of these new hub scores.\nSo at each step \\(t\\), the authority and hub scores are calculated like this:\n\\[  \n   x^h_i(t) = \\sum_j a_{ij} x^a_j(t-1)\n\\]\n\\[\n   x^a_i(t) = \\sum_j a^T_{ij} x^h_j(t)\n\\]\nWhere \\(a^T_{ij}\\) is the corresponding entry in the transpose of the adjacency matrix (t(w) in line 9 of the above function).\nAs you may have guessed this is just an implementation of the “HITS” algorithm developed by Kleinberg (1999).2\nThe results for the Krackhardt advice network are:\n\n   hits.res1 <- status2(A)\n   round(hits.res1$a/max(hits.res1$a), 3)\n\n [1] 0.782 1.000 0.356 0.496 0.330 0.644 0.684 0.711 0.290 0.615 0.769 0.498\n[13] 0.323 0.677 0.267 0.570 0.645 0.871 0.323 0.589 0.776\n\n   round(hits.res1$h/max(hits.res1$h), 3)\n\n [1] 0.370 0.176 0.841 0.709 0.835 0.065 0.492 0.490 0.773 0.672 0.206 0.122\n[13] 0.331 0.279 1.000 0.274 0.313 0.800 0.581 0.687 0.600\n\n\nWhich are equivalent to using the igraph function hits_scores:\n\n   ha <- hits_scores(g, scale = TRUE)\n   round(ha$authority, 3)\n\n [1] 0.782 1.000 0.356 0.496 0.330 0.644 0.684 0.711 0.290 0.615 0.769 0.498\n[13] 0.323 0.677 0.267 0.570 0.645 0.871 0.323 0.589 0.776\n\n   round(ha$hub, 3)\n\n [1] 0.370 0.176 0.841 0.709 0.835 0.065 0.492 0.490 0.773 0.672 0.206 0.122\n[13] 0.331 0.279 1.000 0.274 0.313 0.800 0.581 0.687 0.600\n\n\nNote that the just like the status2 function, the igraph function hits_scores returns the two sets of scores as elements of a list, so we need to access them using the $ operator on the object that we store the results in (in this case ha). We also set the scale argument to TRUE so that the scores are normalized by the maximum."
  },
  {
    "objectID": "prestige.html#hubs-authorities-and-eigenvectors",
    "href": "prestige.html#hubs-authorities-and-eigenvectors",
    "title": "Status and Prestige",
    "section": "Hubs, Authorities and Eigenvectors",
    "text": "Hubs, Authorities and Eigenvectors\nRecall that both the eigenvector and PageRank status scores computed via the network status distribution game routine ended up being equivalent to the eigenvectors of a network proximity matrix (the adjacency matrix \\(\\mathbf{A}\\) and the probability matrix \\(\\mathbf{P}\\) respectively). It would be surprising if the same wasn’t true of the hub and authority status scores.\nLet’s find out which ones!\nConsider the matrices:\n\\[\n\\mathbf{M}_h = \\mathbf{A}\\mathbf{A}^T\n\\]\n\\[\n\\mathbf{M}_a = \\mathbf{A}^T\\mathbf{A}\n\\]\nLet’s see what they look like in the Krackhardt manager’s network:\n\n   M.h = A %*% t(A)\n   M.a = t(A) %*% A\n   M.h[1:10, 1:10]\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n [1,]    6    1    5    5    5    1    3    4    5     5\n [2,]    1    3    3    2    3    1    2    3    3     0\n [3,]    5    3   15   11   12    1    8    8   12     8\n [4,]    5    2   11   12   11    1    7    6   11     8\n [5,]    5    3   12   11   15    1    7    7   12    10\n [6,]    1    1    1    1    1    1    1    1    1     0\n [7,]    3    2    8    7    7    1    8    5    8     4\n [8,]    4    3    8    6    7    1    5    8    7     4\n [9,]    5    3   12   11   12    1    8    7   13     7\n[10,]    5    0    8    8   10    0    4    4    7    14\n\n   M.a[1:10, 1:10]\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n [1,]   13   13    4    5    5    6    8    8    4     8\n [2,]   13   18    5    8    5    9   11   10    4     9\n [3,]    4    5    5    4    4    2    4    4    2     3\n [4,]    5    8    4    8    3    4    6    6    3     4\n [5,]    5    5    4    3    5    1    3    3    3     3\n [6,]    6    9    2    4    1   10    7    7    2     6\n [7,]    8   11    4    6    3    7   13    6    3     7\n [8,]    8   10    4    6    3    7    6   10    3     6\n [9,]    4    4    2    3    3    2    3    3    4     3\n[10,]    8    9    3    4    3    6    7    6    3     9\n\n\nWhat’s in these matrices? Well let’s look at \\(\\mathbf{M}_h\\). The diagonals will look familiar because they happen to be the outdegree of each node:\n\n   degree(g, mode = \"out\")[1:10]\n\n [1]  6  3 15 12 15  1  8  8 13 14\n\n\nYou may have guessed that the diagonals of matrix \\(\\mathbf{M}_a\\) contain the indegrees:\n\n   degree(g, mode = \"in\")[1:10]\n\n [1] 13 18  5  8  5 10 13 10  4  9\n\n\nWhich means that the off-diagonals cells of each matrix \\(m_{ij}\\) and \\(n_{ij}\\), contain the common out-neighbors and common in-neighbors shared by nodes \\(i\\) and \\(j\\) in the graph, respectively.\nIn information science, \\(\\mathbf{M}_h\\) and \\(\\mathbf{M}_a\\) matrices have special interpretations. Consider the subgraph shown in Figure 1, which contains nodes 2 and 11 from the Krackhardt advice network and their respective neighbors:\n\n\n\n\n\nFigure 1: Subgraph from Krackhardt’s Managers Network.\n\n\n\n\nIf these nodes where papers, then we would say that both 2 and 11 point to a common third node 1. In an information network, the papers that other papers point to are their common references. Therefore the number of common out-neighbors of two nodes is is called the bibliographic coupling score between the two papers. In the same way, we can see that 2 and 11 are pointed to by a common third neighbor 21. The number of common in-neighbors between two-papers is called their co-citation score.\nBoth the bibliographic coupling and the co-citation scores get at two ways that nodes can be similar in a directed graph. In the social context of advice seeking, for instance, two people can be similar if they seek advice from the same others, or two people can be similar if they are sought after for advice by the same others.\nThe \\(\\mathbf{M}_h\\) and \\(\\mathbf{M}_a\\) matrices, therefore are two (unweighted) similarity matrices between the nodes in a directed graph. As you may also be suspecting, the hub and authorities scores are the leading eigenvectors of the \\(\\mathbf{M}_h\\) and \\(\\mathbf{M}_a\\) matrices (Kleinberg 1999):\n\n   a <- eigen(M.a)$vector[,1] * -1\n   h <- eigen(M.h)$vector[,1] * -1\n   round(a/max(a), 3)\n\n [1] 0.782 1.000 0.356 0.496 0.330 0.644 0.684 0.711 0.290 0.615 0.769 0.498\n[13] 0.323 0.677 0.267 0.570 0.645 0.871 0.323 0.589 0.776\n\n   round(h/max(h), 3)\n\n [1] 0.370 0.176 0.841 0.709 0.835 0.065 0.492 0.490 0.773 0.672 0.206 0.122\n[13] 0.331 0.279 1.000 0.274 0.313 0.800 0.581 0.687 0.600\n\n\nGiven the connection to the HITS dual status ranking, sometimes the \\(\\mathbf{M}_h\\) is called the hub matrix and the \\(\\mathbf{M}_a\\) is called the authority matrix (Ding et al. 2002).\nNote that this also means we could have obtained the hub and authority scores using our old status1 function, but we would have had to play the game twice, once for the matrix \\(\\mathbf{M}_a\\) and the other one for the matrix \\(\\mathbf{M}_h\\), like this:\n\n   a <- status1(M.a)\n   round(a/max(a), 3)\n\n [1] 0.782 1.000 0.356 0.496 0.330 0.644 0.684 0.711 0.290 0.615 0.769 0.498\n[13] 0.323 0.677 0.267 0.570 0.645 0.871 0.323 0.589 0.776\n\n   h <- status1(M.h)\n   round(h/max(h), 3)\n\n [1] 0.370 0.176 0.841 0.709 0.835 0.065 0.492 0.490 0.773 0.672 0.206 0.122\n[13] 0.331 0.279 1.000 0.274 0.313 0.800 0.581 0.687 0.600\n\n\nThis link once again demonstrates the equivalence between the eigenvectors of the hub and authority matrices, as similarity matrices between nodes in the network, and our prismatic status distribution game!"
  },
  {
    "objectID": "prestige.html#combining-pagerank-and-hits-salsa",
    "href": "prestige.html#combining-pagerank-and-hits-salsa",
    "title": "Status and Prestige",
    "section": "Combining PageRank and HITS: SALSA",
    "text": "Combining PageRank and HITS: SALSA\nLempel and Moran (2001) show that we can combine the logic of PageRank and HITS. Their basic idea is to use the same mutually reinforcing approach as in HITS but with degree-normalized (stochastic) versions of the adjacency matrix (like in PageRank).3\nLet’s see how it works.\nRecall that PageRank works on the \\(\\mathbf{P}\\) matrix, which is defined like this:\n\\[\n\\mathbf{P}_{h} = \\mathbf{D}_{out}^{-1}\\mathbf{A}\n\\]\nIn R we compute it like this:\n\n   P.h <- D.o %*% A\n\nThis matrix is row-stochastic, because each row is divided by the row total (the outdegrees of each node), meaning its rows sum to one, like we saw before:\n\n   rowSums(P.h)\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n\nIt is also possible to compute the indegree normalized version of the \\(\\mathbf{P}\\) matrix, defined like this:\n\\[\n\\mathbf{P}_{a} = \\mathbf{D}_{in}^{-1} \\mathbf{A}^T\n\\]\nWhere \\(\\mathbf{D}_{in}^{-1}\\) is a matrix containing the inverse of the indegrees along the diagonals (and zeroes elsewhere) and \\(\\mathbf{A}^T\\) is the transpose of the adjacency matrix. Each non-zero entry of is thus equal to one divided by that row node’s indegree.\nIn R we compute it like this:\n\n   D.i <- diag(1/colSums(A))\n   P.a <- D.i %*% t(A)\n\nLike \\(\\mathbf{P}_{h}\\) the \\(\\mathbf{P}_{a}\\) matrix is row-stochastic, meaning its rows sum to 1.0:\n\n   rowSums(P.a)\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n\nTo get the SALSA version of the hub and authority scores, we can just play our status game over newly defined versions of the hub and authority matrices (Langville and Meyer 2005, 156).\nThe SALSA hub matrix is defined like this:\n\\[\n\\mathbf{Q}_h = \\mathbf{P}_a\\mathbf{P}_h\n\\]\nAnd the SALSA authority matrix like this:\n\\[\n\\mathbf{Q}_a = \\mathbf{P}_h\\mathbf{P}_a\n\\]\nWhich in R looks like:\n\n   Q.h <- P.a %*% P.h\n   Q.a <- P.h %*% P.a\n\nEach of these matrices are row stochastic:\n\n   rowSums(Q.h)\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n   rowSums(Q.a)\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n\nWhich means that inequalities will be defined according to differences in the in-degrees of each node just like PageRank.\nAnd now to obtain our SALSA hub and authority scores, we simply play our status1 game on (the transpose of) these matrices, just like we did for PageRank:\n\n   salsa.h <- status1(t(Q.a)) \n   salsa.h/max(salsa.h)\n\n [1] 0.30 0.15 0.75 0.60 0.75 0.05 0.40 0.40 0.65 0.70 0.15 0.10 0.30 0.20 1.00\n[16] 0.20 0.25 0.85 0.55 0.60 0.55\n\n   salsa.a <- status1(t(Q.h)) \n   round(salsa.a/max(salsa.a), 2)\n\n [1] 0.72 1.00 0.28 0.44 0.28 0.56 0.72 0.56 0.22 0.50 0.61 0.39 0.22 0.56 0.22\n[16] 0.44 0.50 0.83 0.22 0.44 0.83\n\n\nWhat are these numbers? Well, it turns out that they are equivalent to the out and indegrees of each node, divided by the total number of edges in the network (Fouss, Renders, and Saerens 2004, 451).\nSo the SALSA hub and authority scores can also be obtained like this:\n\n   out.ratio <- rowSums(A)/sum(A)\n   out.ratio/max(out.ratio)\n\n [1] 0.30 0.15 0.75 0.60 0.75 0.05 0.40 0.40 0.65 0.70 0.15 0.10 0.30 0.20 1.00\n[16] 0.20 0.25 0.85 0.55 0.60 0.55\n\n   in.ratio <- colSums(A)/sum(A)\n   round(in.ratio/max(in.ratio), 2)\n\n [1] 0.72 1.00 0.28 0.44 0.28 0.56 0.72 0.56 0.22 0.50 0.61 0.39 0.22 0.56 0.22\n[16] 0.44 0.50 0.83 0.22 0.44 0.83\n\n\nWe could, of course, create “Google” versions of these matrices and compute our SALSA version of the hub and authorities scores by incorporating a damping factor, teleportation, and all the rest (Rafiei and Mendelzon 2000)."
  },
  {
    "objectID": "prestige.html#correspondence-analysis",
    "href": "prestige.html#correspondence-analysis",
    "title": "Status and Prestige",
    "section": "Correspondence Analysis",
    "text": "Correspondence Analysis\nFouss, Renders, and Saerens (2004) show that there is a link between a method to analyze two-way tables called correspondence analysis, and both Lempel and Moran’s SALSA and Kleinberg’s HITS algorithms.\nThey first ask: What if we play our status distribution game not on the transpose of the SALSA hub and authority matrices like we just did but just on the regular matrices without transposition?\nHere’s what happens:\n\n   status1(Q.h) \n\n [1] 0.2182179 0.2182179 0.2182179 0.2182179 0.2182179 0.2182179 0.2182179\n [8] 0.2182179 0.2182179 0.2182179 0.2182179 0.2182179 0.2182179 0.2182179\n[15] 0.2182179 0.2182179 0.2182179 0.2182179 0.2182179 0.2182179 0.2182179\n\n   status1(Q.a) \n\n [1] 0.2182179 0.2182179 0.2182179 0.2182179 0.2182179 0.2182179 0.2182179\n [8] 0.2182179 0.2182179 0.2182179 0.2182179 0.2182179 0.2182179 0.2182179\n[15] 0.2182179 0.2182179 0.2182179 0.2182179 0.2182179 0.2182179 0.2182179\n\n\nOK, so that’s weird. All that we get is a vector with the same number repeated twenty one times (in this case, the number of nodes in the graph). What’s going on?\nRecall from the previous that the status game computes the leading eigenvector of the matrix we play the game on, and spits that vector out as our status scores for that matrix. The leading eigenvector is that associated with the largest eigenvalue (if the matrix contains one).\nSo all that this is telling us is that the first eigenvector of the un-transposed versions of the SALSA hub and authority matrices is pretty useless because it assigns everyone the same status score.\nBut Fouss, Renders, and Saerens (2004) note, like we did at the beginning, that a matrix has many eigenvector/eigenvalue pairs and that perhaps the second leading eigenvector is not that useless; this is the eigenvector associated with the second largest eigenvalue.\nHow do we get that vector? Well, as always, there is a mathematical workaround. The trick is to create a new matrix that removes the influence of that first (useless) eigenvector and then play our status game on that matrix.\nTo do that, let’s create a matrix that is equal to the original useless eigenvector times its own transpose. In R this goes like this:\n\n   v1 <- status1(Q.h)\n   D <- v1 %*% t(v1)\n\nWhat’s in this matrix? Let’s see the first ten rows and columns:\n\n   round(D[1:10, 1:10], 3)\n\n       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10]\n [1,] 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048\n [2,] 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048\n [3,] 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048\n [4,] 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048\n [5,] 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048\n [6,] 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048\n [7,] 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048\n [8,] 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048\n [9,] 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048\n[10,] 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048 0.048\n\n\nSo it’s just a matrix of the same dimension as the SALSA hub matrix with the same number over and over. In fact that number is equal to:\n\n   round(0.2182179^2, 3)\n\n[1] 0.048\n\n\nWhich is just the useless constant status score squared.\nNow, we create new SALSA hub and authority matrices, which are equal to the original minus the constant D matrix above:\n\n   Q.h2 <- Q.h - D\n   Q.a2 <- Q.a - D\n\nAnd now we play our status game on these matrices:\n\n   h2 <- status1(Q.h2)\n   a2 <- status1(Q.a2)\n   names(h2) <- 1:21\n   names(a2) <- 1:21\n\nWe then use the function below to normalize each status score to be within the minus one to plus one interval and have mean zero:\n\n   norm.v <- function(x) {\n      x <- x - min(x)\n      x <- x/max(x)\n      x <- x - mean(x)\n      return(x)\n   }\n   round(norm.v(h2), 3)\n\n     1      2      3      4      5      6      7      8      9     10     11 \n-0.008 -0.146  0.299 -0.106  0.414 -0.329 -0.455 -0.005  0.223 -0.049 -0.027 \n    12     13     14     15     16     17     18     19     20     21 \n-0.158  0.291  0.035  0.323  0.026 -0.057 -0.114  0.291  0.139 -0.586 \n\n   round(norm.v(a2), 3)\n\n     1      2      3      4      5      6      7      8      9     10     11 \n 0.007 -0.436  0.074  0.074  0.138 -0.626 -0.019 -0.098  0.023  0.374 -0.063 \n    12     13     14     15     16     17     18     19     20     21 \n-0.530  0.334 -0.243  0.212  0.119 -0.147  0.292  0.290  0.107  0.115 \n\n\nNow, these scores don’t seem useless. They are different across each node; some are positive and some are negative.\nFouss, Renders, and Saerens (2004) show that these are the same scores you would obtain from a correspondence analysis of the original affilaition matrix.\nLet’s check that out in our case. First, we load the package FactoMineR which can be used to compute the correspondence analysis of any matrix in R using the function CA:\n\n   library(FactoMineR)\n\nAnd the correspondence analysis scores—on the first dimension—fo the Krackhardt advice network can be obtained like this:\n\n   ca.res <- CA(A, graph = FALSE)\n   ca.h <- ca.res$col$coord[, 1]\n   ca.a <- ca.res$row$coord[, 1]\n\nIn line 1 we store the CA results in the object ca.res. We then grab the CA scores associated with the columns of the adjacency matrix and put them in the object ca.h in line 2 and the scores associated with the rows of the adjacency matrix and put them in the object ca.a.\nNow for the big reveal:\n\n   round(norm.v(ca.h), 3)\n\n    V1     V2     V3     V4     V5     V6     V7     V8     V9    V10    V11 \n-0.008 -0.146  0.299 -0.106  0.414 -0.329 -0.455 -0.005  0.223 -0.049 -0.027 \n   V12    V13    V14    V15    V16    V17    V18    V19    V20    V21 \n-0.158  0.291  0.035  0.323  0.026 -0.057 -0.114  0.291  0.139 -0.586 \n\n   round(norm.v(ca.a), 3)\n\n     1      2      3      4      5      6      7      8      9     10     11 \n 0.007 -0.436  0.074  0.074  0.138 -0.626 -0.019 -0.098  0.023  0.374 -0.063 \n    12     13     14     15     16     17     18     19     20     21 \n-0.530  0.334 -0.243  0.212  0.119 -0.147  0.292  0.290  0.107  0.115 \n\n\nWhich shows that indeed the CA scores are the same ones as we obtain from playing our status game on the corrected versions of the un-transposed SALSA hub and authorities matrices!"
  },
  {
    "objectID": "prestige.html#hybrid-pagerankhits-approaches",
    "href": "prestige.html#hybrid-pagerankhits-approaches",
    "title": "Status and Prestige",
    "section": "Hybrid PageRank/HITS Approaches",
    "text": "Hybrid PageRank/HITS Approaches\nBorodin et al. (2005) argue that perhaps a better approach to combining PageRank and HITS is to normalize only one of the scores by degree while leaving the other score alone.\n\nPicky Hubs\nFor instance, in some settings, it might make more sense to assign more authority to nodes that are pointed to by picky hubs (e.g., people who seek advice from a few select others), and discount the authority scores of nodes that are pointed to by indiscriminate hubs (people who seek advice from everyone).\nWe can do this by taking the average of the authority scores of all nodes each hub points to rather than sum, and then feeding this number back to the authority score calculation. This entails slightly modifying the HITS status game as follows:\n\n   status3 <- function(w) {\n     a <- rep(1, nrow(w))  #initializing authority scores\n     d.o <- rowSums(w) #outdegree of each node\n     d <- 1 #initializing delta\n     k <- 0 #initializing counter\n     while (d >= 1e-10) {\n         o.a <- a #old authority scores\n         h <- w %*% o.a #new hub scores a function of previous authority scores of their out-neighbors\n         h <- h/d.o #averaging hub scores by number of out-neighbors\n         h <- h/norm(h, type = \"E\")\n         a <- t(w) %*% h #new authority scores a function of current hub scores of their in-neighbors\n         a <- a/norm(a, type = \"E\")\n         d <- abs(sum(abs(o.a) - abs(a))) #delta between old and new authority scores\n         k <- k + 1\n         }\n   return(list(h = as.vector(h), a = as.vector(a), k = k))\n   }\n\nNote that the only modification is the addition of h <- h/d.o in line 9, which divides the hub score by the outdegree of each hub.\nThe resulting hubs and authorities scores are:\n\n   hits.res2 <- status3(A)\n   round(hits.res2$a/max(hits.res2$a), 3)\n\n [1] 0.693 1.000 0.221 0.414 0.223 0.538 0.759 0.494 0.187 0.470 0.552 0.361\n[13] 0.174 0.498 0.180 0.395 0.451 0.817 0.174 0.375 0.893\n\n   round(hits.res2$h/max(hits.res2$h), 3)\n\n [1] 0.749 0.818 0.635 0.657 0.619 1.000 0.716 0.762 0.683 0.493 0.916 0.925\n[13] 0.639 0.972 0.543 0.835 0.842 0.508 0.590 0.642 0.604\n\n\nThis is an implementation of the “HubAvg” algorithm described by Borodin et al. (2005, 238–39).\n\n\nExclusive Authorities\nIn the same way, depending on the application, it might make more sense to assign a larger hub score to hubs that point to exclusive authorities (authorities that are sought after by a few select others) and discount the hubness of hubs that point to popular authorities (those who are sought after by everyone).\nWe can implement this approach, let’s call it the “AuthAvg” algorithm with a slight modification of the function above:\n\n   status4 <- function(w) {\n     a <- rep(1, nrow(w))  #initializing authority scores\n     d.i <- colSums(w) #indegree of each node\n     d <- 1 #initializing delta\n     k <- 0 #initializing counter\n     while (d >= 1e-10) {\n         o.a <- a #old authority scores\n         h <- w %*% o.a #new hub scores a function of previous authority scores of their out-neighbors\n         h <- h/norm(h, type = \"E\")\n         a <- t(w) %*% h #new authority scores a function of current hub scores of their in-neighbors\n         a <- a/d.i #averaging authority score by number of in-neighbors\n         a <- a/norm(a, type = \"E\")\n         d <- abs(sum(abs(o.a) - abs(a))) #delta between old and new authority scores\n         k <- k + 1\n         }\n   return(list(h = as.vector(h), a = as.vector(a), k = k))\n   }\n\nAnd the resulting hubs and authorities scores are:\n\n   hits.res3 <- status4(A)\n   round(hits.res3$a/max(hits.res3$a), 3)\n\n [1] 0.651 0.579 0.877 0.694 0.819 0.658 0.548 0.784 0.855 0.733 0.761 0.736\n[13] 1.000 0.738 0.808 0.797 0.768 0.600 1.000 0.847 0.524\n\n   round(hits.res3$h/max(hits.res3$h), 3)\n\n [1] 0.266 0.116 0.700 0.564 0.734 0.035 0.358 0.341 0.593 0.734 0.119 0.072\n[13] 0.283 0.150 1.000 0.171 0.200 0.869 0.532 0.561 0.523"
  },
  {
    "objectID": "prestige.html#a-final-ranking-of-prestige-scores",
    "href": "prestige.html#a-final-ranking-of-prestige-scores",
    "title": "Status and Prestige",
    "section": "A Final Ranking of Prestige Scores",
    "text": "A Final Ranking of Prestige Scores\nLike before, we can treat the the Regular Hub, and Authority Scores, their SALSA versions, and their hub and authority averaged versions as “centralities” defined over nodes in the graph. In that case we might be interested in how different nodes in Krackhardt’s High Tech Managers network stack up according to the different status criteria:\n\n\n\n\nTop Prestige Scores Ordered by Indegree.\n \n  \n    Node.ID \n    Hub \n    Aut \n    Hub.Salsa \n    Aut.Salsa \n    Hub.Avg1 \n    Aut.Avg1 \n    Hub.Avg2 \n    Aut.Avg2 \n    Indegree \n  \n \n\n  \n    2 \n    0.176 \n    1.000 \n    0.176 \n    1.000 \n    0.818 \n    1.000 \n    0.116 \n    0.579 \n    18 \n  \n  \n    18 \n    0.800 \n    0.871 \n    0.800 \n    0.871 \n    0.508 \n    0.817 \n    0.869 \n    0.600 \n    15 \n  \n  \n    21 \n    0.600 \n    0.776 \n    0.600 \n    0.776 \n    0.604 \n    0.893 \n    0.523 \n    0.524 \n    15 \n  \n  \n    1 \n    0.370 \n    0.782 \n    0.370 \n    0.782 \n    0.749 \n    0.693 \n    0.266 \n    0.651 \n    13 \n  \n  \n    7 \n    0.492 \n    0.684 \n    0.492 \n    0.684 \n    0.716 \n    0.759 \n    0.358 \n    0.548 \n    13 \n  \n  \n    11 \n    0.206 \n    0.769 \n    0.206 \n    0.769 \n    0.916 \n    0.552 \n    0.119 \n    0.761 \n    11 \n  \n  \n    6 \n    0.065 \n    0.644 \n    0.065 \n    0.644 \n    1.000 \n    0.538 \n    0.035 \n    0.658 \n    10 \n  \n  \n    8 \n    0.490 \n    0.711 \n    0.490 \n    0.711 \n    0.762 \n    0.494 \n    0.341 \n    0.784 \n    10 \n  \n  \n    14 \n    0.279 \n    0.677 \n    0.279 \n    0.677 \n    0.972 \n    0.498 \n    0.150 \n    0.738 \n    10 \n  \n  \n    10 \n    0.672 \n    0.615 \n    0.672 \n    0.615 \n    0.493 \n    0.470 \n    0.734 \n    0.733 \n    9 \n  \n  \n    17 \n    0.313 \n    0.645 \n    0.313 \n    0.645 \n    0.842 \n    0.451 \n    0.200 \n    0.768 \n    9 \n  \n  \n    4 \n    0.709 \n    0.496 \n    0.709 \n    0.496 \n    0.657 \n    0.414 \n    0.564 \n    0.694 \n    8 \n  \n  \n    16 \n    0.274 \n    0.570 \n    0.274 \n    0.570 \n    0.835 \n    0.395 \n    0.171 \n    0.797 \n    8 \n  \n  \n    20 \n    0.687 \n    0.589 \n    0.687 \n    0.589 \n    0.642 \n    0.375 \n    0.561 \n    0.847 \n    8 \n  \n  \n    12 \n    0.122 \n    0.498 \n    0.122 \n    0.498 \n    0.925 \n    0.361 \n    0.072 \n    0.736 \n    7 \n  \n  \n    3 \n    0.841 \n    0.356 \n    0.841 \n    0.356 \n    0.635 \n    0.221 \n    0.700 \n    0.877 \n    5 \n  \n  \n    5 \n    0.835 \n    0.330 \n    0.835 \n    0.330 \n    0.619 \n    0.223 \n    0.734 \n    0.819 \n    5 \n  \n  \n    9 \n    0.773 \n    0.290 \n    0.773 \n    0.290 \n    0.683 \n    0.187 \n    0.593 \n    0.855 \n    4 \n  \n  \n    13 \n    0.331 \n    0.323 \n    0.331 \n    0.323 \n    0.639 \n    0.174 \n    0.283 \n    1.000 \n    4 \n  \n  \n    15 \n    1.000 \n    0.267 \n    1.000 \n    0.267 \n    0.543 \n    0.180 \n    1.000 \n    0.808 \n    4 \n  \n  \n    19 \n    0.581 \n    0.323 \n    0.581 \n    0.323 \n    0.590 \n    0.174 \n    0.532 \n    1.000 \n    4 \n  \n\n\n\n\n\n\n\nTop Prestige Scores Ordered by Outdegree\n \n  \n    Node.ID \n    Hub \n    Aut \n    Hub.Salsa \n    Aut.Salsa \n    Hub.Avg1 \n    Aut.Avg1 \n    Hub.Avg2 \n    Aut.Avg2 \n    Outdegree \n  \n \n\n  \n    15 \n    1.000 \n    0.267 \n    1.000 \n    0.267 \n    0.543 \n    0.180 \n    1.000 \n    0.808 \n    20 \n  \n  \n    18 \n    0.800 \n    0.871 \n    0.800 \n    0.871 \n    0.508 \n    0.817 \n    0.869 \n    0.600 \n    17 \n  \n  \n    3 \n    0.841 \n    0.356 \n    0.841 \n    0.356 \n    0.635 \n    0.221 \n    0.700 \n    0.877 \n    15 \n  \n  \n    5 \n    0.835 \n    0.330 \n    0.835 \n    0.330 \n    0.619 \n    0.223 \n    0.734 \n    0.819 \n    15 \n  \n  \n    10 \n    0.672 \n    0.615 \n    0.672 \n    0.615 \n    0.493 \n    0.470 \n    0.734 \n    0.733 \n    14 \n  \n  \n    9 \n    0.773 \n    0.290 \n    0.773 \n    0.290 \n    0.683 \n    0.187 \n    0.593 \n    0.855 \n    13 \n  \n  \n    4 \n    0.709 \n    0.496 \n    0.709 \n    0.496 \n    0.657 \n    0.414 \n    0.564 \n    0.694 \n    12 \n  \n  \n    20 \n    0.687 \n    0.589 \n    0.687 \n    0.589 \n    0.642 \n    0.375 \n    0.561 \n    0.847 \n    12 \n  \n  \n    19 \n    0.581 \n    0.323 \n    0.581 \n    0.323 \n    0.590 \n    0.174 \n    0.532 \n    1.000 \n    11 \n  \n  \n    21 \n    0.600 \n    0.776 \n    0.600 \n    0.776 \n    0.604 \n    0.893 \n    0.523 \n    0.524 \n    11 \n  \n  \n    7 \n    0.492 \n    0.684 \n    0.492 \n    0.684 \n    0.716 \n    0.759 \n    0.358 \n    0.548 \n    8 \n  \n  \n    8 \n    0.490 \n    0.711 \n    0.490 \n    0.711 \n    0.762 \n    0.494 \n    0.341 \n    0.784 \n    8 \n  \n  \n    1 \n    0.370 \n    0.782 \n    0.370 \n    0.782 \n    0.749 \n    0.693 \n    0.266 \n    0.651 \n    6 \n  \n  \n    13 \n    0.331 \n    0.323 \n    0.331 \n    0.323 \n    0.639 \n    0.174 \n    0.283 \n    1.000 \n    6 \n  \n  \n    17 \n    0.313 \n    0.645 \n    0.313 \n    0.645 \n    0.842 \n    0.451 \n    0.200 \n    0.768 \n    5 \n  \n  \n    14 \n    0.279 \n    0.677 \n    0.279 \n    0.677 \n    0.972 \n    0.498 \n    0.150 \n    0.738 \n    4 \n  \n  \n    16 \n    0.274 \n    0.570 \n    0.274 \n    0.570 \n    0.835 \n    0.395 \n    0.171 \n    0.797 \n    4 \n  \n  \n    2 \n    0.176 \n    1.000 \n    0.176 \n    1.000 \n    0.818 \n    1.000 \n    0.116 \n    0.579 \n    3 \n  \n  \n    11 \n    0.206 \n    0.769 \n    0.206 \n    0.769 \n    0.916 \n    0.552 \n    0.119 \n    0.761 \n    3 \n  \n  \n    12 \n    0.122 \n    0.498 \n    0.122 \n    0.498 \n    0.925 \n    0.361 \n    0.072 \n    0.736 \n    2 \n  \n  \n    6 \n    0.065 \n    0.644 \n    0.065 \n    0.644 \n    1.000 \n    0.538 \n    0.035 \n    0.658 \n    1"
  }
]